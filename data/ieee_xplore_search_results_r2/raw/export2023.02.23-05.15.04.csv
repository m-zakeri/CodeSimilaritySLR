"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"An effective selection policy for load balancing in software DSM","Tyng-Yeu Liang; Ce-Kuen Shieh; Jun-Qi Li","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","Proceedings 2000 International Conference on Parallel Processing","6 Aug 2002","2000","","","105","112","Load balance is an area of current research in software distributed shared memory (DSM) systems. When threads are dynamically migrated from heavily loaded nodes to lightly loaded nodes to achieve load balance, the communication cost of maintaining data consistency is increased if migration threads are carelessly selected. Program performance is degraded when loss from increased communication exceeds the benefit from load balancing. Therefore, load balancing requires careful choice of migration threads. This study addresses the problem with a novel selection policy called Reduce Internode Sharing Cost (RISC). The main characteristic of this thread selection policy is simultaneous consideration of both thread memory access types and global sharing. Experimental application of this policy to a DSM system called Cohesion shows that simultaneous consideration of memory access types and global sharing is necessary for thread selection. RISC can reduce 50% data-consistency communication of benchmark applications during execution of the load balance mechanism.","0190-3918","0-7695-0768-9","10.1109/ICPP.2000.876087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=876087","","Load management;Yarn;Costs;Reduced instruction set computing;Application software;Degradation;Computer networks;User interfaces;Computer interfaces;Distributed computing","distributed shared memory systems;resource allocation;data integrity;software performance evaluation","selection policy;load balancing;software distributed shared memory systems;communication cost;data consistency;program performance;thread selection policy;thread memory access types;global sharing;load balance mechanism","","","7","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Understanding why correlation profiling improves the predictability of data cache misses in nonnumeric applications","T. C. Mowry; C. . -K. Luk","Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; Computer Science Department, University of Toronto, Toronto, ONT, Canada","IEEE Transactions on Computers","6 Aug 2002","2000","49","4","369","384","Latency-tolerance techniques offer the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. However, to fully exploit the benefit of these techniques, one must be careful to apply them only to the dynamic references that are likely to suffer cache misses-otherwise the runtime overheads can potentially offset any gains. In this paper, we focus on isolating dynamic miss instances in nonnumeric applications, which is a difficult but important problem. Although compilers cannot statically analyze data locality in nonnumeric applications, one viable approach is to use profiling information to measure the actual miss behavior. Unfortunately, the state-of-the-art in cache miss profiling (which we call summary profiling) is inadequate for references with intermediate miss ratios-it either misses opportunities to hide latency, or else inserts overhead that is unnecessary. To overcome this problem, we propose and evaluate a new profiling technique that helps predict which dynamic instances of a static memory reference will hit or miss in the cache: correlation profiling Our experimental results demonstrate that roughly half of the 21 nonnumeric applications we study can potentially enjoy significant reductions in memory stall time by exploiting at least one of the three forms of correlation profiling we consider: control-flow correlation, self correlation, and global correlation. In addition, our detailed case studies illustrate that self correlation succeeds because a given reference's cache outcomes often contain repeated patterns and control-flow correlation succeeds because cache outcomes are often call-chain dependent. Finally, we suggest a number of ways to exploit correlation profiling in practice and demonstrate that software prefetching can achieve better performance on a modern superscalar processor when directed by correlation profiling rather than summary profiling information.","1557-9956","","10.1109/12.844349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=844349","","Runtime;Data analysis;Information analysis;Delay;Software performance;Prefetching","cache storage;software performance evaluation;program compilers","correlation profiling;data cache misses;nonnumeric applications;latency-tolerance techniques;memory subsystem;high-performance processors;data locality;control-flow correlation;self correlation;global correlation;software prefetching;superscalar processor;summary profiling","","2","","18","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Object model resurrection-an object oriented maintenance activity","G. V. Subramaniam","Nortel Networks Limited, Richardson, TX, USA","Proceedings of the 2000 International Conference on Software Engineering. ICSE 2000 the New Millennium","6 Aug 2002","2000","","","324","333","This paper addresses the problem of reengineering object-oriented systems that have incurred increased maintenance cost due to long development time-span and project lifecycle. When an Incremental Approach is used to develop an object-oriented system, there is a risk that the class design and the overall object model will deteriorate in quality with each increment. A recent research work suggested a process activity (Class Deterioration Detection and Resurrection-CDDR process activity) and a technique for the detection and resurrection of deteriorated classes. That work focussed on one particular aspect of object-oriented software maintenance-Class Quality Deterioration due to lack of cohesion induced by high coupling. This paper addresses the problem of deteriorating object-oriented design due to code and class growth (increase in the number of classes) within a system. A Code/Class Growth Control process activity (CGC) is suggested to avoid and eliminate Repetitious Code and Classes within the evolving system. The CDDR and CGC process activities are used to build an evolving Maintenance process model for object-oriented systems. The presented maintenance process model is an effective way to periodically assess and resurrect the quality of an object-oriented design during incremental development.","0270-5257","1-58113-206-9","10.1145/337180.337218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=870423","","Object oriented modeling;Degradation;Costs;Permission;Design optimization;Lakes;Software maintenance;Process control;Control systems;Software systems","software quality;software maintenance;object-oriented programming;systems re-engineering","object model resurrection;object oriented maintenance activity;object-oriented systems reengineering;maintenance cost;project lifecycle;class design;software maintenance;incremental development","","","","17","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automating delegation in class-based languages","J. Viega; P. Reynolds; R. Behrends","Reliable Software Technol., USA; Department of Computer Science, University of Virginia, USA; Department of Computer Science and Engineering, Michigan State University","Proceedings. 34th International Conference on Technology of Object-Oriented Languages and Systems - TOOLS 34","6 Aug 2002","2000","","","171","182","Some designers of class-based object oriented languages choose not to support multiple inheritance. As a result, programmers often resort to ad hoc workarounds. The most common of these workarounds is delegation. Even delegation is tedious and error prone, however. We believe that language designers who choose against multiple inheritance should consider automating delegation in order to alleviate these problems. In this paper we present Jamie, a language extension for Java that automates delegation. We also discuss the advantages and disadvantages of both delegation and automating it in a class-based programming language. Many of our observations are based on our experiences with implementing and using Jamie.","1530-2067","0-7695-0774-3","10.1109/TOOLS.2000.868969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=868969","","Java;Computer science;Programming profession;Computer languages;Reliability engineering;Design engineering;Computer errors;Automation;Process design;Concrete","object-oriented languages;object-oriented programming;abstract data types","delegation automation;class-based object oriented languages;Jamie;Java language extension","","2","","21","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Working session: identification of lower-level artifacts [Session intro.]","G. Antoniol",University of Sannio,"Proceedings IWPC 2000. 8th International Workshop on Program Comprehension","6 Aug 2002","2000","","","253","253","Summary form only given. When programmers approach the task of maintaining a software system to modify existing code to correct errors, to add new functionalities or to upgrade it, often their first activity is to build a mentalmodel of the system being studied. In order to understand the organization of a system at different levels of abstractions, maintenance requires the identification of sub-systems, of their functionalities, of their interactions, and of their relation with the different software artifacts. Unfortunately, documentation is often scarce or not up-to-date. Furthermore, since outsourcing has been widely adopted as common practice, people who wrote or maintained a system may no longer be available. Hence, the only reliable source of information about a program is often the program itself. Different approaches and tools have been proposed to help people in building conceptual models of existing systems either at the architectural level or at the code level.","1092-8138","0-7695-0656-9","10.1109/WPC.2000.852502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=852502","","Programming profession;Software maintenance;Computer languages;Software systems;Error correction codes;Cognitive science;Documentation;Outsourcing;Information resources","","","","","","","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Structured design","W. P. Stevens; G. J. Myers; L. L. Constantive","Data Processing Division, Hartford, CT, USA; System Development Division, Poughkeepsie, NY, USA; Tufts University School of Medicine, Medford, Massachusetts, USA","IBM Systems Journal","6 Apr 2010","1999","38","2.3","231","256","The HIPO Hierarchy chart is being used as an aid during general systems design. The considerations and techniques presented here are useful for evaluating alternatives for those portions of the system that will be programmed on a computer. The charting technique used here depicts more details about the interfaces than the HIPO Hierarchy chart. This facilitates consideration during general program design of each individual connection and its associated passed parameters. The resulting design can be documented with the Hero charts. (If the designer decides to have more than one function in any module, the structure chart should show them in the same block. However, the HIPO Hierarchy chart would still show all the functions in separate blocks.) The output of the general program design is the input for the detailed module design. The HIPO input-process-output chart is useful for describing and designing each module.","0018-8670","","10.1147/sj.382.0231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387105","","","","","","19","","","","6 Apr 2010","","","IBM","IBM Journals"
"Extending software quality assessment techniques to Java systems","J. . -F. Patenaude; E. Merlo; M. Dagenais; B. Lague","Department of Electrical and Computer Engineering, Ecole Polytechnique de Montreal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, Ecole Polytechnique de Montreal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, Ecole Polytechnique de Montreal, Montreal, QUE, Canada; Quality Engineering and Research (QER) group, Montreal, QUE, Canada","Proceedings Seventh International Workshop on Program Comprehension","6 Aug 2002","1999","","","49","56","The paper presents extensions to Bell Canada source code quality assessment suite (DATRIX tm) for handling Java language systems. Such extensions are based on source code object metrics, including Java interface metrics, which are presented and explained in detail. The assessment suite helps to evaluate the quality of medium-large software systems identifying parts of the system which have unusual characteristics. The paper also studies and reports the occurrence of clones in medium-large Java software systems. Clone presence affects quality since it increases a system size and often leads to higher maintenance costs. The clone identification process uses Java specific metrics to determine similarities between methods throughout a system. The results obtained from experiments with software evaluation and clone detection techniques, on over 500 KLOC of Java source code, are presented.","1092-8138","0-7695-0180-x","10.1109/WPC.1999.777743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777743","","Software quality;Java;Cloning;Software performance;Software maintenance;Software tools;Performance analysis;ISO standards;Quality assessment;Electrical capacitance tomography","Java;software quality;software metrics;software performance evaluation;reverse engineering","software quality assessment techniques;Bell Canada source code quality assessment suite;DATRIX;Java language systems;source code object metrics;Java interface metrics;assessment suite;medium-large Java software systems;clone presence;system size;maintenance costs;clone identification process;Java specific metrics;software evaluation;clone detection techniques;KLOC;Java source code","","26","","34","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A language independent approach for detecting duplicated code","S. Ducasse; M. Rieger; S. Demeyer","Software Composition Group, University of Bern, Bern, Switzerland; Software Composition Group, University of Bern, Bern, Switzerland; Software Composition Group, University of Bern, Bern, Switzerland","Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360)","6 Aug 2002","1999","","","109","118","Code duplication is one of the factors that severely complicates the maintenance and evolution of large software systems. Techniques for detecting duplicated code exist but rely mostly on parsers, technology that has proven to be brittle in the face of different languages and dialects. In this paper we show that is possible to circumvent this hindrance by applying a language independent and visual approach, i.e. a tool that requires no parsing, yet is able to detect a significant amount of code duplication. We validate our approach on a number of case studies, involving four different implementation languages and ranging from 256 K up to 13 Mb of source code size.","1063-6773","0-7695-0016-1","10.1109/ICSM.1999.792593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792593","","Software maintenance;Programming profession;Testing;Read only memory;Gas detectors;Writing;Costs;Computer industry;Software systems;Application software","software maintenance;program processors;program visualisation;software tools","duplicated code detection;language independent approach;large software maintenance;large software evolution;visual approach","","260","5","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Evolution of differentiated multi-threaded digital organisms","T. S. Ray; J. Hart","ATR Human Information Processing Research Laboratories, Soraku-gun, Kyoto, Japan; ATR Human Information Processing Research Laboratories, Soraku-gun, Kyoto, Japan","Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289)","6 Aug 2002","1999","1","","1","10 vol.1","Presents a descriptive natural history of the results of the evolution of differentiated multi-threaded (multi-cellular) self-replicating machine code programs (digital organisms), living in a network of computers, called the Tierra network. Programs are differentiated in that different threads execute different code (i.e. they express different genes). The seed organism develops into a mature ten-celled form, differentiated into a two-celled reproductive tissue and an eight-celled sensory tissue. The sensory threads obtain data about conditions on the machines in the network, and then process that data to choose the best machine to migrate to or to send the daughter to. Evolution leads to a diversity of algorithms for foraging for resources, primarily CPU time, on the network.","","0-7803-5184-3","10.1109/IROS.1999.812972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812972","","Organisms;Yarn;Genomics;Bioinformatics;DNA;Computer networks;Central Processing Unit;Sequences;Cells (biology);Biomembranes","biocybernetics;evolution (biological);multi-threading;artificial life;computer networks;evolutionary computation;biology computing;mathematics computing","differentiated multi-threaded digital organisms;evolution;descriptive natural history;multi-cellular self-replicating machine code programs;computer network;Tierra network;gene expression;seed organism;mature 10-celled form;reproductive tissue;sensory tissue;sensory threads;resource foraging algorithms;CPU time","","1","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Measuring clone based reengineering opportunities","M. Balazinska; E. Merlo; M. Dagenais; B. Lague; K. Kontogiannis","Department of Electrical and Computer Engineering, École Polytechnique de Montréal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, École Polytechnique de Montréal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, École Polytechnique de Montréal, Montreal, QUE, Canada; Quality Engineering and Research Group, Bell Canada Limited, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ONT, Canada","Proceedings Sixth International Software Metrics Symposium (Cat. No.PR00403)","6 Aug 2002","1999","","","292","303","Code duplication, plausibly caused by copying source code and slightly modifying it, is often observed in large systems. Clone detection and documentation have been investigated by several researchers in the past years. Recently, research focus has shifted towards the investigation of software and process restructuring actions based on clone detection. This paper presents an original definition of a clone classification scheme useful to assess and measure different system reengineering opportunities. The proposed classification considers each group of cloned methods in terms of the meaning of the differences existing between them. The algorithm used for automatic classification of clones is presented together with results obtained by classifying cloned methods and measuring reengineering opportunities in six freely available systems whose total size is about 500 KLOC of Java code.","","0-7695-0403-5","10.1109/METRIC.1999.809750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=809750","","Cloning;Pattern matching;Software systems;Size measurement;Java;Software libraries;Electrical capacitance tomography;Programming profession;Documentation;Fingerprint recognition","systems re-engineering;software reusability;software metrics","clone based reengineering;code duplication;system reengineering;clone classification;process restructuring;clone detection","","55","2","22","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A hybrid reverse engineering approach combining metrics and program visualisation","S. Demeyer; S. Ducasse; M. Lanza","Software Composition Group, University of Bern, Berne, Switzerland; Software Composition Group, University of Bern, Berne, Switzerland; Software Composition Group, University of Bern, Berne, Switzerland","Sixth Working Conference on Reverse Engineering (Cat. No.PR00303)","6 Aug 2002","1999","","","175","186","Surprising as it may seem, many of the early adopters of the object-oriented paradigm already face a number of problems typically encountered in large-scale legacy systems. Consequently, reverse engineering techniques are relevant in an object-oriented context as well. This paper investigates a hybrid approach, combining the immediate appeal of visualisations with the scalability of metrics. We validate such a hybrid approach by showing how CodeCrawler - the experimental platform we built-allowed us to understand the program structure of and identify potential design anomalies in a public-domain software system.","","0-7695-0303-9","10.1109/WCRE.1999.806958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806958","","Reverse engineering;Visualization;Ear;Chromium;Design methodology;Large-scale systems;Displays;Humans;Rain;Software systems","reverse engineering;public domain software;software metrics;program visualisation;object-oriented programming;computer aided software engineering","hybrid reverse engineering approach;software metrics;program visualisation;object-oriented programming paradigm;large-scale legacy systems;scalability;CodeCrawler;program structure understanding;design anomalies identification;public-domain software system","","20","","34","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"QBO: a query tool specially developed to explore programs","F. Balmas","Department Informatique, Université Paris 1, France","Sixth Working Conference on Reverse Engineering (Cat. No.PR00303)","6 Aug 2002","1999","","","270","279","We introduce QBO, or Query by Outlines, a tool specially developed to help explore programs. It relies on a previously implemented system able to automatically construct outlines (F. Balmas, 1997; 1998): every linear loop identified in a program is conceptualized according to the kind of computations it performs. QBO proposes an outline storage mechanism together with a query algorithm that enables outlines to be efficiently retrieved. QBO eases exploration of programs, thus program management, clone detection or plan recognition can be envisaged at lower cost; as outlines are already computed and indexed, only high level constructs have to be checked. Therefore, answering queries is a rather fast process. We sketch our outlining model, present our query tool and discuss how query by outlines may help explore programs.","","0-7695-0303-9","10.1109/WCRE.1999.806966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806966","","Cloning;Ear;Programmable logic arrays;Costs;Programming profession;Large-scale systems;Performance evaluation;Identity-based encryption;Testing","software maintenance;reverse engineering;program control structures;query processing;software tools","QBO;query tool;Query by Outlines;program exploration;linear loop;outline storage mechanism;query algorithm;program management;clone detection;plan recognition;high level constructs;query answering;outlining model","","","","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Partial redesign of Java software systems based on clone analysis","M. Balazinska; E. Merlo; M. Dagenais; B. Lague; K. Kontogiannis","Department of Electrical and Computer Engineering, Ècole Polytechnique de Montréal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, Ècole Polytechnique de Montréal, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, Ècole Polytechnique de Montréal, Montreal, QUE, Canada; Bell Canada, Quality Engineering and Research (QER) Group, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ONT, Canada","Sixth Working Conference on Reverse Engineering (Cat. No.PR00303)","6 Aug 2002","1999","","","326","336","Code duplication, plausibly caused by copying source code and slightly modifying it, is often observed in large systems. Clone detection and documentation have been investigated by several researchers in past years. Recently, research focus has shifted towards the investigation of software and process restructuring actions based on clone detection. The paper presents a new redesign approach developed for Java software systems. The approach factorizes the common parts of cloned methods and parameterizes their differences using the strategy design pattern. The new entities created by such transformations are also decoupled from the original contexts of their use, thus facilitating reuse and increasing maintainability. The applicability and automation of the technique presented in the paper have been verified by partially redesigning JDK 1.1.5.","","0-7695-0303-9","10.1109/WCRE.1999.806971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806971","","Java;Software systems;Cloning;Pattern matching;Documentation;Design automation;Software libraries;Programming profession;Fingerprint recognition;Dynamic programming","Java;software reusability;software maintenance;systems re-engineering","partial redesign;Java software systems redesign;clone analysis;code duplication;large systems;clone detection;process restructuring actions;redesign approach;cloned methods;strategy design pattern;maintainability;partial JDK redesign","","30","","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Restructuring functions with low cohesion","A. Lakhotia; J. . -C. Deprez","The Center for Advanced Computer Studies, University of Southwestern Louisiana, Lafayette, LA, USA; The Center for Advanced Computer Studies, University of Southwestern Louisiana, Lafayette, LA, USA","Sixth Working Conference on Reverse Engineering (Cat. No.PR00303)","6 Aug 2002","1999","","","36","46","We present a technique for restructuring functions with low cohesion into functions with high cohesion. Such restructuring is desirable when re-architecting a legacy system into an object-oriented architecture. The restructured system has functions with higher cohesion and hence lower coupling. This enables finer-grained grouping of functions into objects. Automatically decomposing a function is difficult when its computations are interleaved. The challenge lies in programmatically identifying and separating the various activities performed by a complex code segment. The technique presented partitions the set of output variables of a function on the basis of their pairwise cohesion. Program slicing is then used to identify the statements that perform computations for each variable group in the partition. New functions corresponding to the slices are created to replace the original function. Experiences with restructuring real-world code using a tool that implements the technique are presented.","","0-7695-0303-9","10.1109/WCRE.1999.806945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806945","","Software systems;Computer industry;Reverse engineering;Computer architecture;Programming profession;Data processing;Algorithms;Environmental management","program slicing;software maintenance;reverse engineering;systems re-engineering","function restructuring;low cohesion;high cohesion;legacy system re-architecting;object-oriented architecture;complex code segment;output variable partitioning;pairwise cohesion;program slicing","","10","","43","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"PCI-PipeRench and the SWORDAPI: a system for stream-based reconfigurable computing","R. Laufer; R. R. Taylor; H. Schmit","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA","Seventh Annual IEEE Symposium on Field-Programmable Custom Computing Machines (Cat. No.PR00375)","6 Aug 2002","1999","","","200","208","Reconfigurable hardware accelerators have been shown to be flexible and efficient in stream-based applications. In this paper, we discuss the design of PCI-PipeRench and the SWORDAPI. PCI-PipeRench is a coprocessor utilizing the PipeRench architecture which includes on-chip control and data buffering to interface with a host processor over a PCI bus. SWORDAPI calls resemble standard C file control functions, and allow developers to create applications Independent of underlying reconfigurable hardware details. In addition, the SWORDAPI provides a cosimulation environment so that verification can be performed using unmodified application source with a hardware simulator. Efficient utilization of the bus is of critical importance in the design of such a system; various methods used to address this issue are presented.","","0-7695-0375-6","10.1109/FPGA.1999.803682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=803682","","Hardware;Coprocessors;Pipelines;Fabrics;Streaming media;Bandwidth;Computer buffers;Standards development;Application software;Computational modeling","reconfigurable architectures;system buses;application program interfaces;coprocessors","reconfigurable hardware accelerators;PCI-PipeRench;SWORDAPI;coprocessor;cosimulation environment;reconfigurable hardware","","9","4","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Evidence driven object identification in procedural code","K. Kontogiannis; P. Patil","Department of Electrical & Computer Engineering, University of Waterloo, Waterloo, ONT, Canada; Department of Electrical & Computer Engineering, University of Waterloo, Waterloo, ONT, Canada","STEP '99. Proceedings Ninth International Workshop Software Technology and Engineering Practice","6 Aug 2002","1999","","","12","21","Software evolution is an integrated part of software maintenance. It may take the form of porting a legacy system to a new hardware platform operating system, translating the system to a new language or rearchitecting the system to take advantage of new programming paradigms. This paper presents techniques for the identification and recognition of object-oriented structures in legacy systems that have been implemented using a procedural language. The paper examines methods for the selection of object classes and the recovery of the possible associations between the recovered classes.","","0-7695-0328-4","10.1109/STEP.1999.798403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=798403","","Wrapping;Data analysis;Software metrics;Object oriented modeling;Councils;Context;Control systems;Control system analysis;Encapsulation;Global communication","object-oriented programming;software maintenance;software portability;abstract data types","evidence-driven object identification;procedural code;software evolution;software maintenance;legacy system porting;hardware platform;operating system;programming language translation;system rearchitecting;programming paradigms;object-oriented structures;procedural language;object class selection;class association recovery","","8","","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"An approach to manage variance in legacy systems","A. Karhinen; M. Sandrini; J. Tuominen","Software Technology Laboratory,Nokia Group, Nokia Research Center, Finland; Software Technology Laboratory,Nokia Group, Nokia Research Center, Finland; Software Technology Laboratory,Nokia Group, Nokia Research Center, Finland","Proceedings of the Third European Conference on Software Maintenance and Reengineering (Cat. No. PR00090)","6 Aug 2002","1999","","","190","193","Different market areas set different requirements on software-intensive products. A careful domain analysis yields the predictable variance that can be managed at the architectural and design levels, but the unpredictable variance that is detected only during the maintenance phase is usually managed at the implementation level, due to the high costs of the system re-design or re-architecting. Thus, most legacy systems contain a lot of unpredictable variance in the source code, e.g. in the form of cluttering it with pre-processor directives. The weak overlay technique is a promising reverse engineering approach to abstracting, comprehending, maintaining and re-designing legacy software containing various types of variance.","","0-7695-0090-0","10.1109/CSMR.1999.756698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=756698","","Telephony;Analysis of variance;Software design;Laboratories;Phase detection;Costs;Reverse engineering;Reactive power;Cultural differences;User interfaces","reverse engineering;software maintenance;systems re-engineering;configuration management","legacy systems variance management;market areas;software-intensive product requirements;domain analysis;software maintenance phase;implementation level;system redesign;system re-architecting;unpredictable source code variance;cluttering;pre-processor directives;weak overlay technique;reverse engineering;software abstraction;software comprehension","","3","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Pattern-based reverse-engineering of design components","R. K. Keller; R. Schauer; S. Robitaille; P. Page","Dúpartement IRO, Universite de MontrDúpartement IROal, Montreal, QUE, Canada; Dúpartement IRO, Universite de MontrDúpartement IROal, Montreal, QUE, Canada; Dúpartement IRO, Universite de MontrDúpartement IROal, Montreal, QUE, Canada; Dúpartement IRO, Universite de MontrDúpartement IROal, Montreal, QUE, Canada","Proceedings of the 1999 International Conference on Software Engineering (IEEE Cat. No.99CB37002)","6 Aug 2002","1999","","","226","235","Many reverse-engineering tools have been developed to derive abstract representations from source code. Yet, most of these tools completely ignore recovery of the all-important rationale behind the design decisions that have lead to its physical shape. Design patterns capture the rationale behind proven design solutions and discuss the trade-offs among their alternatives. We argue that it is these patterns of thought that are at the root of many of the key elements of large-scale software systems, and that, in order to comprehend these systems, we need to recover and understand the patterns on which they were built. In this paper, we present our environment for the reverse engineering of design components based on the structural descriptions of design patterns. We give an overview of the environment, explain three case studies, and discuss how pattern-based reverse-engineering helped gain insight into the design rationale of some of the pieces of three large-scale C++ software systems.","0270-5257","1-58113-074-0","10.1145/302405.302622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841012","","Software systems;Large-scale systems;Visualization;Design engineering;Java;Shape;Reverse engineering;Pattern analysis;Application software;State estimation","reverse engineering;object-oriented methods;object-oriented programming;software engineering","pattern-based reverse engineering;design components;abstract representations;source code;design decisions;design patterns;structural descriptions;large-scale C++ software systems","","64","7","30","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A multiparadigm language approach to teaching principles of programming languages","D. S. Westbrook","Computer Science and Elecfrical Engineering, Northern Arizona University, Flagstaff, AZ, USA","FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No.99CH37011","6 Aug 2002","1999","1","","11B3/14","11B3/18 vol.1","This paper describes the authors' experiences in using the multiparadigm language GED to teach the principles of a programming languages course. The benefits of using a multiparadigm language include less time spent on learning new environments for different languages, easier transition to different paradigms, and opportunities for multiparadigm programming. In this paper, they give a brief description of GED (which supports the imperative, functional, logic and object-oriented paradigms), describe how it is used in their course, and discuss the advantages and disadvantages of this approach versus the traditional use of several languages.","0190-5848","0-7803-5643-8","10.1109/FIE.1999.839221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839221","","Education;Computer languages;Logic programming;Programming profession;Object oriented programming;Computer science;Resumes;Java;Heart;Educational programs","computer science education;programming languages;programming;teaching;educational courses;programming environments","programming languages principles teaching;computer science education;multiparadigm language approach;GED language;programming languages course;environments;multiparadigm programming","","1","","3","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A management request broker based on transactional mobile agents for IN-TINA services","K. Sammoud; N. Simoni","INFRES Department, TELECOM ParisTech, Paris, France; NA","Seamless Interconnection for Universal Services. Global Telecommunications Conference. GLOBECOM'99. (Cat. No.99CH37042)","6 Aug 2002","1999","1A","","381","385 vol. 1a","Contemporary computer networks are heterogeneous. But few programming tools embrace, or even acknowledge, this complexity. New methods and approaches are required if next-generation networks are to be configured, administered and utilized to their full potentials. The growing field of ORBs, mobile agents and transactional researches seeks to address problems in this domain. In this paper we discuss the potential uses of a management request broker (MRB) in network and service management. This MRB is based on an ORB, an environment for executing mobile agents and an integrated transaction service. The keyword of our approach is TMA (transactional mobile agent). The core of this paper comprises descriptions of the MRB functions and the potential applications of TMA in the five OSI functional areas of network and service management. Descriptions of the MRBs organization and of the platform on which it can be developed are explained. An IN-TINA integrated telecom architecture is presented as an application domain. Prospects activity in the area conclude the presentation.","","0-7803-5796-5","10.1109/GLOCOM.1999.831667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=831667","","Mobile agents;Protocols;Communication industry;Information management;Computer network management;Conference management;Telecommunication network management;Electronic mail;Computer networks;Next generation networking","computer network management;intelligent networks;information networks;software agents;distributed object management;open systems","management request broker;transactional mobile agents;IN-TINA services;heterogeneous computer networks;programming tools;next-generation networks;network configuration;network administration;ORB;mobile agents;transactional research;network management;service management;OSI functional areas;integrated transaction service;IN-TINA integrated telecom architecture;CORBA","","1","","18","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Towards tool support for reuse","R. L. Biddle; E. D. Tempero","School of Mathematical and Computing Sciences, Victoria University of Wellington, New Zealand; School of Mathematical and Computing Sciences, Victoria University of Wellington, New Zealand","Proceedings. 1998 International Conference Software Engineering: Education and Practice (Cat. No.98EX220)","6 Aug 2002","1998","","","126","133","Many approaches are being pursued to improve the effectiveness of software development. Two important approaches are the development of tools to aid the programmer and reuse of code to reduce the programmer's work. The Renata Project at VUW has begun investigating combining both approaches. We report on our experience in developing tools that directly support the reuse of code in software development.","","0-8186-8828-9","10.1109/SEEP.1998.707642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=707642","","Programming profession;Electrical capacitance tomography;Productivity;Prototypes;Application software;Production;Software reusability;Life testing;Software testing;Code standards","software reusability;software tools","tool support;software reuse;software development;code reuse;Renata Project","","2","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Change and adaptive maintenance detection in Java software systems","D. Rayside; S. Kerr; K. Kontogiannis","Electrical & Computer Engineering, University of Waterloo, Canada; Object Technology International, Inc., Ottawa, Canada; Systems Design Engineering, University of Waterloo, Canada","Proceedings Fifth Working Conference on Reverse Engineering (Cat. No.98TB100261)","6 Aug 2002","1998","","","10","19","Java is a relatively new programming language that is gaining popularity due to its network-centric features and platform independence (Write Once, Run Anywhere). This popularity has caused rapid evolution in the libraries that are available for Java applications. This evolution, in combination with Java's run-time linking, may cause incompatibilities between an application and the library it depends on: an application may execute with a different library version than the one it was compiled for. This paper presents techniques to automatically detect change in a library from its bytecode (binary) representation, and to apply the impact of those changes to any Java application. This paper also includes results of change detection experiments performed on the standard Java library (JDK).","","0-8186-8967-6","10.1109/WCRE.1998.723171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=723171","","Java;Software systems;Application software;Software libraries;Design engineering;Systems engineering and theory;Intelligent networks;Computer networks;Computer languages;Runtime","object-oriented languages;object-oriented programming;software maintenance;reverse engineering;software libraries","adaptive maintenance detection;change detection;Java;programming language;platform independence;software libraries;run-time linking;bytecode representation;standard Java library;JDK","","5","1","28","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Outlining C loops: preliminary results and trends","F. Balmas","Département Informatique, Université Paris 1, France","Proceedings Fifth Working Conference on Reverse Engineering (Cat. No.98TB100261)","6 Aug 2002","1998","","","115","124","We report on a system developed to construct outlines of loops in C programs. It is derived from a model and a system previously defined to outline LISP loops, that have both been enhanced to handle those constructs of an imperative language like C that were out of scope in (pure functional) LISP, especially: iterative control flow structures, variable assignments, and manipulations of arrays, structures or pointers. In this paper, we first introduce our model for C loop outlining. Then we present the architecture of our system and describe the main steps of the outline construction process. We then discuss results obtained from the application of our system to two real-world programs and finally we show that this experiment gives a preview of the use of outlines as indexes for browsing and querying large scale programs.","","0-8186-8967-6","10.1109/WCRE.1998.723181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=723181","","Large-scale systems;Reverse engineering;Programming profession;Visualization;Taxonomy;Computational modeling;Data mining;System testing;Lab-on-a-chip;Performance evaluation","systems re-engineering;C language;program control structures","C loops outlining;LISP loops;imperative language;iterative control flow structures;variable assignments;real-world programs;indexes;browsing;large scale programs querying","","2","1","24","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Clone detection using abstract syntax trees","I. D. Baxter; A. Yahin; L. Moura; M. Sant'Anna; L. Bier","Semantic Designs, Inc., Austin, TX, USA; Semantic Designs, Inc., Austin, TX, USA; Departamento de Informática, Pontificia Universidade Católica Do Rio de Janeiro, Rio de Janeiro, Brazil; NA; SEO, Eaton Corporation, Austin, TX, USA","Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)","6 Aug 2002","1998","","","368","377","Existing research suggests that a considerable fraction (5-10%) of the source code of large scale computer programs is duplicate code (""clones""). Detection and removal of such clones promises decreased software maintenance costs of possibly the same magnitude. Previous work was limited to detection of either near misses differing only in single lexems, or near misses only between complete functions. The paper presents simple and practical methods for detecting exact and near miss clones over arbitrary program fragments in program source code by using abstract syntax trees. Previous work also did not suggest practical means for removing detected clones. Since our methods operate in terms of the program structure, clones could be removed by mechanical methods producing in-lined procedures or standard preprocessor macros. A tool using these techniques is applied to a C production software system of some 400 K source lines, and the results confirm detected levels of duplication found by previous work. The tool produces macro bodies needed for clone removal, and macro invocations to replace the clones. The tool uses a variation of the well known compiler method for detecting common sub expressions. This method determines exact tree matches; a number of adjustments are needed to detect equivalent statement sequences, commutative operands, and nearly exact matches. We additionally suggest that clone detection could also be useful in producing more structured code, and in reverse engineering to discover domain concepts and their implementations.","1063-6773","0-8186-8779-7","10.1109/ICSM.1998.738528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=738528","","Cloning;Software maintenance;Costs;Programming profession;Software engineering;Encapsulation;World Wide Web;Large-scale systems;Production systems;Software systems","software maintenance;reverse engineering;program diagnostics;C language;program compilers;trees (mathematics)","clone detection;abstract syntax trees;large scale computer programs;duplicate code;software maintenance costs;lexems;near miss clones;arbitrary program fragments;program source code;detected clones;program structure;mechanical methods;in-lined procedures;standard preprocessor macros;C production software system;macro bodies;clone removal;macro invocations;compiler method;common sub expressions;exact tree matches;equivalent statement sequences;commutative operands;nearly exact matches;structured code;reverse engineering;domain concepts","","514","12","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Better global scheduling using path profiles","C. Young; M. D. Smith","Bell Laboratories, Lucent Technologies, Inc., Murray Hill, NJ, USA; Division of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA","Proceedings. 31st Annual ACM/IEEE International Symposium on Microarchitecture","6 Aug 2002","1998","","","115","123","Path profiles record the frequencies of execution paths through a program. Until now, the best global instruction schedulers have relied upon profile-gathered frequencies of conditional branch directions to select sequences of basic blocks that only approximate the frequently-executed program paths. The identified sequences are then enlarged using the profile data to improve the scope of scheduling. Finally, the enlarged regions are compacted so that they complete in a small number of cycles. Path profiles remove the need to approximate the frequently-executed paths that are so important to the success of the compaction phase. In this paper, we describe how one can modify a trace-based instruction scheduler and in particular a superblock schedule; to use path profiles in both the selection and enlargement phases of global scheduling. As our experimental results demonstrate, the use of more detailed profile data allows the scheduler to construct superblocks that are more likely to avoid early exits. This effect leads to more useful speculative code motions and an overall improvement in program performance. We also describe how a path-profile based approach can simplify the engineering of a trace-based scheduler by unifying several trace-enlargement heuristics into a single general mechanism.","1072-4451","0-8186-8609-X","10.1109/MICRO.1998.742774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=742774","","Compaction;Frequency;Hardware;Maintenance engineering","scheduling;program compilers;computer architecture","global scheduling;path profiles;scheduling;instruction scheduler;speculative code motions;program performance","","19","7","22","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Assessing the benefits of incorporating function clone detection in a development process","B. Lague; D. Proulx; J. Mayrand; E. M. Merlo; J. Hudepohl","Quality Engineering Advisors, Information Technology Procurement, Bell Canada Limited, Longueuil, QUE, Canada; NA; DGEGI Department of Electrical and Computer Engineering, École Polytechnique, Montreal, QUE, Canada; Vice-president Technology, Telsoft Ventures, Inc., Montreal, QUE, Canada; Manager, Software Reliability, Engineering and Tool Development, Nortel Technologies, Inc., Research Triangle Park, NC, USA","1997 Proceedings International Conference on Software Maintenance","15 Oct 2012","1997","","","314","321","The aim of the experiment presented in this paper is to present an insight into the evaluation of the potential benefits of introducing a function clone detection technology in an industrial software development process. To take advantage of function clone detection, two modifications to the software development process are presented. Our experiment consists of evaluating the impact that these proposed changes would have had on a specific software system if they had been applied over a 3 year period (involving 10000 person-months), where 6 subsequent versions of the software under study were released. The software under study is a large telecommunication system. In total 89 million lines of code have been analyzed. A first result showed that, against our expectations, a significant number of clones are being removed from the system over time. However, this effort is insufficient to prevent the growth of the overall number of clones in the system. In this context the first process change would have added value. We have also found that the second process change would have provided programmers with a significant number of opportunities for correcting problems before customers experienced them. This result shows a potential for improving the software system quality and customer satisfaction","1063-6773","0-8186-8013-X","10.1109/ICSM.1997.624264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5726968","","","software cost estimation;software maintenance;software metrics;software quality;telecommunication computing","customer satisfaction;experiment;function clone detection;industrial software development;process change;product assessment;programmers;software clones;software cost;software development process;software maintenance;software quality;source code metrics;telecommunication system","","96","","13","IEEE","15 Oct 2012","","","IEEE","IEEE Conferences"
"Investigating the maintenance implications of the replication of code","E. Burd; M. Munro","The Centre for Software Maintenance, University of Durham, Durham, UK; The Centre for Software Maintenance, University of Durham, Durham, UK","1997 Proceedings International Conference on Software Maintenance","15 Oct 2012","1997","","","322","329","This paper describes an investigation into the use of code replication within legacy software systems. Two cases of replication are investigated. These are replication with an individual program and replication of an entire or part of a program across a program suite. For each of the cases an example is given from code used within the commercial sector. The instances of replication are then investigated and the implication of their occurrences within the code on the maintenance process are considered. The reasons why code replication is not a form of software reuse are discussed. Finally this paper investigates whether, with reengineering, areas of high usage of code replication are potential candidates for reuse","1063-6773","0-8186-8013-X","10.1109/ICSM.1997.624265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5726969","","","software maintenance;software management;software reusability;systems re-engineering","code replication;commercial sector;legacy software systems;program replication;program suite;reengineering;software maintenance;software reuse","","9","","12","IEEE","15 Oct 2012","","","IEEE","IEEE Conferences"
"The software bookshelf","P. J. Finnigan; R. C. Holt; I. Kalas; S. Kerr; K. Kontogiannis; H. A. Muller; J. Mylopoulos; S. G. Perelgut; M. Stanley; K. Wong","IBM Software Solutions Division, Toronto Laboratory, North York, ONT, Canada; Department of Computer Science, University of Waterloo, Waterloo, ONT, Canada; Centre for Advanced Studies, IBM Software SolutionsDivision, Toronto Laboratory, North York, ONT, Canada; Department of Computer Science, University of Toronto, Toronto, ONT, Canada; Department of Computer Science, University of Waterloo, Waterloo, ONT, Canada; Department of Computer Science, University of Victoria, P.O. Box 3055, MS-7209, B.C., Canada V8W 3P6; Department of Computer Science, University of Toronto, Toronto, ONT, Canada; IBM Software Solutions Division, Toronto Laboratory, North York, ONT, Canada; Technical Knowledge Systems lnc., Toronto, ONT, Canada; Department of Computer Science, University of Victoria, Victoria, BC, Canada","IBM Systems Journal","6 Apr 2010","1997","36","4","564","593","Legacy software systems are typically complex, geriatric, and difficult to change, having evolved over decades and having passed through many developers. Nevertheless, these systems are mature, heavily used, and constitute massive corporate assets. Migrating such systems to modern platforms is a significant challenge due to the loss of information over time. As a result, we embarked on a research project to design and implement an environment to support software migration. In particular, we focused on migrating legacy PL/I source code to C++, with an initial phase of looking at redocumentation strategies. Recent technologies such as reverse engineering tools and World Wide Web standards now make it possible to build tools that greatly simplify the process of redocumenting a legacy software system. In this paper we introduce the concept of a software bookshelf as a means to capture, organize, and manage information about a legacy software system. We distinguish three roles directly involved in the construction, population, and use of such a bookshelf: the builder, the librarian, and the patron. From these perspectives, we describe requirements for the bookshelf, as well as a generic architecture and a prototype implementation. We also discuss various parsing and analysis tools that were developed and integrated to assist in the recovery of useful information about a legacy system. In addition, we illustrate how a software bookshelf is populated with the information of a given software project and how the bookshelf can be used in a program-understanding scenario. Reported results are based on a pilot project that developed a prototype bookshelf for a software system consisting of approximately 300K lines of code written in a PL/I dialect.","0018-8670","","10.1147/sj.364.0564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387168","","","","","","134","2","","","6 Apr 2010","","","IBM","IBM Journals"
"Unconstraining genericity","M. Evered","University of Ulm (EBS), Germany","Proceedings. Technology of Object-Oriented Languages. TOOLS 24 (Cat. No.97TB100240)","6 Aug 2002","1997","","","340","349","Generic classes and especially generic collection libraries can be of great benefit for efficient software production. Constrained genericity is used to guarantee that the type provided as a parameter to a generic class such as a sorted list will offer the methods that class requires. We argue that constrained genericity is not really the appropriate mechanism for this purpose since it restricts a generic class to one kind of use for each element type. We introduce the concept of 'generic procedure parameters' which allow the properties of a collection class to be specified in the instantiation rather than via the properties of the elements. We show that the concept is very efficiently implementable, more powerful than constrained genericity and more useful for the practical construction of complex data collections.","","0-8186-8551-4","10.1109/TOOLS.1997.713561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=713561","","Proposals;Java;Production;Software reusability;Object oriented programming;Packaging;Safety;Data structures;Software libraries;Computer languages","object-oriented programming;software reusability;software libraries","generic classes;generic collection libraries;software production;constrained genericity;sorted list;generic procedure parameters;complex data collections;object oriented programming;software reuse","","2","","24","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Identifying objects in legacy systems","A. Cimitile; A. De Lucia; G. A. Di Lucca; A. R. Fasolino","Dipartimento di Ingegneria dellInformazione ed Ingegneria Elettrica, Faculty of Engineering, Benevento, University of Salerno, Benevento, Italy; Dipartimento di Ingegneria dellInformazione ed Ingegneria Elettrica, Faculty of Engineering, Benevento, University of Salerno, Benevento, Italy; Dipartimento di Informatica e Sistemistica, University of Naples Federico II, Naples, Italy; Dipartimento di Informatica e Sistemistica, University of Naples Federico II, Naples, Italy","Proceedings Fifth International Workshop on Program Comprehension. IWPC'97","6 Aug 2002","1997","","","138","147","We present an approach to decomposing legacy systems written in procedural languages into objects. The identification of the objects is centred around persistent data scores, such as files or tables in the database, while programs and subroutines are candidate to implement object methods. The approach proposed for assigning programs and subroutines as object methods exploits object oriented design metrics. The rationale behind this choice is that any object oriented decomposition of a legacy system should not lead to a poor design, as this would make the reengineered system more difficult to maintain.","1092-8138","0-8186-7993-X","10.1109/WPC.1997.601281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601281","","Object oriented modeling;Software systems;Algorithms;Costs;Production systems;Software maintenance;Object oriented databases;Design methodology;Open systems;Encapsulation","software maintenance;object-oriented programming;software metrics;systems re-engineering;data structures","legacy systems decomposition;procedural languages;persistent data scores;object identification;object methods;subroutines;object oriented design metrics;object oriented decomposition;reengineered system;software maintenance","","18","","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Entropic bounds on FSM switching","A. Tuagi","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","6 Aug 2002","1997","5","4","456","464","Several state assignment algorithms have attempted to minimize the average Hamming distance per transition in the hopes of generating low power assignments. There has not been a reasonable theoretical lower bound on the average Hamming distance per transition that is applicable to every state assignment for a given finite state machine (FSM). Such a lower bound serves many roles-a target for algorithm designers, provides clues about what types of FSM structures are likely to have low average switching per transition, could be incorporated into a high-level power model. We provide two such lower bounds which were also found to be achievable empirically within 17% for MCNC benchmarks. An interesting byproduct of one of these 'theoretical' lower bounds was a greedy state assignment algorithm which is amenable to a very distributed (parallel) implementation. This algorithm also outperforms JEDI by 2.5% for area and by 21% for power over MCNC benchmarks.","1557-9999","","10.1109/92.645072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=645072","","Hamming distance;Entropy;Steady-state;Automata;Probability distribution;Information theory;Turning;Logic;Power generation;Algorithm design and analysis","finite state machines;state assignment;Hamming codes;entropy","FSM switching;state assignment algorithms;average Hamming distance;lower bound;high-level power model;MCNC benchmarks;greedy state assignment;parallel implementation","","7","1","18","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Cots initiative and cost of S/W for aging avionics upgrades","W. A. Hanna","McDonnell Douglas Aerospace MC S064 2374, Advanced Systems and Technology Phantom Works, Saint Louis, MO, USA","16th DASC. AIAA/IEEE Digital Avionics Systems Conference. Reflections to the Future. Proceedings","6 Aug 2002","1997","1","","1.1","1","Tremendous progress has been made in electronic H/W upgrade methodology through the use of reverse engineering; VHDL (VHSIC H/W Description Language); and H/W modeling, simulation, and synthesis tools. There are striking similarities between electronic H/W and S/W design methods due to the increasing popularity of modeling and simulation tools in both areas. In this paper we attempt to apply successful H/W modeling and simulation methodology to S/W upgrades for achieving affordable fast turn around S/W upgrades. The concepts of OOC/OOD (Object Oriented Coding/Object Oriented Design), S/W Wrappers, S/W Partitioning, and Plug and Play are also exploited for fast turnaround and affordable S/W upgrades. 40-60% cost reduction is achievable for both H/W and S/W development based on COTS approach and affordability measures. Commonality among platforms can add even more cost savings to both development and long term (20-30 years) maintenance of fielded upgrades based on the proposed methodology. The paper covers many of the areas that can influence S/W schedule and cost improvements. Some of the opinions presented need validation through R&D based on the proposed S/W upgrade methodology.","","0-7803-4150-3","10.1109/DASC.1997.635005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=635005","","Costs;Aging;Aerospace electronics;Object oriented modeling;Backplanes;Computer displays;Standards development;Operating systems;Software libraries;Military computing","avionics;economics;maintenance engineering;military computing;software management;object-oriented methods;aerospace computing","aging avionics upgrades;H/W upgrade methodology;VHSIC H/W Description Language;modeling;simulation;Object Oriented Coding;Object Oriented Design;S/W Wrappers;S/W Partitioning;Plug and Play;cost reduction;long term maintenance;fielded upgrades;Ada;C language;20 to 30 y","","","","9","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Location, identity and wireless fraud detection","S. Patel","Lucent Technologies, Wireless Secure Communications Laboratory, Whippany, NJ, USA","1997 IEEE International Conference on Personal Wireless Communications (Cat. No.97TH8338)","6 Aug 2002","1997","","","515","521","The detection of fraud in wireless systems is made easier when the network is continuously aware of the location of mobile handsets. For existing cellular systems, location awareness requires handsets to be powered on. We explore ways to exploit the autonomous registration capabilities of handsets in order to detect the presence of illegitimate clones. Throughout the paper, location awareness is used as a new paradigm for organizing new and existing fraud detection techniques.","","0-7803-4298-4","10.1109/ICPWC.1997.655573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=655573","","Cloning;Telephone sets;Communication system security;Authentication;Mobile radio mobility management;Databases;Base stations;Communications technology;Wireless communication;Mobile communication","cellular radio;land mobile radio;fraud;tracking;radio networks;cryptography;protocols","wireless fraud detection;mobile handsets location;cellular systems;location awareness;illegitimate clones;home location register;mobile switching center;visitor location register;wireless network;cryptographic protocols","","2","4","3","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Lazy arithmetic","D. Michelucci; J. . -M. Moreau","Laboratoire LISSE, Ecole des Mines de Saint Etienne, Saint-Etienne, France; Laboratoire LISSE, Ecole des Mines de Saint Etienne, Saint-Etienne, France","IEEE Transactions on Computers","6 Aug 2002","1997","46","9","961","975","Finite-precision leads to many problems in geometric methods from CAD or Computational Geometry. Until now, using exact rational arithmetic was a simple, yet much too slow, solution to be of any practical use in real-scale applications. A recent optimization-the lazy rational arithmetic-seems promising: It defers exact computations until they become either unnecessary (in most cases) or unavoidable; in such a context, only indispensable computations are performed exactly, that is, those without which any given decision cannot be reached safely using only floating-point arithmetic. This paper takes stock of the lazy arithmetic paradigm: principles, functionalities and limits, speed, possible variants and extensions, difficulties, problems solved or left unresolved.","1557-9956","","10.1109/12.620478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620478","","Computational geometry;Solid modeling;Digital arithmetic;Character generation;Floating-point arithmetic;Robustness;Signal design;Calculus;Cryptography;Application software","computational geometry;digital arithmetic","geometric methods;exact rational arithmetic;lazy rational arithmetic;computational geometry;hash coding;interval arithmetic;robustness;lazy arithmetic;inconsistencies","","12","","44","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Evaluation experiments on the detection of programming patterns using software metrics","K. Kontogiannis","Department of Electrical & Computer Engineering, University of Waterloo, Waterloo, ONT, Canada","Proceedings of the Fourth Working Conference on Reverse Engineering","6 Aug 2002","1997","","","44","54","Cloning of code fragments in large systems is a common practice that may result in redundant code, higher maintenance costs, and less modular systems. The paper examines and evaluates the use of five data and control flow related metrics for identifying similar code fragments. The metrics are used as signatures for a code fragment. Matching on such signatures results in fast matching that can be used to locate instances of code cloning even in the presence of modifications such as changes in variable names, and insertion of statements. The paper takes an information retrieval approach and reports on experiments conducted for retrieving code fragments in three different software systems.","","0-8186-8162-4","10.1109/WCRE.1997.624575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624575","","Software metrics;Cloning;Pattern matching;Software systems;Software maintenance;Euclidean distance;Costs;Electric variables control;Information retrieval;Clustering algorithms","software metrics;software reusability;software maintenance;pattern matching;software performance evaluation","evaluation experiments;programming pattern detection;software metrics;code fragment cloning;redundant code;maintenance costs;modular systems;control flow related metrics;similar code fragments;signature matching;fast matching;code cloning;variable names;information retrieval approach;code fragment retrieval;software systems","","55","","26","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Experiment on the automatic detection of function clones in a software system using metrics","Mayrand; Leblanc; Merlo","Telsoft Ventures, Inc., Montreal, QUE, Canada; Information Technology, Procurement Bell Canada, Longueuil, QUE, Canada; Department of Electrical and Computer Engineering, Ecole Polytechnique, Montreal, QUE, Canada","1996 Proceedings of International Conference on Software Maintenance","6 Aug 2002","1996","","","244","253","This paper presents a technique to automatically identify duplicate and near duplicate functions in a large software system. The identification technique is based on metrics extracted from the source code using the tool Datrix/sup TM/. This clone identification technique uses 21 function metrics grouped into four points of comparison. Each point of comparison is used to compare functions and determine their cloning level. An ordinal scale of eight cloning levels is defined. The levels range from an exact copy to distinct functions. The metrics, the thresholds and the process used are fully described. The results of applying the clone detection technique to two telecommunication monitoring systems totaling one million lines of source code are provided as examples. The information provided by this study is useful in monitoring the maintainability of large software systems.","1063-6773","0-8186-7677-9","10.1109/ICSM.1996.565012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565012","","Software metrics","software metrics","function clone detection;software metrics;duplicate function identification;large software system;source code;Datrix tool;clone identification technique;cloning level;thresholds;telecommunication monitoring systems;software maintainability","","260","2","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Binary translation: static, dynamic, retargetable?","Cifuentes; Malhotra","Department of Computer Science, University of Queensland, Brisbane, Australia; Department of Computer Science, University of Tasmania, Hobart, Australia","1996 Proceedings of International Conference on Software Maintenance","6 Aug 2002","1996","","","340","349","The porting of software to newer and faster machines using static binary translation techniques has proved successful to a large extent. Current binary translators are static in nature and require a runtime environment to successfully support the execution of the translated programs on the new machine. On the other hand, dynamic binary translation has not been considered as an alternative to static translation-the authors argue that these translators can achieve at least the same performance as static translators but will require a simpler runtime environment. The paper presents techniques used to migrate legacy software running on register-based machines of the last 10 to 15 years to modern RISC machines. They have developed a second-generation disassembler to aid in the construction of a retargetable binary translation front-end. Retargetability of binary translators is on issue that has nor been addressed in present translators.","1063-6773","0-8186-7677-9","10.1109/ICSM.1996.565037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=565037","","Software portability","software portability","software porting;static binary translation techniques;runtime environment;translated program execution;dynamic binary translation;legacy software migration;register-based machines;RISC machines;second-generation disassembler;retargetable binary translation front-end","","35","12","34","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Correctness, efficiency, extendability and maintainability in neural network simulation","S. Lawrence; Ah Chung Tsoi; C. L. Giles","NEC Research Institute, Inc., Princeton, NJ, USA; Electrical and Computer Engineering, University of Queensland, Saint Lucia, Australia; NEC Research Institute, Inc., Princeton, NJ, USA","Proceedings of International Conference on Neural Networks (ICNN'96)","6 Aug 2002","1996","1","","474","479 vol.1","A large number of neural network simulators are publicly available to researchers. However, when a new paradigm is being developed, as is often the case, the advantages of using existing simulators decrease, causing most researchers to write their own software. It has been estimated that 85% of neural network researchers write their own simulators. We present techniques and principles for the implementation of neural network simulators. First and foremost, we discuss methods for ensuring the correctness of results-avoiding duplication, automating common tasks, using assertions liberally, implementing reverse algorithms, employing multiple algorithms for the same task, and using extensive visualization. Secondly, we discuss efficiency concerns, including using appropriate granularity object-oriented programming, and pre-computing information whenever possible.","","0-7803-3210-5","10.1109/ICNN.1996.548939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=548939","","Intelligent networks;Neural networks;Object oriented modeling;Computational modeling;Computer simulation;National electric code;Computer networks;Maintenance engineering;Australia;Visualization","neural nets","maintainability;neural network simulation;extendability;reverse algorithms;efficiency;object-oriented programming;specification","","1","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Entropic bounds on FSM switching","A. Tyagi","Department of Computer Science, Iowa State University, Ames, IA, USA","Proceedings of 1996 International Symposium on Low Power Electronics and Design","6 Aug 2002","1996","","","323","328","Several state assignment algorithms have attempted to minimize the average Hamming distance per transition in the hopes of generating low power assignments. There has not been a reasonable theoretical lower bound on the average Hamming distance per transition that is applicable to every state assignment for a given FSM. Such a lower bound serves many roles-a target for algorithm designers, provides clues about what types of FSM structures ore likely to have low average switching per transition, could be incorporated into a high-level power model. We provide two such lower bounds which were also found to be achievable empirically within 17% for MCNC benchmarks. An interesting by product of one of these 'theoretical' lower bounds was a greedy state assignment algorithm which is amenable to a very distributed (parallel) implementation. This algorithm also outperforms JEDI by 2.5% for area and by 21% for power over MCNC benchmarks.","","0-7803-3571-6","10.1109/LPE.1996.547533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=547533","","Hamming distance;Steady-state;Entropy;Probability distribution;Computer science;Power generation;Algorithm design and analysis;Upper bound;Turning","entropy;finite state machines;state assignment;Hamming codes","entropic bounds;FSM switching;state assignment algorithms;Hamming distance;MCNC benchmarks;greedy algorithm","","2","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Engineering applications of the self-organizing map","T. Kohonen; E. Oja; O. Simula; A. Visa; J. Kangas","Neural Networks Research Centre, Helsinki University of Technology, Espoo, Finland; Aalto-yliopisto Insinooritieteiden korkeakoulu, Aalto, FI; Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland; Aalto-yliopisto Insinooritieteiden korkeakoulu, Aalto, FI; Neural Networks Res. Centre, Helsinki Univ. of Technol., Espoo, Finland","Proceedings of the IEEE","6 Aug 2002","1996","84","10","1358","1384","The self-organizing map (SOM) method is a new, powerful software tool for the visualization of high-dimensional data. It converts complex, nonlinear statistical relationships between high-dimensional data into simple geometric relationships on a low-dimensional display. As it thereby compresses information while preserving the most important topological and metric relationships of the primary data elements on the display, it may also be thought to produce some kind of abstractions. The term self-organizing map signifies a class of mappings defined by error-theoretic considerations. In practice they result in certain unsupervised, competitive learning processes, computed by simple-looking SOM algorithms. Many industries have found the SOM-based software tools useful. The most important property of the SOM, orderliness of the input-output mapping, can be utilized for many tasks: reduction of the amount of training data, speeding up learning nonlinear interpolation and extrapolation, generalization, and effective compression of information for its transmission.","1558-2256","","10.1109/5.537105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537105","","Software tools;Displays;Power engineering and energy;Application software;Data visualization;Computer errors;Computer industry;Industrial relations;Training data;Interpolation","self-organising feature maps;unsupervised learning;data visualisation;data compression;interpolation;extrapolation;pattern recognition;engineering computing;robots","engineering applications;self-organizing map;software tool;data visualization;orderliness;pattern recognition;data compression;abstractions;unsupervised competitive learning;input-output mapping;interpolation;extrapolation;robotics","","586","8","197","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Towards a framework for program understanding","S. R. Tilley; S. Paul; D. B. Smith","Software Engineering Institute, Carnegie Mellon University, USA; Center for Software Engineering, IBM Thomas J. Watson Research Center; Software Engineering Institute, Carnegie Mellon University, USA","WPC '96. 4th Workshop on Program Comprehension","6 Aug 2002","1996","","","19","28","The paper describes an initial conceptual framework for the classification of reverse engineering tools and techniques that aid program understanding. It is based on a description of the canonical activities that are characteristic of the reverse engineering process. A descriptive model is presented that categorizes important support mechanism features based on a hierarchy of attributes.","1092-8138","0-8186-7283-8","10.1109/WPC.1996.501117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=501117","","Reverse engineering;Software engineering;Software maintenance;Programming profession;Cognition;Relational databases","reverse engineering;computer aided software engineering;cognitive systems","program understanding framework;reverse engineering tools;canonical activities;descriptive model;support mechanism features;attribute hierarchy","","41","2","23","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reverse engineering data requirements","M. A. Hoffman; D. L. Carver","Department of Computer Science, Louisiana State University, Baton Rouge, LA, USA; Department of Computer Science, Louisiana State University, Baton Rouge, LA, USA","1996 IEEE Aerospace Applications Conference. Proceedings","6 Aug 2002","1996","2","","269","277 vol.2","This paper describes some of the difficulties involved in migrating a flat file database accounting system to a relational platform. Included in the discussion are such factors as data reengineering, data collection processing and partial reengineering of legacy systems. We present a case study which was undertaken on a large scale flat file database system. We discuss possible solutions to the problems.","","0-7803-3196-6","10.1109/AERO.1996.495931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=495931","","Reverse engineering;Relational databases;Large-scale systems;Database systems;Software systems;Computer languages;Computer science;Computer aided software engineering;Costs;Software design","reverse engineering;software engineering","flat file database accounting;relational platform;data reengineering;data collection processing;partial reengineering;legacy systems;Datatrieve language;file structure;data collection","","2","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Two applications of parallel processing in power system computation","C. Lemaitre; B. Thomas","Research and Development Division, Electricité de France, Clamart, France; Research and Development Division, Electricité de France, Clamart, France","IEEE Transactions on Power Systems","6 Aug 2002","1996","11","1","246","253","This paper discusses performance improvements achieved in two power system software modules through the use of parallel processing techniques. The first software module, EVARISTE, outputs a voltage stability indicator for various power system situations. This module was designed for extended real-time use and is therefore required to give guaranteed response times. The second module, MEXICO, assesses power system reliability and operating costs by simulating a large number of contingencies for generation and transmission equipment. This module, used for power system planning purposes, uses a Monte-Carlo method to build the various system states, and makes heavy demands on CPU time for running simulations. Like many power system computation packages, both software modules are well-suited to coarse-grain parallel processing. The first module was parallelized on a distributed-memory machine and the second on a shared-memory machine. In this paper, we start by giving a description of the parallelization process used in these two cases, then go on to give details on the performance levels achieved, discussing aspects of programming, parameter selection (number of situations processed, number of processors), and machine characteristics (limitations due to interprocessor communications network, for instance).","1558-0679","","10.1109/59.486102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=486102","","Parallel processing;Power systems;Concurrent computing;Power system stability;Power system reliability;Power system simulation;Power system planning;Computational modeling;Application software;Software performance","power system analysis computing;power system stability;power system reliability;economics;power system planning;parallel processing;distributed memory systems;shared memory systems;Monte Carlo methods","parallel processing;power system computation;power system software modules;EVARISTE;voltage stability indicator;extended real-time use;MEXICO;power system reliability;power system operating costs;transmission equipment;generation equipment;Monte-Carlo method;power system planning;coarse-grain parallel processing;distributed-memory machine;shared-memory machine;parallelization process;programming;parameter selection","","12","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Two applications of parallel processing in power system computation","C. Lemaitre; B. Thomas","Research and Development Division, Electricitð de France, France; Research and Development Division, Electricitð de France, France","Proceedings of Power Industry Computer Applications Conference","6 Aug 2002","1995","","","62","69","This paper discusses performance improvements achieved in two power system software modules through the use of parallel processing techniques. The first software module, EVARISTE outputs a voltage stability indicator for various power system situations. This module was designed for extended real-time use and is therefore required to give guaranteed response times. The second module, MEXICO, assesses power system reliability and operating costs by simulating a large number of contingencies for generation and transmission equipment. This module, used for power system planning purposes, uses a Monte-Carlo method to build the various power system states, and makes heavy demands on CPU time for running simulations. Like many power system computation packages, both software modules are well-suited to coarse-grain parallel processing. The first module was parallelized on a machine capable of integrating up to thirty-two processors. The distributed-memory machine and the second on a shared-memory machine. In this paper, the authors start by a description of the n process used in these two cases, then go on to give details on the performance levels achieved, discussing aspects of programming, parameter selection (number of situations processed, number of processors), and machine characteristics (limitations due to interprocessor communications network, for instance).","","0-7803-2663-6","10.1109/PICA.1995.515166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=515166","","Parallel processing;Power systems;Concurrent computing;Power system reliability;Power system simulation;Power system stability;Power system planning;Computational modeling;Application software;Software performance","power system analysis computing;power system planning;power system stability;power system reliability;economics;digital simulation;parallel processing;real-time systems;Monte Carlo methods;software packages","power system computation;parallel processing;computer simulation;software modules;EVARISTE;MEXICO;voltage stability indicator;real-time;response times;power system reliability;power system operating costs;contingencies;generation;transmission;power system planning;Monte-Carlo method;CPU time;coarse-grain processing;distributed-memory machine;shared-memory machine;performance;parameter selection","","1","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Pattern matching for design concept localization","K. Kontogiannis; R. DeMori; M. Bernstein; M. Galler; E. Merlo","School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada; McGill University, Montreal, QC, CA; Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada","Proceedings of 2nd Working Conference on Reverse Engineering","6 Aug 2002","1995","","","96","103","The effective synergy of a number of different techniques is the key to the successful development of an efficient reverse engineering environment. Compiler technology, pattern matching techniques, visualization tools, and software repositories play an important role for the identification of procedural, data, and abstract-data-type related concepts in the source code. This paper describes a number of techniques used for the development of a distributed reverse engineering environments. Design recovery is investigated through code-to-code and abstract-descriptions-to-code pattern matching techniques used to locate code that may implement a particular plan or algorithm. The code-to-code matching uses dynamic programming techniques to locate similar code fragments and is targeted for large software systems (1MLOC). Patterns are specified either as source code or as a sequence of abstract statements written in an concept language developed for this purpose. Markov models are used to compute similarity measures between an abstract description and or code fragment in terms of the probability that a given abstract statement can generate a given code fragment. The abstract-description-to-code matcher is under implementation and early experiments show it is a promising technique.","","0-8186-711-43","10.1109/WCRE.1995.514698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=514698","","Pattern matching;Reverse engineering;Software systems;Computer science;Data visualization;Software tools;Algorithm design and analysis;Laboratories;Councils;Computer architecture","pattern matching;reverse engineering;program compilers;software libraries;programming environments;software tools;dynamic programming;Markov processes","pattern matching;design concept localization;reverse engineering environment;compiler;visualization tools;software repositories;abstract-data-type;distributed reverse engineering environment;design recovery;code-to-code matching;dynamic programming;large software systems;source code;abstract statements;Markov models;probability","","27","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"On finding duplication and near-duplication in large software systems","B. S. Baker","AT&T Bell Laboratories Engineering Research Center, Murray Hill, NJ, USA","Proceedings of 2nd Working Conference on Reverse Engineering","6 Aug 2002","1995","","","86","95","This paper describes how a program called dup can be used to locate instances of duplication or near-duplication in a software system. Dup reports both textually identical sections of code and sections that are the same textually except for systematic substitution of one set of variable names and constants for another. Further processing locates longer sections of code that are the same except for other small modifications. Experimental results from running dup on millions of lines from two large software systems show dup to be both effective at locating duplication and fast. Applications could include identifying sections of code that should be replaced by procedures, elimination of duplication during reengineering of the system, redocumentation to include references to copies, and debugging.","","0-8186-711-43","10.1109/WCRE.1995.514697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=514697","","Software systems;Programming profession;Application software;Computer bugs;Terminology;Reverse engineering;White spaces;Sections;Scattering parameters","systems re-engineering;systems analysis;software tools;system documentation;program debugging","software duplication;large software systems;dup;systematic substitution;variable names;constants;experimental results;system reengineering;redocumentation;debugging","","342","11","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Identifying reusable functions using specification driven program slicing: a case study","A. Cimitile; A. De Lucia; M. Munro","Department of Ingegneria dellE28099Informazione ed Ingegneria Elettrica, Faculty of Engineering, University of Salerno, Benevento, Italy; Department of Informatica e Sistemistica, University of Napoli Federico II, Naples, Italy; Centre for Software Maintenance, University of Durham, Durham, UK","Proceedings of International Conference on Software Maintenance","6 Aug 2002","1995","","","124","133","We present the results of a case study in identifying and isolating reusable functions from C programs. The work exploits and specializes to programs written in C the theoretical framework of specification driven program slicing, a new program slicing process for isolating code fragments implementing functional abstractions. The specification of the function to be isolated, given in terms of a precondition and a postcondition, is used to identify a suitable slicing criterion. The preconditions for the execution of program statements and predicates are abstracted by using symbolic execution and compared with the conditions of the specification. The statements whose preconditions are equivalent to the pre and postconditions of the functional abstraction are candidates for entry and exit points of the slice implementing the abstraction. Once the slicing criterion has been identified, the slice can be isolated using algorithms based on control flow graphs and dependence graphs.","1063-6773","0-8186-7141-6","10.1109/ICSM.1995.526534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526534","","Computer aided software engineering;Software systems;Software maintenance;Software reusability;Reverse engineering;Control systems;Entropy;Degradation;Hardware;Process design","software reusability;software maintenance;formal specification;symbol manipulation;program control structures;data flow analysis;program diagnostics","reusable function identification;specification driven program slicing;C programs;code fragment isolation;functional abstractions;precondition;postcondition;program statement execution;program predicate execution;symbolic execution;entry points;exit points;algorithms;control flow graphs;dependence graphs","","35","2","30","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"metriX: an approach for characterizing the performance potential of system architectures","R. Jenevein; N. Ullah","Department of Computer Science, University of Texas, Austin, Austin, TX, USA; Motorola RISC Microprocessor Division, Austin, TX, USA","Proceedings International Phoenix Conference on Computers and Communications","6 Aug 2002","1995","","","704","712","This paper discusses using precise measurements of customized code segments for characterizing system architecture performance. A methodology for obtaining precise computer system performance measurements is presented. These techniques allow one to make accurate measurements of code segments as trivial as one machine instruction or complex code sequences involving function calls and I/O. This methodology has been successfully implemented in a software tool called metriX. An accuracy with an error of less than 1% can be obtained for the measurements with a repeatability of 0.1% or better. This tool provides a platform for the experimental design of customized performance studies. We demonstrate how this tool cab be used to develop experiments that evaluate different architectural features and that characterize and quantify the architecture in terms of code performance. Results indicate the metriX's measurements are a very effective means for characterizing quantitatively the performance features of a system, for comparing architectural alternatives, and for judging the difference between two similar architectures.<>","","0-7803-2492-7","10.1109/PCCC.1995.472416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=472416","","Computer architecture;Microprocessors;Application software;System performance;Computer performance;Computer science;Software measurement;Software tools;Computer errors;Design for experiments","performance evaluation;software tools;multiprocessing systems","metriX;performance potential;system architectures;precise measurements;customized code segments;computer system performance measurements;code segments;machine instruction;complex code sequences;function calls;software tool;code performance","","","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Global scheduling with code-motions for high-level synthesis applications","Minjoong Rim; Yaw Fann; Rajiv Jain","Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WI, USA; Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WI, USA; Department of Electrical and Computer Engineering, University of Wisconsin, Madison, WI, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","6 Aug 2002","1995","3","3","379","392","In this paper, we present a global scheduling technique for synthesis applications. The algorithm accepts a specification containing conditional branches and while-loop constructs and schedules it for a given set of resources. The algorithm performs several types of code motions across different basic blocks and trades off cost with performance. Several real-life examples taken from Numerical Recipes in C are used to demonstrate the efficacy of the approach. The results indicate that code-motions are very important for achieving significant speed-ups for synthesis applications.<>","1557-9999","","10.1109/92.406996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=406996","","High level synthesis;Costs;Scheduling algorithm;VLIW;Hardware;Processor scheduling;Coprocessors;Digital systems;Resource management;Computer applications","scheduling;high level synthesis;VLSI;circuit CAD;integrated circuit design","global scheduling;code-motions;high-level synthesis applications;conditional branches;while-loop constructs","","22","1","57","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Investigating reverse engineering technologies for the CAS program understanding project","E. Buss; R. De Mori; W. M. Gentleman; J. Henshaw; H. Johnson; K. Kontogiannis; E. Merlo; H. A. Muller; J. Mylopoulos; S. Paul; A. Prakash; M. Stanley; S. R. Tilley; J. Troster; K. Wong","IBM Software Solutions Division, Toronto Laboratory, IBM Canada Limited, North York, ONT, Canada; School of Computer Science, McGill University, Quebec, Canada; Institutefor Information Technology, National Research Council Canada, Ottawa, ONT, Canada; IBM Software Solutions Division, Toronto Laboratory, IBM Canada Limited, North York, ONT, Canada; Institute for Information Technology, National Research Council Canada, Ottawa, ONT, Canada; School of Computer Science, McGill University, Quebec, Canada; Departement de Genie Electrique, Ecole Polytechnique, Quebec, Canada; Department of Computer Science, University of Victoria, P.O. Box 3055, BC V8W 3P6, Canada; Department of Computer Science, University of Toronto, Toronto, ONT, Canada; Software Systems Research Laboratory, Department of EECS, University of Michigan, Michigan; Software Systems Research Laboratory, Department of EECS, University of Michigan, Michigan; Department of Computer Science, University of Victoria, Victoria, BC, Canada; Department of Computer Science, University of Victoria, Victoria, BC, Canada; IBM Software Solutions Division, Toronto Laboratory, IBM Canada Limited, North York, ONT, Canada; Department of Computer Science, University of Victoria, Victoria, BC, Canada","IBM Systems Journal","6 Apr 2010","1994","33","3","477","500","Corporations face mounting maintenance and re-engineering costs for large legacy systems. Evolving over several years, these systems embody substantial corporate knowledge, including requirements, design decisions, and business rules. Such knowledge is difficult to recover after many years of operation, evolution, and personnel change. To address the problem of program understanding, software engineers are spending an ever-growing amount of effort on reverse engineering technologies. This paper describes the scope and results of an ongoing research project on program understanding undertaken by the IBM Toronto Software Solutions Laboratory Centre for Advanced Studies (CAS). The project involves a team from CAS and five research groups working cooperatively on complementary reverse engineering approaches. All the groups are using the source code of SQL/DS™ (a multimillion-line relational database system) as the reference legacy system. Also discussed is an approach adopted to integrate the various tools under a single reverse engineering environment.","0018-8670","","10.1147/sj.333.0477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387326","","","","","","36","","","","6 Apr 2010","","","IBM","IBM Journals"
"Extracting the parallelism in program with unstructured control statements","Fubo Zhang; E. H. D'Hollander","Department of Electrical Engineering, University of Ghent, Ghent, Belgium; Department of Electrical Engineering, University of Ghent, Ghent, Belgium","Proceedings of 1994 International Conference on Parallel and Distributed Systems","6 Aug 2002","1994","","","264","270","Program parallelization is inhibited by unstructured control statements such as GOTOs, causing interacting and overlapping execution trajectories. In this contribution, a program restructuring method is proposed to convert unstructured control statements into block if statements and while loops. Furthermore, an algorithm is presented to transform a common type of while loops into do loops. The technique works for while loops of which the control variables satisfy a linear recurrence relation. As a result, the loop carried dependencies generated by the control variables are removed. If there are no other loop carried dependencies, the do loop may then be converted into a doall loop. The algorithm has been used to test and convert a significant number of while loops into doall loops for a suite of well-known numerical benchmarks.","","0-8186-6555-6","10.1109/ICPADS.1994.590307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=590307","","Electric variables control;Ear;Benchmark testing;Algorithm design and analysis;Data mining;Flow graphs;Logic testing","parallel programming","parallelism in program;unstructured control statements;program parallelization;GOTOs;program restructuring method;while loops;do loops;linear recurrence relation","","2","","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Low-power software for low-power people","J. H. Snyder; J. B. McKie; B. N. Locanthi","AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA; AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA; AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA","Proceedings of 1994 IEEE Symposium on Low Power Electronics","6 Aug 2002","1994","","","32","35","We have been working towards portable terminals that interact with the user via speech synthesis, automatic speech recognition, and handwriting recognition. Our work includes the development of hardware platforms, operating system (OS) and application software; and the adaptation of signal processing algorithms to these systems. Although we will describe HoBo3, the hardware platform for this work, our primary interest will be in discussing software issues that are relevant to low-power portable computing. We compare a modern Unix implementation with the OS we use, ""Plan9 from Bell Labs"", and draw from this comparison a number of lessons relevant for the design of low-power software.","","0-7803-1953-2","10.1109/LPE.1994.573193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=573193","","Mobile computing;Signal processing algorithms;Energy consumption;Automatic speech recognition;Speech codecs;Speech synthesis;Handwriting recognition;Hardware;Operating systems;Portable computers","portable computers","portable terminals;speech synthesis;automatic speech recognition;handwriting recognition;hardware platforms;operating system;signal processing algorithms;HoBo3;low-power portable computing;software issues;Plan9;low-power software","","3","1","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Notes on the simulation of evolution","W. Atmar","AICS Research, Inc., University Park, NM, USA","IEEE Transactions on Neural Networks","6 Aug 2002","1994","5","1","130","147","The simulation of evolution for the purposes of parameter optimization has generally demonstrated itself to be a robust and rapid optimization technique. But there may exist more value in simulating evolution than simple parameter optimization. The optimizations of system behavior obtained through simulated evolution represent a potentially powerful autopoetic pathway to machine learning and self-organization. Indeed, it may eventually prove to be the only practical path to the development of ontogenetic machine intelligence. As the complexity of the systems being evolved increases, the development of a proper philosophy of analysis and design becomes imperative. The designer of evolutionary algorithms must keep clearly in mind what is being evolved and what evolves only by consequence. Notes on the simulation of evolution are offered in four sections: 1) the basic nature of evolution, 2) its practical simulation, 3) common philosophical errors, and 4) that which remains to be accomplished. Simulated evolutionary optimization is a mechanism of machine learning that can reasonably be expected to continue to grow in importance and practical benefit. As the availability of massively parallel processors increases, the value of simulated evolutionary techniques will become increasingly apparent, if for no other reason than the natural match between the technique and the emerging technology. The method has been repeatedly demonstrated to successfully find points of global optimality when other methods fail, often astonishingly quickly. Specific rules have become apparent and are easily stated.<>","1941-0093","","10.1109/72.265967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=265967","","Reservoirs;Machine learning;Discrete event simulation;Machine intelligence;Aggregates;Robustness;Stochastic processes;Phylogeny;Neurotransmitters","genetic algorithms;optimisation;learning (artificial intelligence);self-adjusting systems","evolution simulation;parameter optimization;autopoetic method;machine learning;self-organization;ontogenetic machine intelligence;massively parallel processors","","50","","59","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Targeting the Motorola RISC compiler for the PowerPC architecture","J. Shipnes","Motorola, Austin, TX, USA","Proceedings of COMPCON '94","6 Aug 2002","1994","","","326","331","Compiler technology has proven to be an integral part of overall performance in RISC systems. Motorola is providing highly optimizing C and FORTRAN compilers as part of the overall solution for the PowerPC and 88000 RISC architectures. This paper gives an introduction to the Motorola RISC compiler technology and describes how the technology was extended to support the PowerPC architecture in addition to the 88000.<>","","0-8186-5380-9","10.1109/CMPCON.1994.282892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=282892","","Reduced instruction set computing;Optimal scheduling;Microprocessors;Optimizing compilers;Debugging;Tree data structures;Assembly systems;Power generation;Computer languages;Availability","reduced instruction set computing;program compilers;FORTRAN;C language","Motorola RISC compiler;PowerPC architecture;C;FORTRAN","","","","4","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Experiences using reverse engineering techniques to analyse documentation","G. Ewart; M. Tomic","IBM Toronto Laboratory, Centre for Advanced Studies, Don Mills, ONT, Canada; IBM Toronto Laboratory, Centre for Advanced Studies, Don Mills, ONT, Canada","Proceedings 1994 IEEE 3rd Workshop on Program Comprehension- WPC '94","6 Aug 2002","1994","","","54","61","This paper discusses an approach taken to analyse IBM product documentation using reverse engineering technologies, which are normally applied to the analysis of system source codes.<>","1092-8138","0-8186-5647-6","10.1109/WPC.1994.341250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=341250","","Reverse engineering;Documentation;Laboratories;Trademarks;Milling machines;Software maintenance;Software systems;Content addressable storage;Paper technology;Life estimation","reverse engineering;system documentation;software maintenance","reverse engineering;documentation;IBM product documentation;system source codes;information structure;Abstract Syntax Tree","","","","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Localization of design concepts in legacy systems","Kontogiannis; DiMori; Bernstein; Merlo","McGill University, Montreal, QC, CA; McGill Univ., Montreal, Que., Canada; McGill University, Montreal, Canada; McGill University, Montreal, Canada","Proceedings 1994 International Conference on Software Maintenance","6 Aug 2002","1994","","","414","423","Complete automation of design recovery of large systems is a desirable but impractical goal due to complexity and size issues, so current research efforts focus on redocumentation and partial design recovery. Pattern matching lies at the center of any design recovery system. In the context of a larger project to develop an integrated reverse engineering environment, we are developing a framework for performing clone detection, code localization, and plan recognition. This paper discusses a plan localization and selection strategy based on a dynamic programming function that records the matching process and identifies parts of the plan and code fragment that are most ""similar"". Program features used for matching are currently based on data flow, control flow, and structural properties. The matching model uses a transition network and allows for the detection of insertions and deletions, and it is targeted for legacy C-based systems.<>","","0-8186-6330-8","10.1109/ICSM.1994.336753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=336753","","Dynamic programming;Software maintenance;Software fault diagnosis;Software design/development;Documentation;Pattern matching;Computer-aided software engineering","dynamic programming;software maintenance;program diagnostics;system documentation;software tools","design concepts;legacy systems;design recovery;large systems;redocumentation;partial design recovery;pattern matching;integrated reverse engineering environment;clone detection;code localization;plan recognition;dynamic programming;data flow;control flow;structural properties;transition network;insertions;deletions;legacy C-based systems","","9","","22","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Substring matching for clone detection and change tracking","Johnson","Software Engineering Laboratory, National Research Council Canada, Ottawa, Canada","Proceedings 1994 International Conference on Software Maintenance","6 Aug 2002","1994","","","120","126","Legacy systems pose problems to maintainers that can be solved partially with effective tools. A prototype tool for determining collections of files sharing a large amount of text has been developed and applied to a 40 megabyte source tree containing two releases of the gcc compiler. Similarities in source code and documentation corresponding to software cloning, movement and inertia between releases, as well as the effects of preprocessing easily stand out in a way that immediately conveys nonobvious structural information to a maintainer taking responsibility for such a system.<>","","0-8186-6330-8","10.1109/ICSM.1994.336783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=336783","","Software maintenance;Computer-aided software engineering;Compilers;Software fault diagnosis","software maintenance;software tools;program compilers;program diagnostics;configuration management","clone detection;change tracking;legacy systems;prototype tool;source tree;gcc compiler;source code;documentation;software cloning;structural information;reverse engineering;design recovery;program understanding","","89","2","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Specifications from source code-alchemists' dream or practical reality?","M. P. Ward","Research Fellow in Computer Science, University of Durham, UK","IEE Colloquium on Reverse Engineering for Software Based Systems","6 Aug 2002","1994","","","5/1","5/3","The author describes the application of formal program transformations to uncover specifications from source code. He takes as an example a small report writing program. Due to errors in the original design, this program had several bugs which were gradually uncovered and fixed in the usual way. The resulting program appears to work, but has a complex and messy structure which makes it extremely difficult to maintain. The aim of the case study is to restructure the program and extract its specification (a concise, high-level description of what the program does which ignores the low-level details of how this result is achieved). It should be noted that his aim is emphatically not that of design recovery in the sense of recovering the original design. Instead he aims to transform the base metal of unstructured code into the gold of a high-level specification. The approach is based on a Wide Spectrum Language (called WSL) which includes both high level abstract specifications and low-level programming constructs within the same language.<>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=385768","","Software requirements and specifications;Reverse engineering;Specification languages","formal specification;reverse engineering;specification languages;systems re-engineering","source code;specifications;formal program transformations;report writing program;program bugs;case study;high-level description;design recovery;unstructured code;high-level specification;Wide Spectrum Language;high level abstract specifications;low-level programming constructs;inverse engineering","","4","","","","6 Aug 2002","","","IET","IET Conferences"
"Measuring limits of parallelism and characterizing its vulnerability to resource constraints","L. Rauchwerger; P. K. Dubey; R. Nair","Computer Sciences Department, University of Wisconsin, Madison, Madison, WI, USA; Dipartimento Elettronico, Elettronico e Sistemistico, Universita di Catania, Catania, Italy; Dipartimento Elettronico, Elettronico e Sistemistico, Universita di Catania, Catania, Italy","Proceedings of the 26th Annual International Symposium on Microarchitecture","6 Aug 2002","1993","","","105","117","Introduces a technique to enhance the ability of dynamic ILP processors to exploit (speculatively executed) parallelism. Existing branch prediction mechanisms used to establish a dynamic window from which ILP can be extracted are limited in their abilities to: (i) create a large, accurate dynamic window, (ii) initiate a large number of instructions into this window in every cycle, and (iii) traverse multiple branches of the control flow graph per prediction. The authors introduce control flow prediction which uses information in the control flow graph of a program to overcome these limitations. They discuss how information present in the control flow graph can be represented using multiblocks, and conveyed to the hardware using Control Flow Tables and Control Flow Prediction Buffers. They evaluate the potential of control flow prediction on an abstract machine and on a dynamic ILP processing model. The results indicate that control flow prediction is a powerful and effective assist to the hardware in making more informed run time decisions about program control flow.<>","","0-8186-5280-2","10.1109/MICRO.1993.282747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=282747","","Hardware;Flow graphs;Concurrent computing;Data mining;Processor scheduling;Dynamic scheduling;Parallel processing;Predictive models;Runtime;Process control","operating systems (computers);parallel processing","dynamic ILP processors;parallelism;control flow prediction;control flow graph;Control Flow Tables;Control Flow Prediction Buffers;program control flow","","11","6","21","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Employing finite automata for resource scheduling","T. Müller","Departmentof Surgery, Uniformed Services University of Health Sciences, Bethesda, MD, USA","Proceedings of the 26th Annual International Symposium on Microarchitecture","6 Aug 2002","1993","","","12","20","The Motorola 88110 is an advanced superscalar design. The processor can issue up to two instructions per cycle among ten functional units, and it includes sophisticated load-store, speculative execution, exception recovery, and branch target buffer facilities. This paper examines several computationally inexpensive instruction scheduling strategies for a post-processor code optimizer for the 88110, including basic block scheduling using reservation tables for writeback buses as well as functional units, delayed branch removal, loop alignment, and special loop entry scheduling. For a set of 32 loop-intensive benchmarks, a combination of delayed branch removal and loop alignment yields the best code improvement.<>","","0-8186-5280-2","10.1109/MICRO.1993.282737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=282737","","Processor scheduling;Delay;Decoding;Reduced instruction set computing;Buffer storage;Computer science;Computer aided instruction;Performance gain;Databases;Springs","microprocessor chips;reduced instruction set computing","instruction scheduling;Motorola 88110;superscalar design;exception recovery;branch target buffer;reservation tables;delayed branch removal;loop alignment;RISC","","11","27","18","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A reuse program in a maintenance environment","A. Nishimoto","Hewlett Packard Company, USA","Digest of Papers. Compcon Spring","6 Aug 2002","1993","","","268","273","The author describes a program of reuse that began in January 1984 with a new product and has evolved to include three other existing products and all new enhancements to these products over the last eight years. The topics covered include: how the program was initiated and how the definition of reuse has changed over time; how the program evolved to include other products; development of the fix process for reuse components; coordination issues between products and the formation of the Shared Components Council; results of reuse metrics; what lessons were learned; and suggested areas for improvement. The emphasis is on what was learned along the way and how the reuse program has evolved to include other products.<>","","0-8186-3400-6","10.1109/CMPCON.1993.289678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=289678","","Software maintenance;Production facilities;Councils;Manufacturing processes;Productivity;Financial management;Inventory management;Technology management;Software tools;Software systems","software maintenance;software metrics;software reusability","reuse program;maintenance environment;coordination issues;Shared Components Council;reuse metrics","","","","1","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automated testing support for a robot tape library","A. von Mayrhauser; S. Crawford-Hines","Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Department of Computer Science, Colorado State University, Fort Collins, CO, USA","Proceedings of 1993 IEEE International Symposium on Software Reliability Engineering","6 Aug 2002","1993","","","6","14","This paper presents a method and tool to automate testing of command-based systems. We use syntactic and semantic information at the parameter, command, and script level. Test data generation is based on a domain model of the application. We applied method and tool to automated testing for a robot tape library.","","0-8186-4010-3","10.1109/ISSRE.1993.624269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=624269","","Automatic testing;Robotics and automation;Libraries;Software testing;Command languages;Computer science;System testing;Application software;Robots;Information analysis","robots","testing tool;syntactic information;command level;parameter level;test data generation;command-based systems;semantic information;script level;domain model;automated testing;robot tape library","","12","","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Speculative execution and reducing branch penalty in a parallel issue machine","H. Ando; C. Nakanishi; H. Machida; T. Hara; S. Kishida; M. Nakaya","LSI Laboratory, Mitsubishi Electric Corporation Limited, Japan; LSI Laboratory, Mitsubishi Electric Corporation Limited, Japan; LSI Laboratory, Mitsubishi Electric Corporation Limited, Japan; LSI Laboratory, Mitsubishi Electric Corporation Limited, Japan; ASIC Design Engineering Center, Mitsubishi Electric Corporation Limited, Japan; LSI Laboratory, Mitsubishi Electric Corporation Limited, Japan","Proceedings of 1993 IEEE International Conference on Computer Design ICCD'93","6 Aug 2002","1993","","","106","113","Parallel instruction issue is essential for performance improvement in current microprocessor designs. Just extra function units are, however, little beneficial in non-numerical applications since control dependence severely limits exploitation of instruction-level parallelism (ILP) and frequent branches consume ILP due to its long latency. Boosting is an interesting technique to reduce control dependence. It allows general speculative execution with little cycle time penalty. From the cost/performance point of view, we propose the efficient implementation of boosting, which requires the small support hardware and maximizes performance gain from boosting in the limited hardware. We also propose a new branch scheme to reduce the branch penalty which has a particularly big performance impact in a parallel issue machine. Our scheme fetches from both directions of the branch with small hardware cost through integration of a code movement and hardware support. We evaluate our schemes and find that they significantly contribute to performance improvement.<>","","0-8186-4230-0","10.1109/ICCD.1993.393396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=393396","","Hardware;Boosting;Microprocessors;Scheduling;Delay effects;Large scale integration;Laboratories;Application specific integrated circuits;Design engineering;Performance gain","parallel programming;parallel architectures","parallel issue machine;instruction-level parallelism;ILP;cost/performance;boosting;performance gain;branch scheme;branch penalty;performance improvement","","2","1","22","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Performance Optimization of Pipelined Primary Caches","K. Olukotun; T. Mudge; R. Brown","Computer Systems Laboratory, University of Stanford, CA, USA; Department Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA","[1992] Proceedings the 19th Annual International Symposium on Computer Architecture","6 Aug 2002","1992","","","181","190","The CPU cycle time of a high-performance processor is usually determined by the the access time of the primary cache. As processor speeds increase, designers will have to increase the number of pipeline stages used to fetch data from the cache in order to reduce the dependence of CPU cycle time on cache access time. This paper studies the performance advantages of a pipelined cache for a GaAs implementation of the MIPS based architecture using a design methodology that includes long traces of multiprogrammed applications and detailed timing analysis, The study evaluates instruction and data caches with various pipeline depths, cache sizes, block sizes, and refill penalties. The impact on CPU cycle time of these alternatives is also factored into the evaluation. Hardware-based and software-based strategies are considered for hiding the branch and load delays which may be required to avoid pipeline hazards. The results show that software-based methods for mitigating the penalty of branch delays can be as successful as the hardware-based branch-target buffer approach, despite the code-expansion inherent in the software methods. The situation is similar for load delays; while hardware-based dynamic methods hide more delay cycles than do static approaches, they may give up the advantage by extending the cycle time. Because these methods are quite successful at hiding small numbers of branch and load delays, and because processors with pipelined caches also have shorter CPU cycle times and larger caches, a significant performance advantage is gained by using two to three pipeline stages to fetch data from the cache.","","0-89791-509-7","10.1109/ISCA.1992.753315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=753315","","Optimization;Delay effects;Permission;Costs;Pipeline processing;Laboratories;Computer science;Gallium arsenide;Design methodology;Application software","","","","9","","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Structural and behavioral code representation for program understanding","E. Merlo; K. Kontogiannis; J. F. Girard","Centre de Recherche informatique de Montreal, Inc., Montreal, Canada; Centre de Recherche informatique de Montreal, Inc., Montreal, Canada; Centre de Recherche informatique de Montreal, Inc., Montreal, Canada","[1992] Proceedings of the Fifth International Workshop on Computer-Aided Software Engineering","6 Aug 2002","1992","","","106","108","Methodologies which could assist the software maintainer are reported, with emphasis on an approach which combines structural and behavioral representation of the code. Structural representation occurs at lower levels of abstraction and uses compiler technology techniques, graph parsing, abstract syntax trees, and control and data flow. Behavioral representation can be achieved at higher levels of abstraction by using some formal representation of source code semantics, such as process algebra, lambda calculus, or denotational semantics. The complexity, concurrency, and interaction levels of the system are good indicators of the best formalism to be chosen. Artificial intelligence techniques can be used to define semantic distances between different behavioral representation plans in order to achieve a full or partial match of the source code to the underlying specifications.<>","","0-8186-2960-6","10.1109/CASE.1992.200136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=200136","","Tree graphs;Flow graphs;Performance analysis;Algebra;Calculus;Concurrent computing;Artificial intelligence;Hardware;Maintenance engineering;Data engineering","computational complexity;grammars;program compilers;software maintenance;tree data structures","structural code representation;artificial intelligence;behavioral code representation;program understanding;software maintainer;behavioral representation;compiler;graph parsing;abstract syntax trees;formal representation;source code semantics;process algebra;lambda calculus;denotational semantics;complexity;concurrency;interaction levels","","1","2","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Optimized CM Fortran compiler for the Connection Machine computer","G. Sabot","Thinking Machines Corporation, Cambridge, MA, USA","Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences","6 Aug 2002","1992","ii","","161","172 vol.2","Describes the techniques that are used in the CM Fortran 1.0 compiler to map the fine-grained array parallelism of Fortran 90 onto the CM-2 architecture. The compiler views the parallel hardware at a much lower level of detail than did previous CM-2 compilers, which had targeted a function library named Paris. In the slicewise machine model used by CM Fortran 1.0, the FPUs, their registers, and the memory hierarchy are directly exposed to the compiler. Thus, the CM-2 target machine is not 64K simple bit-serial processors. Rather, the target is a machine containing 2K PEs (processing elements), where each PE is both superpipelined and superscalar. The compiler uses data distribution to spread the problem out among the 2K processors. A new compiler phase is used to separate the code that runs on the two types of processors in the CM: the parallel PEs, which execute a new RISC-like instruction set called PEAC, and the scalar front-end processor, which executes SPARC or VAX assembler code. The pipelines in PEs are filled by using conventional vector processing techniques along with a new, RISC-like vector instruction set. An innovative scheduler overlaps the execution of a number of RISC operations. This new compiler has greatly increased the performance of Fortran codes on the CM-2 on many important computation kernels, such as climate modeling, seismic processing, and hydrodynamics simulations.<>","","0-8186-2420-5","10.1109/HICSS.1992.183289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=183289","","Optimizing compilers;Parallel processing;Computer architecture;Hardware;Libraries;Registers;Assembly;Pipelines;Processor scheduling;Reduced instruction set computing","FORTRAN;instruction sets;optimisation;parallel languages;pipeline processing;program compilers","compiler optimization;floating point units;superpipelined superscalar processing elements;overlapped execution;Connection Machine;CM Fortran 1.0 compiler;fine-grained array parallelism;Fortran 90;CM-2 architecture;slicewise machine model;registers;memory hierarchy;data distribution;RISC-like instruction set;PEAC;scalar front-end processor;SPARC;VAX assembler code;vector processing;scheduler;performance;computation kernels;climate modeling;seismic processing;hydrodynamics simulations","","3","5","17","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A compiler for a massively parallel distributed memory MIMD computer","G. Sabot","Thinking Machines Corporation, Cambridge, MA, USA","[Proceedings 1992] The Fourth Symposium on the Frontiers of Massively Parallel Computation","6 Aug 2002","1992","","","12","20","The author describes the techniques that are used by the CM Compiler Engine to map the fine-grained array parallelism of languages such as Fortan 90 and C onto the Connection Machine (CM) architectures. The same compiler is used for node-level programming of the CM-5, for global programming of the CM-5, and for global programming of the SIMD (single-instruction multiple-data) CM-2. A new compiler phase is used to generate two classes of output code: code for a scalar control processor, which executes SPARC assembler, and code aimed at a model of the CM-5's parallel-processing elements. The model is embodied in a new RISC (reduced instruction set computer)-like vector instruction set called PEAC. The control program distributes parallel data at runtime among the processor nodes of the target machine. Each of these nodes is itself superpipelined and superscalar. An innovative scheduler overlaps the execution of multiple PEAC operations, while conventional vector processing techniques keep the pipelines filled.<>","","0-8186-2772-7","10.1109/FMPC.1992.234910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=234910","","Engines;Parallel processing;Program processors;Process control;Assembly;Reduced instruction set computing;Computer aided instruction;Runtime;Processor scheduling;Pipelines","distributed memory systems;instruction sets;parallel processing;program compilers","RISC-like vector instruction set;PEAC;compiler;massively parallel distributed memory MIMD computer;CM Compiler Engine;fine-grained array parallelism;Fortan 90;C;node-level programming;global programming;SIMD;scalar control processor;SPARC assembler;parallel-processing elements;scheduler","","7","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A control-flow normalization algorithm and its complexity","Z. Ammarguellat","Center for Supercomputing Research and Development, University of Illinois, Urbana-Champaign, IL, USA","IEEE Transactions on Software Engineering","6 Aug 2002","1992","18","3","237","251","A single method for normalizing the control-flow of programs to facilitate program transformations, program analysis, and automatic parallelization is presented. While previous methods result in programs whose control flowgraphs are reducible, programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods. In particular, all control-flow cycles are normalized into single-entry, single-exit while loops and all GOTOs are eliminated. Furthermore, the method avoids problems of code replication that are characteristic of node-splitting techniques. This restructuring obviates the control dependence graph, since afterwards control dependence relations are manifest in the syntax tree of the program. Transformations that effect this normalization are presented, and the complexity of the method is studied.<>","1939-3520","","10.1109/32.126773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=126773","","Automatic control;Data analysis;Tree graphs;Performance analysis;Algorithm design and analysis;Pathology;Program processors","computational complexity;graph theory;parallel algorithms;structured programming","control-flow normalization algorithm;complexity;automatic parallelization;control flowgraphs;control-flow cycles;GOTOs;node-splitting techniques;control dependence relations;syntax tree","","44","2","48","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Understanding natural programs using proper decomposition","J. Hartman","Department of Computer Sciences, University of Texas, Austin, Austin, TX, USA","[1991 Proceedings] 13th International Conference on Software Engineering","6 Aug 2002","1991","","","62","73","The author presents a practical method for automatic control concept recognition in large, unstructured imperative programs. Control concepts are abstract notions about interactions between control flow, data flow, and computation, e.g., read-process loops. They are recognized by comparing a language-independent abstract program representation against standard implementation plans. Recognition is efficient and scalable because the program representation is hierarchically decomposed by propers (single entry/exit control flow subgraphs). A recognition experiment using the UNPROG program understander shows the method's performance, the role of proper decomposition, and the ability to use standard implementations in a sample of programs. How recognized control concepts are used to perform Cobol restructuring with quality not possible with existing syntactic methods is described.<>","","0-8186-2140-0","10.1109/ICSE.1991.130624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130624","","Automatic control;Data flow computing;Programming profession;Robustness;Joining processes;Testing;Production","programming;software engineering","natural programs;proper decomposition;automatic control concept recognition;unstructured imperative programs;control flow;data flow;computation;read-process loops;language-independent abstract program representation;propers;single entry/exit control flow subgraphs;UNPROG program understander;Cobol restructuring","","37","1","40","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic generation of compiled simulations through program specialization","Wing Yee Au; D. Weise; S. Seligman","Computer Systems Laboratory, University of Stanford, Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, Stanford, CA, USA","28th ACM/IEEE Design Automation Conference","6 Aug 2002","1991","","","205","210","","","0-89791-395-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=979714","","Circuit simulation;Computational modeling;Data structures;Timing;Contracts;Analytical models;Permission;Assembly;Gold;Laboratories","","","","","","12","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Region scheduling: an approach for detecting and redistributing parallelism","R. Gupta; M. L. Soffa","Philips Laboratories, Inc., Briarcliff Manor, NY, USA; Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","6 Aug 2002","1990","16","4","421","431","Region scheduling, a technique applicable to both fine-grain and coarse-grain parallelism, uses a program representation that divides a program into regions consisting of source and intermediate level statements and permits the expression of both data and control dependencies. Guided by estimates of the parallelism present in regions, the region scheduler redistributes code, thus providing opportunities for parallelism in those regions containing insufficient parallelism compared to the capabilities of the executing architecture. The program representation and the transformations are applicable to both structured and unstructured programs, making region scheduling useful for a wide range of applications. The results of experiments conducted using the technique in the generation of code for a reconfigurable long instruction word architecture are presented. The advantages of region scheduling over trace scheduling are discussed.<>","1939-3520","","10.1109/32.54294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=54294","","Program processors;Processor scheduling","parallel programming;program compilers;scheduling","region scheduling;code generation;redistributing parallelism;fine-grain;coarse-grain parallelism;program representation;reconfigurable long instruction word architecture;trace scheduling","","80","2","16","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Compiler-assisted synthesis of algorithm-based checking in multiprocessors","V. Balasubramanian; P. Banerjee","Department of Electrical Engineering and the Coordinated Science Laboratory, University of Illinois, Urbana-Champaign, Urbana-Champaign, IL, USA; Department of Electrical Engineering and the Coordinated Science Laboratory, University of Illinois, Urbana-Champaign, Urbana-Champaign, IL, USA","IEEE Transactions on Computers","6 Aug 2002","1990","39","4","436","446","The task of synthesizing algorithm-based checking techniques for general applications is investigated. The problem is approached at the compiler level by identifying linear transformations in Fortran DO loops and restructuring program statements to convert nonlinear transformations to linear ones. System-level checks based on this property are proposed. The approach is demonstrated with example problems of matrix multiplication and the LINPACK routine: DGEFA.<>","1557-9956","","10.1109/12.54837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=54837","","Fault detection;Message passing;Hypercubes;Matrix decomposition;Costs;Fast Fourier transforms;Program processors;Matrix converters;Encoding;Parallel processing","concurrency control;fault tolerant computing;multiprocessing systems","compiler assisted synthesis;algorithm-based checking;multiprocessors;linear transformations;Fortran DO loops;nonlinear transformations;matrix multiplication;LINPACK routine;DGEFA","","30","4","27","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Using a lookahead window in a compaction-based parallelizing compiler","T. Nakatani; K. Ebcioglu","IBM Tokyo Research Laboratory, Tokyo, Japan; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","[1990] Proceedings of the 23rd Annual Workshop and Symposium@m_MICRO 23: Microprogramming and Microarchitecture","6 Aug 2002","1990","","","57","68","Lookahead is a common technique for high performance uniprocessor design. In general, however, hardware lookahead window is too small to exploit instruction-level parallelism at run time, while compaction-based parallelizing compilers must suffer from worst-case exponential code explosion at compile time. In this paper, the authors propose a software lookahead method, which allows inter-basic block code motions within the prespecified number of operations, called software lookahead window, on any path emanating from the currently processed instruction at compile time. By software lookahead, instruction-level parallelism can be exploited in a much greater code area than the hardware approach, but the lookahead region is still limited to a constant depth with a user-specifiable window, and thus code explosion is restricted. The proposed scheme has been implemented in the authors prototype parallelizing compiler, which can generate code for uniprocessors with multiple functional units and multiway conditional branches, such as VLIW machines, and potentially for super-scalars as well.<>","","0-8186-2124-9","10.1109/MICRO.1990.151427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=151427","","Explosions;Hardware;Block codes;Software prototyping;VLIW;Laboratories;Magnetic heads;Prototypes;Software performance;Prefetching","instruction sets;parallel architectures;program compilers","lookahead window;compaction-based parallelizing compiler;high performance uniprocessor design;instruction-level parallelism;inter-basic block code motions;instruction-level parallelism;multiple functional units;multiway conditional branches;VLIW machines","","16","4","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"New directions in database management systems","C. Beeri","Hebrew University of Jerusalem, Israel","Proceedings of the 5th Jerusalem Conference on Information Technology, 1990. 'Next Decade in Information Technology'","6 Aug 2002","1990","","","500","506","The important concepts concerning object-oriented databases (OODBs) are reviewed. The shortcomings of the relational model in supporting nontraditional applications are outlined. OO features are explained and how they can be used to overcome the above mentioned shortcomings is indicated. The issue of developing a declarative query language for OODBs is briefly discussed. Some practical and theoretical difficulties are addressed.<>","","0-8186-2078-1","10.1109/JCIT.1990.128322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=128322","","Database systems;Object oriented databases;Object oriented modeling;Relational databases;Knowledge management;Data models;User interfaces;Deductive databases;Logic programming;Expert systems","object-oriented databases;query languages","SQL;database management systems;object-oriented databases;relational model;nontraditional applications;declarative query language","","","","14","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Achieving software reuse by conversion and reorganization of software systems","J. N. Sturman","Corporate Research & Development Laboratories, General Electric Company Limited, Schenectady, NY, USA","IEEE Conference on Aerospace and Electronics","6 Aug 2002","1990","","","606","612 vol.2","The author describes advancements made toward a total system for HOL (higher-order language) software conversion which utilizes techniques for applying transformations on software which preserve functionality and improve code readability, understandability, and organization. The research goal is to establish technique for achieving the reuse of software written in one HOL (generally a somewhat arcane one such as Fortran) by translation to a more contemporary HOL (Such as Ada). The objectives of this effort are to achieve functionally equivalent transformation of the code, while preserving and enhancing the clarity of the process which the code represents. Significant aspects of this work include the design of an underlying generic language support system, the provision for human interaction in the code restructuring process via a graphical interface, the development of a text editor which permits mapping between code segments of translated Ada and input Fortran, the development of a data structure analyzer used to transform Ada declarations, and an Ada repackager.<>","","","10.1109/NAECON.1990.112835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=112835","","Software systems;Laboratories;Computer languages;Programming profession;Research and development;Computer displays;Communication system control;Humans;Data structures;Data analysis","Ada;high level languages;software reusability","structured programs;structured transformation;software reuse;reorganization;higher-order language;software conversion;transformations;code readability;understandability;Fortran;graphical interface;text editor;mapping;code segments;data structure analyzer;Ada declarations;Ada repackager","","","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"ROPCO: an environment for micro-incremental reuse","M. Kazerooni-Zand; M. H. Samadzadeh; K. M. George","Department of Computer Science, Langston University, Langston, OK, USA; Department of Computing Information Science, Oklahoma State University, Stillwater, OK, USA; Department of Computing Information Science, Oklahoma State University, Stillwater, OK, USA","Ninth Annual International Phoenix Conference on Computers and Communications. 1990 Conference Proceedings","6 Aug 2002","1990","","","347","354","The authors report on the design of ROPCO (reuse of persistent code and object code), a novel approach to reuse utilizing code and object code stored in persistent structures. One of the main motives in the design of ROPCO is to eliminate the compilation of portions of a program which have not been altered, while the consistency of variables and intra program flow graphs remains intact. Basic blocks are selected as units of reuse, and the notion of a 'use network' is devised to accomplish this goal. Another design motive is to store versions of blocks of a program efficiently, while maintaining direct and sequential access to those blocks. Hierarchical and flat persistency methods are utilized to achieve this goal.<>","","0-8186-2030-7","10.1109/PCCC.1990.101641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101641","","Software systems;Programming profession;Productivity;Information science;Flow graphs;Preforms;Program processors;Software packages;Packaging;Pressing","programming environments;software reusability","direct access;micro-incremental reuse;ROPCO;reuse of persistent code and object code;persistent structures;intra program flow graphs;use network;sequential access;flat persistency methods","","","","31","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Using objects to communicate legal information","E. P. Richards; B. B. Bounds","National Center for Preventive Law, University of Denver; Computer Science, University of Benver","Ninth Annual International Phoenix Conference on Computers and Communications. 1990 Conference Proceedings","6 Aug 2002","1990","","","766","771","The problem of developing a formal knowledge representation system for communicating information about legal events is considered. This system is to be based on the use of the object metaphor embodied in Smalltalk-80. Once a knowledge representation system has been developed, it will be iteratively refined to better approximate the real-world issues in communicating legal information. A litigation event database, which is the necessary precursor to building and validating a litigation event knowledge representation schema, has been developed.<>","","0-8186-2030-7","10.1109/PCCC.1990.101697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=101697","","Law;Legal factors;Knowledge representation;Medical diagnostic imaging;Prototypes;Personnel;Evolution (biology);Diseases;History;Educational institutions","knowledge representation;law administration","legal information;formal knowledge representation system;object metaphor;Smalltalk-80;litigation event database","","","1","32","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A unified approach to building accelerator simulation software for the SSC","V. Paxson; C. Aragon; S. Peggs; C. Saltmarsh; L. Schachinger","SSC Central Design Group, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; SSC Central Design Group, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; SSC Central Design Group, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; SSC Central Design Group, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; SSC Central Design Group, Lawrence Berkeley National Laboratory, Berkeley, CA, USA","Proceedings of the 1989 IEEE Particle Accelerator Conference, . 'Accelerator Science and Technology","6 Aug 2002","1989","","","82","84 vol.1","A large body of software is being developed to simulate the physics and high-level operation of the Superconducting Super Collider (SSC). The state of the SSC simulation software is described, with emphasis on addressing the uniform interface needs by using a standardized data set format and object-oriented approaches to graphics and modeling. It is noted that if this software is to form a unified and flexible whole, it is necessary to solve a number of software engineering problems that will otherwise render the system so bulky as to become effectively useless. To this end, the authors envision (1) the self-describing data standard as providing a software bus on which programs can be plugged in to talk with one another and data transparently moved across heterogeneous networks; (2) a library of InterViews-based graphics classes to provide a way to create new interfaces rapidly and to ensure uniformity across all interfaces; and (3) a library of model/machine classes to make it possible to generalize the simulation to a variety of models of the accelerator and, ultimately, to the actual machine.<>","","","10.1109/PAC.1989.72991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=72991","","Object oriented modeling;Physics;Graphics;Libraries;Buildings;Laboratories;Centralized control;Software systems;Error correction;Hardware","computer graphics;physics computing;software engineering","simulation software;SSC;Superconducting Super Collider;interface;graphics;modeling;software engineering;self-describing data standard;software bus;heterogeneous networks;InterViews-based graphics classes;model/machine classes","","3","1","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Data reengineering for application systems","J. A. Ricketts; J. C. DelMonaco; M. W. Weeks","Peat Marwick Advanced Technology, Chicago, IL, USA; Peat Marwick Advanced Technology, Chicago, IL, USA; Peat Marwick Advanced Technology, Chicago, IL, USA","Proceedings. Conference on Software Maintenance - 1989","6 Aug 2002","1989","","","174","179","Data reengineering methods and tools are discussed, with an emphasis on data definition engineering. Data reengineering extends the life of existing systems by standardizing data definitions and facilitating source code simplification. It can also provide an accurate data model for use as a starting point in data modeling and database technology migration and as a preparation step for reverse engineering. Data definition and data value problems are discussed. Data reengineering is compared with data engineering, highlighting the ways in which they differ. The following data reengineering phases are described: code analysis, data analysis, metadata synthesis, redesign, revision and definition exportation. Three data definition reengineering cases are discussed.<>","","0-8186-1965-1","10.1109/ICSM.1989.65207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=65207","","Application software;Computer aided software engineering;Automatic control;Software tools;Data models;Databases;Reverse engineering;Information systems;Software packages;Packaging","software reliability;software reusability","application systems;data definition engineering;source code simplification;accurate data model;database technology migration;preparation step;reverse engineering;data value problems;data engineering;data reengineering phases;code analysis;data analysis;metadata synthesis;redesign;revision;definition exportation;data definition reengineering cases","","7","3","14","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"How to build a hardware description and measurement system on an object-oriented programming language","W. H. Wolf","AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","6 Aug 2002","1989","8","3","288","301","Techniques are described for applying the mechanisms of object-oriented programming languages to hardware description. Some object-oriented language mechanisms, like inheritance, directly simplify CAD (computer-aided design) programs; others, like data abstraction, allow more powerful CAD mechanisms based on them to be created. The author describes: how to extend class inheritance and to integrate it with procedural construction to simplify the description of hardware; how to create measurement methods than can measure a module whose components are described at different levels of abstraction; and how to implement a consistency-maintenance engine that ensures the consistency of the data kept for the design. The author has implemented these features in Fred, an object-oriented modeling system for VLSI modules. Fred is implemented in Flavors, an object-oriented extension of Lisp. The author also discusses how to implement its features in other languages.<>","1937-4151","","10.1109/43.21848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=21848","","Hardware;Object oriented programming;Design automation;Data structures;Object oriented modeling;Modular construction;Engines;Very large scale integration;Runtime;Programming profession","CAD;LISP;object-oriented programming;specification languages","hardware description;object-oriented programming language;inheritance;CAD;data abstraction;procedural construction;consistency-maintenance engine;Fred;Flavors","","22","11","39","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Designing SAA applications and user interfaces","W. P. Dunfee; J. D. McGehe; R. C. Rauf; K. O. Shipp","IBM System Products Division, White Plains, NY; IBM Application Systems Division, Kingston, NY; IBM Application Systems Division, Cary, NC; IBM Application Systems Division, Texas","IBM Systems Journal","6 Apr 2010","1988","27","3","325","347","This paper describes a framework for developing applications that conform to Systems Application Architecture (SAA). The paper shows a high-level approach to creating a design; it gives examples of early modeling work with the user interface; and it appraises SAA through the eyes of several system designers. The usability of user interfaces has been evaluated through the modeling of office tasks. That experience is described, showing the influence of the SAA Common User Access (CUA) on the model and the influence of the model on CUA. Discussed is a design for distributed applications that fit within the SAA framework and the influence of SAA on the design of integrated distributed applications.","0018-8670","","10.1147/sj.273.0325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387636","","","","","","2","","","","6 Apr 2010","","","IBM","IBM Journals"
"System design metrics: a review and perspective","D. C. Ince; M. J. Sheppard","Open University, UK; Wolverhampton Polytechnic, UK","Second IEE/BCS Conference: Software Engineering, 1988 Software Engineering 88.","6 Aug 2002","1988","","","23","27","In comparison with code metrics relatively little work has been carried out in the area of system design metrics. However, it is potentially a more useful field for research than code metrics: design metrics can be extracted much earlier in a project and can be used to predict the extent of a greater number of activities. The majority of recent system design metrics can be partitioned into three categories: network metrics, stability metrics and information flow metrics. The author discusses each of these metrics before looking at the issue of software tools for system design metrics extraction and processing. He also examines empirical validations in the system design metrics area.<>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=196354","","Software tools;System analysis and design","software tools;systems analysis","systems analysis;system design metrics;network metrics;stability metrics;information flow metrics;software tools;metrics extraction;empirical validations","","6","","","","6 Aug 2002","","","IET","IET Conferences"
"An evaluation of memory management schemes for a multimemory-multiprocessor system","C. . -R. Chou; M. . -Y. Fang","Institute of Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Institute of Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","Seventh Annual International Phoenix Conference on Computers an Communications. 1988 Conference Proceedings","6 Aug 2002","1988","","","94","98","The virtual memory management schemes for a multimemory-multiprocessor system based on bus-structured architecture are dealt with. The design issues peculiar to such a system are pointed out. Various schemes to manage them are introduced. Simulations of the memory management schemes are carried out to evaluate the design alternatives. A queuing network model is established to predict the overall system throughput under various paging system policies. From these experiments, the authors observe the behavior of their memory system, which may provide clues for future improvement.<>","","0-8186-0830-7","10.1109/PCCC.1988.10049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049","","Memory management;Operating systems;Hardware;Microprocessors;Computer architecture;Switches;Multiprocessing systems;Engineering management;Predictive models;Throughput","multiprocessing systems;performance evaluation;queueing theory;semiconductor storage;storage management;virtual storage","performance evaluation;multimemory-multiprocessor system;virtual memory management schemes;bus-structured architecture;queuing network model;system throughput;paging system policies","","","","9","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"On the Implementation and Use of Ada on Fault-Tolerant Distributed Systems","J. C. Knight; J. I. A. Urquhart","Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA","IEEE Transactions on Software Engineering","18 Sep 2006","1987","SE-13","5","553","563","In this paper, we discuss the use of Ada® on distributed systems in which failure of processors has to be tolerated. We assume that tasks are the primary object of distribution, and that communication between tasks on separate processors will take place using the facilities of the Ada language. It would be possible to build a separate set of facilities for communication between processors, and to treat the software on each machine as a separate program. This is unnecessary and undesirable. In addition, the Ada language Reference Manual states specifically that a system consisting of communicating processors with private memories is suitable for executing an Ada program.","1939-3520","","10.1109/TSE.1987.233200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702255","Ada;distributed systems;fault tolerance;highly reliable systems;tolerance of processor failure","Fault tolerant systems;Hardware;Application software;Protocols;Embedded software;Aerospace electronics;Costs;Computer displays;Actuators;Microprocessors","","Ada;distributed systems;fault tolerance;highly reliable systems;tolerance of processor failure","","20","","14","IEEE","18 Sep 2006","","","IEEE","IEEE Journals"
"A distributed VLSI architecture for efficient signal and data processing","J. -L. Gaudiot; R. W. Vedder; G. K. Tucker; D. Finn; M. L. Campbell","Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA, USA; VLSI Systems Department, Electro-Optical and Data Systems Group, Hughes Aircraft Company, El Segundo, CA, USA; VLSI Systems Department, Electro-Optical and Data Systems Group, Hughes Aircraft Company, El Segundo, CA, USA; VLSI Systems Department, Electro-Optical and Data Systems Group, Hughes Aircraft Company, El Segundo, CA, USA; VLSI Systems Department, Electro-Optical and Data Systems Group, Hughes Aircraft Company, El Segundo, CA, USA","IEEE Transactions on Computers","25 Sep 2012","1985","C-34","12","1072","1087","The machine described, the Hughes Data-Flow Multiprocessor (HDFM), is a high-performance, scalable, fault-tolerant, highly programmable multicomputer designed for embedded signal and data processing applications. The architecture of the machine is described in detail, and the influences on the final design of various requirements such as weight, size, power consumption, performance level and reliability are shown. The processing elements have been designed to reduce the number of VLSI component types required and for modularity of the physical system. The modular nature of the architecture allows a range of throughput and reliability requirements to be met. The model of execution, derived from original data-flow principles, is presented, as well as the various software tools which give the system its high-level language programmability. Complex constructs (such as large structure handling) are demonstrated. The results of a deterministic simulation of the machine show that a 64-processing-element machine may provide real throughput of 64 million instructions per second (MIPS).","1557-9956","","10.1109/TC.1985.6312207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6312207","Allocation;asynchronous execution;data-flow multiprocessor;distributed computing;multiprocessor architecture;signal and data processor","High level languages;Very large scale integration;Computer architecture;Hardware;Software;Parallel processing;Throughput","computer architecture;computerised signal processing;distributed processing;parallel processing;signal processing equipment;VLSI","signal processor;scalable multiprocessor;fault-tolerant computer;embedded computer;distributed VLSI architecture;Hughes Data-Flow Multiprocessor;programmable multicomputer;weight;size;power consumption;performance level;reliability;modularity;data-flow principles","","15","","","IEEE","25 Sep 2012","","","IEEE","IEEE Journals"
"PROUST: Knowledge-Based Program Understanding","W. L. Johnson; E. Soloway","Department of Computer Science, Yale University, New Heaven, CT, USA; Department of Computer Science, Yale University, New Heaven, CT, USA","IEEE Transactions on Software Engineering","18 Sep 2006","1985","SE-11","3","267","275","This paper describes a program called PROUST which does on-line analysis and understanding of Pascal written by novice programmers. PROUST takes as input a program and a nonalgorithmic description of the program requirements, and finds the most likely mapping between the requirements and the code. This mapping is in essence a reconstruction of the design and implementation steps that the programmer went through in writing the program. A knowledge base of programming plans and strategies, together with common bugs associated with them, is used in constructing this mapping. Bugs are discovered in the process of relating plans to the code; PROUST can therefore give deep explanations of program bugs by relating the buggy code to its underlying intentions.","1939-3520","","10.1109/TSE.1985.232210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1702003","Artificial inteiligence;program debugging;programmer training;program understanding","Programming profession;Computer bugs;Writing;Debugging;Personnel;Psychology;Computer science;Machinery;Reactive power","","Artificial inteiligence;program debugging;programmer training;program understanding","","163","","20","IEEE","18 Sep 2006","","","IEEE","IEEE Journals"
"TSO Attach: A multipurpose communication channel to IBM Database 2","K. R. Hammond; M. R. Zimowski","Santa Teresa Laboratory, IBM General Products Division, San Jose, CA; Santa Teresa Laboratory, IBM General Products Division, San Jose, CA","IBM Systems Journal","6 Apr 2010","1984","23","2","151","164","TSO Attach provides IBM Database 2 capabilities in a productive work environment that appears as a natural extension of the Time Sharing Option (TSO) and the Interactive System Productivity Facility (ISPF). It was designed and built with careful consideration for the varied and complex user group for which it was intended. Ease of use and ease of development and maintenance were among the significant factors in the design. These factors and others are addressed in this paper, which discusses the basic design decisions made in building the TSO Attachment Facility.","0018-8670","","10.1147/sj.232.0151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5387771","","","","","","","","","","6 Apr 2010","","","IBM","IBM Journals"
"SLAN-4: A Language for the Specification and Design of Large Software Systems","F. Beichter; O. Herzog; H. Petzsch","IBM System Products Division, 7030 Boeblingen I, Federal Republic of Germany; IBM System Products Division, 7030 Boeblingen I, Federal Republic of Germany; Technical University of Aachen, Federal Republic of Germany","IBM Journal of Research and Development","6 Apr 2010","1983","27","6","558","576","The language SLAN-4 has been defined in view of the need for formal tools supporting the specification and design of large software systems. It offers its users language constructs for algebraic and axiomatic specifications as well as for design in pseudocode. One of its major design goals has been to ease subsequent refinements of a (given) specification. The user can start his development with an informal high-level specification which can be formalized and implemented at a later date by using lower-level concepts. This paper provides the formal definitions of the SLAN-4 language, discusses the design decisions, and presents examples for the use of the syntactic constructs.","0018-8646","","10.1147/rd.276.0558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5390404","","","","","","7","","","","6 Apr 2010","","","IBM","IBM Journals"
"The process design system (PDS)","N. A. Vosbury","System Development Corporation, Huntsville, AL, USA","COMPSAC 79. Proceedings. Computer Software and The IEEE Computer Society's Third International Applications Conference, 1979.","10 Dec 2002","1979","","","374","379","PDS, which is designed to aid in the development of large processes, consists of three parts: Configuration Management Processor (CPIP), library system, and Process Design Language (PDL) compiler. CMP manages the outline description of the process; aids in documentation; and is the interface between user, library, and compiler. An important function is the staging of necessary or desired source code for the compiler. Only code that is affected by code changes is compiled. Since PDL is a nested scope language, this capability that enables separate compilation is essential. PDL is a language derived from Pascal. The extensions include tasking primitives; type extensions; COMMONs; array statements and expressions; improved looping, IFF, CASE, and WITH state ments; an ESCAPE statement; an ability to call foreign routines; internal routines; and the ability to restrict the use of variables of selected types to specified procedures.","","","10.1109/CMPSAC.1979.762521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=762521","","Process design;Page description languages;Documentation;Libraries;Program processors;Computer aided software engineering;Data visualization;Testing;Energy management;Data structures","","","","3","","","IEEE","10 Dec 2002","","","IEEE","IEEE Conferences"
"ACS.1: An Experimental Automated Command Support System","M. C. Pease","Stanford Research Institute, Menlo Park, CA, USA","IEEE Transactions on Systems, Man, and Cybernetics","12 Nov 2007","1978","8","10","725","735","A laboratory system for studying the design of knowledge-based systems to support a manager in the development and execution of operational plans is described. The system, called ACS.1 for Automated Command Support, operates in the simulated environment of a naval air squadron, but the techniques used are believed applicable to a wide spectrum of environments requiring the planning of operations, the administration and monitoring of approved plans, and the retrospectrive analysis of completed operations. The development of ACS.1 has shown that the concept is feasible and has included the development of programming techniques to support it. The design of ACS.1 is predicated on the importance of system flexibility. This quality is considered essential to permit incremental growth of the system, to facilitate its adaptation to a changing environment or to new requirements, and to enable the manager to intervene in exceptional situations. Much of the design of ACS.1 was chosen to provide flexibility. As viewed from the top level, ACS.1 is highly modular, with coordination being achieved through a process of negotiation carried on via messages. The manager can intervene in this process in a central switching unit called the message handler. He can, for example, direct that certain messages be rerouted to new modules or to the terminal. This is a useful capability, facilitating the orderly incremental growth of the system, as well as permitting alteration of the system's behavior in preplanned ways to meet particular needs.","2168-2909","","10.1109/TSMC.1978.4309842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4309842","","Humans;Control systems;Laboratories;Knowledge management;Knowledge based systems;Analytical models;Computerized monitoring;Environmental management;Quality management;System testing","","","","2","","11","IEEE","12 Nov 2007","","","IEEE","IEEE Journals"
"EASCON 1977","",,"IEEE Transactions on Aerospace and Electronic Systems","20 Feb 2007","1977","AES-13","6","722","732","Presents abstracts of papers that were presented at this conference.","1557-9603","","10.1109/TAES.1977.308517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4101904","","","","","","","","","IEEE","20 Feb 2007","","","IEEE","IEEE Journals"
"Structured design","W. P. Stevens; G. J. Myers; L. L. Constantine",NA; NA; NA,"IBM Systems Journal","6 Apr 2010","1974","13","2","115","139","The HIPO Hierarchy chart is being used as an aid during general systems design. The considerations and techniques presented here are useful for evaluating alternatives for thos portions of the system that will be programmed on a computer. The charting technique used here depicts more details about the interfaces than the HIPO Hierarchy chart. This facilitates consideration during general program design of each individual connection and its associated passed parameters. The resulting design can be documented with the HIPO charts. (if the designer decides to have more than one function in any module, the structure chart should show them in the same block. However, the HIPO Hierarchy chart would still show all the functions in separate blocks.) The output of the general program design is the input for the detailed module design. The HIPO input-process-output chart is useful for describing and designing each module.","0018-8670","","10.1147/sj.132.0115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5388187","","","","","","681","2","","","6 Apr 2010","","","IBM","IBM Journals"
"Self-Diagnosis and Self-Repair in Memory: An Integrated System Approach","S. A. Szygenda; M. J. Flynn","Computer Science/Operations Research Center, Institute of Technology, Southern Methodist University, Dallas, TX, USA; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Reliability","25 Aug 2009","1973","R-22","1","2","12","This paper presents an integrated approach to the design of an ultrareliable memory system using a variety of coding and modularization techniques on each of the memory subsystem elements. The overall objective is to provide a properly operating memory system in spite of any single indigenous fault (regardless of the number of failures which might ensue). In other words, the system has the capability automatically to: (1) detect single faults and multiple failures, (2) mask failures to prevent malfunctions, without interrupting service, (3) isolate the fault to a replaceable module, and (4) reconfigure the faulty unit out of the system. The storage medium and retrieval circuits are checked and corrected by coding techniques. Some redundancy is used on the subunits, but the total redundancy is less than 20% of the system cost, and diagnostic software is eliminated.","1558-1721","","10.1109/TR.1973.5216015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5216015","","Circuit faults;Redundancy;Solids;Costs;Hardware;Embedded software;Error correction codes;Fault detection;Intrusion detection;Failure analysis","","","","3","3","8","IEEE","25 Aug 2009","","","IEEE","IEEE Journals"
"Compiling Optimized Code from Decision Tables","H. J. Myers","IBM Systems Development Division laboratory, CA","IBM Journal of Research and Development","6 Apr 2010","1972","16","5","489","503","This paper reviews the structure of decision tables and methods for converting them into procedural code. It describes new optimization methods, which are applied before, during, and after code generation. Some results from an experimental decision table processor are provided.","0018-8646","","10.1147/rd.165.0489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5391456","","","","","","6","2","","","6 Apr 2010","","","IBM","IBM Journals"
"Miniature Light Sources for Timing Signal Recording","M. A. Kerr","General Electric Co., Room 4337, Apollo Park, P.O. Box 2500, Daytona Beach, Fla. 32015","Journal of the SMPTE","19 Oct 2015","1968","77","3","210","214","Photooptical recording of timing signals on the edge of motion-picture films is a regular requirement on many cameras used by the aerospace industry. The recording consists of regularly spaced, fine-line timing marks, plus pulse-width modulated rectangular blocks which define discrete time codes at regular intervals. Several previous papers have defined the problems of using miniature neon or argon lamps to record timing signals. Some improvements in mechanical-optical packaging have been proposed. This paper presents neon lamp data on new types, standard vs. high brightness, more certain ionization and “keep alive” circuits, lamp current vs. life, and operating ranges. Improvement in optical efficiency of signal recording blocks is examined. Wider operating range and more reliable recording by use of new electroluminescent (EL) diodes is considered. Sample 16mm recordings at 4 to 600 frames/s are analyzed. Available EL diodes with visible range emission are listed.","0361-4573","","10.5594/J07028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7264187","","","","","","4","","7","","19 Oct 2015","","","SMPTE","SMPTE Journals"
"Assembled Switchgear Designed by Computer II-A General Computer Method for Translating Functional and Descriptive Input into Parts Lists","H. B. Wortman; H. K. Gallimore; M. H. Waller; R. H. Davis","Westinghouse Electric Corporation, East Pittsburgh, PA, USA; Westinghouse Electric Corporation, East Pittsburgh, PA, USA; Westinghouse Electric Corporation, East Pittsburgh, PA, USA; Westinghouse Electric Corporation, East Pittsburgh, PA, USA","Transactions of the American Institute of Electrical Engineers. Part III: Power Apparatus and Systems","7 May 2008","1962","81","3","793","800","The problem of converting descriptive input into parts lists implies a common coding that will permit association of application demands with the required parts. A general method has been devised whereby parts and descriptive conditions are suitably coded and their patterns, developed according to the same definitions, are matched. This method has been used to translate metal-clad switchgear application descriptions into parts lists.","2379-6766","","10.1109/AIEEPAS.1962.4501424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4501424","","Assembly;Switchgear;Production;Manufacturing;Computer peripherals;Costs;Job design;Logic;Engineering drawings;Design engineering","","","","","","","IEEE","7 May 2008","","","IEEE","IEEE Journals"
"Journal-First Papers","",,"2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)","21 Jul 2022","2022","","","xxviii","xxix","","1534-5351","978-1-6654-3786-8","10.1109/SANER53432.2022.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825906","","","","","","","","","IEEE","21 Jul 2022","","","IEEE","IEEE Conferences"
"Guest Editorial: Industrial Cyber–Physical Systems—New Trends in Computing and Communications","F. Tramarin; M. Luvisotto; A. Willig; K. Yu","Department of Engineering E. Ferrari, University of Modena and Reggio Emilia, Modena, Italy; University of Canterbury, Christchurch, New Zealand; Hitachi ABB Power Grids Research, Västerås, Sweden; Department of Computer Science and Information Technology, La Trobe University, Melbourne, Vic, Australia","IEEE Transactions on Industrial Informatics","24 Feb 2021","2021","17","5","3518","3522","The papers in this special section focus on industrial cyber-physical systems (CPS), with an emphasis on computing and communications applications. CPS systems are defined by integrating computation and communication facilities on the one hand and the monitoring and control of physical processes on the other hand. Industrial cyber–physical systems (ICPS) refer to the science and art of designing and using cyber–physical systems for industrial and process control applications, for example in smart factories, smart energy grids, smart transportation systems, smart cities, and several other areas. Many of these applications are time- and mission-critical and hence often require low latency and high reliability. In parallel, there has been a strong growth in integrating intelligence, often in the form of machine/deep learning, into applications, which also leads to vastly increasing computational requirements. These innovations then will be integrated into complex systems, which need to be properly engineered to become safe, reliable, trustworthy, and secure while at the same time being cost-efficient.","1941-0050","","10.1109/TII.2020.3033818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361682","","Special issues and sections;Cyber-physical systems;Industrial electronics;Machine learning;Wireless sensor networks;Smart devices;Deep learning;5G mobile communication;6G mobile communication","","","","1","","0","IEEE","24 Feb 2021","","","IEEE","IEEE Journals"
"IEEE Access Special Section Editorial: Proximity Service (Prose) Challenges and Applications","Y. Wang; Q. Jin; J. Ma; K. Ntalianis; M. Z. Alam Bhuiyan; M. Luvisotto","College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Human Informatics and Cognitive Sciences, Waseda University, Tokyo, Japan; Faculty of Computer and Information Sciences, Hosei University, Tokyo, Japan; Department of Business Administration, University of West Attica, Psachna, Greece; Department of Computer and Information Sciences, Fordham University, New York City, USA; ABB Power Grids Research, Västerås, Sweden","IEEE Access","22 Sep 2020","2020","8","","169106","169109","The mobile revolution is changing the way we interact with the people and things around us. Proximity awareness, the ability to actively/passively and continuously search for relevant value in one’s physical/virtual proximity, is at the core of this phenomenon.","2169-3536","","10.1109/ACCESS.2020.3023559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203997","","","","","","","","0","CCBY","22 Sep 2020","","","IEEE","IEEE Journals"
"Guest Editorial: Recent Advances in Cyber-Physical Security in Industrial Environments","Z. Lv; W. Mazurczyk; S. Wendzel; H. Song","Qingdao University, Shandong, China; Warsaw University of Technology, Warszawa, Poland; Worms University of Applied Sciences, Worms, Germany; Embry-Riddle Aeronautical University, Oklahoma City, USA","IEEE Transactions on Industrial Informatics","5 Dec 2019","2019","15","12","6468","6471","“Smart” has gradually infiltrating all areas of people's daily life and the environments where we lead our life. The term of “Smart Industrial Environment” can be used to refer to each aspect of the industrial environments focused on the future, being smart vehicles, smart systems of transportation, smart devices (wearables and smartphones), smart services (such as just-in-time production pipelines adjusted to the requirements of the supply-chain), smart grids, smart factories and smart plants management utilizing information technology. It includes the inter-connection of all the smart technologies, involving every type of political and technological borders besides being a term that involves all the aspects.","1941-0050","","10.1109/TII.2019.2945971","National Natural Science Foundation of China(grant numbers:No. 61902203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861403","Industrial Environment;Security;Cyber-Physical System","Special issues and sections;Authentication;Cyber-physical systems;Cloud computing;Encryption;Big Data","","","","6","","0","IEEE","7 Oct 2019","","","IEEE","IEEE Journals"
