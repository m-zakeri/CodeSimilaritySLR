"Item type","Authors","Title","Journal","Publication year","Volume","Issue","Pages","Publisher","Address","Proceedings title","Conference location","Date published","ISBN","ISSN","URLs","DOI","Abstract","Keywords","Notes","Series"
"Journal Article","Keller P,Kaboré AK,Plein L,Klein J,Le Traon Y,Bissyandé TF","What You See is What It Means! Semantic Representation Learning of Code Based on Visualization and Transfer Learning","ACM Trans. Softw. Eng. Methodol.","2021","31","2","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3485135;http://dx.doi.org/10.1145/3485135","10.1145/3485135","Recent successes in training word embeddings for Natural Language Processing (NLP) tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the WySiWiM (‘‘What You See Is What It Means”) approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on the task of vulnerable code prediction in source code and on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the BigCloneBench (Java), Open Judge (C) that although simple, our WySiWiM approach performs as effectively as state-of-the-art approaches such as ASTNN or TBCNN. We also showed with data from NVD and SARD that WySiWiM representation can be used to learn a vulnerable code detector with reasonable performance (accuracy ∼90%). We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.","embedding, representation learning, Semantic clones, visual representation","",""
"Journal Article","Maddila C,Nagappan N,Bird C,Gousios G,van Deursen A","ConE: A Concurrent Edit Detection Tool for Large-Scale Software Development","ACM Trans. Softw. Eng. Methodol.","2021","31","2","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3478019;http://dx.doi.org/10.1145/3478019","10.1145/3478019","Modern, complex software systems are being continuously extended and adjusted. The developers responsible for this may come from different teams or organizations, and may be distributed over the world. This may make it difficult to keep track of what other developers are doing, which may result in multiple developers concurrently editing the same code areas. This, in turn, may lead to hard-to-merge changes or even merge conflicts, logical bugs that are difficult to detect, duplication of work, and wasted developer productivity. To address this, we explore the extent of this problem in the pull-request-based software development model. We study half a year of changes made to six large repositories in Microsoft in which at least 1,000 pull requests are created each month. We find that files concurrently edited in different pull requests are more likely to introduce bugs. Motivated by these findings, we design, implement, and deploy a service named Concurrent Edit Detector (ConE) that proactively detects pull requests containing concurrent edits, to help mitigate the problems caused by them. ConE has been designed to scale, and to minimize false alarms while still flagging relevant concurrently edited files. Key concepts of ConE include the detection of the Extent of Overlap between pull requests, and the identification of Rarely Concurrently Edited Files. To evaluate ConE, we report on its operational deployment on 234 repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775 recommendations about conflicting changes, which were rated as useful in over 70% (554) of the cases. From interviews with 48 users, we learned that they believed ConE would save time in conflict resolution and avoiding duplicate work, and that over 90% intend to keep using the service on a daily basis.","Pull-based software development, merge conflict, pull request, distributed software development","",""
"Journal Article","Chen J,Xia X,Lo D,Grundy J","Why Do Smart Contracts Self-Destruct? Investigating the Selfdestruct Function on Ethereum","ACM Trans. Softw. Eng. Methodol.","2021","31","2","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3488245;http://dx.doi.org/10.1145/3488245","10.1145/3488245","The selfdestruct function is provided by Ethereum smart contracts to destroy a contract on the blockchain system. However, it is a double-edged sword for developers. On the one hand, using the selfdestruct function enables developers to remove smart contracts (SCs) from Ethereum and transfers Ethers when emergency situations happen, e.g., being attacked. On the other hand, this function can increase the complexity for the development and open an attack vector for attackers. To better understand the reasons why SC developers include or exclude the selfdestruct function in their contracts, we conducted an online survey to collect feedback from them and summarize the key reasons. Their feedback shows that 66.67% of the developers will deploy an updated contract to the Ethereum after destructing the old contract. According to this information, we propose a method to find the self-destructed contracts (also called predecessor contracts) and their updated version (successor contracts) by computing the code similarity. By analyzing the difference between the predecessor contracts and their successor contracts, we found five reasons that led to the death of the contracts; two of them (i.e., Unmatched ERC20 Token and Limits of Permission) might affect the life span of contracts. We developed a tool named LifeScope to detect these problems. LifeScope reports 0 false positives or negatives in detecting Unmatched ERC20 Token. In terms of Limits of Permission, LifeScope achieves 77.89% of F-measure and 0.8673 of AUC in average. According to the feedback of developers who exclude selfdestruct functions, we propose suggestions to help developers use selfdestruct functions in Ethereum smart contracts better.","selfdestruct function, ethereum, empirical study, Smart contract","",""
"Conference Paper","Yuan S,Talpin JP","Verified Functional Programming of an IoT Operating System's Bootloader","","2021","","","89–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th ACM-IEEE International Conference on Formal Methods and Models for System Design","Virtual Event, China","2021","9781450391276","","https://doi.org/10.1145/3487212.3487347;http://dx.doi.org/10.1145/3487212.3487347","10.1145/3487212.3487347","The fault of one device on a grid may incur severe economical or physical damages. Among the many critical components in such IoT devices, the operating system's bootloader comes first to initiate the trusted function of the device on the network. However, a bootloader uses hardware-dependent features that make its functional correctness proof difficult. This paper uses verified programming to automate the verification of both the C libraries and assembly boot-sequence of such a, real-world, bootloader in an operating system for ARM-based IoT devices: RIoT. We first define the ARM ISA specification, semantics and properties in F* to model its critical assembly code boot sequence. We then use Low*, a DSL rendering a C-like memory model in F*, to implement the complete bootloader library and verify its functional correctness and memory safety. Other than fixing potential faults and vulnerabilities in the source C and ASM bootloader, our evaluation provides an optimized and formally documented code structure, a reasonable specification/implementation ratio, a high degree of proof automation and an equally efficient generated code.","boot loader, case study, verified programming, IoT kernel","","MEMOCODE '21"
"Conference Paper","Lim G,Ham M,Moon J,Song W","LightSys: Lightweight and Efficient CI System for Improving Integration Speed of Software","","2021","","","1–10","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice","","2021","9780738146690","","https://doi.org/10.1109/ICSE-SEIP52600.2021.00009;http://dx.doi.org/10.1109/ICSE-SEIP52600.2021.00009","10.1109/ICSE-SEIP52600.2021.00009","The complexity and size increase of software has extended the delay for developers as they wait for code analysis and code merge. With the larger and more complex software, more developers nowadays are developing software with large source code repositories. The tendency for software platforms to immediately update software packages with feature updates and bug-fixes is a significant obstacle. Continuous integration systems may help prevent software flaws during the active development of software packages, even when they are deployed and updated frequently. Herein, we present a portable and modular code review automation system that inspects incoming code changes such as code format and style, performance regression, static analysis, build and deployment tests, and dynamic analysis before merging and changing code. The proposed mechanisms are sufficiently lightweight to be hosted on a regular desktop computer even for numerous developers. The resulting reduced costs allow developers to apply the proposed mechanism to many source code repositories. Experimental results demonstrate that the proposed mechanism drastically reduces overheads and improves usability compared with conventional mechanisms: execution time (6x faster), CPU usage (40% lower), memory consumption (1/180), and no out-of-memory occurrence.","code review automation, software update, continuous integration, software regression, continuous test","","ICSE-SEIP '21"
"Conference Paper","Reichenbach C","Software Ticks Need No Specifications","","2021","","","61–65","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results","","2021","9780738133249","","https://doi.org/10.1109/ICSE-NIER52604.2021.00021;http://dx.doi.org/10.1109/ICSE-NIER52604.2021.00021","10.1109/ICSE-NIER52604.2021.00021","Software bugs cost time, money, and lives. They drive software research and development efforts, and are central to modern software engineering. Yet we lack a clear and general definition of what bugs are. Some bugs are defects, clearly defined as failures to meet some requirement or specification. However, there are many forms of undesirable program behaviour that are completely compatible with a typical program's specification.In this paper, we argue that the lack of a criterion for identifying non-defect bugs is hampering the development of tools that find and fix bugs. We propose such a criterion, based on the idea of wasted effort, discuss how bugs that meet our definition of software ticks can complement defects, and sketch how our definition can help guide future work on software tools.","software ticks, software bugs, software defects","","ICSE-NIER '21"
"Conference Paper","Mukelabai M,Berger T,Borba P","Semi-Automated Test-Case Propagation in Fork Ecosystems","","2021","","","46–50","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results","","2021","9780738133249","","https://doi.org/10.1109/ICSE-NIER52604.2021.00018;http://dx.doi.org/10.1109/ICSE-NIER52604.2021.00018","10.1109/ICSE-NIER52604.2021.00018","Forking provides a flexible and low-cost strategy for developers to adapt an existing project to new requirements, for instance, when addressing different market segments, hardware constraints, or runtime environments. Then, small ecosystems of forked projects are formed, with each project in the ecosystem maintained by a separate team or organization. The software quality of projects in fork ecosystems varies with the resources available as well as team experience, and expertise, especially when the forked projects are maintained independently by teams that are unaware of the evolution of other's forks. Consequently, the quality of forked projects could be improved by reusing test cases as well as code, thereby leveraging community expertise and experience, and commonalities between the projects. We propose a novel technique for recommending and propagating test cases across forked projects. We motivate our idea with a pre-study we conducted to investigate the extent to which test cases are shared or can potentially be reused in a fork ecosystem. We also present the theoretical and practical implications underpinning the proposed idea, together with a research agenda.","test propagation, code transplantation, forking","","ICSE-NIER '21"
"Conference Paper","Chong CY,Thongtanunam P,Tantithamthavorn C","Assessing the Students' Understanding and Their Mistakes in Code Review Checklists: An Experience Report of 1,791 Code Review Checklist Questions from 394 Students","","2021","","","20–29","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training","","2021","9780738133201","","https://doi.org/10.1109/ICSE-SEET52601.2021.00011;http://dx.doi.org/10.1109/ICSE-SEET52601.2021.00011","10.1109/ICSE-SEET52601.2021.00011","Code review is a widely-used practice in software development companies to identify defects. Hence, code review has been included in many software engineering curricula at universities worldwide. However, teaching code review is still a challenging task because the code review effectiveness depends on the code reading and analytical skills of a reviewer. While several studies have investigated the code reading techniques that students should use to find defects during code review, little has focused on a learning activity that involves analytical skills. Indeed, developing a code review checklist should stimulate students to develop their analytical skills to anticipate potential issues (i.e., software defects). Yet, it is unclear whether students can anticipate potential issues given their limited experience in software development (programming, testing, etc.). We perform a qualitative analysis to investigate whether students are capable of creating code review checklists, and if the checklists can be used to guide reviewers to find defects. In addition, we identify common mistakes that students make when developing a code review checklist. Our results show that while there are some misconceptions among students about the purpose of code review, students are able to anticipate potential defects and create a relatively good code review checklist. Hence, our results lead us to conclude that developing a code review checklist can be a part of the learning activities for code review in order to scaffold students' skills.","software engineering education, checklist-based code review, assessment methods for software quality assurance","","ICSE-JSEET '21"
"Conference Paper","Tan SH,Hu C,Li Z,Zhang X,Zhou Y","GitHub-OSS Fixit","","2021","","","1–10","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training","","2021","9780738133201","","https://doi.org/10.1109/ICSE-SEET52601.2021.00009;http://dx.doi.org/10.1109/ICSE-SEET52601.2021.00009","10.1109/ICSE-SEET52601.2021.00009","Many studies have shown the benefits of introducing open-source projects into teaching Software Engineering (SE) courses. However, there are several limitations of existing studies that limit the wide adaptation of open-source projects in a classroom setting, including (1) the selected project is limited to one particular project, (2) most studies only investigated on its effect on teaching a specific SE concept, and (3) students may make mistakes in their contribution which leads to poor quality code. Meanwhile, software companies have successfully launched programs like Google Summer of Code (GSoC) and FindBugs ""fixit"" to contribute to open-source projects. Inspired by the success of these programs, we propose GitHub-OSS Fixit, a team-based course project where students are taught to contribute to open-source Java projects by fixing bugs reported in GitHub. We described our course outline to teach students SE concepts by encouraging the usages of several automated program analysis tools. We also included the carefully designed instructions that we gave to students for participating in GitHub-OSS Fixit. As all lectures and labs are conducted online, we think that our course design could help in guiding future online SE courses. Overall, our survey results show that students think that GitHub-OSS Fixit could help them to improve many skills and apply the knowledge taught in class. In total, 154 students have submitted 214 pull requests to 24 different Java projects, in which 93 of them have been merged, and 46 have been closed by developers.","program repair, open-source software, software engineering","","ICSE-JSEET '21"
"Conference Paper","Fraser G,Heuer U,Körber N,Obermüller F,Wasmeier E","LitterBox: A Linter for Scratch Programs","","2021","","","183–188","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training","","2021","9780738133201","","https://doi.org/10.1109/ICSE-SEET52601.2021.00028;http://dx.doi.org/10.1109/ICSE-SEET52601.2021.00028","10.1109/ICSE-SEET52601.2021.00028","Creating programs with block-based programming languages like Scratch is easy and fun. Block-based programs can nevertheless contain bugs, in particular when learners have misconceptions about programming. Even when they do not, Scratch code is often of low quality and contains code smells, further inhibiting understanding, reuse, and fun. To address this problem, in this paper we introduce LitterBox, a linter for Scratch programs. Given a program or its public project ID, LitterBox checks the program against patterns of known bugs and code smells. For each issue identified, LitterBox provides not only the location in the code, but also a helpful explanation of the underlying reason and possible misconceptions. Learners can access LitterBox through an easy to use web interface with visual information about the errors in the block-code, while for researchers LitterBox provides a general, open source, and extensible framework for static analysis of Scratch programs.","linting, scratch, bug patterns, code smells","","ICSE-JSEET '21"
"Conference Paper","AlOmar EA,AlRubaye H,Mkaouer MW,Ouni A,Kessentini M","Refactoring Practices in the Context of Modern Code Review: An Industrial Case Study at Xerox","","2021","","","348–357","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice","","2021","9780738146690","","https://doi.org/10.1109/ICSE-SEIP52600.2021.00044;http://dx.doi.org/10.1109/ICSE-SEIP52600.2021.00044","10.1109/ICSE-SEIP52600.2021.00044","Modern code review is a common and essential practice employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure conformance with coding standards. During code review, developers may inspect and discuss various changes including refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring activities are being reviewed, perceived, and practiced.This study aims to reveal insights into how reviewers develop a decision about accepting or rejecting a submitted refactoring request, and what makes such review challenging. We present an industrial case study with 24 professional developers at Xerox. Particularly, we study the motivations, documentation practices, challenges, verification, and implications of refactoring activities during code review.Our study delivers several important findings. Our results report the lack of a proper procedure to follow by developers when documenting their refactorings for review. Our survey with reviewers has also revealed several difficulties related to understanding the refactoring intent and implications on the functional and non-functional aspects of the software. In light of our findings, we recommended a procedure to properly document refactoring activities, as part of our survey feedback.","refactoring, software quality, code review","","ICSE-SEIP '21"
"Conference Paper","Detofeno T,Reinehr S,Andreia M","Technical Debt Guild: When Experience and Engagement Improve Technical Debt Management","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XX Brazilian Symposium on Software Quality","Virtual Event, Brazil","2021","9781450395533","","https://doi.org/10.1145/3493244.3493271;http://dx.doi.org/10.1145/3493244.3493271","10.1145/3493244.3493271","Efficient Technical Debt Management (TDM) requires specialized guidance so that decisions taken are oriented to add value to the business. Because it is a complex problem that involves several variables, TDM requires a systemic look that considers professionals' experiences from different specialties. Guilds have been a means technology companies using the Spotify methodology have found to unite specialized professionals around a common interest. This paper presents the experience in implementing a guild to support TDM's source code activities in a software development organization, using the action research method. The project lasted two years, and approximately 100 developers were involved in updating about 63,300 source-code files. The actions resulting from the TDM guild's efforts impacted the company's culture by introducing new development practices and standards. Besides, they positively influenced the quality of the artifact delivered by the developers. This study shows that, as the company acquires maturity in TDM, it increases the need for professionals dedicated to TDM's activities.","Technical Debt Guild, Community of Practice, Technical Debt Management, Technical Debt","","SBQS '21"
"Conference Paper","Gomes RF,Lelli V","GAMUT: GAMe-Based Learning Approach for Teaching Unit Testing","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XX Brazilian Symposium on Software Quality","Virtual Event, Brazil","2021","9781450395533","","https://doi.org/10.1145/3493244.3493263;http://dx.doi.org/10.1145/3493244.3493263","10.1145/3493244.3493263","Software testing is essential to ensure the quality of a system. One of the first levels of testing is the Unit Testing, which aims to test the smallest part of the software, such as objects, methods or classes or modules. Motivated by the relevance of unit tests in the software development process and the lack of undergraduate courses where students can relate the theoretical concepts of tests to practical classes, we propose a game-based learning approach, called GAMUT, linked by a narrative for teaching unit tests. The approach consists of three steps: a game to introduce the concepts of unit testing such as testing doubles and the given-when-then structure; a video lesson that uses similar code of the game to explain and exemplify the previous concepts; and an activity with a challenge, in which the students can practice what they learned for example the writing unit tests. The approach was applied to an undergraduate class of a Verification and Validation course at a university. The results showed that the approach helped to engage the students in the learning process of unit testing since most of them were able to successfully complete the proposed activity. Also, the students enjoyed the game, the narrative and the lucidity of the proposed activity.","Game based., Unit Testing, Learning","","SBQS '21"
"Conference Paper","Yu Y,Qin X,Gan S","HDBFuzzer–Target-Oriented Hybrid Directed Binary Fuzzer","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Computer Science and Application Engineering","Sanya, China","2021","9781450389853","","https://doi.org/10.1145/3487075.3487124;http://dx.doi.org/10.1145/3487075.3487124","10.1145/3487075.3487124","In this paper, we propose a target-oriented hybrid directed binary fuzzer (HDBFuzzer) to solve the vulnerability confirmation problem based on binary code similarity comparison. HDBFuzzer combines macro function level direction fuzzing and micro path-constraint directed solving. For some branches with simple or loose constraints, it still uses directed mutation of the directed fuzzing to penetrate while for some really hard-to-penetrate constraints, it resorts to guided concolic execution. At the same time, in order to improve the efficiency of constraint solving, we propose a constraint solving method based on “path abstraction”, which approximates the solution space by the linear expression and generates effective input utilizing the highly-effective sampling method towards the linear space. Then, under the guidance by the directed greybox fuzzing, HDBFuzzer can generate input that can quickly reach the vulnerable code region and finally crash the program under the test to confirm the vulnerability hidden in the binary program. We evaluate HDBFuzzer against AFLGo-B and QSYM on LAVA-M dataset and ten real-world programs, and the results show that HDBFuzzer is superior to AFLGo-B and QSYM on the bug discovery, bug reproduction and target reaching capabilities.","Binary program, Target site, Directed Greybox Fuzzing, Vulnerability confirmation, Symbolic execution","","CSAE '21"
"Conference Paper","Hong H,Woo S,Lee H","Dicos: Discovering Insecure Code Snippets from Stack Overflow Posts by Leveraging User Discussions","","2021","","","194–206","Association for Computing Machinery","New York, NY, USA","Annual Computer Security Applications Conference","Virtual Event, USA","2021","9781450385794","","https://doi.org/10.1145/3485832.3488026;http://dx.doi.org/10.1145/3485832.3488026","10.1145/3485832.3488026","Online Q&A fora such as Stack Overflow assist developers to solve their faced coding problems. Despite the advantages, Stack Overflow has the potential to provide insecure code snippets that, if reused, can compromise the security of the entire software. We present Dicos, an accurate approach by examining the change history of Stack Overflow posts for discovering insecure code snippets. When a security issue was detected in a post, the insecure code is fixed to be safe through user discussions, leaving a change history. Inspired by this process, Dicos first extracts the change history from the Stack Overflow post, and then analyzes the history whether it contains security patches, by utilizing pre-selected features that can effectively identify security patches. Finally, when such changes are detected, Dicos determines that the code snippet before applying the security patch is insecure. To evaluate Dicos, we collected 1,958,283 Stack Overflow posts tagged with C, C++, and Android. When we applied Dicos on the collected posts, Dicos discovered 12,458 insecure posts (i.e., 14,719 insecure code snippets) from the collected posts with 91% precision and 93% recall. We further confirmed that the latest versions of 151 out of 2,000 popular C/C++ open-source software contain at least one insecure code snippet taken from Stack Overflow, being discovered by Dicos. Our proposed approach, Dicos, can contribute to preventing further propagation of insecure codes and thus creating a safe code reuse environment.","Insecure code snippet discovery, Q&A forum, Software security.","","ACSAC '21"
"Conference Paper","Xylogiannopoulos KF,Karampelas P","Identifying Social Networks of Programmers Using Text Mining for Code Similarity Detection","","2021","","","643–650","IEEE Press","Virtual Event, Netherlands","Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining","","2021","9781728110561","","https://doi.org/10.1109/ASONAM49781.2020.9381381;http://dx.doi.org/10.1109/ASONAM49781.2020.9381381","10.1109/ASONAM49781.2020.9381381","The availability of code in many online repositories and collaborating platforms has posed new challenges in source code attribution not only for plagiarism detection but also in other settings such as in the use of insecure copied code in commercial application, etc. The sophistication of different type of attacks in the code sequence used especially by the students requires more effective code similarity detection algorithms. In this paper, a novel source code detection method is proposed that can identify programmers' social network based on advanced pattern detection text mining techniques. The proposed methodology has significant advantages against existing methods since ARPaD algorithm can detect all common patterns between all possible code sequences in one run. Therefore, the computational time is massively reduced to O(mn log n). In order to assess the performance of the methodology, a new dataset was created by assigning to 46 students a code project with specific instructions. The assessment results have been visualized, producing the social network graphs of possible collaboration teams.","code plagiarism detection, text mining, code similarity detection, ARPaD, LERP-RSA, social network analysis","","ASONAM '20"
"Conference Paper","Xylogiannopoulos KF,Karampelas P","Visualization of Repeated Patterns in Multivariate Discrete Sequences","","2021","","","862–869","IEEE Press","Virtual Event, Netherlands","Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining","","2021","9781728110561","","https://doi.org/10.1109/ASONAM49781.2020.9381316;http://dx.doi.org/10.1109/ASONAM49781.2020.9381316","10.1109/ASONAM49781.2020.9381316","The availability and affordability of mobile devices, wearables, sensors, IoT devices and electronic social networks produce big data in the form of complex systems of multivariate discrete sequences such as bio-informatics, natural language processing, social network corpus, etc. or non-discrete time series such as weather data, network traffic, workout data, etc. At the same time, the increased availability of advanced hardware in the form of powerful computers or high performance clusters provide us with the opportunity to analyze the aforementioned datasets that could produce vast amounts of results in diverse forms. One problem that has recently got focus is the one of discovering all repeated patterns in multivariate sequences. Novel algorithms have appeared such as ARPaD that address the specific problem however there are still no appropriate visualization methods to represent the complex results of the algorithm. In this paper, we attempt to create a visualization method that presents the common repeated patterns in multivariate discrete sequences. The visualization algorithm has been applied in a dataset of different text sequences of varying length and the results are presented in two novel type of plots, the Pattern Positional Alignment (PaPA) plot and the Stacked PaPA plot.","repeated patterns, visualization, ARPaD, complex information representation","","ASONAM '20"
"Journal Article","Gross M,Hohentanner K,Wiehler S,Sigl G","Enhancing the Security of FPGA-SoCs via the Usage of ARM TrustZone and a Hybrid-TPM","ACM Trans. Reconfigurable Technol. Syst.","2021","15","1","","Association for Computing Machinery","New York, NY, USA","","","2021-11","","1936-7406","https://doi.org/10.1145/3472959;http://dx.doi.org/10.1145/3472959","10.1145/3472959","Isolated execution is a concept commonly used for increasing the security of a computer system. In the embedded world, ARM TrustZone technology enables this goal and is currently used on mobile devices for applications such as secure payment or biometric authentication. In this work, we investigate the security benefits achievable through the usage of ARM TrustZone on FPGA-SoCs. We first adapt Microsoft’s implementation of a firmware Trusted Platform Module (fTPM) running inside ARM TrustZone for the Zynq UltraScale+ platform. This adaptation consists in integrating hardware accelerators available on the device to fTPM’s implementation and to enhance fTPM with an entropy source derived from on-chip SRAM start-up patterns. With our approach, we transform a software implementation of a TPM into a hybrid hardware/software design that could address some of the security drawbacks of the original implementation while keeping its flexibility. To demonstrate the security gains obtained via the usage of ARM TrustZone and our hybrid-TPM on FPGA-SoCs, we propose a framework that combines them for enabling a secure remote bitstream loading. The approach consists in preventing the insecure usages of a bitstream reconfiguration interface that are made possible by the manufacturer and to integrate the interface inside a Trusted Execution Environment.","secure device reconfiguration, TrustZone, trusted execution environment, hardware entropy, firmware TPM, FPGA-SoC","",""
"Conference Paper","Rosiak K","Extractive Multi Product-Line Engineering","","2021","","","263–265","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings","","2021","","","https://doi.org/10.1109/ICSE-Companion52605.2021.00122;http://dx.doi.org/10.1109/ICSE-Companion52605.2021.00122","10.1109/ICSE-Companion52605.2021.00122","Cloning is a general approach to create new functionality within variants as well as new system variants. It is a fast, flexible, intuitive, and economical approach to evolve systems in the short run. However, in the long run, the maintenance effort increases. A common solution to this problem is the extraction of a product line from a set of cloned variants. This process requires a detailed analysis of variants to extract variability information. However, clones within a variant are usually not considered in the process, but are also a cause for unsustainable software. This thesis proposes an extractive multi product-line engineering approach to re-establish the sustainable development of software variants. We propose an approach to re-engineer intra-system and inter-system clones into reusable, configurable components stored in an integrated platform and synthesize a matching multilayer feature model.","clone detection, refactoring, multi product-line, variability mining","","ICSE '21"
"Conference Paper","Heumüller R","Learning to Boost the Efficiency of Modern Code Review","","2021","","","275–277","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings","","2021","","","https://doi.org/10.1109/ICSE-Companion52605.2021.00126;http://dx.doi.org/10.1109/ICSE-Companion52605.2021.00126","10.1109/ICSE-Companion52605.2021.00126","Modern Code Review (MCR) is a standard in all kinds of organizations that develop software. MCR pays for itself through perceived and proven benefits in quality assurance and knowledge transfer. However, the time invest in MCR is generally substantial. The goal of this thesis is to boost the efficiency of MCR by developing AI techniques that can partially replace or assist human reviewers. The envisioned techniques distinguish from existing MCR-related AI models in that we interpret these challenges as graph-learning problems. This should allow us to use state-of-science algorithms from that domain to learn coding and reviewing standards directly from existing projects. The required training data will be mined from online repositories and the experiments will be designed to use standard, quantitative evaluation metrics. This research proposal defines the motivation, research-questions, and solution components for the thesis, and gives an overview of the relevant related work.","modern code review, deep learning, automated software engineering","","ICSE '21"
"Conference Paper","Chen B,Abedjan Z","RPT: Effective and Efficient Retrieval of Program Translations from Big Code","","2021","","","252–253","IEEE Press","Virtual Event, Spain","Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings","","2021","","","https://doi.org/10.1109/ICSE-Companion52605.2021.00117;http://dx.doi.org/10.1109/ICSE-Companion52605.2021.00117","10.1109/ICSE-Companion52605.2021.00117","Program translation is a growing demand in software engineering. Manual program translation requires programming expertise in source and target language. One way to automate this process is to make use of the big data of programs, i.e., Big Code. In particular, one can search for program translations in Big Code. However, existing code retrieval techniques are not designed for cross-language code retrieval. Other data-driven approaches require human efforts in constructing cross-language parallel datasets to train translation models. In this paper, we present Rpt, a novel code translation retrieval system. We propose a lightweight but informative program representation, which can be generalized to all imperative PLs. Furthermore, we present our index structure and hierarchical filtering mechanism for efficient code retrieval from a Big Code database.","","","ICSE '21"
"Conference Paper","Almonte L,Pérez-Soler S,Guerra E,Cantador I,de Lara J","Automating the Synthesis of Recommender Systems for Modelling Languages","","2021","","","22–35","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering","Chicago, IL, USA","2021","9781450391115","","https://doi.org/10.1145/3486608.3486905;http://dx.doi.org/10.1145/3486608.3486905","10.1145/3486608.3486905","We are witnessing an increasing interest in building recommender systems (RSs) for all sorts of Software Engineering activities. Modelling is no exception to this trend, as modelling environments are being enriched with RSs that help building models by providing recommendations based on previous solutions to similar problems in the same domain. However, building a RS from scratch requires considerable effort and specialized knowledge. To alleviate this problem, we propose an automated approach to the generation of RSs for modelling languages. Our approach is model-based, and we provide a domain-specific language called Droid to configure every aspect of the RS (like the type and features of the recommended items, the recommendation method, and the evaluation metrics). The RS so configured can be deployed as a service, and we offer out-of-the-box integration of this service with the EMF tree editor. To assess the usefulness of our proposal, we present a case study on the integration of a generated RS with a modelling chatbot, and report on an offline experiment measuring the precision and completeness of the recommendations.","Model-Driven Engineering, Recommender Systems, Domain-Specific Languages, Modelling Languages","","SLE 2021"
"Conference Paper","Barash M","Vision: The next 700 Language Workbenches","","2021","","","16–21","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th ACM SIGPLAN International Conference on Software Language Engineering","Chicago, IL, USA","2021","9781450391115","","https://doi.org/10.1145/3486608.3486907;http://dx.doi.org/10.1145/3486608.3486907","10.1145/3486608.3486907","Language workbenches (LWBs) are tools to define software languages together with tailored Integrated Development Environments for them. A comprehensive review of language workbenches by Erdweg et al. (Comput. Lang. Syst. Struct. 44, 2015) presented a feature model of functionality of LWBs from the point of view of ""languages that can be defined with a LWB, and not the definition mechanism of the LWB itself"". This vision paper discusses possible functionality of LWBs with regard to language definition mechanisms. We have identified five groups of such functionality, related to: metadefinitions, metamodifications, metaprocess, LWB itself, and programs written in languages defined in a LWB. We design one of the features (""ability to define dependencies between language concerns"") based on our vision.","algebraic specifications, metaprogramming, software languages, Language workbenches","","SLE 2021"
"Conference Paper","Guri M","USBCulprit: USB-Borne Air-Gap Malware","","2021","","","7–13","Association for Computing Machinery","New York, NY, USA","European Interdisciplinary Cybersecurity Conference","Virtual Event, Romania","2021","9781450390491","","https://doi.org/10.1145/3487405.3487412;http://dx.doi.org/10.1145/3487405.3487412","10.1145/3487405.3487412","Air-gapped networks are disconnected from the Internet due to the sensitive data they store and process. These networks are usually maintained by military organizations, defense industries, critical infrastructures, and more. Malware that is capable of jumping air-gaps is rare findings. In June 2020, researchers in Kaspersky security firm reported USBCulprit, an Advanced Persistent Threat (APT) which seems to be designed to reach air-gapped networks. The malware includes lateral movement, spreading, and data exfiltrations mechanisms via USB thumb drives. We tested and reverse-engineered the sample of USBCulprit, and investigated its internal design, modules, and techniques. Especially, we revised the data collection and air-gap exfiltration mechanisms. We also present a video clip showing the actual attack on our in-lab air-gapped network and discuss a set of defensive countermeasures. This analysis in important for the understanding and mitigation of USB-borne APTs.","APT, malware, Air-gap, USB, covert channels, exfiltration","","EICC"
"Conference Paper","Jacobsen B,Rahaman S,Debray S","Optimization to the Rescue: Evading Binary Code Stylometry with Adversarial Use of Code Optimizations","","2021","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 Research on Offensive and Defensive Techniques in the Context of Man At The End (MATE) Attacks","Virtual Event, Republic of Korea","2021","9781450385527","","https://doi.org/10.1145/3465413.3488574;http://dx.doi.org/10.1145/3465413.3488574","10.1145/3465413.3488574","Recent work suggests that it may be possible to determine the author of a binary program simply by analyzing stylistic features preserved within it. As this poses a threat to the privacy of programmers who wish to distribute their work anonymously, we consider steps that can be taken to mislead such analysis. We begin by exploring the effect of compiler optimizations on the features used for stylistic analysis. Building on these findings, we propose a gray-box attack on a state-of-the-art classifier using compiler optimizations. Finally, we discuss our results, as well as implications for the field of binary stylometry.","stylometry, adversarial machine learning, privacy, bayesian optimization","","Checkmate '21"
"Conference Paper","Liu D,Wu Q,Ji S,Lu K,Liu Z,Chen J,He Q","Detecting Missed Security Operations Through Differential Checking of Object-Based Similar Paths","","2021","","","1627–1644","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3485373;http://dx.doi.org/10.1145/3460120.3485373","10.1145/3460120.3485373","Missing a security operation such as a bound check has been a major cause of security-critical bugs. Automatically checking whether the code misses a security operation in large programs is challenging since it has to understand whether the security operation is indeed necessary in the context. Recent methods typically employ cross-checking to identify deviations as security bugs, which collects functionally similar program slices and infers missed security operations through majority-voting. An inherent limitation of such approaches is that they heavily rely on a substantial number of similar code pieces to enable cross-checking. In practice, many code pieces are unique, and thus we may be unable to find adequate similar code snippets to utilize cross-checking.In this paper, we present IPPO (Inconsistent Path Pairs as a bug Oracle), a static analysis framework for detecting security bugs based on differential checking. IPPO defines several novel rules to identify code paths that share similar semantics with respect to an object, and collects them as similar-path pairs. It then investigates the path pairs for identifying inconsistent security operations with respect to the object. If one path in a path pair enforces a security operation while the other does not, IPPO reports it as a potential security bug. By utilizing on object-based path-similarity analysis, IPPO achieves a higher precision, compared to conventional code-similarity analysis methods. Through differential checking of a similar-path pair, IPPO eliminates the requirement of constructing a large number of similar code pieces, addressing the limitation of traditional cross-checking approaches. We implemented IPPO and extensively evaluated it on four widely used open-source programs: Linux kernel, OpenSSL library, FreeBSD kernel, and PHP. IPPO found 154, 5, 1, and 1 new security bugs in the above systems, respectively. We have submitted patches for all these bugs, and 136 of them have been accepted by corresponding maintainers. The results confirm the effectiveness and usefulness of IPPO in practice.","static analysis, similar path, missing security operation, bug detection","","CCS '21"
"Conference Paper","Dai J,Zhang Y,Xu H,Lyu H,Wu Z,Xing X,Yang M","Facilitating Vulnerability Assessment through PoC Migration","","2021","","","3300–3317","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484594;http://dx.doi.org/10.1145/3460120.3484594","10.1145/3460120.3484594","Recent research shows that, even for vulnerability reports archived by MITRE/NIST, they usually contain incomplete information about the software's vulnerable versions, making users of under-reported vulnerable versions at risk. In this work, we address this problem by introducing a fuzzing-based method. Technically, this approach first collects the crashing trace on the reference version of the software. Then, it utilizes the trace to guide the mutation of the PoC input so that the target version could follow the trace similar to the one observed on the reference version. Under the mutated input, we argue that the target version's execution could have a higher chance of triggering the bug and demonstrating the vulnerability's existence. We implement this idea as an automated tool, named VulScope. Using 30 real-world CVEs on 470 versions of software, VulScope is demonstrated to introduce no false positives and only 7.9% false negatives while migrating PoC from one version to another. Besides, we also compare our method with two representative fuzzing tools AFL and AFLGO. We find VulScope outperforms both of these existing techniques while taking the task of PoC migration. Finally, by using VulScope, we identify 330 versions of software that MITRE/NIST fails to report as vulnerable.","trace alignment, vulnerability assessment, PoC adjustment","","CCS '21"
"Conference Paper","Fu L,Ji S,Lu K,Liu P,Zhang X,Duan Y,Zhang Z,Chen W,Wu Y","CPscan: Detecting Bugs Caused by Code Pruning in IoT Kernels","","2021","","","794–810","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484738;http://dx.doi.org/10.1145/3460120.3484738","10.1145/3460120.3484738","To reduce the development costs, IoT vendors tend to construct IoT kernels by customizing the Linux kernel. Code pruning is common in this customization process. However, due to the intrinsic complexity of the Linux kernel and the lack of long-term effective maintenance, IoT vendors may mistakenly delete necessary security operations in the pruning process, which leads to various bugs such as memory leakage and NULL pointer dereference. Yet detecting bugs caused by code pruning in IoT kernels is difficult. Specifically, (1) a significant structural change makes precisely locating the deleted security operations (DSO ) difficult, and (2) inferring the security impact of a DSO is not trivial since it requires complex semantic understanding, including the developing logic and the context of the corresponding IoT kernel.In this paper, we present CPscan, a system for automatically detecting bugs caused by code pruning in IoT kernels. First, using a new graph-based approach that iteratively conducts a structure-aware basic block matching, CPscan can precisely and efficiently identify theDSOs in IoT kernels. Then, CPscan infers the security impact of a DSO by comparing the bounded use chains (where and how a variable is used within potentially influenced code segments) of the security-critical variable associated with it. Specifically, CPscan reports the deletion of a security operation as vulnerable if the bounded use chain of the associated security-critical variable remains the same before and after the deletion. This is because the unchanged uses of a security-critical variable likely need the security operation, and removing it may have security impacts. The experimental results on 28 IoT kernels from 10 popular IoT vendors show that CPscan is able to identify 3,193DSO s and detect 114 new bugs with a reasonably low false-positive rate. Many such bugs tend to have a long latent period (up to 9 years and 5 months). We believe CPscan paves a way for eliminating the bugs introduced by code pruning in IoT kernels. We will open-source CPscan to facilitate further research.","static analysis, inconsistency analysis, missing security operation, bug detection","","CCS '21"
"Conference Paper","Tan X,Zhang Y,Mi C,Cao J,Sun K,Lin Y,Yang M","Locating the Security Patches for Disclosed OSS Vulnerabilities with Vulnerability-Commit Correlation Ranking","","2021","","","3282–3299","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484593;http://dx.doi.org/10.1145/3460120.3484593","10.1145/3460120.3484593","Security patches play an important role in defending against the security threats brought by the increasing OSS vulnerabilities. However, the collection of security patches still remains a challenging problem. Existing works mainly adopt a matching-based design that uses auxiliary information in CVE/NVD to reduce the search scope of patch commits. However, our preliminary study shows that these approaches can only cover a small part of disclosed OSS vulnerabilities (about 12%-53%) even with manual assistance.To facilitate the collection of OSS security patches, this paper proposes a ranking-based approach, named PatchScout, which ranks the code commits in the OSS code repository based on their correlations to a given vulnerability. By exploiting the broad correlations between a vulnerability and code commits, patch commits are expected to be put to front positions in the ranked results. Compared with existing works, our approach could help to locate more security patches and meet a balance between the patch coverage and the manual efforts involved. We evaluate PatchScout with 685 OSS CVEs and the results show that it helps to locate 92.70% patches with acceptable manual workload. To further demonstrate the utility of PatchScout, we perform a study on 5 popular OSS projects and 225 CVEs to understand the patch deployment practice across branches, and we obtain many new findings.","vulnerability-commit correlation, patch ranking, security patches","","CCS '21"
"Conference Paper","Li X,Qu Y,Yin H","PalmTree: Learning an Assembly Language Model for Instruction Embedding","","2021","","","3236–3251","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484587;http://dx.doi.org/10.1145/3460120.3484587","10.1145/3460120.3484587","Deep learning has demonstrated its strengths in numerous binary analysis tasks, including function boundary detection, binary code search, function prototype inference, value set analysis, etc. When applying deep learning to binary analysis tasks, we need to decide what input should be fed into the neural network model. More specifically, we need to answer how to represent an instruction in a fixed-length vector. The idea of automatically learning instruction representations is intriguing, but the existing schemes fail to capture the unique characteristics of disassembly. These schemes ignore the complex intra-instruction structures and mainly rely on control flow in which the contextual information is noisy and can be influenced by compiler optimizations. In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora. PalmTree utilizes three pre-training tasks to capture various characteristics of assembly language. These training tasks overcome the problems in existing schemes, thus can help to generate high-quality representations. We conduct both intrinsic and extrinsic evaluations, and compare PalmTree with other instruction embedding schemes. PalmTree has the best performance for intrinsic metrics, and outperforms the other instruction embedding schemes for all downstream tasks.","binary analysis, deep learning, representation learning, language model","","CCS '21"
"Conference Paper","He X,Xie X,Li Y,Sun J,Li F,Zou W,Liu Y,Yu L,Zhou J,Shi W,Huo W","SoFi: Reflection-Augmented Fuzzing for JavaScript Engines","","2021","","","2229–2242","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484823;http://dx.doi.org/10.1145/3460120.3484823","10.1145/3460120.3484823","JavaScript engines have been shown prone to security vulnerabilities, which can lead to serious consequences due to their popularity. Fuzzing is an effective testing technique to discover vulnerabilities. The main challenge of fuzzing JavaScript engines is to generate syntactically and semantically valid inputs such that deep functionalities can be explored. However, due to the dynamic nature of JavaScript and the special features of different engines, it is quite challenging to generate semantically meaningful test inputs.We observed that state-of-the-art semantic-aware JavaScript fuzzers usually require manually written rules to analyze the semantics for a JavaScript engine, which is labor-intensive, incomplete and engine-specific. Moreover, the error rate of generated test cases is still high. Another challenge is that existing fuzzers cannot generate new method calls that are not included in the initial seed corpus or pre-defined rules, which limits the bug-finding capability. To this end, we propose a novel semantic-aware fuzzing technique named SoFi. To guarantee the validity of the generated test cases, SoFi adopts a fine-grained program analysis to identify available variables and infer types of these variables for the mutation. Moreover, an automatic repair strategy is proposed to repair syntax/semantic errors in invalid test cases. To improve the exploration capability of SoFi, we propose a reflection-based analysis to identify unseen attributes and methods of objects, which are further used in the mutation. With fine-grained analysis and reflection-based augmentation, SoFi can generate more valid and diverse test cases. Besides, SoFi is general in different JavaScript engines without any manual configuration (e.g., the grammar rules). The evaluation results have shown that SoFi outperforms state-of-the-art techniques in generating semantically valid inputs, improving code coverage and detecting more bugs. SoFi discovered 51 bugs in popular JavaScript engines, 28 of which have been confirmed or fixed by the developers and 10 CVE IDs have been assigned.","fuzzing, vulnerability","","CCS '21"
"Conference Paper","Jiang M,Ma L,Zhou Y,Liu Q,Zhang C,Wang Z,Luo X,Wu L,Ren K","ECMO: Peripheral Transplantation to Rehost Embedded Linux Kernels","","2021","","","734–748","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484753;http://dx.doi.org/10.1145/3460120.3484753","10.1145/3460120.3484753","Dynamic analysis based on the full-system emulator QEMU is widely used for various purposes.However, it is challenging to run firmware images of embedded devices in QEMU, especially the process to boot the Linux kernel (we call this process rehosting the Linux kernel in this paper). That's because embedded devices usually use different system-on-chips (SoCs) from multiple vendors and only a limited number of SoCs are currently supported in QEMU.In this work, we propose a technique called peripheral transplantation. The main idea is to transplant the device drivers of designated peripherals into the Linux kernel binary. By doing so, it can replace the peripherals in the kernel that are currently unsupported in QEMU with supported ones, thus making the Linux kernel rehostable. After that, various applications can be built.We implemented this technique inside a prototype system called ECMO and applied it to 815 firmware images, which consist of 20 kernel versions and 37 device models. The result shows that ECMO can successfully transplant peripherals for all the 815 Linux kernels. Among them, 710 kernels can be successfully rehosted, i.e., launching a user-space shell (87.1% success rate). The failed cases are mainly because the root file system format (ramfs) is not supported by the kernel. Meanwhile, we are able to inject rather complex drivers (i.e., NIC driver) for all the rehosted Linux kernels by installing kernel modules. We further build three applications, i.e., kernel crash analysis, rootkit forensic analysis, and kernel fuzzing, based on the rehosted kernels to demonstrate the usage scenarios of ECMO.","Linux kernel, rehosting, peripheral transplantation","","CCS '21"
"Conference Paper","Yong Wong M,Landen M,Antonakakis M,Blough DM,Redmiles EM,Ahamad M","An Inside Look into the Practice of Malware Analysis","","2021","","","3053–3069","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484759;http://dx.doi.org/10.1145/3460120.3484759","10.1145/3460120.3484759","Malware analysis aims to understand how malicious software carries out actions necessary for a successful attack and identify the possible impacts of the attack. While there has been substantial research focused on malware analysis and it is an important tool for practitioners in industry, the overall malware analysis process used by practitioners has not been studied. As a result, an understanding of common malware analysis workflows and their goals is lacking. A better understanding of these workflows could help identify new research directions that are impactful in practice. In order to better understand malware analysis processes, we present the results of a user study with 21 professional malware analysts with diverse backgrounds who work at 18 different companies. The study focuses on answering three research questions: (1) What are the different objectives of malware analysts in practice?, (2) What comprises a typical professional malware analyst workflow, and (3) When analysts decide to conduct dynamic analysis, what factors do they consider when setting up a dynamic analysis system? Based on participant responses, we propose a taxonomy of malware analysts and identify five common analysis workflows. We also identify challenges that analysts face during the different stages of their workflow. From the results of the study, we propose two potential directions for future research, informed by challenges described by the participants. Finally, we recommend guidelines for developers of malware analysis tools to consider in order to improve the usability of such tools.","malware analysis, usable security","","CCS '21"
"Conference Paper","Chen W,Wang Y,Zhang Z,Qian Z","SyzGen: Automated Generation of Syscall Specification of Closed-Source MacOS Drivers","","2021","","","749–763","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, Republic of Korea","2021","9781450384544","","https://doi.org/10.1145/3460120.3484564;http://dx.doi.org/10.1145/3460120.3484564","10.1145/3460120.3484564","Kernel drivers are a critical part of the attack surface since they constitute a large fraction of kernel codebase and oftentimes lack proper vetting, especially for those closed-source ones. Unfortunately, the complex input structure and unknown relationships/dependencies among interfaces make them very challenging to understand. Thus, security analysts primarily rely on manual audit for interface recovery to generate meaningful fuzzing test cases. In this paper, we present SyzGen, a first attempt to automate the generation of syscall specifications for closed-source macOS drivers and facilitate interface-aware fuzzing. We leverage two insights to overcome the challenges of binary analysis: (1) iterative refinement of syscall knowledge and (2) extraction and extrapolation of dependencies from a small number of execution traces. We evaluated our approach on 25 targets. The results show that SyzGen can effectively produce high-quality specifications, leading to 34 bugs, including one that attackers can exploit to escalate privilege, and 2 CVEs to date.","vulnerability discovery, fuzzing, operating systems security","","CCS '21"
"Conference Paper","Grund F,Chowdhury S,Bradley NC,Hall B,Holmes R","CodeShovel: Constructing Method-Level Source Code Histories","","2021","","","1510–1522","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00135;http://dx.doi.org/10.1109/ICSE43902.2021.00135","10.1109/ICSE43902.2021.00135","Source code histories are commonly used by developers and researchers to reason about how software evolves. Through a survey with 42 professional software developers, we learned that developers face significant mismatches between the output provided by developers' existing tools for examining source code histories and what they need to successfully complete their historical analysis tasks. To address these shortcomings, we propose CodeShovel, a tool for uncovering method histories that quickly produces complete and accurate change histories for 90% methods (including 97% of all method changes) outperforming leading tools from both research (e.g, FinerGit) and practice (e.g., IntelliJ / git log). CodeShovel helps developers to navigate the entire history of source code methods so they can better understand how the method evolved. A field study on industrial code bases with 16 industrial developers confirmed our empirical findings of CodeShovel's correctness, low runtime overheads, and additionally showed that the approach can be useful for a wide range of industrial development tasks.","","","ICSE '21"
"Conference Paper","Woo S,Park S,Kim S,Lee H,Oh H","Centris: A Precise and Scalable Approach for Identifying Modified Open-Source Software Reuse","","2021","","","860–872","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00083;http://dx.doi.org/10.1109/ICSE43902.2021.00083","10.1109/ICSE43902.2021.00083","Open-source software (OSS) is widely reused as it provides convenience and efficiency in software development. Despite evident benefits, unmanaged OSS components can introduce threats, such as vulnerability propagation and license violation. Unfortunately, however, identifying reused OSS components is a challenge as the reused OSS is predominantly modified and nested. In this paper, we propose CENTRIS, a precise and scalable approach for identifying modified OSS reuse. By segmenting an OSS code base and detecting the reuse of a unique part of the OSS only, CENTRIS is capable of precisely identifying modified OSS reuse in the presence of nested OSS components. For scalability, CENTRIS eliminates redundant code comparisons and accelerates the search using hash functions. When we applied CENTRIS on 10,241 widely-employed GitHub projects, comprising 229,326 versions and 80 billion lines of code, we observed that modified OSS reuse is a norm in software development, occurring 20 times more frequently than exact reuse. Nonetheless, CENTRIS identified reused OSS components with 91% precision and 94% recall in less than a minute per application on average, whereas a recent clone detection technique, which does not take into account modified and nested OSS reuse, hardly reached 10% precision and 40% recall.","Open-Source Software, Software Composition Analysis, Software Security","","ICSE '21"
"Conference Paper","Bui ND,Yu Y,Jiang L","InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees","","2021","","","1186–1197","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00109;http://dx.doi.org/10.1109/ICSE43902.2021.00109","10.1109/ICSE43902.2021.00109","Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units.We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.","","","ICSE '21"
"Conference Paper","Fan Y,Xia X,Lo D,Hassan AE,Wang Y,Li S","A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms","","2021","","","1174–1185","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00108;http://dx.doi.org/10.1109/ICSE43902.2021.00108","10.1109/ICSE43902.2021.00108","Abstract syntax tree (AST) mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorithm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback, we observe that our approach achieves a precision of 0.98-1.00 and a recall of 0.65-0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%-29%, 25%-36% and 21%-30% of the file revisions, respectively. Our experimental results show that state-of-the-art AST mapping algorithms still need improvements.","software evolution, abstract syntax trees, Program element mapping","","ICSE '21"
"Conference Paper","Jiang N,Lutellier T,Tan L","CURE: Code-Aware Neural Machine Translation for Automatic Program Repair","","2021","","","1161–1173","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00107;http://dx.doi.org/10.1109/ICSE43902.2021.00107","10.1109/ICSE43902.2021.00107","Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.","automatic program repair, software reliability","","ICSE '21"
"Conference Paper","Hata H,Kula RG,Ishio T,Treude C","Same File, Different Changes: The Potential of Meta-Maintenance on GitHub","","2021","","","773–784","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00076;http://dx.doi.org/10.1109/ICSE43902.2021.00076","10.1109/ICSE43902.2021.00076","Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance---i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential.","","","ICSE '21"
"Conference Paper","Nusrat F,Hassan F,Zhong H,Wang X","How Developers Optimize Virtual Reality Applications: A Study of Optimization Commits in Open Source Unity Projects","","2021","","","473–485","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00052;http://dx.doi.org/10.1109/ICSE43902.2021.00052","10.1109/ICSE43902.2021.00052","Virtual Reality (VR) is an emerging technique that provides immersive experience for users. Due to the high computation cost of rendering real-time animation twice (for both eyes) and the resource limitation of wearable devices, VR applications often face performance bottlenecks and performance optimization plays an important role in VR software development. Performance optimizations of VR applications can be very different from those in traditional software as VR involves more elements such as graphics rendering and real-time animation. In this paper, we present the first empirical study on 183 real-world performance optimizations from 45 VR software projects. In particular, we manually categorized the optimizations into 11 categories, and applied static analysis to identify how they affect different life-cycle phases of VR applications. Furthermore, we studied the complexity and design / behavior effects of performance optimizations, and how optimizations are different between large organizational software projects and smaller personal software projects. Our major findings include: (1) graphics simplification (24.0%), rendering optimization (16.9%), language / API optimization (15.3%), heap avoidance (14.8%), and value caching (12.0%) are the most common categories of performance optimization in VR applications; (2) game logic updates (30.4%) and before-scene initialization (20.0%) are the most common life-cycle phases affected by performance issues; (3) 45.9% of the optimizations have behavior and design effects and 39.3% of the optimizations are systematic changes; (4) the distributions of optimization classes are very different between organizational VR projects and personal VR projects.","Performance Optimization, Virtual Reality, Empirical Study","","ICSE '21"
"Conference Paper","Li Y,Wang S,Nguyen TN","A Context-Based Automated Approach for Method Name Consistency Checking and Suggestion","","2021","","","574–586","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00060;http://dx.doi.org/10.1109/ICSE43902.2021.00060","10.1109/ICSE43902.2021.00060","Misleading method names in software projects can confuse developers, which may lead to software defects and affect code understandability. In this paper, we present DeepName, a context-based, deep learning approach to detect method name inconsistencies and suggest a proper name for a method. The key departure point is the philosophy of ""Show Me Your Friends, I'll Tell You Who You Are"". Unlike the state-of-the-art approaches, in addition to the method's body, we also consider the interactions of the current method under study with the other ones including the caller and callee methods, and the sibling methods in the same enclosing class. The sequences of sub-tokens in the program entities' names in the contexts are extracted and used as the input for an RNN-based encoder-decoder to produce the representations for the current method. We modify that RNN model to integrate the copy mechanism and our newly developed component, called the non-copy mechanism, to emphasize on the possibility of a certain sub-token not to be copied to follow the current sub-token in the currently generated method name.We conducted several experiments to evaluate DEEPNAME on large datasets with +14M methods. For consistency checking, DeepName improves the state-of-the-art approach by 2.1%, 19.6%, and 11.9% relatively in recall, precision, and F-score, respectively. For name suggestion, DeepName improves relatively over the state-of-the-art approaches in precision (1.8%-30.5%), recall (8.8%-46.1%), and F-score (5.2%-38.2%). To assess DEEPNAME's usefulness, we detected inconsistent methods and suggested new method names in active projects. Among 50 pull requests, 12 were merged into the main branch. In total, in 30/50 cases, the team members agree that our suggested method names are more meaningful than the current names.","Inconsistent Method Name Checking, Naturalness of Software, Deep Learning, Entity Name Suggestion","","ICSE '21"
"Conference Paper","Xu X,Zheng Q,Yan Z,Fan M,Jia A,Liu T","Interpretation-Enabled Software Reuse Detection Based on a Multi-Level Birthmark Model","","2021","","","873–884","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00084;http://dx.doi.org/10.1109/ICSE43902.2021.00084","10.1109/ICSE43902.2021.00084","Software reuse, especially partial reuse, poses legal and security threats to software development. Since its source codes are usually unavailable, software reuse is hard to be detected with interpretation. On the other hand, current approaches suffer from poor detection accuracy and efficiency, far from satisfying practical demands. To tackle these problems, in this paper, we propose ISRD, an interpretation-enabled software reuse detection approach based on a multi-level birthmark model that contains function level, basic block level, and instruction level. To overcome obfuscation caused by cross-compilation, we represent function semantics with Minimum Branch Path (MBP) and perform normalization to extract core semantics of instructions. For efficiently detecting reused functions, a process for ""intent search based on anchor recognition"" is designed to speed up reuse detection. It uses strict instruction match and identical library call invocation check to find anchor functions (in short anchors) and then traverses neighbors of the anchors to explore potentially matched function pairs. Extensive experiments based on two real-world binary datasets reveal that ISRD is interpretable, effective, and efficient, which achieves 97.2% precision and 94.8% recall. Moreover, it is resilient to cross-compilation, outperforming state-of-the-art approaches.","Software Reuse Detection, Multi-Level Software Birthmark, Interpretation, Binary Similarity Analysis","","ICSE '21"
"Conference Paper","Li Y,Wang S,Nguyen TN","Fault Localization with Code Coverage Representation Learning","","2021","","","661–673","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00067;http://dx.doi.org/10.1109/ICSE43902.2021.00067","10.1109/ICSE43902.2021.00067","In this paper, we propose DEEPRL4FL, a deep learning fault localization (FL) approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem. DEEPRL4FL does so via novel code coverage representation learning (RL) and data dependencies RL for program statements. Those two types of RL on the dynamic information in a code coverage matrix are also combined with the code representation learning on the static information of the usual suspicious source code. This combination is inspired by crime scene investigation in which investigators analyze the crime scene (failed test cases and statements) and related persons (statements with dependencies), and at the same time, examine the usual suspects who have committed a similar crime in the past (similar buggy code in the training data).For the code coverage information, DEEPRL4FL first orders the test cases and marks error-exhibiting code statements, expecting that a model can recognize the patterns discriminating between faulty and non-faulty statements/methods. For dependencies among statements, the suspiciousness of a statement is seen taking into account the data dependencies to other statements in execution and data flows, in addition to the statement by itself. Finally, the vector representations for code coverage matrix, data dependencies among statements, and source code are combined and used as the input of a classifier built from a Convolution Neural Network to detect buggy statements/methods. Our empirical evaluation shows that DEEPRL4FL improves the top-1 results over the state-of-the-art statement-level FL baselines from 173.1% to 491.7%. It also improves the top-1 results over the existing method-level FL baselines from 15.0% to 206.3%.","fault localization, deep learning, code coverage, representation learning, machine learning","","ICSE '21"
"Conference Paper","Mahmood W,Strüber D,Berger T,Lämmel R,Mukelabai M","Seamless Variability Management With the Virtual Platform","","2021","","","1658–1670","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00147;http://dx.doi.org/10.1109/ICSE43902.2021.00147","10.1109/ICSE43902.2021.00147","Customization is a general trend in software engineering, demanding systems that support variable stakeholder requirements. Two opposing strategies are commonly used to create variants: software clone&own and software configuration with an integrated platform. Organizations often start with the former, which is cheap, agile, and supports quick innovation, but does not scale. The latter scales by establishing an integrated platform that shares software assets between variants, but requires high up-front investments or risky migration processes. So, could we have a method that allows an easy transition or even combine the benefits of both strategies? We propose a method and tool that supports a truly incremental development of variant-rich systems, exploiting a spectrum between both opposing strategies. We design, formalize, and prototype the variability-management framework virtual platform. It bridges clone&own and platform-oriented development. Relying on programming-language-independent conceptual structures representing software assets, it offers operators for engineering and evolving a system, comprising: traditional, asset-oriented operators and novel, feature-oriented operators for incrementally adopting concepts of an integrated platform. The operators record meta-data that is exploited by other operators to support the transition. Among others, they eliminate expensive feature-location effort or the need to trace clones. Our evaluation simulates the evolution of a real-world, clone-based system, measuring its costs and benefits.","software product lines, clone management, re-engineering, variability management, framework","","ICSE '21"
"Conference Paper","Tang Y,Khatchadourian R,Bagherzadeh M,Singh R,Stewart A,Raja A","An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems","","2021","","","238–250","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00033;http://dx.doi.org/10.1109/ICSE43902.2021.00033","10.1109/ICSE43902.2021.00033","Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major crosscutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.","software repository mining, refactoring, technical debt, machine learning systems, empirical studies","","ICSE '21"
"Conference Paper","Kim DJ,Tsantalis N,Chen TH,Yang J","Studying Test Annotation Maintenance in the Wild","","2021","","","62–73","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00019;http://dx.doi.org/10.1109/ICSE43902.2021.00019","10.1109/ICSE43902.2021.00019","Since the introduction of annotations in Java 5, the majority of testing frameworks, such as JUnit, TestNG, and Mockito, have adopted annotations in their core design. This adoption affected the testing practices in every step of the test life-cycle, from fixture setup and test execution to fixture teardown. Despite the importance of test annotations, most research on test maintenance has mainly focused on test code quality and test assertions. As a result, there is little empirical evidence on the evolution and maintenance of test annotations. To fill this gap, we perform the first fine-grained empirical study on annotation changes. We developed a tool to mine 82,810 commits and detect 23,936 instances of test annotation changes from 12 open-source Java projects. Our main findings are: (1) Test annotation changes are more frequent than rename and type change refactorings. (2) We recover various migration efforts within the same testing framework or between different frameworks by analyzing common annotation replacement patterns. (3) We create a taxonomy by manually inspecting and classifying a sample of 368 test annotation changes and documenting the motivations driving these changes. Finally, we present a list of actionable implications for developers, researchers, and framework designers.","Software Quality, Annotation, Empirical Study, Software Evolution","","ICSE '21"
"Conference Paper","Wang Y,Qiao L,Xu C,Liu Y,Cheung SC,Meng N,Yu H,Zhu Z","Hero: On the Chaos When PATH Meets Modules","","2021","","","99–111","IEEE Press","Madrid, Spain","Proceedings of the 43rd International Conference on Software Engineering","","2021","9781450390859","","https://doi.org/10.1109/ICSE43902.2021.00022;http://dx.doi.org/10.1109/ICSE43902.2021.00022","10.1109/ICSE43902.2021.00022","Ever since its first release in 2009, the Go programming language (Golang) has been well received by software communities. A major reason for its success is the powerful support of library-based development, where a Golang project can be conveniently built on top of other projects by referencing them as libraries. As Golang evolves, it recommends the use of a new library-referencing mode to overcome the limitations of the original one. While these two library modes are incompatible, both are supported by the Golang ecosystem. The heterogeneous use of library-referencing modes across Golang projects has caused numerous dependency management (DM) issues, incurring reference inconsistencies and even build failures. Motivated by the problem, we conducted an empirical study to characterize the DM issues, understand their root causes, and examine their fixing solutions. Based on our findings, we developed HERO, an automated technique to detect DM issues and suggest proper fixing solutions. We applied HERO to 19,000 popular Golang projects. The results showed that HERO achieved a high detection rate of 98.5% on a DM issue benchmark and found 2,422 new DM issues in 2,356 popular Golang projects. We reported 280 issues, among which 181 (64.6%) issues have been confirmed, and 160 of them (88.4%) have been fixed or are under fixing. Almost all the fixes have adopted our fixing suggestions.","Golang Ecosystem, Dependency Management","","ICSE '21"
"Journal Article","Lee DJ,Tang D,Agarwal K,Boonmark T,Chen C,Kang J,Mukhopadhyay U,Song J,Yong M,Hearst MA,Parameswaran AG","Lux: Always-on Visualization Recommendations for Exploratory Dataframe Workflows","Proc. VLDB Endow.","2021","15","3","727–738","VLDB Endowment","","","","2021-11","","2150-8097","https://doi.org/10.14778/3494124.3494151;http://dx.doi.org/10.14778/3494124.3494151","10.14778/3494124.3494151","Exploratory data science largely happens in computational notebooks with dataframe APIs, such as pandas, that support flexible means to transform, clean, and analyze data. Yet, visually exploring data in dataframes remains tedious, requiring substantial programming effort for visualization and mental effort to determine what analysis to perform next. We propose Lux, an always-on framework for accelerating visual insight discovery in dataframe workflows. When users print a dataframe in their notebooks, Lux recommends visualizations to provide a quick overview of the patterns and trends and suggests promising analysis directions. Lux features a high-level language for generating visualizations on demand to encourage rapid visual experimentation with data. We demonstrate that through the use of a careful design and three system optimizations, Lux adds no more than two seconds of overhead on top of pandas for over 98% of datasets in the UCI repository. We evaluate Lux in terms of usability via interviews with early adopters, finding that Lux helps fulfill the needs of data scientists for visualization support within their dataframe workflows. Lux has already been embraced by data science practitioners, with over 3.1k stars on Github.","","",""
"Conference Paper","Wang J,Yang J,Zhang W","Top-k Tree Similarity Join","","2021","","","1939–1948","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM International Conference on Information & Knowledge Management","Virtual Event, Queensland, Australia","2021","9781450384469","","https://doi.org/10.1145/3459637.3482304;http://dx.doi.org/10.1145/3459637.3482304","10.1145/3459637.3482304","Tree similarity join is useful for analyzing tree structured data. The traditional threshold-based tree similarity join requires a similarity threshold, which is usually a difficult task for users. To remedy this issue, we advocate the problem of top-k tree similarity join. Given a collection of trees and a parameter k, the top-k tree similarity join aims to find k tree pairs with minimum tree edit distance (TED). Although we show that this problem can be resolved by utilizing the threshold-based join, the efficiency is unsatisfactory. In this paper, we propose an efficient algorithm, namely TopKTJoin, which generates the candidate tree pairs incrementally using an inverted index. We also derive TED lower bound for the unseen tree pairs. Together with TED value of the k-th best join result seen so far, we have a chance to terminate the algorithm early without missing any correct results. To further improve the efficiency, we propose two optimization techniques in terms of index structure and verification mechanism. We conduct comprehensive performance studies on real and synthetic datasets. The experimental results demonstrate that TopKTJoin significantly outperforms the baseline method.","label histogram, tree similarity join, inverted index, tree edit distance, incremental algorithm","","CIKM '21"
"Journal Article","Liu C,Gao C,Xia X,Lo D,Grundy J,Yang X","On the Reproducibility and Replicability of Deep Learning in Software Engineering","ACM Trans. Softw. Eng. Methodol.","2021","31","1","","Association for Computing Machinery","New York, NY, USA","","","2021-10","","1049-331X","https://doi.org/10.1145/3477535;http://dx.doi.org/10.1145/3477535","10.1145/3477535","Context: Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge.Objective: Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) reproducibility—whether the reported experimental results can be obtained by other researchers using authors’ artifacts (i.e., source code and datasets) with the same experimental setup; and (2) replicability—whether the reported experimental result can be obtained by other researchers using their re-implemented artifacts with a different experimental setup. We observed that DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process, unlike classical supervised machine learning (ML) methods (e.g., random forest). This study aims to investigate the urgency and importance of reproducibility and replicability for DL studies on SE tasks.Method: In this study, we conducted a literature review on 147 DL studies recently published in 20 SE venues and 20 AI (Artificial Intelligence) venues to investigate these issues. We also re-ran four representative DL models in SE to investigate important factors that may strongly affect the reproducibility and replicability of a study.Results: Our statistics show the urgency of investigating these two factors in SE, where only 10.2% of the studies investigate any research question to show that their models can address at least one issue of replicability and/or reproducibility. More than 62.6% of the studies do not even share high-quality source code or complete data to support the reproducibility of their complex models. Meanwhile, our experimental results show the importance of reproducibility and replicability, where the reported performance of a DL model could not be reproduced for an unstable optimization process. Replicability could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data.Conclusion: It is urgent for the SE community to provide a long-lasting link to a high-quality reproduction package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.","reproducibility, replicability, Deep learning, software engineering","",""
"Conference Paper","Verma A,Udhayanan P,Shankar RM,Kn N,Chakrabarti SK","Source-Code Similarity Measurement: Syntax Tree Fingerprinting for Automated Evaluation","","2021","","","","Association for Computing Machinery","New York, NY, USA","The First International Conference on AI-ML-Systems","Bangalore, India","2021","9781450385947","","https://doi.org/10.1145/3486001.3486228;http://dx.doi.org/10.1145/3486001.3486228","10.1145/3486001.3486228","A majority of the current automated evaluation tools focus on grading a program based only on functionally testing the outputs. This approach suffers both false positives (i.e. finding errors where there are not any) and false negatives (missing out on actual errors). In this paper, we present a novel system which emulates manual evaluation of programming assignments based on the structure and not the functional output of the program using structural similarity between the given program and a reference solution. We propose an evaluation rubric for scoring structural similarity with respect to a reference solution. We present an ML based approach to map the system predicted scores to the scores computed using the rubric. Empirical evaluation of the system is done on a corpus of Python programs extracted from the popular programming platform, HackerRank, in combination with programming assignments submitted by students undertaking an undergraduate Python programming course. The preliminary results have been encouraging with the errors reported being as low as 12 percent with a deviation of about 3 percent, showing that the automatically generated scores are in high correlation with the instructor assigned scores.","Syntax Tree Fingerprinting, Program Structural Similarity, Evaluation Rubric, Automated Evaluation","","AIMLSystems 2021"
"Journal Article","Ullah F,Naeem MR,Bajahzar AS,Al-Turjman F","IoT-Based Cloud Service for Secured Android Markets Using PDG-Based Deep Learning Classification","ACM Trans. Internet Technol.","2021","22","2","","Association for Computing Machinery","New York, NY, USA","","","2021-10","","1533-5399","https://doi.org/10.1145/3418206;http://dx.doi.org/10.1145/3418206","10.1145/3418206","Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48% among clones from different application stores.","deep learning, program dependency graph, Clone detection, cloud services, Internet of Things","",""
"Conference Paper","Obermüller F,Bloch L,Greifenstein L,Heuer U,Fraser G","Code Perfumes: Reporting Good Code to Encourage Learners","","2021","","","","Association for Computing Machinery","New York, NY, USA","The 16th Workshop in Primary and Secondary Computing Education","Virtual Event, Germany","2021","9781450385718","","https://doi.org/10.1145/3481312.3481346;http://dx.doi.org/10.1145/3481312.3481346","10.1145/3481312.3481346","Block-based programming languages like enable children to be creative while learning to program. Even though the block-based approach simplifies the creation of programs, learning to program can nevertheless be challenging. Automated tools such as linters therefore support learners by providing feedback about potential bugs or code smells in their programs. Even when this feedback is elaborate and constructive, it still represents purely negative criticism and by construction ignores what learners have done correctly in their programs. In this paper we introduce an orthogonal approach to linting: We complement the criticism produced by a linter with positive feedback. We introduce the concept of code perfumes as the counterpart to code smells, indicating the correct application of programming practices considered to be good. By analysing not only what learners did wrong but also what they did right we hope to encourage learners, to provide teachers and students a better understanding of learners’ progress, and to support the adoption of automated feedback tools. Using a catalogue of 25 code perfumes for, we empirically demonstrate that these represent frequent practices in, and we find that better programs indeed contain more code perfumes.","Code quality, Linting, Block-based programming, Scratch","","WiPSCE '21"
"Conference Paper","Fu Y,Osei-Owusu J,Astorga A,Zhao ZN,Zhang W,Xie T","PaCon: A Symbolic Analysis Approach for Tactic-Oriented Clustering of Programming Submissions","","2021","","","32–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGPLAN International Symposium on SPLASH-E","Chicago, IL, USA","2021","9781450390897","","https://doi.org/10.1145/3484272.3484963;http://dx.doi.org/10.1145/3484272.3484963","10.1145/3484272.3484963","Enrollment in programming courses increasingly surges. To maintain the quality of education in programming courses, instructors need ways to understand the performance of students and give feedback accordingly at scale. For example, it is important for instructors to identify different problem-solving ways (named as tactics in this paper) used in programming submissions. However, because there exist many abstraction levels of tactics and high implementation diversity of the same tactic, it is challenging and time-consuming for instructors to manually tackle the task of tactic identification. Toward this task, we propose PaCon, a symbolic analysis approach for clustering functionally correct programming submissions to provide a way of identifying tactics. In particular, PaCon clusters submissions according to path conditions, a semantic feature of programs. Because of the focus on program semantics, PaCon does not struggle with the issue of an excessive number of clusters caused by subtle syntactic differences between submissions. Our experimental results on real-world data sets show that PaCon can produce a reasonable number of clusters each of which effectively groups together those submissions with high syntax diversity while sharing equivalent path-condition-based semantics, providing a promising way toward identifying tactics.","symbolic analysis, path condition, programming education, clustering","","SPLASH-E 2021"
"Conference Paper","Trilla D,Wellman JD,Buyuktosunoglu A,Bose P","NOVIA: A Framework for Discovering Non-Conventional Inline Accelerators","","2021","","","507–521","Association for Computing Machinery","New York, NY, USA","MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture","Virtual Event, Greece","2021","9781450385572","","https://doi.org/10.1145/3466752.3480094;http://dx.doi.org/10.1145/3466752.3480094","10.1145/3466752.3480094","Accelerators provide an increasingly valuable source of performance in modern computing systems. In most cases, accelerators are implemented as stand-alone, offload engines to which the processor can send large computation tasks. For many edge devices, as performance needs increase accelerators become essential, but the tight constraints on these devices limit the extent to which offload engines can be incorporated. An alternative is inline accelerators, which can be integrated as part of the core and provide performance with much smaller start-up times and area overheads. While inline accelerators allow greater flexibility in the interface and acceleration of finer grain code, determining good inline candidate accelerators is non-trivial. In this paper, we present NOVIA, a framework to derive inline accelerators by examining the workload source code and identifying inline accelerator candidates that provide benefits across many different regions of the workload. These NOVIA-derived accelerators are then integrated into an embedded core. For this core, NOVIA produces inline accelerators that improve the performance of various benchmark suites like EEMBC Autobench 2.0 and Mediabench by 1.37x with only a 3% core area increase.","hardware-software co-design, inline accelerator, accelerator discovery","","MICRO '21"
"Conference Paper","Luo X,Wu D,Ma Z,Chen C,Deng M,Huang J,Hua XS","A Statistical Approach to Mining Semantic Similarity for Deep Unsupervised Hashing","","2021","","","4306–4314","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM International Conference on Multimedia","Virtual Event, China","2021","9781450386517","","https://doi.org/10.1145/3474085.3475570;http://dx.doi.org/10.1145/3474085.3475570","10.1145/3474085.3475570","The majority of deep unsupervised hashing methods usually first construct pairwise semantic similarity information and then learn to map images into compact hash codes while preserving the similarity structure, which implies that the quality of hash codes highly depends on the constructed semantic similarity structure. However, since the features of images for each kind of semantics usually scatter in high-dimensional space with unknown distribution, previous methods could introduce a large number of false positives and negatives for boundary points of distributions in the local semantic structure based on pairwise cosine distances. Towards this limitation, we propose a general distribution-based metric to depict the pairwise distance between images. Specifically, each image is characterized by its random augmentations that can be viewed as samples from the corresponding latent semantic distribution. Then we estimate the distances between images by calculating the sample distribution divergence of their semantics. By applying this new metric to deep unsupervised hashing, we come up with Distribution-based similArity sTructure rEconstruction (DATE). DATE can generate more accurate semantic similarity information by using non-parametric ball divergence. Moreover, DATE explores both semantic-preserving learning and contrastive learning to obtain high-quality hash codes. Extensive experiments on several widely-used datasets validate the superiority of our DATE.","statistical approach, image retrieval, deep unsupervised hashing","","MM '21"
"Conference Paper","Fernandez-Reyes K,Gariano IO,Noble J,Greenwood-Thessman E,Homer M,Wrigstad T","Dala: A Simple Capability-Based Dynamic Language Design for Data Race-Freedom","","2021","","","1–17","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Chicago, IL, USA","2021","9781450391108","","https://doi.org/10.1145/3486607.3486747;http://dx.doi.org/10.1145/3486607.3486747","10.1145/3486607.3486747","Dynamic languages like Erlang, Clojure, JavaScript, and E adopted data-race freedom by design. To enforce data-race freedom, these languages either deep copy objects during actor (thread) communication or proxy back to their owning thread. We present Dala, a simple programming model that ensures data-race freedom while supporting efficient inter-thread communication. Dala is a dynamic, concurrent, capability-based language that relies on three core capabilities: immutable values can be shared freely; isolated mutable objects can be transferred between threads but not aliased; local objects can be aliased within their owning thread but not dereferenced by other threads. Objects with capabilities can co-exist with unsafe objects, that are unchecked and may suffer data races, without compromising the safety of safe objects. We present a formal model of Dala, prove data race-freedom and state and prove a dynamic gradual guarantee. These theorems guarantee data race-freedom when using safe capabilities and show that the addition of capabilities is semantics preserving modulo permission and cast errors.","immutability, capability, isolation, concurrency, permission","","Onward! 2021"
"Conference Paper","Gupta U,Hsia S,Zhang J,Wilkening M,Pombra J,Lee HH,Wei GY,Wu CJ,Brooks D","RecPipe: Co-Designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance","","2021","","","870–884","Association for Computing Machinery","New York, NY, USA","MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture","Virtual Event, Greece","2021","9781450385572","","https://doi.org/10.1145/3466752.3480127;http://dx.doi.org/10.1145/3466752.3480127","10.1145/3466752.3480127","Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 × and 6 ×.","hardware accelerator, deep learning, personalized recommendation, datacenter","","MICRO '21"
"Journal Article","Brown MD,Pruett M,Bigelow R,Mururu G,Pande S","Not so Fast: Understanding and Mitigating Negative Impacts of Compiler Optimizations on Code Reuse Gadget Sets","Proc. ACM Program. Lang.","2021","5","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2021-10","","","https://doi.org/10.1145/3485531;http://dx.doi.org/10.1145/3485531","10.1145/3485531","Despite extensive testing and correctness certification of their functional semantics, a number of compiler optimizations have been shown to violate security guarantees implemented in source code. While prior work has shed light on how such optimizations may introduce semantic security weaknesses into programs, there remains a significant knowledge gap concerning the impacts of compiler optimizations on non-semantic properties with security implications. In particular, little is currently known about how code generation and optimization decisions made by the compiler affect the availability and utility of reusable code segments called gadgets required for implementing code reuse attack methods such as return-oriented programming. In this paper, we bridge this gap through a study of the impacts of compiler optimization on code reuse gadget sets. We analyze and compare 1,187 variants of 20 different benchmark programs built with two production compilers (GCC and Clang) to determine how their optimization behaviors affect the code reuse gadget sets present in program variants with respect to both quantitative and qualitative metrics. Our study exposes an important and unexpected problem; compiler optimizations introduce new gadgets at a high rate and produce code containing gadget sets that are generally more useful to an attacker than those in unoptimized code. Using differential binary analysis, we identify several undesirable behaviors at the root of this phenomenon. In turn, we propose and evaluate several strategies to mitigate these behaviors. In particular, we show that post-production binary recompilation can effectively mitigate these behaviors with negligible performance impacts, resulting in optimized code with significantly smaller and less useful gadget sets.","Computer security, Code reuse gadgets, Code generation, Compilers, Code reuse attacks, Software security, Code optimization, Binary recompilation","",""
"Journal Article","Gao X,Radhakrishna A,Soares G,Shariffdeen R,Gulwani S,Roychoudhury A","APIfix: Output-Oriented Program Synthesis for Combating Breaking Changes in Libraries","Proc. ACM Program. Lang.","2021","5","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2021-10","","","https://doi.org/10.1145/3485538;http://dx.doi.org/10.1145/3485538","10.1145/3485538","Use of third-party libraries is extremely common in application software. The libraries evolve to accommodate new features or mitigate security vulnerabilities, thereby breaking the Application Programming Interface(API) used by the software. Such breaking changes in the libraries may discourage client code from using the new library versions thereby keeping the application vulnerable and not up-to-date. We propose a novel output-oriented program synthesis algorithm to automate API usage adaptations via program transformation. Our aim is not only to rely on the few example human adaptations of the clients from the old library version to the new library version, since this can lead to over-fitting transformation rules. Instead, we also rely on example usages of the new updated library in clients, which provide valuable context for synthesizing and applying the transformation rules. Our tool APIFix provides an automated mechanism to transform application code using the old library versions to code using the new library versions - thereby achieving automated API usage adaptation to fix the effect of breaking changes. Our evaluation shows that the transformation rules inferred by APIFix achieve 98.7% precision and 91.5% recall. By comparing our approach to state-of-the-art program synthesis approaches, we show that our approach significantly reduces over-fitting while synthesizing transformation rules for API usage adaptations.","Program transformation, API usage adaptation, Programming by example, Program synthesis, Breaking changes","",""
"Journal Article","Madsen M,van de Pol J","Relational Nullable Types with Boolean Unification","Proc. ACM Program. Lang.","2021","5","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2021-10","","","https://doi.org/10.1145/3485487;http://dx.doi.org/10.1145/3485487","10.1145/3485487","We present a simple, practical, and expressive relational nullable type system. A relational nullable type system captures whether an expression may evaluate to null based on its type, but also based on the type of other related expressions. The type system extends the Hindley-Milner type system with Boolean constraints, supports parametric polymorphism, and preserves principal types modulo Boolean equivalence. We show how to support full Hindley-Milner style type inference with an extension of Algorithm W. We conduct a preliminary study of open source projects showing that there is a need for relational nullable type systems across a wide range of programming languages. The most important findings from the study are: (i) programmers use programming patterns where the nullability of one expression depends on the nullability of other related expressions, (ii) such invariants are commonly enforced with run-time exceptions, and (iii) reasoning about these programming patterns requires not only knowledge of when an expression may evaluate to null, but also when it may evaluate to a non-null value. We incorporate these observations in the design of the proposed relational nullable type system.","relational pattern matching, relational nullable type system, choose construct, type inference, Boolean unification, successive variable elimination algorithm, Algorithm W","",""
"Conference Paper","Ni W,Sunshine J,Le V,Gulwani S,Barik T","ReCode : A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example","","2021","","","258–269","Association for Computing Machinery","New York, NY, USA","The 34th Annual ACM Symposium on User Interface Software and Technology","Virtual Event, USA","2021","9781450386357","","https://doi.org/10.1145/3472749.3474748;http://dx.doi.org/10.1145/3472749.3474748","10.1145/3472749.3474748","Software developers frequently confront a recurring challenge of making code transformations—similar but not entirely identical code changes in many places—in their integrated development environments. Through formative interviews (n = 7), we found that developers were aware of many tools intended to help with code transformations, but often made their changes manually because these tools required too much expertise or effort to be able to use effectively. To address these needs, we built an extension for Visual Studio Code, called reCode. reCode improves the familiar find-and-replace experience by allowing the developer to specify a straightforward search term to identify relevant locations, and then demonstrate their intended changes by simply typing a change directly in the editor. Using programming by example, reCode automatically learns a more general code transformation and displays these transformations as before-and-after differences inline, with clickable actions to interactively accept, reject, or refine the proposed changes. In our usability evaluation (n = 12), developers reported that this mixed-initiative, example-driven experience is intuitive, complements their existing workflow, and offers a unified approach to conveniently tackle a variety of common yet frustrating scenarios for code transformations.","find-and-replace, code transformation, program synthesis","","UIST '21"
"Conference Paper","Mo R,Zhao Y,Feng Q,Li Z","The Existence and Co-Modifications of Code Clones within or across Microservices","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","Bari, Italy","2021","9781450386654","","https://doi.org/10.1145/3475716.3475784;http://dx.doi.org/10.1145/3475716.3475784","10.1145/3475716.3475784","In recent years, microservice architecture has been widely applied in software design. In addition, more and more monolithic software systems have been migrated into a microservice architecture. The core idea is to decompose the concerns of software projects into small and loosely-coupled services. Each service is supposed to be developed and even managed independently, which in turn improving the efficiency of development and maintenance. Code clone is common during software implementations, and many prior studies have revealed that code clones could cause maintenance difficulties. However, there is little work exploring the impacts of code clones on microservice projects. To bridge this gap, we focus on exploring the existence and co-modifications of within-service and cross-service code clones. With our evaluation of eight microservice projects, we have presented that there still exist code clones within services or across services. In addition, both within-service and cross-service code clones have been involved in co-modifications, meaning that these clones have caused maintenance difficulties. Finally, we have explored the characteristics of co-modifications in terms of changed LOC for both within-service and cross-service code clones.","Microservice, Co-modification, Code Clone","","ESEM '21"
"Conference Paper","Corazza A,Di Martino S,Peron A,Starace LL","Web Application Testing: Using Tree Kernels to Detect Near-Duplicate States in Automated Model Inference","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","Bari, Italy","2021","9781450386654","","https://doi.org/10.1145/3475716.3484187;http://dx.doi.org/10.1145/3475716.3484187","10.1145/3475716.3484187","Background: In the context of End-to-End testing of web applications, automated exploration techniques (a.k.a. crawling) are widely used to infer state-based models of the site under test. These models, in which states represent features of the web application and transitions represent reachability relationships, can be used for several model-based testing tasks, such as test case generation. However, current exploration techniques often lead to models containing many near-duplicate states, i.e., states representing slightly different pages that are in fact instances of the same feature. This has a negative impact on the subsequent model-based testing tasks, adversely affecting, for example, size, running time, and achieved coverage of generated test suites. Aims: As a web page can be naturally represented by its tree-structured DOM representation, we propose a novel near-duplicate detection technique to improve the model inference of web applications, based on Tree Kernel (TK) functions. TKs are a class of functions that compute similarity between tree-structured objects, largely investigated and successfully applied in the Natural Language Processing domain. Method: To evaluate the capability of the proposed approach in detecting near-duplicate web pages, we conducted preliminary classification experiments on a freely-available massive dataset of about 100k manually annotated web page pairs. We compared the classification performance of the proposed approach with other state-of-the-art near-duplicate detection techniques. Results: Preliminary results show that our approach performs better than state-of-the-art techniques in the near-duplicate detection classification task. Conclusions: These promising results show that TKs can be applied to near-duplicate detection in the context of web application model inference, and motivate further research in this direction to assess the impact of the technique on the quality of the inferred models and on the subsequent application of model-based testing techniques.","Model-based testing, Reverse engineering, Web Application Testing, Near-duplicate detection, Tree kernels, Model inference","","ESEM '21"
"Conference Paper","Carvalho L,Seco JC","Deep Semantic Versioning for Evolution and Variability","","2021","","","","Association for Computing Machinery","New York, NY, USA","23rd International Symposium on Principles and Practice of Declarative Programming","Tallinn, Estonia","2021","9781450386890","","https://doi.org/10.1145/3479394.3479416;http://dx.doi.org/10.1145/3479394.3479416","10.1145/3479394.3479416","The development cycles in the software industry are shrinking due to the increasing demands for shorter time to market and the incremental development style of agile methodologies. Pragmatic software engineering approaches rely on careful product management, a strong versioning discipline, and a feature development strategy to avoid that newly merged code disrupts existing systems. Versioning is critical when managing software product lines and ensuring that all their variants are kept in operation amidst all the performed changes. Such methodologies enforce functional correctness through strong efforts in regression testing, with the associated exponential growth in complexity. In this paper, we propose a language-based approach to software versioning. Unlike the traditional approach of mainstream VCS, where each evolution step is represented by a textual diff, we treat versions as first-class citizens. Each evolution step, merge operation, and version relationship, is represented as code in the program. We extend prior work, Versioned Featherweight Java, to support a full-fledged version control system. First, we introduce multi-branching and merge operations, which allow for more advanced workflows. We also present a slicing procedure implemented in a compile-time tool that extracts well-typed Featherweight Java code for any single version out of a versioned codebase. We present formal soundness results that ensure that the sliced code for any version is well-behaved and has the same behaviour as the multi-version source code. We believe that our developments effectively model relevant domains of software evolution, such as feature-oriented programming, software product lines, and continuous delivery scenarios. By lifting the versioning aspect, usually represented by text diffs, to the language level, we pave the way for tools that interact with software repositories (e.g. CI/CD, GitHub Actions) to have more insight regarding the evolution of the software semantics.","Program evolution, Type system, Program versioning","","PPDP 2021"
"Conference Paper","Sobrinho E,Maia M","On the Interplay of Smells Large Class, Complex Class and Duplicate Code","","2021","","","64–73","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXV Brazilian Symposium on Software Engineering","Joinville, Brazil","2021","9781450390613","","https://doi.org/10.1145/3474624.3474716;http://dx.doi.org/10.1145/3474624.3474716","10.1145/3474624.3474716","Bad smells have been defined to describe potential problems in code, possibly pointing out refactoring opportunities. Several empirical studies have highlighted that smells have a negative impact on comprehension and maintainability. Consequently, several approaches have been proposed to detect and restructure them. However, studies on the inter-relationship of occurrence of different types of smells in source code are still lacking, especially those focused on the quantification of this inter-relationship. In this work, we aim at understand and quantify the possible the inter-relation of smells Large Class - LC, Complex Class - CC and Duplicate Code - DC. In particular, we investigate patterns of LC and CC regarding the presence or absence of duplicate code. We conduct a quantitative study on five open source projects, and also a qualitative analysis to measure and understand the association of specific smells. As one of the main results, we highlight that there are ”occurrence patterns” among these smells, for example: either in Complex Class or in the co-occurrence of Large Class and Complex Class, clones tend to be more prevalent in highly complex classes than less complex classes. The found patterns could be used to improve the performance of detection tools or even help in refactoring tasks.","reengineering, Software maintenance, bad smell","","SBES '21"
"Conference Paper","Silva D,Mendonça D","SCPL: A Markup Language for Source Code Patterns Localization","","2021","","","127–132","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXV Brazilian Symposium on Software Engineering","Joinville, Brazil","2021","9781450390613","","https://doi.org/10.1145/3474624.3476017;http://dx.doi.org/10.1145/3474624.3476017","10.1145/3474624.3476017","Context: Static analysis tools (SATs) have become commonly used to locate defects in source code. Many SATs allow for development of custom static analysis rules, which intend to help users to find application-specific defects. However, custom static analysis rules are not well adopted in practice. One possible reason for the low adoption is the difficulty to develop custom rules. Aims: In this paper, we present Source Code Pattern Language (SCPL), a pattern-finding language which uses markups in code examples, instead of more complex abstractions, to facilitate development of custom static analysis rules. Method: SCPL uses markups in the source code and tree isomorphism methods to locate the patterns. In order to perform a proof-of-concept, we developed custom static analysis rules to automatically check four GoF design patterns. Result: We spent approximately 30 man-hours to develop 33 custom static analysis rules that automatically check the four design patterns. Conclusion: SCPL provide a rich feature set and potentially facilitates the programming of custom static analysis rules by using markups directly in source code. Video: https://youtu.be/B-Ovi3zurnM","tool, custom static analysis rules, patterns","","SBES '21"
"Conference Paper","Faustino A","Graphs Based on IR as Representation of Code: Types and Insights","","2021","","","75–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th Brazilian Symposium on Programming Languages","Joinville, Brazil","2021","9781450390620","","https://doi.org/10.1145/3475061.3475063;http://dx.doi.org/10.1145/3475061.3475063","10.1145/3475061.3475063","Mainstream compilers infer code properties from data structures, such as trees and graphs. The latter is useful to represent the control flow and the data dependencies in a code. In addition, graphs can also be used in learning tasks, such as classifying applications given their raw code, predicting the best-performing compute device (e.g., CPU, GPU) or predicting the optimal thread coarsening factor. This paper investigates the performance of graph neural networks on classifying applications given their raw code, for different type of graphs extracted from LLVM’s intermediate representation. The results indicate that adding new (different) edges and/or nodes is not a fact of performance improvement. This paper shows a compact representation tends to achieve the best performance. As a result of such investigation, this paper has three main contributions: (1) an infrastructure to explore such graphs on different tasks, (2) compact graphs from LLVM’s intermediate representation, (3) and a detailed evaluation of different types of graphs on a learning task.","LLVM, Graph, Compilers, Program Reasoning, Neural Network","","SBLP '21"
"Conference Paper","Colanzi T,Amaral A,Assunção W,Zavadski A,Tanno D,Garcia A,Lucena C","Are We Speaking the Industry Language? The Practice and Literature of Modernizing Legacy Systems with Microservices","","2021","","","61–70","Association for Computing Machinery","New York, NY, USA","15th Brazilian Symposium on Software Components, Architectures, and Reuse","Joinville, Brazil","2021","9781450384193","","https://doi.org/10.1145/3483899.3483904;http://dx.doi.org/10.1145/3483899.3483904","10.1145/3483899.3483904","Microservice architecture has gained much attention in the last few years in both industry and academia. Microservice is an architectural style that enables developing systems as a suite of small loosely coupled, and autonomous (micro)services that encapsulate business capabilities and communicate with each other using language-agnostic APIs. Despite the microservice adoption for modernizing legacy systems, few studies investigate how microservices are used in practice. Furthermore, the literature still scarce on presenting studies on why and how the modernization is conducted in practice in comparison to existing literature on the subject. Thus, our goal is to investigate if industry and academy are speaking the same language concerning the modernization of legacy systems with microservices, by means of a rigorous study on the use of microservices in the industry. For doing so, we design a survey to understand the state of practice from the perspective of a modernization process roadmap derived from the literature. In this paper, we report the results of a survey with 56 software companies, from which 35 (63.6%) adopt the microservice architecture in their legacy systems. Results pointed out the most expected benefits that drive the migration to microservices are easier software maintenance, better scalability, ease of deployment, and technology flexibility. Besides, we verified, based on a set of activities defined in the modernization process, that the practitioners are performing their migration process according to the best literature practices.","Software Migration, Microservices, Software Re-engineering","","SBCARS '21"
"Conference Paper","Martins J,Bezerra C,Uchôa A,Garcia A","How Do Code Smell Co-Occurrences Removal Impact Internal Quality Attributes? A Developers’ Perspective","","2021","","","54–63","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXV Brazilian Symposium on Software Engineering","Joinville, Brazil","2021","9781450390613","","https://doi.org/10.1145/3474624.3474642;http://dx.doi.org/10.1145/3474624.3474642","10.1145/3474624.3474642","Code smells are poor code structures that might harm the software quality and evolution. However, previous studies has shown that only individual occurrences of smells may not be enough to assess the real impact that these smells can bring on systems. In this context, the co-occurrences of code smells, i.e., occurrences of more than one code smell in the same class or same method, can be better indicators of design problems for software quality. Despite its importance as an indicator of design problems, we have little known about the impact of removing the co-occurrence of smells via software refactoring on internal quality attributes, such as coupling, cohesion, complexity, and inheritance. It is even less clear on what is the developers’ perspective on the co-occurrences removal. We aim at addressing this gap through a qualitative study with 14 developers. To this end, we analyze the refactorings employed by developers during the removal of 60 code smells co-occurrences, during 3 months in 5 closed-source projects. We observe (i) impact of code smells co-occurrences on internal quality attributes, (ii) which are the most harmful co-occurrences from the developers’ perspective, (iii) developers’ perceptions during the removal of code smells co-occurrence via refactoring activities; and (iv) what are the main difficulties faced by developers during the removal of code smells co-occurrences in practice. Our findings indicate that: (i) the refactoring of some types of code smells co-occurrences (e.g., Dispersed Coupling–God Class) indicated improvement for the quality attributes; (ii) refactoring code smells co-occurrences according to the developers is difficult mainly due to the understanding of the code and complexity refactoring methods; and (iii) developers still have insecurities regarding the identification and refactoring of code smells and their co-occurrences.","Refactoring, Code Smells Co-occurrences, Internal Quality Attributes","","SBES '21"
"Conference Paper","Silva G,Andrade V,Ré R,Meneses R","A Quasi-Experiment to Investigating the Impact of the Strategy Design Pattern on Maintainability","","2021","","","105–114","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXV Brazilian Symposium on Software Engineering","Joinville, Brazil","2021","9781450390613","","https://doi.org/10.1145/3474624.3474636;http://dx.doi.org/10.1145/3474624.3474636","10.1145/3474624.3474636","GoF Design Patterns (DPs) are optimal solutions for several recurring problems in software development. As maintenance accounts for a massive amount of development costs, understanding the impact of DPs on maintainability is key to decide whether their benefits outweigh their costs. To test the hypothesis that the Strategy DP impacts maintainability, we carried out a quasi-experiment with 19 final-year undergraduate students. Participants performed enhancement tasks in two identical versions of Java classes that differed only in that a version implemented the Strategy DP whereas another version did not. As recommended in the literature, we used effectiveness as a representative proxy of maintainability and correctness as a measure of effectiveness. We found a statistically significant correctness change between treatments. Moreover, the effect size shows that a participant is 47.7 percentage points more likely to succeed when the Strategy DP is not used. Conversely, a participant is 5.5 times more likely to fail when the DP is introduced. Unlike most related studies, our findings suggest that the Strategy DP may reduce software maintainability although these findings need to be confirmed by independent and extended replications.","Experimental Software Engineering, Software Architecture, Software Maintenance and Evolution, Design Patterns","","SBES '21"
"Conference Paper","Sousa R,Soares G,Gheyi R,Barik T,D'Antoni L","Learning Quick Fixes from Code Repositories","","2021","","","74–83","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXV Brazilian Symposium on Software Engineering","Joinville, Brazil","2021","9781450390613","","https://doi.org/10.1145/3474624.3474650;http://dx.doi.org/10.1145/3474624.3474650","10.1145/3474624.3474650","Code analyzers such as Error Prone and FindBugs detect code patterns symptomatic of bugs, performance issues, or bad style. These tools express patterns as quick fixes that detect and rewrite unwanted code. However, it is difficult to come up with new quick fixes and decide which ones are useful and frequently appear in real code. We propose to rely on the collective wisdom of programmers and learn quick fixes from revision histories in software repositories. We present Revisar, a tool for discovering common Java edit patterns in code repositories. Given code repositories and their revision histories, Revisar (i) identifies code edits from revisions and (ii) clusters edits into sets that can be described using an edit pattern. The designers of code analyzers can then inspect the patterns and add the corresponding quick fixes to their tools. We ran Revisar on nine popular GitHub projects, and it discovered 89 useful edit patterns that appeared in 3 or more projects. Moreover, 64% of the discovered patterns did not appear in existing tools. We then conducted a survey with 164 programmers from 124 projects and found that programmers significantly preferred eight out of the nine of the discovered patterns. Finally, we submitted 16 pull requests applying our patterns to 9 projects and, at the time of the writing, programmers accepted 7 (63.6%) of them. The results of this work aid toolsmiths in discovering quick fixes and making informed decisions about which quick fixes to prioritize based on patterns programmers actually apply in practice.","Education, Software Engineering, Survey.","","SBES '21"
"Conference Paper","Weichbrodt N,Heinemann J,Almstedt L,Aublin PL,Kapitza R","Experience Paper: Sgx-Dl: Dynamic Loading and Hot-Patching for Secure Applications","","2021","","","91–103","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Middleware Conference","Québec city, Canada","2021","9781450385343","","https://doi.org/10.1145/3464298.3476134;http://dx.doi.org/10.1145/3464298.3476134","10.1145/3464298.3476134","Trusted execution as offered by Intel's Software Guard Extensions (SGX) is considered as an enabler to protect the integrity and confidentiality of stateful workloads such as key-value stores and databases in untrusted environments. These systems are typically long running and require extension mechanisms built on top of dynamic loading as well as hot-patching to avoid downtimes and apply security updates faster. However, such essential mechanisms are currently neglected or even missing in combination with trusted execution.We present sgx-dl, a lean framework that enables dynamic loading of enclave code at the function level and hot-patching of dynamically loaded code. Additionally, sgx-dl is the first framework to utilize the new SGX version 2 features and also provides a versioning mechanism for dynamically loaded code. Our evaluation shows that sgx-dl introduces a performance overhead of less than 5% and shrinks application downtime by an order of magnitude in the case of a database system.","dynamic code loading, hot-patching, trusted execution, Intel software guard extensions","","Middleware '21"
"Conference Paper","Limaye A,Adegbija T","DOSAGE: Generating Domain-Specific Accelerators for Resource-Constrained Computing","","2021","","","","IEEE Press","Boston, Massachusetts","Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design","","2021","","","https://doi.org/10.1109/ISLPED52811.2021.9502501;http://dx.doi.org/10.1109/ISLPED52811.2021.9502501","10.1109/ISLPED52811.2021.9502501","Integrating low-overhead domain-specific accelerators with low-energy general-purpose processors can improve the processors' performance efficiency in resource-constrained systems (e.g., embedded systems). However, current function-based approaches for designing domain-specific accelerators require substantial programmer efforts for hardware/software partitioning and program modifications to access the best available hardware accelerators. This paper presents DOSAGE, an LLVM compiler-based methodology to generate domain-specific accelerators for resource-constrained computing systems. Given a set of applications, DOSAGE automatically identifies and ranks the recurrent and similar code blocks that would benefit the most from hardware acceleration, based on the code blocks' composition. We illustrate the benefits of the proposed approach using a case study that involves generating domain-specific accelerators for a diverse set of healthcare applications and evaluate the accelerators via FPGA-based prototyping. Compared to a base low-resource RISC-V processor, DOSAGE accelerators improved the system's performance and energy by 24.85% and 8.54%, respectively. Furthermore, compared to a state-of-the-art function-based accelerator generation approach, DOSAGE eliminated the function-level granularity constraint of the generation process and reduced the number of required accelerators---and, in effect, the interfacing overhead---by 33.33%, while achieving equal or better program coverage and performance/energy results.","domain-specific accelerators, embedded systems, resource-constrained computing, low-energy, hardware software co-design","","ISLPED '21"
"Journal Article","Liu C,Xia X,Lo D,Liu Z,Hassan AE,Li S","CodeMatcher: Searching Code Based on Sequential Semantics of Important Query Words","ACM Trans. Softw. Eng. Methodol.","2021","31","1","","Association for Computing Machinery","New York, NY, USA","","","2021-09","","1049-331X","https://doi.org/10.1145/3465403;http://dx.doi.org/10.1145/3465403","10.1145/3465403","To accelerate software development, developers frequently search and reuse existing code snippets from a large-scale codebase, e.g., GitHub. Over the years, researchers proposed many information retrieval (IR)-based models for code search, but they fail to connect the semantic gap between query and code. An early successful deep learning (DL)-based model DeepCS solved this issue by learning the relationship between pairs of code methods and corresponding natural language descriptions. Two major advantages of DeepCS are the capability of understanding irrelevant/noisy keywords and capturing sequential relationships between words in query and code. In this article, we proposed an IR-based model CodeMatcher that inherits the advantages of DeepCS (i.e., the capability of understanding the sequential semantics in important query words), while it can leverage the indexing technique in the IR-based model to accelerate the search response time substantially. CodeMatcher first collects metadata for query words to identify irrelevant/noisy ones, then iteratively performs fuzzy search with important query words on the codebase that is indexed by the Elasticsearch tool and finally reranks a set of returned candidate code according to how the tokens in the candidate code snippet sequentially matched the important words in a query. We verified its effectiveness on a large-scale codebase with 41K repositories. Experimental results showed that CodeMatcher achieves an MRR (a widely used accuracy measure for code search) of 0.60, outperforming DeepCS, CodeHow, and UNIF by 82%, 62%, and 46%, respectively. Our proposed model is over 1.2K times faster than DeepCS. Moreover, CodeMatcher outperforms two existing online search engines (GitHub and Google search) by 46% and 33%, respectively, in terms of MRR. We also observed that: fusing the advantages of IR-based and DL-based models is promising; improving the quality of method naming helps code search, since method name plays an important role in connecting query and code.","information retrieval, Code search, mining software repositories, code indexing","",""
"Conference Paper","Wang S,Liu J,Liu Z,Li P,Ran Y","Research on Multi-Peaks Non-Fuzzy Tracking Algorithm of BOC Signal Based on Improved Bump-Jumping Method","","2021","","","151–156","Association for Computing Machinery","New York, NY, USA","2021 5th International Conference on Digital Signal Processing","Chengdu, China","2021","9781450389365","","https://doi.org/10.1145/3458380.3458406;http://dx.doi.org/10.1145/3458380.3458406","10.1145/3458380.3458406","The GPS-III navigation signal adopts the BOC modulation mode. The signal modulated by BOC will have multiple subpeaks when it is captured and tracked, so there is a large error caused by the subpeak wrong locked in the time delay difference measurement. In this paper, the bump-jumping algorithm is proposed, which can quickly identify the subpeak and ensure that the PLL is ready to lock the main peak. After locking the main peak, the algorithm is not easy to be disturbed and lead to loss of lock. Finally, the high-precision tracking of non-fuzzy BOC signal can be realized by using bump-jumping tracking algorithm.","BOC modulation technology, spectrum splitting, bump-jumping, subpeak blur","","ICDSP 2021"
"Conference Paper","Chabbi M,Lin J,Barik R","An Experience with Code-Size Optimization for Production IOS Mobile Applications","","2021","","","363–366","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization","","2021","9781728186139","","https://doi.org/10.1109/CGO51591.2021.9370306;http://dx.doi.org/10.1109/CGO51591.2021.9370306","10.1109/CGO51591.2021.9370306","Modern mobile application binaries are bulky for many reasons: software and its dependencies, fast-paced addition of new features, high-level language constructs, and statically linked platform libraries. Reduced application size is critical not only for the end-user experience but also for vendor's download size limitations. Moreover, download size restrictions may impact revenues for critical businesses.In this paper, we highlight some of the key reasons of code-size bloat in iOS mobile applications, specifically apps written using a mix of Swift and Objective-C. Our observation reveals that machine code sequences systematically repeat throughout the app's binary. We highlight source-code patterns and high-level language constructs that lead to an increase in the code size. We propose whole-program, fine-grained machine-code outlining as an effective optimization to constrain the code-size growth. We evaluate the effectiveness of our new optimization pipeline on the UberRider iOS app used by millions of customers daily. Our optimizations reduce the code size by 23%. The impact of our optimizations on the code size grows in magnitude over time as the code evolves. For a set of performance spans defined by the app developers, the optimizations do not statistically regress production performance. We applied the same optimizations to Uber's UberDriver and UberEats apps and gained 17% and 19% size savings, respectively.","code-size, intermodule optimization, iOS, swift, machine outlining, whole-program optimization","","CGO '21"
"Conference Paper","da Silva AF,Kind BC,de Souza Magalhães JW,Rocha JN,Guimarães BC,Pereira FM","AnghaBench: A Suite with One Million Compilable C Benchmarks for Code-Size Reduction","","2021","","","378–390","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization","","2021","9781728186139","","https://doi.org/10.1109/CGO51591.2021.9370322;http://dx.doi.org/10.1109/CGO51591.2021.9370322","10.1109/CGO51591.2021.9370322","A predictive compiler uses properties of a program to decide how to optimize it. The compiler is trained on a collection of programs to derive a model which determines its actions in face of unknown codes. One of the challenges of predictive compilation is how to find good training sets. Regardless of the programming language, the availability of humanmade benchmarks is limited. Moreover, current synthesizers produce code that is very different from actual programs, and mining compilable code from open repositories is difficult, due to program dependencies. In this paper, we use a combination of web crawling and type inference to overcome these problems for the C programming language. We use a type reconstructor based on Hindley-Milner's algorithm to produce AnghaBench, a virtually unlimited collection of real-world compilable C programs. Although AnghaBench programs are not executable, they can be transformed into object files by any C compliant compiler. Therefore, they can be used to train compilers for code size reduction. We have used thousands of AnghaBench programs to train YaCoS, a predictive compiler based on LLVM. The version of YaCoS autotuned with AnghaBench generates binaries for the LLVM test suite over 10% smaller than clang -Oz. It compresses code impervious even to the state-of-the-art Function Sequence Alignment technique published in 2019, as it does not require large binaries to work well.","benchmark, training, synthesis, repository","","CGO '21"
"Journal Article","Wang D,Chen K,Wang W","Demystifying the Vetting Process of Voice-Controlled Skills on Markets","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","2021","5","3","","Association for Computing Machinery","New York, NY, USA","","","2021-09","","","https://doi.org/10.1145/3478101;http://dx.doi.org/10.1145/3478101","10.1145/3478101","Smart speakers, such as Google Home and Amazon Echo, have become popular. They execute user voice commands via their built-in functionalities together with various third-party voice-controlled applications, called skills. Malicious skills have brought significant threats to users in terms of security and privacy. As a countermeasure, only skills passing the strict vetting process can be released onto markets. However, malicious skills have been reported to exist on markets, indicating that the vetting process can be bypassed. This paper aims to demystify the vetting process of skills on main markets to discover weaknesses and protect markets better. To probe the vetting process, we carefully design numerous skills, perform the Turing test, a test for machine intelligence, to determine whether humans or machines perform vetting, and leverage natural language processing techniques to analyze their behaviors. Based on our comprehensive experiments, we gain a good understanding of the vetting process (e.g., machine or human testers and skill exploration strategies) and discover some weaknesses. In this paper, we design three types of attacks to verify our results and prove an attacker can embed sensitive behaviors in skills and bypass the strict vetting process. Accordingly, we also propose countermeasures to these attacks and weaknesses.","Alexa, vetting process, Skill, Google-Home","",""
"Conference Paper","Yildirim A,Mader CA,Martins JR","Accelerating Parallel CFD Codes on Modern Vector Processors Using Blockettes","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Platform for Advanced Scientific Computing Conference","Geneva, Switzerland","2021","9781450385633","","https://doi.org/10.1145/3468267.3470615;http://dx.doi.org/10.1145/3468267.3470615","10.1145/3468267.3470615","The performance and scalability of computational fluid dynamics (CFD) solvers are essential for many applications, including multidisciplinary design optimization. With the evolution of highperformance computing resources such as Intel's Knights Landing and Skylake architectures in the Stampede2 cluster, CFD solver performance can be improved by modifying how the core computations are performed while keeping the mathematical formulation unchanged. In this work, we introduce a cache-blocking method to improve memory-bound CFD codes that use structured grids. The overall idea is to split computational blocks into smaller, fixed-sized blockettes that are sufficiently small to completely fit into the available cache size for each core on a given architecture. We can fully take advantage of modern vector instruction sets such as AVX2 and AVX512 on these modern architectures with this approach. Using this method, we have achieved up to 3.27 times speedup in the core routines of the open-source CFD solver, ADflow.","cache blocking, memory hierarchies, blockette, computational fluid dynamics","","PASC '21"
"Conference Paper","Mens K,Nijssen S,Pham HS","The Good, the Bad, and the Ugly: Mining for Patterns in Student Source Code","","2021","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Workshop on Education through Advanced Software Engineering and Artificial Intelligence","Athens, Greece","2021","9781450386241","","https://doi.org/10.1145/3472673.3473958;http://dx.doi.org/10.1145/3472673.3473958","10.1145/3472673.3473958","Research on source code mining has been explored to discover interesting structural regularities, API usage patterns, refactoring opportunities, bugs, crosscutting concerns, code clones and systematic changes. In this paper we present a pattern mining algorithm that uses frequent tree mining to mine for interesting good, bad or ugly coding idioms made by undergraduate students taking an introductory programming course. We do so by looking for patterns that distinguish positive examples, corresponding to the more correct answers to a question, from negative examples, corresponding to solutions that failed the question. We report promising initial results of this algorithm applied to the source code of over 500 students. Even though more work is needed to fine-tune and validate the algorithm further, we hope that it can lead to interesting insights that can eventually be integrated into an intelligent recommendation system to help students learn from their errors.","pattern mining, CS education, programming, source code mining","","EASEAI 2021"
"Conference Paper","Pravilov M,Bogomolov E,Golubev Y,Bryksin T","Unsupervised Learning of General-Purpose Embeddings for Code Changes","","2021","","","7–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution","Athens, Greece","2021","9781450386258","","https://doi.org/10.1145/3472674.3473979;http://dx.doi.org/10.1145/3472674.3473979","10.1145/3472674.3473979","Applying machine learning to tasks that operate with code changes requires their numerical representation. In this work, we propose an approach for obtaining such representations during pre-training and evaluate them on two different downstream tasks — applying changes to code and commit message generation. During pre-training, the model learns to apply the given code change in a correct way. This task requires only code changes themselves, which makes it unsupervised. In the task of applying code changes, our model outperforms baseline models by 5.9 percentage points in accuracy. As for the commit message generation, our model demonstrated the same results as supervised models trained for this specific task, which indicates that it can encode code changes well and can be improved in the future by pre-training on a larger dataset of easily gathered code changes.","Code changes, Commit message generation, Unsupervised learning","","MaLTESQuE 2021"
"Conference Paper","Terra-Neves M,Nadkarni J,Ventura M,Resende P,Veiga H,Alegria A","Duplicated Code Pattern Mining in Visual Programming Languages","","2021","","","1348–1359","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3473928;http://dx.doi.org/10.1145/3468264.3473928","10.1145/3468264.3473928","Visual Programming Languages (VPLs), coupled with the high-level abstractions that are commonplace in visual programming environments, enable users with less technical knowledge to become proficient programmers. However, the lower skill floor required by VPLs also entails that programmers are more likely to not adhere to best practices of software development, producing systems with high technical debt, and thus poor maintainability. Duplicated code is one important example of such technical debt. In fact, we observed that the amount of duplication in the OutSystems VPL code bases can reach as high as 39%. Duplicated code detection in text-based programming languages is still an active area of research with important implications regarding software maintainability and evolution. However, to the best of our knowledge, the literature on duplicated code detection for VPLs is very limited. We propose a novel and scalable duplicated code pattern mining algorithm that leverages the visual structure of VPLs in order to not only detect duplicated code, but also highlight duplicated code patterns that explain the reported duplication. The performance of the proposed approach is evaluated on a wide range of real-world mobile and web applications developed using OutSystems.","maximum common sub-graph, maximum satisfiability, visual programming, duplicated code","","ESEC/FSE 2021"
"Conference Paper","Chirkova N,Troshin S","Empirical Study of Transformers for Source Code","","2021","","","703–715","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468611;http://dx.doi.org/10.1145/3468264.3468611","10.1145/3468264.3468611","Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.","code completion, neural networks, function naming, variable misuse detection, transformer","","ESEC/FSE 2021"
"Conference Paper","Bogomolov E,Kovalenko V,Rebryk Y,Bacchelli A,Bryksin T","Authorship Attribution of Source Code: A Language-Agnostic Approach and Applicability in Software Engineering","","2021","","","932–944","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468606;http://dx.doi.org/10.1145/3468264.3468606","10.1145/3468264.3468606","Authorship attribution (i.e., determining who is the author of a piece of source code) is an established research topic. State-of-the-art results for the authorship attribution problem look promising for the software engineering field, where they could be applied to detect plagiarized code and prevent legal issues. With this article, we first introduce a new language-agnostic approach to authorship attribution of source code. Then, we discuss limitations of existing synthetic datasets for authorship attribution, and propose a data collection approach that delivers datasets that better reflect aspects important for potential practical use in software engineering. Finally, we demonstrate that high accuracy of authorship attribution models on existing datasets drastically drops when they are evaluated on more realistic data. We outline next steps for the design and evaluation of authorship attribution models that could bring the research efforts closer to practical use for software engineering.","Security, Software process, Copyrights, Methods of data collection, Software maintenance, Machine learning","","ESEC/FSE 2021"
"Conference Paper","Nakagawa T,Higo Y,Kusumoto S","NIL: Large-Scale Detection of Large-Variance Clones","","2021","","","830–841","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468564;http://dx.doi.org/10.1145/3468264.3468564","10.1145/3468264.3468564","A code clone (in short, clone) is a code fragment that is identical or similar to other code fragments in source code. Clones generated by a large number of changes to copy-and-pasted code fragments are called large-variance (modifications are scattered) or large-gap (modifications are in one place) clones. It is difficult for general clone detection techniques to detect such clones and thus specialized techniques are necessary. In addition, with the rapid growth of software development, scalable clone detectors that can detect clones in large codebases are required. However, there are no existing techniques for quickly detecting large-variance or large-gap clones in large codebases. In this paper, we propose a scalable clone detection technique that can detect large-variance clones from large codebases and describe its implementation, called NIL. NIL is a token-based clone detector that efficiently identifies clone candidates using an N-gram representation of token sequences and an inverted index. Then, NIL verifies the clone candidates by measuring their similarity based on the longest common subsequence between their token sequences. We evaluate NIL in terms of large- variance clone detection accuracy, general Type-1, Type-2, and Type- 3 clone detection accuracy, and scalability. Our experimental results show that NIL has higher accuracy in terms of large-variance clone detection, equivalent accuracy in terms of general clone detection, and the shortest execution time for inputs of various sizes (1–250 MLOC) compared to existing state-of-the-art tools.","Clone Detection, Scalability, Large-Variance Clone","","ESEC/FSE 2021"
"Conference Paper","Mathew G,Stolee KT","Cross-Language Code Search Using Static and Dynamic Analyses","","2021","","","205–217","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468538;http://dx.doi.org/10.1145/3468264.3468538","10.1145/3468264.3468538","As code search permeates most activities in software development,code-to-code search has emerged to support using code as a query and retrieving similar code in the search results. Applications include duplicate code detection for refactoring, patch identification for program repair, and language translation. Existing code-to-code search tools rely on static similarity approaches such as the comparison of tokens and abstract syntax trees (AST) to approximate dynamic behavior, leading to low precision. Most tools do not support cross-language code-to-code search, and those that do, rely on machine learning models that require labeled training data. We present Code-to-Code Search Across Languages (COSAL), a cross-language technique that uses both static and dynamic analyses to identify similar code and does not require a machine learning model. Code snippets are ranked using non-dominated sorting based on code token similarity, structural similarity, and behavioral similarity. We empirically evaluate COSAL on two datasets of 43,146Java and Python files and 55,499 Java files and find that 1) code search based on non-dominated ranking of static and dynamic similarity measures is more effective compared to single or weighted measures; and 2) COSAL has better precision and recall compared to state-of-the-art within-language and cross-language code-to-code search tools. We explore the potential for using COSAL on large open-source repositories and discuss scalability to more languages and similarity metrics, providing a gateway for practical,multi-language code-to-code search.","static analysis, non-dominated sorting, dynamic analysis, cross-language code search, code-to-code search","","ESEC/FSE 2021"
"Conference Paper","Patra J,Pradel M","Semantic Bug Seeding: A Learning-Based Approach for Creating Realistic Bugs","","2021","","","906–918","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468623;http://dx.doi.org/10.1145/3468264.3468623","10.1145/3468264.3468623","When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens. This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.","bugs, machine learning, bug injection, dataset, token embeddings","","ESEC/FSE 2021"
"Conference Paper","Bittner PM,Schultheiß A,Thüm T,Kehrer T,Young JM,Linsbauer L","Feature Trace Recording","","2021","","","1007–1020","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468531;http://dx.doi.org/10.1145/3468264.3468531","10.1145/3468264.3468531","Tracing requirements to their implementation is crucial to all stakeholders of a software development process. When managing software variability, requirements are typically expressed in terms of features, a feature being a user-visible characteristic of the software. While feature traces are fully documented in software product lines, ad-hoc branching and forking, known as clone-and-own, is still the dominant way for developing multi-variant software systems in practice. Retroactive migration to product lines suffers from uncertainties and high effort because knowledge of feature traces must be recovered but is scattered across teams or even lost. We propose a semi-automated methodology for recording feature traces proactively, during software development when the necessary knowledge is present. To support the ongoing development of previously unmanaged clone-and-own projects, we explicitly deal with the absence of domain knowledge for both existing and new source code. We evaluate feature trace recording by replaying code edit patterns from the history of two real-world product lines. Our results show that feature trace recording reduces the manual effort to specify traces. Recorded feature traces could improve automation in change-propagation among cloned system variants and could reduce effort if developers decide to migrate to a product line.","clone-and-own, software product lines, feature traceability, feature location, disciplined annotations","","ESEC/FSE 2021"
"Conference Paper","van der Leij D,Binda J,van Dalen R,Vallen P,Luo Y,Aniche M","Data-Driven Extract Method Recommendations: A Study at ING","","2021","","","1337–1347","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3473927;http://dx.doi.org/10.1145/3468264.3473927","10.1145/3468264.3473927","The sound identification of refactoring opportunities is still an open problem in software engineering. Recent studies have shown the effectiveness of machine learning models in recommending methods that should undergo different refactoring operations. In this work, we experiment with such approaches to identify methods that should undergo an Extract Method refactoring, in the context of ING, a large financial organization. More specifically, we (i) compare the code metrics distributions, which are used as features by the models, between open-source and ING systems, (ii) measure the accuracy of different machine learning models in recommending Extract Method refactorings, (iii) compare the recommendations given by the models with the opinions of ING experts. Our results show that the feature distributions of ING systems and open-source systems are somewhat different, that machine learning models can recommend Extract Method refactorings with high accuracy, and that experts tend to agree with most of the recommendations of the model.","Machine Learning for Software Engineering, Software Refactoring, Software Engineering","","ESEC/FSE 2021"
"Conference Paper","Zhu Q,Sun Z,Xiao YA,Zhang W,Yuan K,Xiong Y,Zhang L","A Syntax-Guided Edit Decoder for Neural Program Repair","","2021","","","341–353","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468544;http://dx.doi.org/10.1145/3468264.3468544","10.1145/3468264.3468544","Automated Program Repair (APR) helps improve the efficiency of software development and maintenance. Recent APR techniques use deep learning, particularly the encoder-decoder architecture, to generate patches. Though existing DL-based APR approaches have proposed different encoder architectures, the decoder remains to be the standard one, which generates a sequence of tokens one by one to replace the faulty statement. This decoder has multiple limitations: 1) allowing to generate syntactically incorrect programs, 2) inefficiently representing small edits, and 3) not being able to generate project-specific identifiers. In this paper, we propose Recoder, a syntax-guided edit decoder with placeholder generation. Recoder is novel in multiple aspects: 1) Recoder generates edits rather than modified code, allowing efficient representation of small edits; 2) Recoder is syntax-guided, with the novel provider/decider architecture to ensure the syntactic correctness of the patched program and accurate generation; 3) Recoder generates placeholders that could be instantiated as project-specific identifiers later. We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2, 420 additional bugs from Defects4J v2.0, 297 bugs from IntroClassJava and 40 bugs from QuixBugs. Our results show that Recoder repairs 53 bugs on Defects4J v1.2, which achieves 26.2% (11 bugs) improvement over the previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to our knowledge, Recoder is the first DL-based APR approach that has outperformed the traditional APR approaches on this benchmark. Furthermore, Recoder repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5% (11 bugs) more than TBar and 850% (17 bugs) more than SimFix. Recoder also achieves 775% (31 bugs) and 30.8% (4 bugs) improvement on IntroClassJava and QuixBugs over the baselines respectively. These results suggest that Recoder has better generalizability than existing APR approaches.","Automated program repair, Neural networks","","ESEC/FSE 2021"
"Conference Paper","Fernandes S","A Live Environment for Inspection and Refactoring of Software Systems","","2021","","","1655–1659","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3473100;http://dx.doi.org/10.1145/3468264.3473100","10.1145/3468264.3473100","Refactoring helps to improve the design of software systems, making them more readable, maintainable, cleaner, and easy to expand. Most of the tools that already exist on this concept allow developers to select and execute the best refactoring techniques for a particular programming context. However, they aren’t interactive and prompt enough, providing a poor programming experience. In this gap, we can introduce and combine the topic of liveness with refactoring methods. Live Refactoring allows to know continuously, while programming, the blocks of code that we should refactor and why they were classified as problematic. Therefore, it shortens the time needed to create high-quality systems, due to early and continuous refactoring feedback, support, and guidance. This paper presents our research project based on a live refactoring environment. This environment is focused on a refactoring tool that aims to explore the concept of Live Refactoring and its main components --- recommendation, visualization, and application.","code smells, refactoring, code quality, visualization, liveness","","ESEC/FSE 2021"
"Conference Paper","Wang S,Wen M,Lin B,Mao X","Lightweight Global and Local Contexts Guided Method Name Recommendation with Prior Knowledge","","2021","","","741–753","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468567;http://dx.doi.org/10.1145/3468264.3468567","10.1145/3468264.3468567","The quality of method names is critical for the readability and maintainability of source code. However, it is often challenging to construct concise method names. To alleviate this problem, a number of approaches have been proposed to automatically recommend high-quality names for methods. Despite being effective, existing approaches meet their bottlenecks mainly in two aspects: (1) the leveraged information is restricted to the target method itself; and (2) lack of distinctions towards the contributions of tokens extracted from different program contexts. Through a large-scale empirical analysis on +12M methods from +14K real-world projects, we found that (1) the tokens composing a method’s name can be frequently observed in its callers/callees; and (2) tokens extracted from different specific contexts have diverse probabilities to compose the target method’s name. Motivated by our findings, we propose, in this paper, a context-guided method name recommender, which mainly embodies two key ideas: (1) apart from the local context, which is extracted from the target method itself, we also consider the global context, which is extracted from other methods in the project that have call relations with the target method, to include more useful information; and (2) we utilize our empirical results as the prior knowledge to guide the generation of method names and also to restrict the number of tokens extracted from the global contexts. We implemented the idea as Cognac and performed extensive experiments to assess its effectiveness. Results reveal that can (1) perform better than existing approaches on the method name recommendation task (e.g., it achieves an F-score of 63.2%, 60.8%, 66.3%, and 68.5%, respectively, on four widely-used datasets, which all outperform existing techniques); and (2) achieve higher performance than existing techniques on the method name consistency checking task (e.g., its overall accuracy reaches 76.6%, outperforming the state-of-the-art MNire by 11.2%). Further results reveal that the caller/callee information and the prior knowledge all contribute significantly to the overall performance of Cognac.","Code embedding, Deep learning, Method name recommendation","","ESEC/FSE 2021"
"Conference Paper","Gao Z,Xia X,Lo D,Grundy J,Li YF","Code2Que: A Tool for Improving Question Titles from Mined Code Snippets in Stack Overflow","","2021","","","1525–1529","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3473114;http://dx.doi.org/10.1145/3468264.3473114","10.1145/3468264.3473114","Stack Overflow is one of the most popular technical Q&A sites used by software developers. Seeking help from Stack Overflow has become an essential part of software developers’ daily work for solving programming-related questions. Although the Stack Overflow community has provided quality assurance guidelines to help users write better questions, we observed that a significant number of questions submitted to Stack Overflow are of low quality. In this paper, we introduce a new web-based tool, Code2Que, which can help developers in writing higher quality questions for a given code snippet. Code2Que consists of two main stages: offline learning and online recommendation. In the offline learning phase, we first collect a set of good quality ⟨code snippet, question⟩ pairs as training samples. We then train our model on these training samples via a deep sequence-to-sequence approach, enhanced with an attention mechanism, a copy mechanism and a coverage mechanism. In the online recommendation phase, for a given code snippet, we use the offline trained model to generate question titles to assist less experienced developers in writing questions more effectively. To evaluate Code2Que, we first sampled 50 low quality ⟨code snippet, question⟩ pairs from the Python and Java datasets on Stack Overflow. Then we conducted a user study to evaluate the question titles generated by our approach as compared to human-written ones using three metrics: Clearness, Fitness and Willingness to Respond. Our experimental results show that for a large number of low-quality questions in Stack Overflow, Code2Que can improve the question titles in terms of Clearness, Fitness and Willingness measures.","Deep Learning, Question Quality, Seq2Seq Model, Stack Overflow","","ESEC/FSE 2021"
"Conference Paper","Wong CP,Santiesteban P,Kästner C,Le Goues C","VarFix: Balancing Edit Expressiveness and Search Effectiveness in Automated Program Repair","","2021","","","354–366","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468600;http://dx.doi.org/10.1145/3468264.3468600","10.1145/3468264.3468600","Automatically repairing a buggy program is essentially a search problem, searching for code transformations that pass a set of tests. Various search strategies have been explored, but they either navigate the search space in an ad hoc way using heuristics, or systemically but at the cost of limited edit expressiveness in the kinds of supported program edits. In this work, we explore the possibility of systematically navigating the search space without sacrificing edit expressiveness. The key enabler of this exploration is variational execution, a dynamic analysis technique that has been shown to be effective at exploring many similar executions in large search spaces. We evaluate our approach on IntroClassJava and Defects4J, showing that a systematic search is effective at leveraging and combining fixing ingredients to find patches, including many high-quality patches and multi-edit patches.","automatic program repair, variational execution","","ESEC/FSE 2021"
"Conference Paper","Wang X,Xiao L,Yu T,Woepse A,Wong S","An Automatic Refactoring Framework for Replacing Test-Production Inheritance by Mocking Mechanism","","2021","","","540–552","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468590;http://dx.doi.org/10.1145/3468264.3468590","10.1145/3468264.3468590","Unit testing focuses on verifying the functions of individual units of a software system. It is challenging due to the high inter-dependencies among software units. Developers address this by mocking-replacing the dependency by a ""faked"" object. Despite the existence of powerful, dedicated mocking frameworks, developers often turn to a ""hand-rolled"" approach-inheritance. That is, they create a subclass of the dependent class and mock its behavior through method overriding. However, this requires tedious implementation and compromises the design quality of unit tests. This work contributes a fully automated refactoring framework to identify and replace the usage of inheritance by using Mockito-a well received mocking framework. Our approach is built upon the empirical experience from five open source projects that use inheritance for mocking. We evaluate our approach on four other projects. Results show that our framework is efficient, generally applicable to new datasets, mostly preserves test case behaviors in detecting defects (in the form of mutants), and decouples test code from production code. The qualitative evaluation by experienced developers suggests that the auto-refactoring solutions generated by our framework improve the quality of the unit test cases in various aspects, such as making test conditions more explicit, as well as improved cohesion, readability, understandability, and maintainability with test cases.","software refactoring, software testing","","ESEC/FSE 2021"
"Conference Paper","Shen B,Zhang W,Kästner C,Zhao H,Wei Z,Liang G,Jin Z","SmartCommit: A Graph-Based Interactive Assistant for Activity-Oriented Commits","","2021","","","379–390","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Athens, Greece","2021","9781450385626","","https://doi.org/10.1145/3468264.3468551;http://dx.doi.org/10.1145/3468264.3468551","10.1145/3468264.3468551","In collaborative software development, it is considered to be a best practice to submit code changes as a sequence of cohesive commits, each of which records the work result of a specific development activity, such as adding a new feature, bug fixing, and refactoring. However, rather than following this best practice, developers often submit a set of loosely-related changes serving for different development activities as a composite commit, due to the tedious manual work and lack of effective tool support to decompose such a tangled changeset. Composite commits often obfuscate the change history of software artifacts and bring challenges to efficient collaboration among developers. To encourage activity-oriented commits, we propose SmartCommit, a graph-partitioning-based interactive approach to tangled changeset decomposition that leverages not only the efficiency of algorithms but also the knowledge of developers. To evaluate the effectiveness of our approach, we (1) deployed SmartCommit in an international IT company, and analyzed usage data collected from a field study with 83 engineers over 9 months; and (2) conducted a controlled experiment on 3,000 synthetic composite commits from 10 diverse open-source projects. Results show that SmartCommit achieves a median accuracy between 71–84% when decomposing composite commits without developer involvement, and significantly helps developers follow the best practice of submitting activity-oriented commits with acceptable interaction effort and time cost in real collaborative software development.","revision control system, changes decomposition, code commit","","ESEC/FSE 2021"
"Conference Paper","Singh S,Chaturvedy K,Mishra B","Multi-View Learning for Repackaged Malware Detection","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Availability, Reliability and Security","Vienna, Austria","2021","9781450390514","","https://doi.org/10.1145/3465481.3470040;http://dx.doi.org/10.1145/3465481.3470040","10.1145/3465481.3470040","Repackaging refers to the core process of unpacking a software package, then repackaging it after a probable modification to the decompiled code and/or to other resource files. In the case of repackaged malware, benign apps are injected with a malicious payload. Repackaged malware pose a serious threat to the Android ecosystem. Moreover, repackaged malware and benign apps share more than 80% of their features, which makes detection a challenging problem. This paper presents a novel technique based on multi-view learning to address this challenge of detecting repackaged malware. Multi-View Learning is a technique where data from multiple distinct feature groups, referred to as views, are fused to improve the model’s generalization performance. In the context of Android, we define views as different components of the app, such as permissions, APIs, sensor usage, etc. We analyzed 15,297 repackaged app pairs and extracted seven different views to represent an app. We perform an ablation study to identify which view(s) contribute more to the classification. The model was trained end-to-end to jointly learn appropriate features and to perform the classification. We show that our approach achieves accuracy and an F1-score of 97.46% and 0.98, respectively.","Malware detection, Mobile apps, Repackaged malware, Multi-view learning","","ARES 21"
"Conference Paper","Puodzius C,Zendra O,Heuser A,Noureddine L","Accurate and Robust Malware Analysis through Similarity of External Calls Dependency Graphs (ECDG)","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Availability, Reliability and Security","Vienna, Austria","2021","9781450390514","","https://doi.org/10.1145/3465481.3470115;http://dx.doi.org/10.1145/3465481.3470115","10.1145/3465481.3470115","Malware is a primary concern in cybersecurity, being one of the attacker’s favorite cyberweapons. Over time, malware evolves not only in complexity but also in diversity and quantity. Malware analysis automation is thus crucial. In this paper we present ECDGs, a shorter call graph representation, and a new similarity function that is accurate and robust. Toward this goal, we revisit some principles of malware analysis research to define basic primitives and an evaluation paradigm addressed for the setup of more reliable experiments. Our benchmark shows that our similarity function is very efficient in practice, achieving speedup rates of 3.30x and 354,11x wrt. radiff2 for the standard and the cache-enhanced implementations, respectively. Our evaluations generate clusters that produce almost unerring results - homogeneity score of 0.983 for the accuracy phase - and marginal information loss for a highly polluted dataset - NMI score of 0.974 between initial and final clusters of the robustness phase. Overall, ECDGs and our similarity function enable autonomous frameworks for malware search and clustering that can assist human-based analysis or improve classification models for malware analysis.","binary code analysis, similarity, malware, call graph","","ARES 21"
"Conference Paper","Piva M,Maselli G,Restuccia F","The Tags Are Alright: Robust Large-Scale RFID Clone Detection Through Federated Data-Augmented Radio Fingerprinting","","2021","","","41–50","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing","Shanghai, China","2021","9781450385589","","https://doi.org/10.1145/3466772.3467033;http://dx.doi.org/10.1145/3466772.3467033","10.1145/3466772.3467033","Millions of RFID tags are pervasively used all around the globe to inexpensively identify a wide variety of everyday-use objects. One of the key issues of RFID is that tags cannot use energy-hungry cryptography, and thus can be easily cloned. For this reason, radio fingerprinting (RFP) is a compelling approach that leverages the unique imperfections in the tag's wireless circuitry to achieve large-scale RFID clone detection. Recent work, however, has unveiled that time-varying channel conditions can significantly decrease the accuracy of the RFP process. Prior art in RFID identification does not consider this critical aspect, and instead focuses on custom-tailored feature extraction techniques and data collection with static channel conditions. For this reason, we propose the first large-scale investigation into RFP of RFID tags with dynamic channel conditions. Specifically, we perform a massive data collection campaign on a testbed composed by 200 off-the-shelf identical RFID tags and a software-defined radio (SDR) tag reader. We collect data with different tag-reader distances in an over-the-air configuration. To emulate implanted RFID tags, we also collect data with two different kinds of porcine meat inserted between the tag and the reader. We use this rich dataset to train and test several convolutional neural network (CNN)-based classifiers in a variety of channel conditions. Our investigation reveals that training and testing on different channel conditions drastically degrades the classifier's accuracy. For this reason, we propose a novel training framework based on federated machine learning (FML) and data augmentation (DAG) to boost the accuracy. Extensive experimental results indicate that (i) our FML approach improves accuracy by up to 48%; (ii) our DAG approach improves the FML performance by up to 19% and the single-dataset performance by 31%. To the best of our knowledge, this is the first paper experimentally demonstrating the efficacy of FML and DAG on a large device population. To allow full replicability, we are sharing with the research community our fully-labeled 200-GB RFID waveform dataset, as well as the entirety of our code and trained models, concurrently with our submission.","Federated Learning, Fingerprint, Data Augmentation, RFID","","MobiHoc '21"
"Journal Article","Wang H,Xia X,Lo D,He Q,Wang X,Grundy J","Context-Aware Retrieval-Based Deep Commit Message Generation","ACM Trans. Softw. Eng. Methodol.","2021","30","4","","Association for Computing Machinery","New York, NY, USA","","","2021-07","","1049-331X","https://doi.org/10.1145/3464689;http://dx.doi.org/10.1145/3464689","10.1145/3464689","Commit messages recorded in version control systems contain valuable information for software development, maintenance, and comprehension. Unfortunately, developers often commit code with empty or poor quality commit messages. To address this issue, several studies have proposed approaches to generate commit messages from commit diffs. Recent studies make use of neural machine translation algorithms to try and translate git diffs into commit messages and have achieved some promising results. However, these learning-based methods tend to generate high-frequency words but ignore low-frequency ones. In addition, they suffer from exposure bias issues, which leads to a gap between training phase and testing phase.In this article, we propose CoRec to address the above two limitations. Specifically, we first train a context-aware encoder-decoder model that randomly selects the previous output of the decoder or the embedding vector of a ground truth word as context to make the model gradually aware of previous alignment choices. Given a diff for testing, the trained model is reused to retrieve the most similar diff from the training set. Finally, we use the retrieval diff to guide the probability distribution for the final generated vocabulary. Our method combines the advantages of both information retrieval and neural machine translation. We evaluate CoRec on a dataset from Liu et al. and a large-scale dataset crawled from 10K popular Java repositories in Github. Our experimental results show that CoRec significantly outperforms the state-of-the-art method NNGen by 19% on average in terms of BLEU.","information retrieval, neural machine translation, Commit message generation","",""
"Journal Article","Bogart C,Kästner C,Herbsleb J,Thung F","When and How to Make Breaking Changes: Policies and Practices in 18 Open Source Software Ecosystems","ACM Trans. Softw. Eng. Methodol.","2021","30","4","","Association for Computing Machinery","New York, NY, USA","","","2021-07","","1049-331X","https://doi.org/10.1145/3447245;http://dx.doi.org/10.1145/3447245","10.1145/3447245","Open source software projects often rely on package management systems that help projects discover, incorporate, and maintain dependencies on other packages, maintained by other people. Such systems save a great deal of effort over ad hoc ways of advertising, packaging, and transmitting useful libraries, but coordination among project teams is still needed when one package makes a breaking change affecting other packages. Ecosystems differ in their approaches to breaking changes, and there is no general theory to explain the relationships between features, behavioral norms, ecosystem outcomes, and motivating values. We address this through two empirical studies. In an interview case study, we contrast Eclipse, NPM, and CRAN, demonstrating that these different norms for coordination of breaking changes shift the costs of using and maintaining the software among stakeholders, appropriate to each ecosystem’s mission. In a second study, we combine a survey, repository mining, and document analysis to broaden and systematize these observations across 18 ecosystems. We find that all ecosystems share values such as stability and compatibility, but differ in other values. Ecosystems’ practices often support their espoused values, but in surprisingly diverse ways. The data provides counterevidence against easy generalizations about why ecosystem communities do what they do.","semantic versioning, Software ecosystems, dependency management, qualitative research, collaboration","",""
"Journal Article","Hemel A,Kalleberg KT,Vermaas R,Dolstra E","Finding Software License Violations Through Binary Code Clone Detection - A Retrospective","SIGSOFT Softw. Eng. Notes","2021","46","3","24–25","Association for Computing Machinery","New York, NY, USA","","","2021-07","","0163-5948","https://doi.org/10.1145/3468744.3468752;http://dx.doi.org/10.1145/3468744.3468752","10.1145/3468744.3468752","Ten years ago, we published the article Finding software license violations through binary code clone detection at the MSR 2011 conference. Our paper was motivated by the tendency of em- bedded hardware vendors to only release binary blobs of their rmware, often violating the licensing terms of open-source soft- ware present inside those blobs. The techniques presented in our paper were designed to accurately identify open-source code hid- den inside binary blobs. Here, we give our perspectives on the impact of our work, both industrially and academically, and re- visit the original problem statement to see what has happened in the eld of open-source compliance in the intervening decade.","code clone detection, binary analysis, repository mining, rmware","",""
"Conference Paper","Perdpunya T,Nuchitprasitchai S,Boonrawd P","Augmented Reality with Mask R-CNN (ARR-CNN) Inspection for Intelligent Manufacturing","","2021","","","","Association for Computing Machinery","New York, NY, USA","The 12th International Conference on Advances in Information Technology","Bangkok, Thailand","2021","9781450390125","","https://doi.org/10.1145/3468784.3468788;http://dx.doi.org/10.1145/3468784.3468788","10.1145/3468784.3468788","A machine is an essential factor for industrial production. Industry 4.0 is the revolution that causes improvement of machines to have higher efficiency. Accordingly, inspection and maintenance are becoming more important. However, most of factories are not changed the operating process, there is no data logging for evaluation and analysis for preventive maintenance. This research aims to develop a model for machine inspection using augmented reality with object detection and marker techniques on real world machines and mask R-CNN algorithm allowing inspector to perform inspections. This study, we demonstrate the process of development of the proposed model by showing steps of data acquisition from a machine in a factory. The dataset is images of machines in different perspectives, and they were used for training and testing the model. The testing is done on a mobile device of an inspector. With computer vision technique and the proposed model, the instant precision tracking and detection are provided. Then the trained model is transferred to the mobile devices for testing without any modification by an expert. Some images of machines are randomly selected to verify the accuracy of the model. The result shows that the efficiency of the model is acceptable in real usage.","Object Detection, Smart maintenance, Augmented Reality, Mask R-CNN, industry 4.0","","IAIT2021"
"Journal Article","Zeltner T,Speierer S,Georgiev I,Jakob W","Monte Carlo Estimators for Differential Light Transport","ACM Trans. Graph.","2021","40","4","","Association for Computing Machinery","New York, NY, USA","","","2021-07","","0730-0301","https://doi.org/10.1145/3450626.3459807;http://dx.doi.org/10.1145/3450626.3459807","10.1145/3450626.3459807","Physically based differentiable rendering algorithms propagate derivatives through realistic light transport simulations and have applications in diverse areas including inverse reconstruction and machine learning. Recent progress has led to unbiased methods that can simultaneously compute derivatives with respect to millions of parameters. At the same time, elementary properties of these methods remain poorly understood.Current algorithms for differentiable rendering are constructed by mechanically differentiating a given primal algorithm. While convenient, such an approach is simplistic because it leaves no room for improvement. Differentiation produces major changes in the integrals that occur throughout the rendering process, which indicates that the primal and differential algorithms should be decoupled so that the latter can suitably adapt.This leads to a large space of possibilities: consider that even the most basic Monte Carlo path tracer already involves several design choices concerning the techniques for sampling materials and emitters, and their combination, e.g. via multiple importance sampling (MIS). Differentiation causes a veritable explosion of this decision tree: should we differentiate only the estimator, or also the sampling technique? Should MIS be applied before or after differentiation? Are specialized derivative sampling strategies of any use? How should visibility-related discontinuities be handled when millions of parameters are differentiated simultaneously? In this paper, we provide a taxonomy and analysis of different estimators for differential light transport to provide intuition about these and related questions.","radiative backpropagation, inverse rendering, differentiating visibility, differentiable rendering","",""
"Conference Paper","Bui ND,Yu Y,Jiang L","Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations","","2021","","","511–521","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval","Virtual Event, Canada","2021","9781450380379","","https://doi.org/10.1145/3404835.3462840;http://dx.doi.org/10.1145/3404835.3462840","10.1145/3404835.3462840","We propose Corder, a self-supervised contrastive learning framework for source code model. Corder is designed to alleviate the need of labeled data for code retrieval and code summarization tasks. The pre-trained model of Corder can be used in two ways: (1) it can produce vector representation of code which can be applied to code retrieval tasks that do not have labeled data; (2) it can be used in a fine-tuning process for tasks that might still require label data such as code summarization. The key innovation is that we train the source code model by asking it to recognize similar and dissimilar code snippets through acontrastive learning objective. To do so, we use a set of semantic-preserving transformation operators to generate code snippets that are syntactically diverse but semantically equivalent. Through extensive experiments, we have shown that the code models pretrained by Corder substantially outperform the other baselines for code-to-code retrieval, text-to-code retrieval, and code-to-text summarization tasks.","code-retrieval, code-learning, source-code-model, code-search, code-representation, code-summarization, compiler, contrastive-learning, big-code, program-transformation","","SIGIR '21"
"Conference Paper","Xue L,Yan Y,Yan L,Jiang M,Luo X,Wu D,Zhou Y","Parema: An Unpacking Framework for Demystifying VM-Based Android Packers","","2021","","","152–164","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, Denmark","2021","9781450384599","","https://doi.org/10.1145/3460319.3464839;http://dx.doi.org/10.1145/3460319.3464839","10.1145/3460319.3464839","Android packers have been widely adopted by developers to protect apps from being plagiarized. Meanwhile, various unpacking tools unpack the apps through direct memory dumping. To defend against these off-the-shelf unpacking tools, packers start to adopt virtual machine (VM) based protection techniques, which replace the original Dalvik bytecode (DCode) with customized bytecode (PCode) in memory. This defeats the unpackers using memory dumping mechanisms. However, little is known about whether such packers can provide enough protection to Android apps. In this paper, we aim to shed light on these questions and take the first step towards demystifying the protections provided to the apps by the VM-based packers. We proposed novel program analysis techniques to investigate existing commercial VM-based packers including a learning phase and a deobfuscation phase.We aim at deobfuscating the VM-protection DCode in three scenarios, recovering original DCode or its semantics with training apps, and restoring the semantics without training apps. We also develop a prototype named Parema to automate much work of the deobfuscation procedure. By applying it to the online VM-based Android packers, we reveal that all evaluated packers do not provide adequate protection and could be compromised.","App Protection, Code Similarity, Obfuscation","","ISSTA 2021"
"Conference Paper","Li Y,Zhang Z,Liu B,Yang Z,Liu Y","ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection","","2021","","","139–151","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, Denmark","2021","9781450384599","","https://doi.org/10.1145/3460319.3464816;http://dx.doi.org/10.1145/3460319.3464816","10.1145/3460319.3464816","The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.","model reuse, Deep neural networks, vulnerability propagation, similarity comparison, intellectual property","","ISSTA 2021"
"Conference Paper","Shariffdeen R,Gao X,Duck GJ,Tan SH,Lawall J,Roychoudhury A","Automated Patch Backporting in Linux (Experience Paper)","","2021","","","633–645","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, Denmark","2021","9781450384599","","https://doi.org/10.1145/3460319.3464821;http://dx.doi.org/10.1145/3460319.3464821","10.1145/3460319.3464821","Whenever a bug or vulnerability is detected in the Linux kernel, the kernel developers will endeavour to fix it by introducing a patch into the mainline version of the Linux kernel source tree. However, many users run older “stable” versions of Linux, meaning that the patch should also be “backported” to one or more of these older kernel versions. This process is error-prone and there is usually along delay in publishing the backported patch. Based on an empirical study, we show that around 8% of all commits submitted to Linux mainline are backported to older versions,but often more than one month elapses before the backport is available. Hence, we propose a patch backporting technique that can automatically transfer patches from the mainline version of Linux into older stable versions. Our approach first synthesizes a partial transformation rule based on a Linux mainline patch. This rule can then be generalized by analysing the alignment between the mainline and target versions. The generalized rule is then applied to the target version to produce a backported patch. We have implemented our transformation technique in a tool called FixMorph and evaluated it on 350 Linux mainline patches. FixMorph correctly backports 75.1% of them. Compared to existing techniques, FixMorph improves both the precision and recall in backporting patches. Apart from automation of software maintenance tasks, patch backporting helps in reducing the exposure to known security vulnerabilities in stable versions of the Linux kernel.","Linux Kernel, Patch Backporting, Program Transformation","","ISSTA 2021"
"Journal Article","Roy SK,Devaraj R,Sarkar A,Senapati D","SLAQA: Quality-Level Aware Scheduling of Task Graphs on Heterogeneous Distributed Systems","ACM Trans. Embed. Comput. Syst.","2021","20","5","","Association for Computing Machinery","New York, NY, USA","","","2021-07","","1539-9087","https://doi.org/10.1145/3462776;http://dx.doi.org/10.1145/3462776","10.1145/3462776","Continuous demands for higher performance and reliability within stringent resource budgets is driving a shift from homogeneous to heterogeneous processing platforms for the implementation of today’s cyber-physical systems (CPSs). These CPSs are typically represented as Directed-acyclic Task Graph (DTG) due to the complex interactions between their functional components that are often distributed in nature. In this article, we consider the problem of scheduling a real-time application modelled as a single DTG, where tasks may have multiple implementations designated as quality-levels, with higher quality-levels producing more accurate results and contributing to higher rewards/Quality-of-Service for the system. First, we introduce an optimal solution using Integer Linear Programming (ILP) for a DTG with multiple quality-levels, to be executed on a heterogeneous distributed platform. However, this ILP-based optimal solution exhibits high computational complexity and does not scale for moderately large problem sizes. Hence, we propose two low-overhead heuristic algorithms called Global Slack Aware Quality-level Allocator (G-SLAQA) and Total Slack Aware Quality-level Allocator (T-SLAQA), which are able to produce satisfactorily efficient as well as fast solutions within a reasonable time. G-SLAQA, the baseline heuristic, is greedier and faster than its counter-part T-SLAQA, whose performance is at least as efficient as G-SLAQA. The efficiency of all the proposed schemes have been extensively evaluated through simulation-based experiments using benchmark and randomly generated DTGs. Through the case study of a real-world automotive traction controller, we generate schedules using our proposed schemes to demonstrate their practical applicability.","optimal scheduling, directed-acyclic task graphs, heterogeneous platform, Distributed systems, integer linear programming, quality-of-service","",""
"Conference Paper","Buffardi K,Aguirre-Ayala J","Unit Test Smells and Accuracy of Software Engineering Student Test Suites","","2021","","","234–240","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456328;http://dx.doi.org/10.1145/3430665.3456328","10.1145/3430665.3456328","With an increasing emphasis on unit testing in computer science curricula, we examined students' work on testing assignments to investigate their adoption of test smells---practices that indicate potential problems in unit tests. We discovered three common causes of test smells in students' unit tests: multiple member function calls, multiple assertions, and conditional logic. We also explored how each might be associated with test inaccuracies.In a quasi-experimental study, we evaluated the quality of students' unit tests by evaluating test accuracy---tests' reliability in distinguishing between a corpus of acceptable production code and a separate corpus containing faults. Correlational and comparative analyses revealed that unit tests with calls to multiple member functions and/or conditional logic were associated with worse test accuracy. However, no relationship was found between test accuracy and whether or not unit tests contained multiple assertions.","test accuracy, fault detection, computer science education, test effectiveness, code smell, software engineering education, coverage, verification, software testing, test smell, unit testing","","ITiCSE '21"
"Conference Paper","Chow SP,Komarlu T,Conrad PT","Teaching Testing with Modern Technology Stacks in Undergraduate Software Engineering Courses","","2021","","","241–247","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456352;http://dx.doi.org/10.1145/3430665.3456352","10.1145/3430665.3456352","Students' experience with software testing in undergraduate computing courses is often relatively shallow, as compared to the importance of the topic. This experience report describes introducing industrial-strength testing into CMPSC 156, an upper division course in software engineering at UC Santa Barbara. We describe our efforts to modify our software engineering course to introduce rigorous test-coverage requirements into full-stack web development projects, requirements similar to those the authors had experienced in a professional software development setting. We present student feedback on the course and coverage metrics for the projects. We reflect on what about these changes worked (or didn't), and provide suggestions for other instructors that would like to give their students a deeper experience with software testing in their software engineering courses.","continuous integration, integration testing, unit testing, software engineering education, web applications, test coverage, testing, computer science education","","ITiCSE '21"
"Conference Paper","Wang W,Kwatra A,Skripchuk J,Gomes N,Milliken A,Martens C,Barnes T,Price T","Novices' Learning Barriers When Using Code Examples in Open-Ended Programming","","2021","","","394–400","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456370;http://dx.doi.org/10.1145/3430665.3456370","10.1145/3430665.3456370","Open-ended programming increases students' motivation by allowing them to solve authentic problems and connect programming to their own interests. However, such open-ended projects are also challenging, as they often encourage students to explore new programming features and attempt tasks that they have not learned before. Code examples are effective learning materials for students and are well-suited to supporting open-ended programming. However, there is little work to understand how novices learn with examples during open-ended programming, and few real-world deployments of such tools. In this paper, we explore novices' learning barriers when interacting with code examples during open-ended programming. We deployed Example Helper, a tool that offers galleries of code examples to search and use, with 44 novice students in an introductory programming classroom, working on an open-ended project in Snap. We found three high-level barriers that novices encountered when using examples: decision, search, and integration barriers. We discuss how these barriers arise and design opportunities to address them.","opportunistic learning, code examples, open-ended programming, API learning","","ITiCSE '21"
"Conference Paper","Kirk D,Crow T,Luxton-Reilly A,Tempero E","Mind the Gap: Searching for Clarity in NCEA","","2021","","","192–198","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456318;http://dx.doi.org/10.1145/3430665.3456318","10.1145/3430665.3456318","The introduction of programming into secondary schools in New Zealand (NZ) has meant that many teachers are preparing programming students at senior level for the New Zealand Certificate of Educational Achievement (NCEA). A recent study showed that many teachers struggle with providing feedback to students on code quality, and indicated a possible issue with the available resources. In this paper, we describe a study to help us understand this phenomenon. We first analysed the data from the earlier study concerning teachers' viewpoints on the available resources. We then analysed the defining curriculum and assessment documents for NCEA, extracting quality-relating terms. We found that all teachers interviewed reported issues with the resources and that there was no clear mapping for quality concepts between the two sets of resources. Our contribution is to expose gaps in the online resources that are problematic from the perspective of teacher and student outcomes.","code quality, computing education, k12, programming","","ITiCSE '21"
"Conference Paper","Liao SN,Shah K,Griswold WG,Porter L","A Quantitative Analysis of Study Habits Among Lower- and Higher-Performing Students in CS1","","2021","","","366–372","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456350;http://dx.doi.org/10.1145/3430665.3456350","10.1145/3430665.3456350","Our prior work found differences in study habits between high- and low-performers in a small-scale qualitative study, and this work seeks to verify and extend these findings by examining the study habits of a larger population of CS1 students. To do this, we devised a survey based on the findings of our prior qualitative study. The responses of CS1 students reveals that some study habits are more frequently practiced by higher-performers then lower-performers or vice versa. One concern with these findings is that the differences in study habits might simply be explained by prior experience. As such, we compare study habits between students with and without prior experience as well. We find that although prior experience translates to better class performance, it is not associated with the same study habits as lower- and higher-performers, suggesting that prior experience and study habits are separately associated with better student performance. These findings encourage further inquiry into the role of study habits in student success and whether explicit instruction on better study habits might be the basis for successful future interventions.","higher-performing students, lower-performing students, self-regulation, CS1, study habits","","ITiCSE '21"
"Conference Paper","Goletti O,Mens K,Hermans F","Tutors' Experiences in Using Explicit Strategies in a Problem-Based Learning Introductory Programming Course","","2021","","","157–163","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1","Virtual Event, Germany","2021","9781450382144","","https://doi.org/10.1145/3430665.3456348;http://dx.doi.org/10.1145/3430665.3456348","10.1145/3430665.3456348","In programming education, explicit strategies are gaining traction. The reason for this study was to improve an introductory programming course based on a problem-based methodology, by using more explicit programming strategies. After analysing a previous run of this course for first year undergraduate students, we concluded that such strategies could improve learning transfer for students across the different weeks of the semester. We introduced four instructional strategies to tutors with close to no pedagogical background: explicit tracing, subgoal labeled worked examples, Parsons problems and explicit problem solving. These explicit programming strategies aim to decrease cognitive load. Tutors tested these four strategies in the course.Our goal was to explore how tutors could benefit in their tutoring from explicit strategies. Interviews with the tutors show that the easiest and most effective of the tested strategies were best used. For the more elaborate strategies, more time should be devoted to explain and model them or they can be misunderstood and misapplied.We conclude that four criteria are key to successfully using an explicit strategy: easy to understand, straightforward to apply, useful on the long term and supported by literature.","cognitive load, explicit programming strategies, problem-based learning","","ITiCSE '21"
"Conference Paper","Rocha RC,Petoumenos P,Wang Z,Cole M,Hazelwood K,Leather H","HyFM: Function Merging for Free","","2021","","","110–121","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems","Virtual, Canada","2021","9781450384728","","https://doi.org/10.1145/3461648.3463852;http://dx.doi.org/10.1145/3461648.3463852","10.1145/3461648.3463852","Function merging is an important optimization for reducing code size. It merges multiple functions into a single one, eliminating duplicate code among them. The existing state-of-the-art relies on a well-known sequence alignment algorithm to identify duplicate code across whole functions. However, this algorithm is quadratic in time and space on the number of instructions. This leads to very high time overheads and prohibitive levels of memory usage even for medium-sized benchmarks. For larger programs, it becomes impractical. This is made worse by an overly eager merging approach. All selected pairs of functions will be merged. Only then will this approach estimate the potential benefit from merging and decide whether to replace the original functions with the merged one. Given that most pairs are unprofitable, a significant amount of time is wasted producing merged functions that are simply thrown away. In this paper, we propose HyFM, a novel function merging technique that delivers similar levels of code size reduction for significantly lower time overhead and memory usage. Unlike the state-of-the-art, our alignment strategy works at the block level. Since basic blocks are usually much shorter than functions, even a quadratic alignment is acceptable. However, we also propose a linear algorithm for aligning blocks of the same size at a much lower cost. We extend this strategy with a multi-tier profitability analysis that bails out early from unprofitable merging attempts. By aligning individual pairs of blocks, we are able to decide their alignment’s profitability separately and before actually generating code. Experimental results on SPEC 2006 and 2017 show that HyFM needs orders of magnitude less memory, using up to 48 MB or 5.6 MB, depending on the variant used, while the state-of-the-art requires 32 GB in the worst case. HyFM also runs over 4.5×× faster, while still achieving comparable code size reduction. Combined with the speedup of later compilation stages due to the reduced number of functions, HyFM contributes to a reduced end-to-end compilation time.","Code-Size Reduction, LLVM, Function Merging, Link-Time Optimization, Interprocedural Optimization","","LCTES 2021"
"Conference Paper","Bora P,Awalgaonkar T,Palve H,Joshi R,Goel P","ICodeNet - A Hierarchical Neural Network Approach For Source Code Author Identification","","2021","","","180–185","Association for Computing Machinery","New York, NY, USA","2021 13th International Conference on Machine Learning and Computing","Shenzhen, China","2021","9781450389310","","https://doi.org/10.1145/3457682.3457709;http://dx.doi.org/10.1145/3457682.3457709","10.1145/3457682.3457709","With the open-source revolution, source codes are now more easily accessible than ever. This has, however, made it easier for malicious users and institutions to copy the code without giving regards to the license, or credit to the original author. Therefore, source code author identification is a critical task with paramount importance. In this paper, we propose ICodeNet - a hierarchical neural network that can be used for source code file-level tasks. The ICodeNet processes source code in image format and is employed for the task of per file author identification. The ICodeNet consists of an ImageNet trained VGG encoder followed by a shallow neural network. The shallow network is based either on CNN or LSTM. Different variations of models are evaluated on a source code author classification dataset. We have also compared our image-based hierarchical neural network model with simple image-based CNN architecture and text-based CNN and LSTM models to highlight its novelty and efficiency.","Transfer Learning, Long Short Term Memory, Convolutional Neural Network, Author Identification","","ICMLC 2021"
"Conference Paper","Aumpansub A,Huang Z","Detecting Software Vulnerabilities Using Neural Networks","","2021","","","166–171","Association for Computing Machinery","New York, NY, USA","2021 13th International Conference on Machine Learning and Computing","Shenzhen, China","2021","9781450389310","","https://doi.org/10.1145/3457682.3457707;http://dx.doi.org/10.1145/3457682.3457707","10.1145/3457682.3457707","As software vulnerabilities remain prevalent, automatically detecting software vulnerabilities is crucial for software security. Recently neural networks have been shown to be a promising tool in detecting software vulnerabilities. In this paper, we use neural networks trained with program slices, which extract the syntax and semantic characteristics of the source code of programs, to detect software vulnerabilities in C/C++ programs. To achieve a strong prediction model, we combine different types of program slices and optimize different types of neural networks. Our result shows that combining different types of characteristics of source code and using a balanced ratio of vulnerable program slices and non-vulnerable program slices a balanced accuracy in predicting both vulnerable code and non-vulnerable code. Among different neural networks, BGRU performs the best in detecting software vulnerabilities with an accuracy of 94.89%.","software vulnerability, neural network, deep learning, vulnerability detection","","ICMLC 2021"
"Conference Paper","Aljedaani W,Peruma A,Aljohani A,Alotaibi M,Mkaouer MW,Ouni A,Newman CD,Ghallab A,Ludi S","Test Smell Detection Tools: A Systematic Mapping Study","","2021","","","170–180","Association for Computing Machinery","New York, NY, USA","Evaluation and Assessment in Software Engineering","Trondheim, Norway","2021","9781450390538","","https://doi.org/10.1145/3463274.3463335;http://dx.doi.org/10.1145/3463274.3463335","10.1145/3463274.3463335","Test smells are defined as sub-optimal design choices developers make when implementing test cases. Hence, similar to code smells, the research community has produced numerous test smell detection tools to investigate the impact of test smells on the quality and maintenance of test suites. However, little is known about the characteristics, type of smells, target language, and availability of these published tools. In this paper, we provide a detailed catalog of all known, peer-reviewed, test smell detection tools.We start with performing a comprehensive search of peer-reviewed scientific publications to construct a catalog of 22 tools. Then, we perform a comparative analysis to identify the smell types detected by each tool and other salient features that include programming language, testing framework support, detection strategy, and adoption, among others. From our findings, we discover tools that detect test smells in Java, Scala, Smalltalk, and C++ test suites, with Java support favored by most tools. These tools are available as command-line and IDE plugins, among others. Our analysis also shows that most tools overlap in detecting specific smell types, such as General Fixture. Further, we encounter four types of techniques these tools utilize to detect smells. We envision our study as a one-stop source for researchers and practitioners in determining the tool appropriate for their needs. Our findings also empower the community with information to guide future tool development.","","","EASE 2021"
"Conference Paper","Yang C,Liang P,Fu L,Li Z","Self-Claimed Assumptions in Deep Learning Frameworks: An Exploratory Study","","2021","","","139–148","Association for Computing Machinery","New York, NY, USA","Evaluation and Assessment in Software Engineering","Trondheim, Norway","2021","9781450390538","","https://doi.org/10.1145/3463274.3463333;http://dx.doi.org/10.1145/3463274.3463333","10.1145/3463274.3463333","Deep learning (DL) frameworks have been extensively designed, implemented, and used in software projects across many domains. However, due to the lack of knowledge or information, time pressure, complex context, etc., various uncertainties emerge during the development, leading to assumptions made in DL frameworks. Though not all the assumptions are negative to the frameworks, being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures). As the first step of addressing the critical problems, there is a need to explore and understand the assumptions made in DL frameworks. To this end, we conducted an exploratory study to understand self-claimed assumptions (SCAs) about their distribution, classification, and impacts using code comments from nine popular DL framework projects on GitHub. The results are that: (1) 3,084 SCAs are scattered across 1,775 files in the nine DL frameworks, ranging from 1,460 (TensorFlow) to 8 (Keras) SCAs. (2) There are four types of validity of SCAs: Valid SCA, Invalid SCA, Conditional SCA, and Unknown SCA, and four types of SCAs based on their content: Configuration and Context SCA, Design SCA, Tensor and Variable SCA, and Miscellaneous SCA. (3) Both valid and invalid SCAs may have an impact within a specific scope (e.g., in a function) on the DL frameworks. Certain technical debt is induced when making SCAs. There are source code written and decisions made based on SCAs. This is the first study on investigating SCAs in DL frameworks, which helps researchers and practitioners to get a comprehensive understanding on the assumptions made. We also provide the first dataset of SCAs for further research and practice in this area.","Deep Learning Framework, GitHub, Self-Claimed Assumption","","EASE 2021"
"Conference Paper","Lehr JP,Jammer T,Bischof C","MPI-CorrBench: Towards an MPI Correctness Benchmark Suite","","2021","","","69–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing","Virtual Event, Sweden","2021","9781450382175","","https://doi.org/10.1145/3431379.3460652;http://dx.doi.org/10.1145/3431379.3460652","10.1145/3431379.3460652","The Message Passing Interface (MPI) is the de-facto standard for distributed memory computing in high-performance computing (HPC). To aid developers write correct MPI programs, different tools have been proposed, e.g., Intel Trace Analyzer and Collector (ITAC), MUST, Parcoach and MPI-Checker. Unfortunately, the effectiveness of these tools is hard to compare, as they have not been evaluated on a common set of applications. More importantly, well-known and widespread benchmarks, which tend to be well-tested and error free, were used for their evaluation. To enable a structured comparison and improve the coverage and reliability of available MPI correctness tools, we propose MPI-CorrBench as a common test harness. MPI-CorrBench enables a structured comparison of the different tools available w.r.t. various types of errors. In our evaluation, we use MPI-CorrBench to provide a well-defined set of error-cases to MUST, ITAC, Parcoach and MPI-Checker. In particular, we find that ITAC and MUST complement each other in many cases. In general, MUST works better for detecting type errors while ITAC is better in detecting errors in non-blocking operations. Although the most-used functions of MPI are well supported, MPI-CorrBench shows that for one sided communication, the error detection capability of all evaluated tools needs improvement. Moreover, our experiments reveal a MPI standard violation in the MPICH test suite as well as several cases of discouraged use of MPI functionality.","hpc, mpi, correctness, message passing interface","","HPDC '21"
"Conference Paper","Garg S,Moghaddam RZ,Sundaresan N,Wu C","PerfLens: A Data-Driven Performance Bug Detection and Fix Platform","","2021","","","19–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis","Virtual, Canada","2021","9781450384681","","https://doi.org/10.1145/3460946.3464318;http://dx.doi.org/10.1145/3460946.3464318","10.1145/3460946.3464318","The wealth of open-source software development artifacts available online creates a great opportunity to learn the patterns of performance improvements from data. In this paper, we present a data-driven approach to software performance improvement in C#. We first compile a large dataset of hundreds of performance improvements made in open source projects. We then leverage this data to build a tool called PerfLens for performance improvement recommendations via code search. PerfLens indexes the performance improvements, takes a codebase as an input and searches a pool of performance improvements for similar code. We show that when our system is further augmented with profiler data information our recommendations are more accurate. Our experiments show that PerfLens can suggest performance improvements with 90% accuracy when profiler data is available and 55% accuracy when it analyzes source code only.","Machine Learning, Software Performance","","SOAP 2021"
"Conference Paper","Ren X,Ho M,Ming J,Lei Y,Li L","Unleashing the Hidden Power of Compiler Optimization on Binary Code Difference: An Empirical Study","","2021","","","142–157","Association for Computing Machinery","New York, NY, USA","Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation","Virtual, Canada","2021","9781450383912","","https://doi.org/10.1145/3453483.3454035;http://dx.doi.org/10.1145/3453483.3454035","10.1145/3453483.3454035","Hunting binary code difference without source code (i.e., binary diffing) has compelling applications in software security. Due to the high variability of binary code, existing solutions have been driven towards measuring semantic similarities from syntactically different code. Since compiler optimization is the most common source contributing to binary code differences in syntax, testing the resilience against the changes caused by different compiler optimization settings has become a standard evaluation step for most binary diffing approaches. For example, 47 top-venue papers in the last 12 years compared different program versions compiled by default optimization levels (e.g., -Ox in GCC and LLVM). Although many of them claim they are immune to compiler transformations, it is yet unclear about their resistance to non-default optimization settings. Especially, we have observed that adversaries explored non-default compiler settings to amplify malware differences. This paper takes the first step to systematically studying the effectiveness of compiler optimization on binary code differences. We tailor search-based iterative compilation for the auto-tuning of binary code differences. We develop BinTuner to search near-optimal optimization sequences that can maximize the amount of binary code differences. We run BinTuner with GCC 10.2 and LLVM 11.0 on SPEC benchmarks (CPU2006 & CPU2017), Coreutils, and OpenSSL. Our experiments show that at the cost of 279 to 1,881 compilation iterations, BinTuner can find custom optimization sequences that are substantially better than the general -Ox settings. BinTuner's outputs seriously undermine prominent binary diffing tools' comparisons. In addition, the detection rate of the IoT malware variants tuned by BinTuner falls by more than 50%. Our findings paint a cautionary tale for security analysts that attackers have a new way to mutate malware code cost-effectively, and the research community needs to step back to reassess optimization-resistance evaluations.","Compiler Optimization, Binary Code Difference","","PLDI 2021"
"Conference Paper","Erdweg S,Szabó T,Pacak A","Concise, Type-Safe, and Efficient Structural Diffing","","2021","","","406–419","Association for Computing Machinery","New York, NY, USA","Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation","Virtual, Canada","2021","9781450383912","","https://doi.org/10.1145/3453483.3454052;http://dx.doi.org/10.1145/3453483.3454052","10.1145/3453483.3454052","A structural diffing algorithm compares two pieces of tree-shaped data and computes their difference. Existing structural diffing algorithms either produce concise patches or ensure type safety, but never both. We present a new structural diffing algorithm called truediff that achieves both properties by treating subtrees as mutable, yet linearly typed resources. Mutation is required to derive concise patches that only mention changed nodes, but, in contrast to prior work, truediff guarantees all intermediate trees are well-typed. We formalize type safety, prove truediff has linear run time, and evaluate its performance and the conciseness of the derived patches empirically for real-world Python documents. While truediff ensures type safety, the size of its patches is on par with Gumtree, a popular untyped diffing implementation. Regardless, truediff outperforms Gumtree and a typed diffing implementation by an order of magnitude.","incremental computing, tree diffing","","PLDI 2021"
"Journal Article","Bowman B,Huang HH","Towards Next-Generation Cybersecurity with Graph AI","SIGOPS Oper. Syst. Rev.","2021","55","1","61–67","Association for Computing Machinery","New York, NY, USA","","","2021-06","","0163-5980","https://doi.org/10.1145/3469379.3469386;http://dx.doi.org/10.1145/3469379.3469386","10.1145/3469379.3469386","Cybersecurity professionals are inundated with large amounts of data, and require intelligent algorithms capable of distinguishing vulnerable from patched, normal from anomalous, and malicious from benign. Unfortunately, not all machine learning (ML) and artificial intelligence (AI) algorithms are created equal, and in this position paper we posit that a new breed of ML, specifically graph-based machine learning (Graph AI), is poised to make a significant impact in this domain. We will discuss the primary differentiators between traditional ML and graph ML, and provide reasons and justifications for why the latter is well-suited to many aspects of cybersecurity. We will present several example applications and result of graph ML in cybersecurity, followed by a discussion of the challenges that lie ahead.","","",""
"Conference Paper","Ji Y,Cui L,Huang HH","BugGraph: Differentiating Source-Binary Code Similarity with Graph Triplet-Loss Network","","2021","","","702–715","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security","Virtual Event, Hong Kong","2021","9781450382878","","https://doi.org/10.1145/3433210.3437533;http://dx.doi.org/10.1145/3433210.3437533","10.1145/3433210.3437533","Binary code similarity detection, which answers whether two pieces of binary code are similar, has been used in a number of applications,such as vulnerability detection and automatic patching. Existing approaches face two hurdles in their efforts to achieve high accuracy and coverage: (1) the problem of source-binary code similarity detection, where the target code to be analyzed is in the binary format while the comparing code (with ground truth) is in source code format. Meanwhile, the source code is compiled to the comparing binary code with either a random or fixed configuration (e.g.,architecture, compiler family, compiler version, and optimization level), which significantly increases the difficulty of code similarity detection; and (2) the existence of different degrees of code similarity. Less similar code is known to be more, if not equally, important in various applications such as binary vulnerability study. To address these challenges, we design BugGraph, which performs source-binary code similarity detection in two steps. First, BugGraph identifies the compilation provenance of the target binary and compiles the comparing source code to a binary with the same provenance.Second, BugGraph utilizes a new graph triplet-loss network on the attributed control flow graph to produce a similarity ranking. The experiments on four real-world datasets show that BugGraph achieves 90% and 75% true positive rate for syntax equivalent and similar code, respectively, an improvement of 16% and 24% overstate-of-the-art methods. Moreover, BugGraph is able to identify 140 vulnerabilities in six commercial firmware.","code similarity, binary code, vulnerability, graph embedding","","ASIA CCS '21"
"Conference Paper","Meng D,Guerriero M,Machiry A,Aghakhani H,Bose P,Continella A,Kruegel C,Vigna G","Bran: Reduce Vulnerability Search Space in Large Open Source Repositories by Learning Bug Symptoms","","2021","","","731–743","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security","Virtual Event, Hong Kong","2021","9781450382878","","https://doi.org/10.1145/3433210.3453115;http://dx.doi.org/10.1145/3433210.3453115","10.1145/3433210.3453115","Software is continually increasing in size and complexity, and therefore, vulnerability discovery would benefit from techniques that identify potentially vulnerable regions within large code bases, as this allows for easing vulnerability detection by reducing the search space. Previous work has explored the use of conventional code-quality and complexity metrics in highlighting suspicious sections of (source) code. Recently, researchers also proposed to reduce the vulnerability search space by studying code properties with neural networks. However, previous work generally failed in leveraging the rich metadata that is available for long-running, large code repositories.In this paper, we present an approach, named Bran, to reduce the vulnerability search space by combining conventional code metrics with fine-grained repository metadata. Bran locates code sections that are more likely to contain vulnerabilities in large code bases, potentially improving the efficiency of both manual and automatic code audits. In our experiments on four large code bases, Bran successfully highlights potentially vulnerable functions, outperforming several baselines, including state-of-art vulnerability prediction tools. We also assess Bran's effectiveness in assisting automated testing tools. We use Bran to guide syzkaller, a known kernel fuzzer, in fuzzing a recent version of the Linux kernel. The guided fuzzer identifies 26 bugs (10 are zero-day flaws), including arbitrary writes and reads.","static analysis, machine learning, vulnerabilities","","ASIA CCS '21"
"Journal Article","Chen W,Li X,Sui Y,He N,Wang H,Wu L,Luo X","SADPonzi: Detecting and Characterizing Ponzi Schemes in Ethereum Smart Contracts","Proc. ACM Meas. Anal. Comput. Syst.","2021","5","2","","Association for Computing Machinery","New York, NY, USA","","","2021-06","","","https://doi.org/10.1145/3460093;http://dx.doi.org/10.1145/3460093","10.1145/3460093","Ponzi schemes are financial scams that lure users under the promise of high profits. With the prosperity of Bitcoin and blockchain technologies, there has been growing anecdotal evidence that this classic fraud has emerged in the blockchain ecosystem. Existing studies have proposed machine-learning based approaches for detecting Ponzi schemes, i.e., either based on the operation codes (opcodes) of the smart contract binaries or the transaction patterns of addresses. However, state-of-the-art approaches face several major limitations, including lacking interpretability and high false positive rates. Moreover, machine-learning based methods are susceptible to evasion techniques, and transaction-based techniques do not work on smart contracts that have a small number of transactions. These limitations render existing methods for detecting Ponzi schemes ineffective. In this paper, we propose SADPonzi, a semantic-aware detection approach for identifying Ponzi schemes in Ethereum smart contracts. Specifically, by strictly following the definition of Ponzi schemes, we propose a heuristic-guided symbolic execution technique to first generate the semantic information for each feasible path in smart contracts and then identify investor-related transfer behaviors and the distribution strategies adopted. Experimental result on a well-labelled benchmark suggests that SADPonzi can achieve 100% precision and recall, outperforming all existing machine-learning based techniques. We further apply SADPonzi to all 3.4 million smart contracts deployed by EOAs in Ethereum and identify 835 Ponzi scheme contracts, with over 17 million US Dollars invested by victims. Our observations confirm the urgency of identifying and mitigating Ponzi schemes in the blockchain ecosystem.","ethereum, smart contract, Ponzi scheme, symbolic execution","",""
"Journal Article","Bijlani A,Ramachandran U,Campbell R","Where Did My 256 GB Go? A Measurement Analysis of Storage Consumption on Smart Mobile Devices","Proc. ACM Meas. Anal. Comput. Syst.","2021","5","2","","Association for Computing Machinery","New York, NY, USA","","","2021-06","","","https://doi.org/10.1145/3460095;http://dx.doi.org/10.1145/3460095","10.1145/3460095","This work presents the first-ever detailed and large-scale measurement analysis of storage consumption behavior of applications (apps) on smart mobile devices. We start by carrying out a five-year longitudinal static analysis of millions of Android apps to study the increase in their sizes over time and identify various sources of app storage consumption. Our study reveals that mobile apps have evolved as large monolithic packages that are packed with features to monetize/engage users and optimized for performance at the cost of redundant storage consumption.We also carry out a mobile storage usage study with 140 Android participants. We built and deployed a lightweight context-aware storage tracing tool, called cosmos, on each participant's device. Leveraging the traces from our user study, we show that only a small fraction of apps/features are actively used and usage is correlated to user context. Our findings suggest a high degree of app feature bloat and unused functionality, which leads to inefficient use of storage. Furthermore, we found that apps are not constrained by storage quota limits, and developers freely abuse persistent storage by frequently caching data, creating debug logs, user analytics, and downloading advertisements as needed.Finally, drawing upon our findings, we discuss the need for efficient mobile storage management, and propose an elastic storage design to reclaim storage space when unused. We further identify research challenges and quantify expected storage savings from such a design. We believe our findings will be valuable to the storage research community as well as mobile app developers.","smartphones, mobile, storage management","",""
"Conference Paper","Gao Y,Wang H,Li L,Luo X,Xu G,Liu X","Demystifying Illegal Mobile Gambling Apps","","2021","","","1447–1458","Association for Computing Machinery","New York, NY, USA","Proceedings of the Web Conference 2021","Ljubljana, Slovenia","2021","9781450383127","","https://doi.org/10.1145/3442381.3449932;http://dx.doi.org/10.1145/3442381.3449932","10.1145/3442381.3449932","Mobile gambling app, as a new type of online gambling service emerging in the mobile era, has become one of the most popular and lucrative underground businesses in the mobile app ecosystem. Since its born, mobile gambling app has received strict regulations from both government authorities and app markets. However, to the best of our knowledge, mobile gambling apps have not been investigated by our research community. In this paper, we take the first step to fill the void. Specifically, we first perform a 5-month dataset collection process to harvest illegal gambling apps in China, where mobile gambling apps are outlawed. We have collected 3,366 unique gambling apps with 5,344 different versions. We then characterize the gambling apps from various perspectives including app distribution channels, network infrastructure, malicious behaviors, abused third-party and payment services. Our work has revealed a number of covert distribution channels, the unique characteristics of gambling apps, and the abused fourth-party payment services. At last, we further propose a “guilt-by-association” expansion method to identify new suspicious gambling services, which help us further identify over 140K suspicious gambling domains and over 57K gambling app candidates. Our study demonstrates the urgency for detecting and regulating illegal gambling apps.","","","WWW '21"
"Conference Paper","Ashizawa N,Yanai N,Cruz JP,Okamura S","Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts","","2021","","","47–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM International Symposium on Blockchain and Secure Critical Infrastructure","Virtual Event, Hong Kong","2021","9781450384001","","https://doi.org/10.1145/3457337.3457841;http://dx.doi.org/10.1145/3457337.3457841","10.1145/3457337.3457841","Ethereum smart contracts are programs that run on the Ethereum blockchain, and many smart contract vulnerabilities have been discovered in the past decade. Many security analysis tools have been created to detect such vulnerabilities, but their performance decreases drastically when codes to be analyzed are being rewritten. In this paper, we propose Eth2Vec, a machine-learning-based static analysis tool for vulnerability detection in smart contracts. It is also robust against code rewrites, i.e., it can detect vulnerabilities even in rewritten codes. Existing machine-learning-based static analysis tools for vulnerability detection need features, which analysts create manually, as inputs. In contrast, Eth2Vec automatically learns features of vulnerable Ethereum Virtual Machine (EVM) bytecodes with tacit knowledge through a neural network for natural language processing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by comparing the code similarity between target EVM bytecodes and the EVM bytecodes it already learned. We conducted experiments with existing open databases, such as Etherscan, and our results show that Eth2Vec outperforms a recent model based on support vector machine in terms of well-known metrics, i.e., precision, recall, and F1-score.","code similarity, neural networks, vulnerability detection, static analysis, ethereum, smart contracts","","BSCI '21"
"Conference Paper","Che H","Detection Method of Malicious Mirroring Site in Mass Network Traffic","","2021","","","654–658","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Conference on Information Technologies and Electrical Engineering","Changde City, Hunan, China","2021","9781450388665","","https://doi.org/10.1145/3452940.3453068;http://dx.doi.org/10.1145/3452940.3453068","10.1145/3452940.3453068","This paper proposes a method for detecting malicious mirrored websites under large-scale network traffic. This method passively extracts webpage source code from network traffic and actively obtains webpage snapshots through a combination of active and passive methods, extracts corresponding features for similarity comparison, and detects malicious Mirror web pages. The experiment used 1447 malicious webpages as benchmark webpages. In a large-scale network flow environment, 49 phishing webpages, 13 gambling webpages, 23 obscene and pornographic webpages, and 8 illegal webpages were detected. The accuracy rate of the algorithm is 93.94%, the recall rate is 92.08%, and the F value is 0.93, which verifies that the malicious mirror webpage detection algorithm proposed in this paper is practical and effective.","Image-aware hash, Malicious mirrored webpage, SIFT, Simhash","","ICITEE2020"
"Journal Article","Zhao Y,Li L,Wang H,Cai H,Bissyandé TF,Klein J,Grundy J","On the Impact of Sample Duplication in Machine-Learning-Based Android Malware Detection","ACM Trans. Softw. Eng. Methodol.","2021","30","3","","Association for Computing Machinery","New York, NY, USA","","","2021-05","","1049-331X","https://doi.org/10.1145/3446905;http://dx.doi.org/10.1145/3446905","10.1145/3446905","Malware detection at scale in the Android realm is often carried out using machine learning techniques. State-of-the-art approaches such as DREBIN and MaMaDroid are reported to yield high detection rates when assessed against well-known datasets. Unfortunately, such datasets may include a large portion of duplicated samples, which may bias recorded experimental results and insights. In this article, we perform extensive experiments to measure the performance gap that occurs when datasets are de-duplicated. Our experimental results reveal that duplication in published datasets has a limited impact on supervised malware classification models. This observation contrasts with the finding of Allamanis on the general case of machine learning bias for big code. Our experiments, however, show that sample duplication more substantially affects unsupervised learning models (e.g., malware family clustering). Nevertheless, we argue that our fellow researchers and practitioners should always take sample duplication into consideration when performing machine-learning-based (via either supervised or unsupervised learning) Android malware detections, no matter how significant the impact might be.","Duplication, dataset, android, malware detection, machine learning","",""
"Journal Article","Zou D,Wu Y,Yang S,Chauhan A,Yang W,Zhong J,Dou S,Jin H","IntDroid: Android Malware Detection Based on API Intimacy Analysis","ACM Trans. Softw. Eng. Methodol.","2021","30","3","","Association for Computing Machinery","New York, NY, USA","","","2021-05","","1049-331X","https://doi.org/10.1145/3442588;http://dx.doi.org/10.1145/3442588","10.1145/3442588","Android, the most popular mobile operating system, has attracted millions of users around the world. Meanwhile, the number of new Android malware instances has grown exponentially in recent years. On the one hand, existing Android malware detection systems have shown that distilling the program semantics into a graph representation and detecting malicious programs by conducting graph matching are able to achieve high accuracy on detecting Android malware. However, these traditional graph-based approaches always perform expensive program analysis and suffer from low scalability on malware detection. On the other hand, because of the high scalability of social network analysis, it has been applied to complete large-scale malware detection. However, the social-network-analysis-based method only considers simple semantic information (i.e., centrality) for achieving market-wide mobile malware scanning, which may limit the detection effectiveness when benign apps show some similar behaviors as malware.In this article, we aim to combine the high accuracy of traditional graph-based method with the high scalability of social-network-analysis--based method for Android malware detection. Instead of using traditional heavyweight static analysis, we treat function call graphs of apps as complex social networks and apply social-network--based centrality analysis to unearth the central nodes within call graphs. After obtaining the central nodes, the average intimacies between sensitive API calls and central nodes are computed to represent the semantic features of the graphs. We implement our approach in a tool called IntDroid and evaluate it on a dataset of 3,988 benign samples and 4,265 malicious samples. Experimental results show that IntDroid is capable of detecting Android malware with an F-measure of 97.1% while maintaining a True-positive Rate of 99.1%. Although the scalability is not as fast as a social-network-analysis--based method (i.e., MalScan), compared to a traditional graph-based method, IntDroid is more than six times faster than MaMaDroid. Moreover, in a corpus of apps collected from GooglePlay market, IntDroid is able to identify 28 zero-day malware that can evade detection of existing tools, one of which has been downloaded and installed by more than ten million users. This app has also been flagged as malware by six anti-virus scanners in VirusTotal, one of which is Symantec Mobile Insight.","Android malware, centrality, API intimacy, social network","",""
"Conference Paper","Goree S,Doosti B,Crandall D,Su NM","Investigating the Homogenization of Web Design: A Mixed-Methods Approach","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","Yokohama, Japan","2021","9781450380966","","https://doi.org/10.1145/3411764.3445156;http://dx.doi.org/10.1145/3411764.3445156","10.1145/3411764.3445156","Visual design provides the backdrop to most of our interactions over the Internet, but has not received as much analytical attention as textual content. Combining computational with qualitative approaches, we investigate the growing concern that visual design of the World Wide Web has homogenized over the past decade. By applying computer vision techniques to a large dataset of representative websites images from 2003–2019, we show that designs have become significantly more similar since 2007, especially for page layouts where the average distance between sites decreased by over 30%. Synthesizing interviews from 11 experienced web design professionals with our computational analyses, we discuss causes of this homogenization including overlap in source code and libraries, color scheme standardization, and support for mobile devices. Our results seek to motivate future discussion of the factors that influence designers and their implications on the future trajectory of web design.","historical analysis, computational social science, design homogenization, web design, interviews","","CHI '21"
"Conference Paper","Weinman N,Drucker SM,Barik T,DeLine R","Fork It: Supporting Stateful Alternatives in Computational Notebooks","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","Yokohama, Japan","2021","9781450380966","","https://doi.org/10.1145/3411764.3445527;http://dx.doi.org/10.1145/3411764.3445527","10.1145/3411764.3445527","Computational notebooks, which seamlessly interleave code with results, have become a popular tool for data scientists due to the iterative nature of exploratory tasks. However, notebooks provide a single execution state for users to manipulate through creating and manipulating variables. When exploring alternatives, data scientists must carefully create many-step manipulations in visually distant cells. We conducted formative interviews with 6 professional data scientists, motivating design principles behind exposing multiple states. We introduce forking — creating a new interpreter session — and backtracking — navigating through previous states. We implement these interactions as an extension to notebooks that help data scientists more directly express and navigate through decision points a single notebook. In a qualitative evaluation, 11 professional data scientists found the tool would be useful for exploring alternatives and debugging code to create a predictive model. Their insights highlight further challenges to scaling this functionality.","computational notebooks, Alternatives, exploratory programming, code history","","CHI '21"
"Conference Paper","McNutt AM,Chugh R","Integrated Visualization Editing via Parameterized Declarative Templates","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","Yokohama, Japan","2021","9781450380966","","https://doi.org/10.1145/3411764.3445356;http://dx.doi.org/10.1145/3411764.3445356","10.1145/3411764.3445356","Interfaces for creating visualizations typically embrace one of several common forms. Textual specification enables fine-grained control, shelf building facilitates rapid exploration, while chart choosing promotes immediacy and simplicity. Ideally these approaches could be unified to integrate the user- and usage-dependent benefits found in each modality, yet these forms remain distinct. We propose parameterized declarative templates, a simple abstraction mechanism over JSON-based visualization grammars, as a foundation for multimodal visualization editors. We demonstrate how templates can facilitate organization and reuse by factoring the more than 160 charts that constitute Vega-Lite’s example gallery into approximately 40 templates. We exemplify the pliability of abstracting over charting grammars by implementing—as a template—the functionality of the shelf builder Polestar (a simulacra of Tableau) and a set of templates that emulate the Google Sheets chart chooser. We show how templates support multimodal visualization editing by implementing a prototype and evaluating it through an approachability study.","Information Visualization, Systems, Templates, Declarative Grammars, User Interfaces, Ivy","","CHI '21"
"Conference Paper","Weinman N,Fox A,Hearst MA","Improving Instruction of Programming Patterns with Faded Parsons Problems","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","Yokohama, Japan","2021","9781450380966","","https://doi.org/10.1145/3411764.3445228;http://dx.doi.org/10.1145/3411764.3445228","10.1145/3411764.3445228","Learning to recognize and apply programming patterns — reusable abstractions of code — is critical to becoming a proficient computer scientist. However, many introductory Computer Science courses do not teach patterns, in part because teaching these concepts requires significant curriculum changes. As an alternative, we explore how a novel user interface for practicing coding — Faded Parsons Problems — can support introductory Computer Science students in learning to apply programming patterns. We ran a classroom-based study with 237 students which found that Faded Parsons Problems, or rearranging and completing partially blank lines of code into a valid program, are an effective exercise interface for teaching programming patterns, significantly surpassing the performance of the more standard approaches of code writing and code tracing exercises. Faded Parsons Problems also improve overall code writing ability at a comparable level to code writing exercises, but are preferred by students.","Computing Education, CS1, Programming Patterns, Parsons Problems","","CHI '21"
"Conference Paper","Li X,Wang Y,Wang H,Wang Y,Zhao J","NBSearch: Semantic Search and Visual Exploration of Computational Notebooks","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","Yokohama, Japan","2021","9781450380966","","https://doi.org/10.1145/3411764.3445048;http://dx.doi.org/10.1145/3411764.3445048","10.1145/3411764.3445048","Code search is an important and frequent activity for developers using computational notebooks (e.g., Jupyter). The flexibility of notebooks brings challenges for effective code search, where classic search interfaces for traditional software code may be limited. In this paper, we propose, NBSearch, a novel system that supports semantic code search in notebook collections and interactive visual exploration of search results. NBSearch leverages advanced machine learning models to enable natural language search queries and intuitive visualizations to present complicated intra- and inter-notebook relationships in the returned results. We developed NBSearch through an iterative participatory design process with two experts from a large software company. We evaluated the models with a series of experiments and the whole system with a controlled user study. The results indicate the feasibility of our analytical pipeline and the effectiveness of NBSearch to support code search in large notebook collections.","document and text analysis., computational notebooks, search result visualization, Semantic code search","","CHI '21"
"Journal Article","Chen B,Jiang ZM","A Survey of Software Log Instrumentation","ACM Comput. Surv.","2021","54","4","","Association for Computing Machinery","New York, NY, USA","","","2021-05","","0360-0300","https://doi.org/10.1145/3448976;http://dx.doi.org/10.1145/3448976","10.1145/3448976","Log messages have been used widely in many software systems for a variety of purposes during software development and field operation. There are two phases in software logging: log instrumentation and log management. Log instrumentation refers to the practice that developers insert logging code into source code to record runtime information. Log management refers to the practice that operators collect the generated log messages and conduct data analysis techniques to provide valuable insights of runtime behavior. There are many open source and commercial log management tools available. However, their effectiveness highly depends on the quality of the instrumented logging code, as log messages generated by high-quality logging code can greatly ease the process of various log analysis tasks (e.g., monitoring, failure diagnosis, and auditing). Hence, in this article, we conducted a systematic survey on state-of-the-art research on log instrumentation by studying 69 papers between 1997 and 2019. In particular, we have focused on the challenges and proposed solutions used in the three steps of log instrumentation: (1) logging approach; (2) logging utility integration; and (3) logging code composition. This survey will be useful to DevOps practitioners and researchers who are interested in software logging.","Systematic survey, software logging, instrumentation","",""
"Journal Article","Rogers P","From Ada to Platinum SPARK: A Case Study","Ada Lett.","2021","40","2","76–91","Association for Computing Machinery","New York, NY, USA","","","2021-04","","1094-3641","https://doi.org/10.1145/3463478.3463488;http://dx.doi.org/10.1145/3463478.3463488","10.1145/3463478.3463488","An effective approach to learning a new programming language is to implement data structures common to computer programming. The approach is effective because the problem to be solved is well understood, allowing one to focus on the language details. Moreover, several different forms of a given data structure are often possible: bounded versus unbounded, sequential versus thread-safe, and so on. These multiple forms likely require a wide range of language features.","","",""
"Conference Paper","Kapur R,Sodhi B,Rao PU,Sharma S","Using Paragraph Vectors to Improve Our Existing Code Review Assisting Tool-CRUSO","","2021","","","","Association for Computing Machinery","New York, NY, USA","14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)","Bhubaneswar, Odisha, India","2021","9781450390460","","https://doi.org/10.1145/3452383.3452393;http://dx.doi.org/10.1145/3452383.3452393","10.1145/3452383.3452393","Code reviews are one of the effective methods to estimate defectiveness in source code. However, the existing methods are dependent on experts or inefficient. In this paper, we improve the performance (in terms of speed and memory usage) of our existing code review assisting tool–CRUSO. The central idea of the approach is to estimate the defectiveness for an input source code by using the defectiveness score of similar code fragments present in various StackOverflow (SO) posts. The significant contributions of our paper are i) SOpostsDB: a dataset containing the PVA vectors and the SO posts information, ii) CRUSO-P: a code review assisting system based on PVA models trained on SOpostsDB. For a given input source code, CRUSO-P labels it as Likely to be defective, Unlikely to be defective, Unpredictable. To develop CRUSO-P, we processed >3 million SO posts and 188200+ GitHub source files. CRUSO-P is designed to work with source code written in the popular programming languages C, C#, Java, JavaScript, and Python. CRUSO-P outperforms CRUSO with an improvement of 97.82% in response time and a storage reduction of 99.15%. CRUSO-P achieves the highest mean accuracy score of 99.6% when tested with the C programming language, thus achieving an improvement of 5.6% over the existing method.","Paragraph Vector, Software maintenance, Automated code review, StackOverflow, Code quality","","ISEC 2021"
"Conference Paper","Lano K,Alwakeel L,Rahimi SK,Haughton H","Synthesis of Mobile Applications Using AgileUML","","2021","","","","Association for Computing Machinery","New York, NY, USA","14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)","Bhubaneswar, Odisha, India","2021","9781450390460","","https://doi.org/10.1145/3452383.3452409;http://dx.doi.org/10.1145/3452383.3452409","10.1145/3452383.3452409","In this paper we describe a method to apply the AgileUML toolset to synthesise mobile apps from UML models. This is a lightweight model-driven engineering (MDE) approach suitable for app developers who need to rapidly produce native apps for either or both Android or iOS platforms. In contrast to other MDE approaches for app development, the approach aims to minimise the extent of manual effort by using very concise high-level specifications which abstract from technical details as far as possible, while still providing explicit definitions of functional app behaviour.","MDE, Mobile apps, Agile development","","ISEC 2021"
"Journal Article","Cheng X,Wang H,Hua J,Xu G,Sui Y","DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network","ACM Trans. Softw. Eng. Methodol.","2021","30","3","","Association for Computing Machinery","New York, NY, USA","","","2021-04","","1049-331X","https://doi.org/10.1145/3436877;http://dx.doi.org/10.1145/3436877","10.1145/3436877","Static bug detection has shown its effectiveness in detecting well-defined memory errors, e.g., memory leaks, buffer overflows, and null dereference. However, modern software systems have a wide variety of vulnerabilities. These vulnerabilities are extremely complicated with sophisticated programming logic, and these bugs are often caused by different bad programming practices, challenging existing bug detection solutions. It is hard and labor-intensive to develop precise and efficient static analysis solutions for different types of vulnerabilities, particularly for those that may not have a clear specification as the traditional well-defined vulnerabilities.This article presents DeepWukong, a new deep-learning-based embedding approach to static detection of software vulnerabilities for C/C++ programs. Our approach makes a new attempt by leveraging advanced recent graph neural networks to embed code fragments in a compact and low-dimensional representation, producing a new code representation that preserves high-level programming logic (in the form of control- and data-flows) together with the natural language information of a program. Our evaluation studies the top 10 most common C/C++ vulnerabilities during the past 3 years. We have conducted our experiments using 105,428 real-world programs by comparing our approach with four well-known traditional static vulnerability detectors and three state-of-the-art deep-learning-based approaches. The experimental results demonstrate the effectiveness of our research and have shed light on the promising direction of combining program analysis with deep learning techniques to address the general static code analysis challenges.","graph embedding, vulnerabilities, Static analysis","",""
"Journal Article","Uddin G,Khomh F,Roy CK","Automatic API Usage Scenario Documentation from Technical Q&A Sites","ACM Trans. Softw. Eng. Methodol.","2021","30","3","","Association for Computing Machinery","New York, NY, USA","","","2021-04","","1049-331X","https://doi.org/10.1145/3439769;http://dx.doi.org/10.1145/3439769","10.1145/3439769","The online technical Q&A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.","crowd-sourced developer forum, usage scenario, documentation, API","",""
"Conference Paper","Ferreira F,Silva LL,Valente MT","Software Engineering Meets Deep Learning: A Mapping Study","","2021","","","1542–1549","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th Annual ACM Symposium on Applied Computing","Virtual Event, Republic of Korea","2021","9781450381048","","https://doi.org/10.1145/3412841.3442029;http://dx.doi.org/10.1145/3412841.3442029","10.1145/3412841.3442029","Deep Learning (DL) is being used nowadays in many traditional Software Engineering (SE) problems and tasks. However, since the renaissance of DL techniques is still very recent, we lack works that summarize and condense the most recent and relevant research conducted at the intersection of DL and SE. Therefore, in this paper, we describe the first results of a mapping study covering 81 papers about DL & SE. Our results confirm that DL is gaining momentum among SE researchers over the years and that the top-3 research problems tackled by the analyzed papers are documentation, defect prediction, and testing.","deep learning, software engineering","","SAC '21"
"Conference Paper","Jin B,Choi J,Kim H,Hong JB","FUMVar: A Practical Framework for Generating Fully-Working and Unseen Malware Variants","","2021","","","1656–1663","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th Annual ACM Symposium on Applied Computing","Virtual Event, Republic of Korea","2021","9781450381048","","https://doi.org/10.1145/3412841.3442039;http://dx.doi.org/10.1145/3412841.3442039","10.1145/3412841.3442039","It is crucial to understand how malware variants are generated to bypass malware detection systems and understand their characteristics to improve the detectors' performances. To achieve this goal, we propose an evolutionary-based framework named FUMVar to generate Fully-working and Unseen Malware Variants. In particular, we applied FUMVar on portable executable (PE) files that have been used extensively to infect Windows operating systems. Compared to the state-of-the-art approach named AIMED, our experimental results show that FUMVar generated 25% more evasive malware variants while reducing the time taken to generate them by 23%. Furthermore, FUMVar generated malware variants that bypassed commercial anti-malware engines, such as TrendMicro, with an alarming rate of up to 73% false-negative rate. To improve the detection techniques, we evaluate how different perturbations enhance the evasiveness and how different malware categories are affected by those perturbations. The results show that perturbations' effectiveness varies significantly by up to 6 times (e.g., section add v.s. unpack), and more suitable perturbations can be selected for different malware categories due to their varying applications. This information can then be used to develop more robust malware detection systems to detect unseen malware variants more effectively.","windows PE, malware variation, malware generation","","SAC '21"
"Conference Paper","Chen X,Chen W,Liu K,Chen C,Li L","A Comparative Study of Smartphone and Smartwatch Apps","","2021","","","1484–1493","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th Annual ACM Symposium on Applied Computing","Virtual Event, Republic of Korea","2021","9781450381048","","https://doi.org/10.1145/3412841.3442023;http://dx.doi.org/10.1145/3412841.3442023","10.1145/3412841.3442023","Despite that our community has spent numerous efforts on analyzing mobile apps, there is no study proposed for characterizing the relationship between smartphone and smartwatch apps. To fill this gap, we present to the community a comparative study of smartphone and smartwatch apps, aiming at understanding the status quo of cross-phone/watch apps. Specifically, in this work, we first collect a set of cross-phone/watch app pairs and then experimentally look into them to explore their similarities or dissimilarities from different perspectives. Experimental results show that (1) Approximately, up to 40% of resource files, 30% of code methods are reused between smartphone/watch app pairs, (2) Smartphone apps may require more than twice as many as permissions and adopt more than five times as many as user interactions than their watch counterparts, and (3) Smartwatch apps can be released as either standalone (can be run independently) or companion versions (i.e., have to co-work with their smartphone counterparts), for which the former type of apps tends to require more permissions and reuse more code, involve more user interactions than the latter type. Our findings can help developers and researchers understand the ecosystem of smartwatch apps and further gain insight into migrating smartphone apps for smartwatches.","Android, mobile software engineering, static code analysis, smartwatch","","SAC '21"
"Journal Article","Jiang JA,Wade K,Fiesler C,Brubaker JR","Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis","Proc.  ACM Hum. -Comput.  Interact.","2021","5","CSCW1","","Association for Computing Machinery","New York, NY, USA","","","2021-04","","","https://doi.org/10.1145/3449168;http://dx.doi.org/10.1145/3449168","10.1145/3449168","Qualitative inductive methods are widely used in CSCW and HCI research for their ability to generatively discover deep and contextualized insights, but these inherently manual and human-resource-intensive processes are often infeasible for analyzing large corpora. Researchers have been increasingly interested in ways to apply qualitative methods to ""big"" data problems, hoping to achieve more generalizable results from larger amounts of data while preserving the depth and richness of qualitative methods. In this paper, we describe a study of qualitative researchers' work practices and their challenges, with an eye towards whether this is an appropriate domain for human-AI collaboration and what successful collaborations might entail. Our findings characterize participants' diverse methodological practices and nuanced collaboration dynamics, and identify areas where they might benefit from AI-based tools. While participants highlight the messiness and uncertainty of qualitative inductive analysis, they still want full agency over the process and believe that AI should not interfere. Our study provides a deep investigation of task delegability in human-AI collaboration in the context of qualitative analysis, and offers directions for the design of AI assistance that honor serendipity, human agency, and ambiguity.","ai, human-ai collaboration, interview, qualitative research","",""
"Journal Article","Haq IU,Caballero J","A Survey of Binary Code Similarity","ACM Comput. Surv.","2021","54","3","","Association for Computing Machinery","New York, NY, USA","","","2021-04","","0360-0300","https://doi.org/10.1145/3446371;http://dx.doi.org/10.1145/3446371","10.1145/3446371","Binary code similarityapproaches compare two or more pieces of binary code to identify their similarities and differences. The ability to compare binary code enables many real-world applications on scenarios where source code may not be available such as patch analysis, bug search, and malware detection and analysis. Over the past 22 years numerous binary code similarity approaches have been proposed, but the research area has not yet been systematically analyzed. This article presents the first survey of binary code similarity. It analyzes 70 binary code similarity approaches, which are systematized on four aspects: (1) the applications they enable, (2) their approach characteristics, (3) how the approaches are implemented, and (4) the benchmarks and methodologies used to evaluate them. In addition, the survey discusses the scope and origins of the area, its evolution over the past two decades, and the challenges that lie ahead.","cross-architecture, executable, code diffing, code search, Binary code similarity","",""
"Conference Paper","Weisz JD,Muller M,Houde S,Richards J,Ross SI,Martinez F,Agarwal M,Talamadupula K","Perfection Not Required? Human-AI Partnerships in Code Translation","","2021","","","402–412","Association for Computing Machinery","New York, NY, USA","26th International Conference on Intelligent User Interfaces","College Station, TX, USA","2021","9781450380171","","https://doi.org/10.1145/3397481.3450656;http://dx.doi.org/10.1145/3397481.3450656","10.1145/3397481.3450656","Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency. New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another. The artifacts produced in this way may contain imperfections, such as compilation or logical errors. We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors. Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task of translating source code from one language to another. Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system’s outputs would be established, and future opportunities for generative AI in application modernization. Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.","application modernization, code translation, imperfect AI, generative AI, neural machine translation, NMT","","IUI '21"
"Journal Article","Alzahrani N,Vahid F","Progression Highlighting for Programming Courses","J. Comput. Sci. Coll.","2021","36","10","17–23","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2021-04","","1937-4771","","","New program auto-graders can provide a log file having an entry for each student code run, either during development or when submitting for points. Using that log file as input, we introduce ""code progression highlighting"" for instructors to gain visibility into a student's programming process. For any student, an instructor can view the student's program for every run, highlighted to show changes from the previous run (the ""progression""), and with statistics per entry like time spent, characters changed, and current score. The progression highlighter opens several new opportunities, like aiding instructors in helping students during office hours, allowing awarding points for good process (starting early, developing incrementally, etc.), detecting some cheating not detectable by similarity checkers, and helping discover where students are struggling.","","",""
"Conference Paper","Lister R","On the Cognitive Development of the Novice Programmer: And the Development of a Computing Education Researcher","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Computer Science Education Research Conference","Virtual Event, Netherlands","2021","9781450388726","","https://doi.org/10.1145/3442481.3442498;http://dx.doi.org/10.1145/3442481.3442498","10.1145/3442481.3442498","This paper is a companion to my keynote address at the 9th Computer Science Education Research Conference (CSERC '20). I review the research that led to my three stage neo-Piagetian model of how novices understand code. Code tracing is the key. In the first stage, the novice cannot trace code. In the second stage, the novice has mastered tracing, but, crucially, that is the only skill they have mastered. It is only when novices reach the third stage that they begin to reason about code in a more general, abstract way. The principal failure of traditional approaches to teaching programming has been the assumption that the novices begin at the third stage.","novice programmers, neo-piagetian theory","","CSERC '20"
"Conference Paper","Júnior LC,Belgamo A,Mendonça VR,Vincenzi AM","WarningsFIX: A Recommendation System for Prioritizing Warnings Generated by Automated Static Analyzers","","2021","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XIX Brazilian Symposium on Software Quality","São Luís, Brazil","2021","9781450389235","","https://doi.org/10.1145/3439961.3439987;http://dx.doi.org/10.1145/3439961.3439987","10.1145/3439961.3439987","Recommendation systems try to guide the users in carrying out a task providing them with useful information about it. Considering the context of software development, programs are ever-increasing, making it difficult to conduct a detailed verification and validation. Automated static analyzers help to detect possible faults on software products earlier and quickly but, in general, the issue maybe a false-positive warning. In this sense, this work presents and evaluates a recommendation system, called WarningsFIX (WFX), which combines several static analyzers aim at: i) Expand the possible fault domain approached by each static analysis tool increasing the range of warnings types covered, allowing the concentration of a higher number of true-positive warnings. ii) Establish different prioritization strategies of warnings aiming at suggesting for reviewers first analyze the ones with a higher chance of being true-positive. WFX organizes the warnings information via treemaps considering four levels of abstraction: program, package, class, and line. The nodes of the treemap on each level may be classified by three different prioritization strategies based on the number of warnings, the number of tools, and the suspicions rate. The use of these strategies enables the reviewer to handle the set of warnings in a coordinated way depending on the cost and time constraint available. We perform a feasibility study to evaluate the WFX effectiveness whose results shown that: i) WFX was able to improve the results obtained from combined static analyzers to 44% of the analyzed programs, concentrating for them a greater number of true-positives. ii) WFX, depending on the adopted prioritization strategy, improved from 67.5% to 55% the ranking of lines with real bugs when compared with the list of warnings provided by the automated static analyzers without the WFX support.","recommendation system, WarningsFix, warnings prioritization, static analysis, software visualization","","SBQS '20"
"Conference Paper","Karnalim O,Simon","Common Code Segment Selection: Semi-Automated Approach and Evaluation","","2021","","","335–341","Association for Computing Machinery","New York, NY, USA","Proceedings of the 52nd ACM Technical Symposium on Computer Science Education","Virtual Event, USA","2021","9781450380621","","https://doi.org/10.1145/3408877.3432436;http://dx.doi.org/10.1145/3408877.3432436","10.1145/3408877.3432436","When comparing student programs to check for evidence of plagiarism or collusion, the goal is to identify code segments that are common to two or more programs. Yet some code segments are common for reasons other than plagiarism or collusion, and so should not be considered. A few code similarity detection tools automatically remove very common segment, but they are prone to false results as no human validation is involved. This paper proposes a semi-automated approach for excluding common segments, where human validation is introduced before excluding the segments. As existing selection techniques are not detachable from their similarity detection tools, we propose a new tool to independently select the segments (C2S2), along with several adjustable selection constraints to keep the number of suggested segments reasonable for manual observation. In order to independently evaluate automated selection techniques, we propose and apply three metrics. The evaluation shows our selection technique to be more effective and efficient than the basis underlying existing selection techniques, and establishes the benefit of each of its selection features.","code similarity, plagiarism, n-gram, semi-automated approach, common code segment, collusion","","SIGCSE '21"
"Conference Paper","Crichton W,Sampaio GG,Hanrahan P","Automating Program Structure Classification","","2021","","","1177–1183","Association for Computing Machinery","New York, NY, USA","Proceedings of the 52nd ACM Technical Symposium on Computer Science Education","Virtual Event, USA","2021","9781450380621","","https://doi.org/10.1145/3408877.3432358;http://dx.doi.org/10.1145/3408877.3432358","10.1145/3408877.3432358","When students write programs, their program structure provides insight into their learning process. However, analyzing program structure by hand is time-consuming, and teachers need better tools for computer-assisted exploration of student solutions. As a first step towards an education-oriented program analysis toolkit, we show how supervised machine learning methods can automatically classify student programs into a predetermined set of high-level structures. We evaluate two models on classifying student solutions to the Rainfall problem: a nearest-neighbors classifier using syntax tree edit distance and a recurrent neural network. We demonstrate that these models can achieve 91% classification accuracy when trained on 108 programs. We further explore the generality, trade-offs, and failure cases of each model.","neural networks, program classification, machine learning","","SIGCSE '21"
"Journal Article","Qasem A,Shirani P,Debbabi M,Wang L,Lebel B,Agba BL","Automatic Vulnerability Detection in Embedded Devices and Firmware: Survey and Layered Taxonomies","ACM Comput. Surv.","2021","54","2","","Association for Computing Machinery","New York, NY, USA","","","2021-03","","0360-0300","https://doi.org/10.1145/3432893;http://dx.doi.org/10.1145/3432893","10.1145/3432893","In the era of the internet of things (IoT), software-enabled inter-connected devices are of paramount importance. The embedded systems are very frequently used in both security and privacy-sensitive applications. However, the underlying software (a.k.a. firmware) very often suffers from a wide range of security vulnerabilities, mainly due to their outdated systems or reusing existing vulnerable libraries; which is evident by the surprising rise in the number of attacks against embedded systems. Therefore, to protect those embedded systems, detecting the presence of vulnerabilities in the large pool of embedded devices and their firmware plays a vital role. To this end, there exist several approaches to identify and trigger potential vulnerabilities within deployed embedded systems firmware. In this survey, we provide a comprehensive review of the state-of-the-art proposals, which detect vulnerabilities in embedded systems and firmware images by employing various analysis techniques, including static analysis, dynamic analysis, symbolic execution, and hybrid approaches. Furthermore, we perform both quantitative and qualitative comparisons among the surveyed approaches. Moreover, we devise taxonomies based on the applications of those approaches, the features used in the literature, and the type of the analysis. Finally, we identify the unresolved challenges and discuss possible future directions in this field of research.","internet of things (IoT), vulnerability detection, firmware analysis, embedded device security, Binary code analysis","",""
"Conference Paper","Begel A,Dominic J,Phillis C,Beeson T,Rodeghero P","How a Remote Video Game Coding Camp Improved Autistic College Students' Self-Efficacy in Communication","","2021","","","142–148","Association for Computing Machinery","New York, NY, USA","Proceedings of the 52nd ACM Technical Symposium on Computer Science Education","Virtual Event, USA","2021","9781450380621","","https://doi.org/10.1145/3408877.3432516;http://dx.doi.org/10.1145/3408877.3432516","10.1145/3408877.3432516","Communication and teamwork are essential skills for software developers. However, these skills are often difficult to learn for students with autism spectrum disorder (ASD). We designed, developed, and ran a 13-day, remote video game coding camp for incoming college first-year students with ASD. We developed instructional materials to teach computer programming, video game design, and communication and teaming skills. Students used the MakeCode Arcade development environment to build their games and Zoom to remotely collaborate with their teammates. In summative interviews, students reported improved programming skills, increased confidence in communication, and better experiences working with others. We also found that students valued the opportunity to practice teaming, such as being more vocal in expressing ideas to their peers and working out differences of opinion with their teammates. Two students reported the remote learning environment decreased their anxiety and stress, both are frequent challenges for autistic people. We plan to rerun the camp next year with materials that we have made available online.","video games, coding camp, autism","","SIGCSE '21"
"Conference Paper","Murtaza G,Benson TA","WebOptProfiler: Providing Performance Clarity for Mobile Webpage Optimizations","","2021","","","140–146","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications","Virtual, United Kingdom","2021","9781450383233","","https://doi.org/10.1145/3446382.3449073;http://dx.doi.org/10.1145/3446382.3449073","10.1145/3446382.3449073","Despite decades of research on mobile webpage optimizations, little is known about how these optimizations interoperate. Moreover, there has been little systematic work to understand the scenarios wherein combinations of these optimizations excel. Without a comprehensive understanding of how these optimizations compose with each other and under what conditions they excel, operators cannot determine which optimizations to adopt, and, similarly, developers do not know where to focus their efforts.In this paper, we argue that developers should be required to evaluate and characterize the broader interactions between their proposed optimizations and other optimizations - this is in addition to demonstrating the potential benefits of their approach. To aide developers in characterizing these broader interactions, we propose an analytical model which decomposes web optimizations into virtual speedup functions that operate on well-understood browser processing phases (e.g., processing, rendering, layout, etc., for an object) and we present a web browser-oriented causal profiler which empirically explores interactions between optimizations by using their analytical models to speed up different parts of the Browser during a page load. Our system, WebOptProfiler, identifies and addresses practical issues in extending causal profiling to the webpage optimization domain and provides an algorithm for extracting an analytical model from readily available browser traces.","","","HotMobile '21"
"Conference Paper","Meng X,Anderson JM,Mellor-Crummey J,Krentel MW,Miller BP,Milaković S","Parallel Binary Code Analysis","","2021","","","76–89","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming","Virtual Event, Republic of Korea","2021","9781450382946","","https://doi.org/10.1145/3437801.3441604;http://dx.doi.org/10.1145/3437801.3441604","10.1145/3437801.3441604","Binary code analysis is widely used to help assess a program's correctness, performance, and provenance. Binary analysis applications often construct control flow graphs, analyze data flow, and use debugging information to understand how machine code relates to source lines, inlined functions, and data types. To date, binary analysis has been single-threaded, which is too slow for convenient use in performance tuning workflows where it is used to help attribute performance to complex applications with large binaries.This paper describes our design and implementation for accelerating the task of constructing control flow graphs (CFGs) from binaries by using multithreading. Prior research focuses on algorithms for analysis of challenging code constructs encountered while constructing CFGs, including functions sharing code, jump tables, non-returning functions, and tail calls. These algorithms are described from a program analysis perspective and are not suitable for direct parallel implementation. We abstract the task of constructing CFGs as repeated applications of several core CFG operations that include creating functions, basic blocks, and edges. We then derive CFG operation dependency, commutativity, and monotonicity. These operation properties guide our design of a new parallel analysis for constructing CFGs. Using 64 threads, we achieved as much as 25× speedup for constructing CFGs and 8× for a performance analysis tool that leverages our new analysis to recover program structure.","operation dependencies and properties, control flow graph construction, performance analysis","","PPoPP '21"
"Conference Paper","Choi YK,Chi Y,Qiao W,Samardzic N,Cong J","HBM Connect: High-Performance HLS Interconnect for FPGA HBM","","2021","","","116–126","Association for Computing Machinery","New York, NY, USA","The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays","Virtual Event, USA","2021","9781450382182","","https://doi.org/10.1145/3431920.3439301;http://dx.doi.org/10.1145/3431920.3439301","10.1145/3431920.3439301","With the recent release of High Bandwidth Memory (HBM) based FPGA boards, developers can now exploit unprecedented external memory bandwidth. This allows more memory-bounded applications to benefit from FPGA acceleration. However, fully utilizing the available bandwidth may not be an easy task. If an application requires multiple processing elements to access multiple HBM channels, we observed a significant drop in the effective bandwidth. The existing high-level synthesis (HLS) programming environment had limitation in producing an efficient communication architecture. In order to solve this problem, we propose HBM Connect, a high-performance customized interconnect for FPGA HBM board. Novel HLS-based optimization techniques are introduced to increase the throughput of AXI bus masters and switching elements. We also present a high-performance customized crossbar that may replace the built-in crossbar. The effectiveness of HBM Connect is demonstrated using Xilinx's Alveo U280 HBM board. Based on bucket sort and merge sort case studies, we explore several design spaces and find the design point with the best resource-performance trade-off. The result shows that HBM Connect improves the resource-performance metrics by 6.5X-211X.","performance optimization, high bandwidth memory, on-chip network, field-programmable gate array, high-level synthesis","","FPGA '21"
"Journal Article","Chen Q,Xia X,Hu H,Lo D,Li S","Why My Code Summarization Model Does Not Work: Code Comment Improvement with Category Prediction","ACM Trans. Softw. Eng. Methodol.","2021","30","2","","Association for Computing Machinery","New York, NY, USA","","","2021-02","","1049-331X","https://doi.org/10.1145/3434280;http://dx.doi.org/10.1145/3434280","10.1145/3434280","Code summarization aims at generating a code comment given a block of source code and it is normally performed by training machine learning algorithms on existing code block-comment pairs. Code comments in practice have different intentions. For example, some code comments might explain how the methods work, while others explain why some methods are written. Previous works have shown that a relationship exists between a code block and the category of a comment associated with it. In this article, we aim to investigate to which extent we can exploit this relationship to improve code summarization performance. We first classify comments into six intention categories and manually label 20,000 code-comment pairs. These categories include “what,” “why,” “how-to-use,” “how-it-is-done,” “property,” and “others.” Based on this dataset, we conduct an experiment to investigate the performance of different state-of-the-art code summarization approaches on the categories. We find that the performance of different code summarization approaches varies substantially across the categories. Moreover, the category for which a code summarization model performs the best is different for the different models. In particular, no models perform the best for “why” and “property” comments among the six categories. We design a composite approach to demonstrate that comment category prediction can boost code summarization to reach better results. The approach leverages classified code-category labeled data to train a classifier to infer categories. Then it selects the most suitable models for inferred categories and outputs the composite results. Our composite approach outperforms other approaches that do not consider comment categories and obtains a relative improvement of 8.57% and 16.34% in terms of ROUGE-L and BLEU-4 score, respectively.","comment classification, Code summarization, code comment","",""
"Journal Article","Zhang H,Wang S,Chen TH,Hassan AE","Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?","ACM Trans. Softw. Eng. Methodol.","2021","30","2","","Association for Computing Machinery","New York, NY, USA","","","2021-02","","1049-331X","https://doi.org/10.1145/3434279;http://dx.doi.org/10.1145/3434279","10.1145/3434279","Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87% of the comments have 0-score) and (2) in 97.3% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier.","Empirical software engineering, crowdsourced knowledge sharing, Q8A website, stack overflow, commenting","",""
"Conference Paper","Lin Z,Lin L","A Code Similarity Detection Algorithm Based on Maximum Common Subtree Optimization","","2021","","","104–110","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering","Xiamen, China","2021","9781450387811","","https://doi.org/10.1145/3443467.3443737;http://dx.doi.org/10.1145/3443467.3443737","10.1145/3443467.3443737","The code similarity detection is different from the traditional text duplication checking. The former has a lot of the same syntax content in the code. There are two code duplication detection algorithms. One is realized by extracting and counting characteristic attributes, which can result in a lack of the logical relationship between code structures. The other is realized by abstracting code into a string, tree structure or graph structure, which can lead to a lack of codes' semantic features. To rectify these deficiencies, an optimization algorithm based on maximum common subtree is proposed. First of all, the structural information based on the largest common subtree is extracted to calculate the structural similarity of codes. After this, the semantic information based on the longest common subsequence is extracted to calculate the semantic similarity of codes. Finally, the semantic similarity and structural similarity are assigned different weights using TF-IDF algorithm. Experimental results show that, the optimization for code similarity detection based on the maximum common subtree is able to reduce the non-plagiarism similarity of codes and keeps more feature information than the traditional code duplication checking algorithm.","Longest Common Subsequence, Code Similarity Detection, TF-IDF, Maximum Common Subtree","","EITCE 2020"
"Conference Paper","Zou Y,Ban B,Xue Y,Xu Y","CCGraph: A PDG-Based Code Clone Detector with Approximate Graph Matching","","2021","","","931–942","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416541;http://dx.doi.org/10.1145/3324884.3416541","10.1145/3324884.3416541","Software clone detection is an active research area, which is very important for software maintenance, bug detection, etc. The two pieces of cloned code reflect some similarities or equivalents in the syntax or structure of the code representations. There are many representations of code like AST, token, PDG, etc. The PDG (Program Dependency Graph) of source code can contain both syntactic and structural information. However, most existing PDG-based tools are quite time-consuming and miss many clones because they detect code clones with exact graph matching by using subgraph isomorphism. In this paper, we propose a novel PDG-based code clone detector, CCGraph, that uses graph kernels. Firstly, we normalize the structure of PDGs and design a two-stage filtering strategy by measuring the characteristic vectors of codes. Then we detect the code clones by using an approximate graph matching algorithm based on the reforming WL (Weisfeiler-Lehman) graph kernel. Experiment results show that CCGraph retains a high accuracy, has both better recall and F1-score values, and detects more semantic clones than other two related state-of-the-art tools. Besides, CCGraph is much more efficient than the existing PDG-based tools.","WL graph kernel, clone detection, program dependence graph","","ASE '20"
"Conference Paper","Wu Y,Zou D,Dou S,Yang S,Yang W,Cheng F,Liang H,Jin H","SCDetector: Software Functional Clone Detection Based on Semantic Tokens Analysis","","2021","","","821–833","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416562;http://dx.doi.org/10.1145/3324884.3416562","10.1145/3324884.3416562","Code clone detection is to find out code fragments with similar functionalities, which has been more and more important in software engineering. Many approaches have been proposed to detect code clones, in which token-based methods are the most scalable but cannot handle semantic clones because of the lack of consideration of program semantics. To address the issue, researchers conduct program analysis to distill the program semantics into a graph representation and detect clones by matching the graphs. However, such approaches suffer from low scalability since graph matching is typically time-consuming.In this paper, we propose SCDetector to combine the scalability of token-based methods with the accuracy of graph-based methods for software functional clone detection. Given a function source code, we first extract the control flow graph by static analysis. Instead of using traditional heavyweight graph matching, we treat the graph as a social network and apply social-network-centrality analysis to dig out the centrality of each basic block. Then we assign the centrality to each token in a basic block and sum the centrality of the same token in different basic blocks. By this, a graph is turned into certain tokens with graph details (i.e., centrality), called semantic tokens. Finally, these semantic tokens are fed into a Siamese architecture neural network to train a code clone detector. We evaluate SCDetector on two large datasets of functionally similar code. Experimental results indicate that our system is superior to four state-of-the-art methods (i.e., SourcererCC, Deckard, RtvNN, and ASTNN) and the time cost of SCDetector is 14 times less than a traditional graph-based method (i.e., CCSharp) on detecting semantic clones.","siamese network, social network centrality, semantic tokens","","ASE '20"
"Conference Paper","Wei B,Li Y,Li G,Xia X,Jin Z","Retrieve and Refine: Exemplar-Based Neural Comment Generation","","2021","","","349–360","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416578;http://dx.doi.org/10.1145/3324884.3416578","10.1145/3324884.3416578","Code comment generation which aims to automatically generate natural language descriptions for source code, is a crucial task in the field of automatic software development. Traditional comment generation methods use manually-crafted templates or information retrieval (IR) techniques to generate summaries for source code. In recent years, neural network-based methods which leveraged acclaimed encoder-decoder deep learning framework to learn comment generation patterns from a large-scale parallel code corpus, have achieved impressive results. However, these emerging methods only take code-related information as input. Software reuse is common in the process of software development, meaning that comments of similar code snippets are helpful for comment generation. Inspired by the IR-based and template-based approaches, in this paper, we propose a neural comment generation approach where we use the existing comments of similar code snippets as exemplars to guide comment generation. Specifically, given a piece of code, we first use an IR technique to retrieve a similar code snippet and treat its comment as an exemplar. Then we design a novel seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to assist in the target comment generation based on the semantic similarity between the source code and the similar code. We evaluate our approach on a large-scale Java corpus, which contains about 2M samples, and experimental results demonstrate that our model outperforms the state-of-the-art methods by a substantial margin.","comment generation, deep learning","","ASE '20"
"Conference Paper","Tian H,Liu K,Kaboré AK,Koyuncu A,Li L,Klein J,Bissyandé TF","Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair","","2021","","","981–992","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416532;http://dx.doi.org/10.1145/3324884.3416532","10.1145/3324884.3416532","A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.","program repair, patch correctness, machine learning, embeddings, distributed representation learning","","ASE '20"
"Conference Paper","Zhan X,Fan L,Liu T,Chen S,Li L,Wang H,Xu Y,Luo X,Liu Y","Automated Third-Party Library Detection for Android Applications: Are We There Yet?","","2021","","","919–930","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416582;http://dx.doi.org/10.1145/3324884.3416582","10.1145/3324884.3416582","Third-party libraries (TPLs) have become a significant part of the Android ecosystem. Developers can employ various TPLs with different functionalities to facilitate their app development. Unfortunately, the popularity of TPLs also brings new challenges and even threats. TPLs may carry malicious or vulnerable code, which can infect popular apps to pose threats to mobile users. Besides, the code of third-party libraries could constitute noises in some downstream tasks (e.g., malware and repackaged app detection). Thus, researchers have developed various tools to identify TPLs. However, no existing work has studied these TPL detection tools in detail; different tools focus on different applications with performance differences, but little is known about them.To better understand existing TPL detection tools and dissect TPL detection techniques, we conduct a comprehensive empirical study to fill the gap by evaluating and comparing all publicly available TPL detection tools based on four criteria: effectiveness, efficiency, code obfuscation-resilience capability, and ease of use. We reveal their advantages and disadvantages based on a systematic and thorough empirical study. Furthermore, we also conduct a user study to evaluate the usability of each tool. The results show that LibScout outperforms others regarding effectiveness, LibRadar takes less time than others and is also regarded as the most easy-to-use one, and LibPecker performs the best in defending against code obfuscation techniques. We further summarize the lessons learned from different perspectives, including users, tool implementation, and researchers. Besides, we enhance these open-sourced tools by fixing their limitations to improve their detection ability. We also build an extensible framework that integrates all existing available TPL detection tools, providing online service for the research community. We make publicly available the evaluation dataset and enhanced tools. We believe our work provides a clear picture of existing TPL detection techniques and also give a road-map for future directions.","empirical study, Android, library detection, third-party library","","ASE '20"
"Conference Paper","Zhang J,Wang X,Zhang H,Sun H,Pu Y,Liu X","Learning to Handle Exceptions","","2021","","","29–41","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416568;http://dx.doi.org/10.1145/3324884.3416568","10.1145/3324884.3416568","Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.","neural network, deep learning, exception handling, code generation","","ASE '20"
"Conference Paper","Gros D,Sezhiyan H,Devanbu P,Yu Z","Code to Comment ""Translation"": Data, Metrics, Baselining & Evaluation","","2021","","","746–757","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416546;http://dx.doi.org/10.1145/3324884.3416546","10.1145/3324884.3416546","The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task---specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using ""affinity pairs"" of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.","","","ASE '20"
"Conference Paper","Collie B,Ginsbach P,Woodruff J,Rajan A,O'Boyle MF","M3: Semantic API Migrations","","2021","","","90–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416618;http://dx.doi.org/10.1145/3324884.3416618","10.1145/3324884.3416618","Library migration is a challenging problem, where most existing approaches rely on prior knowledge. This can be, for example, information derived from changelogs or statistical models of API usage.This paper addresses a different API migration scenario where there is no prior knowledge of the target library. We have no historical changelogs and no access to its internal representation. To tackle this problem, this paper proposes a novel approach (M3), where probabilistic program synthesis is used to semantically model the behavior of library functions. Then, we use an SMT-based code search engine to discover similar code in user applications. These discovered instances provide potential locations for API migrations.We evaluate our approach against 7 well-known libraries from varied application domains, learning correct implementations for 94 functions. Our approach is integrated with standard compiler tooling, and we use this integration to evaluate migration opportunities in 9 existing C/C++ applications with over 1MLoC. We discover over 7,000 instances of these functions, of which more than 2,000 represent migration opportunities.","","","ASE '20"
"Conference Paper","Li Z,Chen TH,Shang W","Where Shall We Log? Studying and Suggesting Logging Locations in Code Blocks","","2021","","","361–372","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416636;http://dx.doi.org/10.1145/3324884.3416636","10.1145/3324884.3416636","Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations (e.g., exception logging) or at a coarse-grained level (e.g., method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80.1% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.","","","ASE '20"
"Conference Paper","Liu F,Li G,Zhao Y,Jin Z","Multi-Task Learning Based Pre-Trained Language Model for Code Completion","","2021","","","473–485","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416591;http://dx.doi.org/10.1145/3324884.3416591","10.1145/3324884.3416591","Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.","pre-trained language model, multi-task learning, code completion, transformer networks","","ASE '20"
"Conference Paper","Liu Z,Xia X,Yan M,Li S","Automating Just-in-Time Comment Updating","","2021","","","585–597","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","Virtual Event, Australia","2021","9781450367684","","https://doi.org/10.1145/3324884.3416581;http://dx.doi.org/10.1145/3324884.3416581","10.1145/3324884.3416581","Code comments are valuable for program comprehension and software maintenance, and also require maintenance with code evolution. However, when changing code, developers sometimes neglect updating the related comments, bringing in inconsistent or obsolete comments (aka., bad comments). Such comments are detrimental since they may mislead developers and lead to future bugs. Therefore, it is necessary to fix and avoid bad comments. In this work, we argue that bad comments can be reduced and even avoided by automatically performing comment updates with code changes. We refer to this task as ""Just-In-Time (JIT) Comment Updating"" and propose an approach named CUP (Comment UPdater) to automate this task. CUP can be used to assist developers in updating comments during code changes and can consequently help avoid the introduction of bad comments. Specifically, CUP leverages a novel neural sequence-to-sequence model to learn comment update patterns from extant code-comment co-changes and can automatically generate a new comment based on its corresponding old comment and code change. Several customized enhancements, such as a special tokenizer and a novel co-attention mechanism, are introduced in CUP by us to handle the characteristics of this task. We build a dataset with over 108K comment-code co-change samples and evaluate CUP on it. The evaluation results show that CUP outperforms an information-retrieval-based and a rule-based baselines by substantial margins, and can reduce developers' edits required for JIT comment updating. In addition, the comments generated by our approach are identical to those updated by developers in 1612 (16.7%) test samples, 7 times more than the best-performing baseline.","Seq2seq model, comment updating, code-comment co-evolution","","ASE '20"
"Journal Article","Arceri V,Mastroeni I","Analyzing Dynamic Code: A Sound Abstract Interpreter for Evil Eval","ACM Trans. Priv. Secur.","2021","24","2","","Association for Computing Machinery","New York, NY, USA","","","2021-01","","2471-2566","https://doi.org/10.1145/3426470;http://dx.doi.org/10.1145/3426470","10.1145/3426470","Dynamic languages, such as JavaScript, employ string-to-code primitives to turn dynamically generated text into executable code at run-time. These features make standard static analysis extremely hard if not impossible, because its essential data structures, i.e., the control-flow graph and the system of recursive equations associated with the program to analyze, are themselves dynamically mutating objects. Nevertheless, assembling code at run-time by manipulating strings, such as by eval in JavaScript, has been always strongly discouraged, since it is often recognized that “eval is evil,” leading static analyzers to not consider such statements or ignoring their effects. Unfortunately, the lack of formal approaches to analyze string-to-code statements pose a perfect habitat for malicious code, that is surely evil and do not respect good practice rules, allowing them to hide malicious intents as strings to be converted to code and making static analyses blind to the real malicious aim of the code. Hence, the need to handle string-to-code statements approximating what they can execute, and therefore allowing the analysis to continue (even in the presence of dynamically generated program statements) with an acceptable degree of precision, should be clear. To reach this goal, we propose a static analysis allowing us to collect string values and to soundly over-approximate and analyze the code potentially executed by a string-to-code statement.","dynamic languages, Abstract interpretation, static analysis","",""
"Conference Paper","Ke H,Wu H,Yang D","Towards Evolving Security Requirements of Industrial Internet: A Layered Security Architecture Solution Based on Data Transfer Techniques","","2021","","","504–511","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies","Guangzhou, China","2021","9781450387828","","https://doi.org/10.1145/3444370.3444620;http://dx.doi.org/10.1145/3444370.3444620","10.1145/3444370.3444620","Industrial Internet has been incrementally established in recent years to support smart manufacturing, which involves a collection of technology and value chain organization concepts. Along with the continuous development of the industrial Internet, an increasing number of threats have been exposed. In particular, the integration of traditional industrial settings and industrial Internet platforms leads to incompatible security countermeasures, making the industrial Internet vulnerable. This article comprehensively and deeply investigates the security challenges of the current industry Internet based on relevant research studies and realistic security scenarios. In response to our identified security challenges, we propose a data security transfer architecture solution for industrial platforms. Specifically, our proposal contains seven sub-modules, which enhances the industrial cloud platform with secure data security transfer technology to protect industrial data when it is connected to the cloud platforms and industrial equipment. To evaluate the utility of our proposal, we have specialized our security architecture solution to a realistic case of the industrial Internet.","Industrial safety issues, Industrial data security, Industrial Internet","","CIAT 2020"
"Journal Article","Shariffdeen RS,Tan SH,Gao M,Roychoudhury A","Automated Patch Transplantation","ACM Trans. Softw. Eng. Methodol.","2021","30","1","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3412376;http://dx.doi.org/10.1145/3412376","10.1145/3412376","Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted and inserted into a “similar” target program. We observe that despite standard procedures for vulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patch insertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.","Program repair, dynamic program analysis, code transplantation, patch transplantation","",""
"Journal Article","Siegmund J,Peitek N,Apel S,Siegmund N","Mastering Variation in Human Studies: The Role of Aggregation","ACM Trans. Softw. Eng. Methodol.","2021","30","1","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3406544;http://dx.doi.org/10.1145/3406544","10.1145/3406544","The human factor is prevalent in empirical software engineering research. However, human studies often do not use the full potential of analysis methods by combining analysis of individual tasks and participants with an analysis that aggregates results over tasks and/or participants. This may hide interesting insights of tasks and participants and may lead to false conclusions by overrating or underrating single-task or participant performance. We show that studying multiple levels of aggregation of individual tasks and participants allows researchers to have both insights from individual variations as well as generalized, reliable conclusions based on aggregated data. Our literature survey revealed that most human studies perform either a fully aggregated analysis or an analysis of individual tasks. To show that there is important, non-trivial variation when including human participants, we reanalyze 12 published empirical studies, thereby changing the conclusions or making them more nuanced. Moreover, we demonstrate the effects of different aggregation levels by answering a novel research question on published sets of fMRI data. We show that when more data are aggregated, the results become more accurate. This proposed technique can help researchers to find a sweet spot in the tradeoff between cost of a study and reliability of conclusions.","guidelines, data aggregation, Human studies","",""
"Journal Article","Gao Z,Xia X,Lo D,Grundy J","Technical Q8A Site Answer Recommendation via Question Boosting","ACM Trans. Softw. Eng. Methodol.","2021","30","1","","Association for Computing Machinery","New York, NY, USA","","","2021-12","","1049-331X","https://doi.org/10.1145/3412845;http://dx.doi.org/10.1145/3412845","10.1145/3412845","Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is “answer hungriness,” i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network–based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given a post, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network–based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives.","deep neural network, question answering, question boosting, sequence-to-sequence, weakly supervised learning, CQA","",""
"Conference Paper","Simon,Karnalim O,Sheard J,Dema I,Karkare A,Leinonen J,Liut M,McCauley R","Choosing Code Segments to Exclude from Code Similarity Detection","","2020","","","1–19","Association for Computing Machinery","New York, NY, USA","Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education","Trondheim, Norway","2020","9781450382939","","https://doi.org/10.1145/3437800.3439201;http://dx.doi.org/10.1145/3437800.3439201","10.1145/3437800.3439201","When student programs are compared for similarity as a step in the detection of academic misconduct, certain segments of code are always sure to be similar but are no cause for suspicion. Some of these segments are boilerplate code (e.g. public static void main String [] args) and some will be code that was provided to students as part of the assessment specification. This working group explores these and other types of code that are legitimately common in student assessments and can therefore be excluded from similarity checking. From their own institutions, working group members collected assessment submissions that together encompass a wide variety of assessment tasks in a wide variety of programming languages. The submissions were analysed to determine what sorts of code segment arose frequently in each assessment task. The group has found that common code can arise in programming assessment tasks when it is required for compilation purposes; when it reflects an intuitive way to undertake part or all of the task in question; when it can be legitimately copied from external sources; and when it has been suggested by people with whom many of the students have been in contact. A further finding is that the nature and size of the common code fragments vary with course level and with task complexity. An informal survey of programming educators confirms the group's findings and gives some reasons why various educators include code when setting programming assignments.","collusion, plagiarism, academic integrity, code similarity detection","","ITiCSE-WGR '20"
"Conference Paper","Freire WM,Massago M,Zavadski AC,Malachini AM,Amaral M,Colanzi TE","OPLA-Tool v2.0: A Tool for Product Line Architecture Design Optimization","","2020","","","818–823","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422498;http://dx.doi.org/10.1145/3422392.3422498","10.1145/3422392.3422498","The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.","multi-objective evolutionary algorithms, Software product line, product line architecture","","SBES '20"
"Conference Paper","Gomes RA,Pinheiro LB,Maciel RS","Anticipating Identification of Technical Debt Items in Model-Driven Software Projects","","2020","","","740–749","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422434;http://dx.doi.org/10.1145/3422392.3422434","10.1145/3422392.3422434","Model-driven development (MDD) and Technical Debt (TD) are software engineering approaches that look for promoting the quality of systems under development. Most research on TD focuses on application code as primary TD sources. In an MDD project, however, dealing with technical debt only on the source code may not be an adequate strategy because MDD projects should focus their software building efforts on models. Besides, in MDD projects, code generation is often done at a later stage than creating models, then dealing with TD only in source code can lead to unnecessary interest payments due to unmanaged debts, such as model and source codes artifacts desynchronization. Recent works concluded that MDD project codes are not technical debt free, making it necessary to investigate the possibility and benefits of applying TD identification techniques in earlier stages of the development process, such as in modeling phases. The use of TD concept in an MDD context is also known as Model-Driven Technical Debt (MDTD). This paper intends to analyze whether it is possible to use source code technical debt detection strategies to identify TD on code-generating models in the context of model-driven development projects. A catalog of nine different model technical debt items for platform-independent code-generating models was specified. An evaluation was performed to observe the effectiveness of the proposed catalog compared to existing source code identification techniques found in the literature. Through three different open source software projects, more than 78 thousand lines of code were investigated. Results revealed that, although the catalog items present different precision rates, it is possible to identify these model-driven technical debts before source code is generated. We hope that sharing this catalog version provides future contributions and improvements.","model-driven development, model smell, models, code smell, technical debt","","SBES '20"
"Conference Paper","Trindade RP,da Silva Bigonha MA,Ferreira KA","Oracles of Bad Smells: A Systematic Literature Review","","2020","","","62–71","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422415;http://dx.doi.org/10.1145/3422392.3422415","10.1145/3422392.3422415","A bad smell is an evidence of a design problem that may be harmful to the software maintenance. Several studies have been carried out to aid the identification of bad smells, by defining approaches or tools. Usually, the evaluation of these studies' results relies on data of oracles bad smells. An oracle is a set of data of bad smells found in a given software system. Such data serves as a referential template or a benchmark to evaluate the proposals on detecting bad smells. The availability and the quality of bad smell oracles are crucial to assert the quality of detection strategies of bad smells. This study aims to compile the bad smell oracles proposed in the literature. To achieve this, we conducted a Systematic Literature Review (SLR) to identify bad smell oracles and their characteristics. The main result of this study is a catalog of bad smell oracles that may be useful for research on bad smells, especially the studies that propose tools or detection strategies for bad smells.","oracle, bad smell, benchmark, design anomaly, code smell, systematic literature review","","SBES '20"
"Conference Paper","Martins J,Bezerra C,Uchôa A,Garcia A","Are Code Smell Co-Occurrences Harmful to Internal Quality Attributes? A Mixed-Method Study","","2020","","","52–61","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422419;http://dx.doi.org/10.1145/3422392.3422419","10.1145/3422392.3422419","Previous studies demonstrated how code smells (i.e., symptoms of the presence of system degradation) impact the software maintainability. However, few studies have investigated which code smell types tend to co-occur in the source code. Moreover, it is not clear to what extent the removal of code smell co-occurrences -through refactoring operations - has a positive impact on quality attributes such as cohesion, coupling, inheritance, complexity, and size. We aim at addressing these gaps through an empirical study. By investigating the impact of the smells co-occurrences in 11 releases of 3 closed-source systems, we observe (i) which code smells tend to co-occur together, (ii) the impact of the removal of code smell co-occurrences on quality internal attributes before and after refactoring, and (iii) which are the most difficult co-occurrences to refactoring from the developers' perspective. Our results show that 2 types of code smell co-occurrences generally tend to co-occur. Moreover, we observed that the removal of code smells co-occurrences lead to a significant reduction in the complexity of the systems studied was obtained. Conversely, cohesion and coupling tend to get worse. We also found that two code smells cooccurrences (God Class-Long Method and Disperse Coupling-Long Method) as the most difficult to refactor indicating that attention is needed not to insert these anomalies in the source code. Based on our findings, we argue that further research is needed on the impact of code smells co-occurrences on internal quality attributes.","Quality Attributes, Code Smells Co-occurrences, Refactoring","","SBES '20"
"Conference Paper","Gama E,Freire S,Mendonça M,Spínola RO,Paixao M,Cortés MI","Using Stack Overflow to Assess Technical Debt Identification on Software Projects","","2020","","","730–739","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422429;http://dx.doi.org/10.1145/3422392.3422429","10.1145/3422392.3422429","Context. The accumulation of technical debt (TD) items can lead to risks in software projects, such a gradual decrease in product quality, difficulties in their maintenance, and ultimately the cancellation of the project. To mitigate these risks, developers need means to identify TD items, which enable better documentation and improvements in TD management. Recent literature has proposed different indicator-based strategies for TD identification. However, there is limited empirical evidence to support that developers use these indicators to identify TD in practice. In this context, data from Q&A websites, such as Stack Overflow (SO), have been extensively leveraged in recent studies to investigate software engineering practices from a developers' point of view. Goal. This paper seeks to investigate, from the point of view of practitioners, how developers commonly identify TD items in their projects. Method. We mined, curated, and selected a total of 140 TD-related discussions on SO, from which we performed both quantitative and qualitative analyses. Results. We found that SO's practitioners commonly discuss TD identification, revealing 29 different low-level indicators for recognizing TD items on code, infrastructure, architecture, and tests. We grouped low-level indicators based on their themes, producing an aggregated set of 13 distinct high-level indicators. We then classified all low- and high-level indicators into three different categories according to which type of debt each of them is meant to identify. Conclusions. We organize the empirical evidence on the low- and high-level indicators and their relationship to types of TD in a conceptual framework, which may assist developers and serve as guidance for future research, shedding new light on TD identification state-of-practice.","Technical Debt, Mining Software Repositories, Stack Overflow, Indicators","","SBES '20"
"Conference Paper","Santana R,Martins L,Rocha L,Virgínio T,Cruz A,Costa H,Machado I","RAIDE: A Tool for Assertion Roulette and Duplicate Assert Identification and Refactoring","","2020","","","374–379","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIV Brazilian Symposium on Software Engineering","Natal, Brazil","2020","9781450387538","","https://doi.org/10.1145/3422392.3422510;http://dx.doi.org/10.1145/3422392.3422510","10.1145/3422392.3422510","Test smells are fragments of code that can affect the comprehensibility and the maintainability of the test code. Preventing, detecting, and correcting test smells are tasks that may require a lot of effort, and might not scale to large-sized projects when carried out manually. Currently, there are many tools available to support test smells detection. However, they usually do not provide neither a user-friendly interface nor automated support for refactoring the test code to remove test smells. In this work, we propose RAIDE, an open-source and IDE-integrated tool. RAIDE assists testers with an environment for automated detection of lines of code affected by test smells, as well as a semi-automated refactoring for Java projects using the JUnit framework.","Unit Test, Test Smells, Test Refactoring, Automated Test","","SBES '20"
"Conference Paper","Cozzi E,Vervier PA,Dell'Amico M,Shen Y,Bilge L,Balzarotti D","The Tangled Genealogy of IoT Malware","","2020","","","1–16","Association for Computing Machinery","New York, NY, USA","Annual Computer Security Applications Conference","Austin, USA","2020","9781450388580","","https://doi.org/10.1145/3427228.3427256;http://dx.doi.org/10.1145/3427228.3427256","10.1145/3427228.3427256","The recent emergence of consumer off-the-shelf embedded (IoT) devices and the rise of large-scale IoT botnets has dramatically increased the volume and sophistication of Linux malware observed in the wild. The security community has put a lot of effort to document these threats but analysts mostly rely on manual work, which makes it difficult to scale and hard to regularly maintain. Moreover, the vast amount of code reuse that characterizes IoT malware calls for an automated approach to detect similarities and identify the phylogenetic tree of each family. In this paper we present the largest measurement of IoT malware to date. We systematically reconstruct – through the use of binary code similarity – the lineage of IoT malware families, and track their relationships, evolution, and variants. We apply our technique on a dataset of more than 93k samples submitted to VirusTotal over a period of 3.5 years. We discuss the findings of our analysis and present several case studies to highlight the tangled relationships of IoT malware.","Classification, IoT, Lineage, Malware, Measurement","","ACSAC '20"
"Conference Paper","Patrick-Evans J,Cavallaro L,Kinder J","Probabilistic Naming of Functions in Stripped Binaries","","2020","","","373–385","Association for Computing Machinery","New York, NY, USA","Annual Computer Security Applications Conference","Austin, USA","2020","9781450388580","","https://doi.org/10.1145/3427228.3427265;http://dx.doi.org/10.1145/3427228.3427265","10.1145/3427228.3427265","Debugging symbols in binary executables carry the names of functions and global variables. When present, they greatly simplify the process of reverse engineering, but they are almost always removed (stripped) for deployment. We present the design and implementation of punstrip, a tool which combines a probabilistic fingerprint of binary code based on high-level features with a probabilistic graphical model to learn the relationship between function names and program structure. As there are many naming conventions and developer styles, functions from different applications do not necessarily have the exact same name, even if they implement the exact same functionality. We therefore evaluate punstrip across three levels of name matching: exact; an approach based on natural language processing of name components; and using Symbol2Vec, a new embedding of function names based on random walks of function call graphs. We show that our approach is able to recognize functions compiled across different compilers and optimization levels and then demonstrate that punstrip can predict semantically similar function names based on code structure. We evaluate our approach over open source C binaries from the Debian Linux distribution and compare against the state of the art.","binaries, machine learning, function names","","ACSAC '20"
"Conference Paper","Vaidyan VM,Tyagi A","Instruction Level Disassembly through Electromagnetic Side-Chanel: Machine Learning Classification Approach with Reduced Combinatorial Complexity","","2020","","","124–130","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning","Beijing, China","2020","9781450375733","","https://doi.org/10.1145/3432291.3432300;http://dx.doi.org/10.1145/3432291.3432300","10.1145/3432291.3432300","EM side-channel can be quite effective at instruction level disassembly of the executing program. This leaks IP from Internet of Things (IoT) networks. This may also serve as a benign capability to reverse engineer IoT malware binaries. Power Side Channel instruction level disassembly state-of-the-art is capable of identifying instructions in a 2-3 stage pipeline at 50-200 MHz clock frequency with reasonable accuracy by grouping instructions. EM side-channel works at distance unlike power side-channel. Machine Learning models for instruction identification, Principal Component Analysis (PCA) for feature selection, Gaussian Process Classifiers (GPC), Adaptive Boosting (AB), Quadratic Discriminant Analysis (QDA), Naïve Bayes (NB), Support Vector Machines (SVM) and Convolutional Neural Network (CNN) for instruction classification were developed. Our results of implementation on a 2-stage pipelined architecture demonstrate that the EM side-channel classification approach identifies instructions in flight with 99% accuracy.","Instruction Disassembly, Electromagnetics, Computer Architecture, Machine Learning, Hardware Security, IoT Devices","","SPML 2020"
"Journal Article","Coblenz M,Oei R,Etzel T,Koronkevich P,Baker M,Bloem Y,Myers BA,Sunshine J,Aldrich J","Obsidian: Typestate and Assets for Safer Blockchain Programming","ACM Trans. Program. Lang. Syst.","2020","42","3","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","0164-0925","https://doi.org/10.1145/3417516;http://dx.doi.org/10.1145/3417516","10.1145/3417516","Blockchain platforms are coming into use for processing critical transactions among participants who have not established mutual trust. Many blockchains are programmable, supporting smart contracts, which maintain persistent state and support transactions that transform the state. Unfortunately, bugs in many smart contracts have been exploited by hackers. Obsidian is a novel programming language with a type system that enables static detection of bugs that are common in smart contracts today. Obsidian is based on a core calculus, Silica, for which we proved type soundness. Obsidian uses typestate to detect improper state manipulation and uses linear types to detect abuse of assets. We integrated a permissions system that encodes a notion of ownership to allow for safe, flexible aliasing. We describe two case studies that evaluate Obsidian’s applicability to the domains of parametric insurance and supply chain management, finding that Obsidian’s type system facilitates reasoning about high-level states and ownership of resources. We compared our Obsidian implementation to a Solidity implementation, observing that the Solidity implementation requires much boilerplate checking and tracking of state, whereas Obsidian does this work statically.","linearity, Typestate, permissions, blockchain, type systems, alias control, smart contracts, ownership","",""
"Conference Paper","Karnalim O,Simon,Chivers W","Preprocessing for Source Code Similarity Detection in Introductory Programming","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th Koli Calling International Conference on Computing Education Research","Koli, Finland","2020","9781450389211","","https://doi.org/10.1145/3428029.3428065;http://dx.doi.org/10.1145/3428029.3428065","10.1145/3428029.3428065","It is well documented that some students either work together on programming assessments when required to work individually (collusion) or make unauthorised use of existing code from external sources (plagiarism). One approach used in the detection of these violations of academic integrity is source code similarity detection, the automatic checking of student programs for unduly high levels of similarity. Preprocessing of source code files has the potential to increase the effectiveness, the efficiency, or both, of the source code comparison process. There are many possible steps in the preprocessing, and examination of the literature suggests that these steps are selected and implemented without any empirical evidence as to their value. This paper lists 19 preprocessing steps that have been used in code similarity detection, and assesses the effectiveness and the efficiency of 16 of these steps on data sets of student programs from introductory programming courses. The results should help researchers to decide what preprocessing steps to include when designing source code similarity detection techniques or software. According to the study, identifier removal increases both effectiveness and efficiency. Token renaming and syntax tree linearisation increase effectiveness at a cost of efficiency. Other preprocessing steps are dependent upon characteristics of the data set and should ideally be empirically tested before being applied. The paper should also help alert programming educators to the sorts of disguise that students can apply to copied programs.","source code similarity detection, computing education, collusion, programming, plagiarism","","Koli Calling '20"
"Conference Paper","Cateté V,Isvik A,Barnes T","Infusing Computing: A Scaffolding and Teacher Accessibility Analysis of Computing Lessons Designed by Novices","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th Koli Calling International Conference on Computing Education Research","Koli, Finland","2020","9781450389211","","https://doi.org/10.1145/3428029.3428056;http://dx.doi.org/10.1145/3428029.3428056","10.1145/3428029.3428056","Creators of computing curricula do not always have formal pedagogical training. We investigated if exposing novice lesson designers to pedagogical best practices would result in the creation of lessons where evidence of successful use of these practices could be identified. We trained 29 high school students who were in a full-time computer science summer internship on how to create Snap! programming lessons for non-computing courses. Over the course of three weeks they developed computing-infused lessons on their choice of learning topic (science, business, language, etc.). We examined these lessons for their use of scaffolding, teacher accessibility, equity, and content. We found that students implemented many of the scaffolding techniques that they themselves experienced and created lessons that were detailed enough to be accessible for teacher use. We also identified significant relationships between both subject area and gender on equity scores, as well as an impact of collaboration on scaffolding type included. No difference in artifact quality was identified by prior student coding experience. This project represents an innovative way to engage students in learning more computer science while creating educational materials for computing in K-12 classrooms.","virtual learning, professional development, computing education","","Koli Calling '20"
"Conference Paper","Sotoudeh M,Thakur AV","Analogy-Making as a Core Primitive in the Software Engineering Toolbox","","2020","","","101–121","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Virtual, USA","2020","9781450381789","","https://doi.org/10.1145/3426428.3426918;http://dx.doi.org/10.1145/3426428.3426918","10.1145/3426428.3426918","An analogy is an identification of structural similarities and correspondences between two objects. Computational models of analogy making have been studied extensively in the field of cognitive science to better understand high-level human cognition. For instance, Melanie Mitchell and Douglas Hofstadter sought to better understand high-level perception by developing the Copycat algorithm for completing analogies between letter sequences. In this paper, we argue that analogy making should be seen as a core primitive in software engineering. We motivate this argument by showing how complex software engineering problems such as program understanding and source-code transformation learning can be reduced to an instance of the analogy-making problem. We demonstrate this idea using Sifter, a new analogy-making algorithm suitable for software engineering applications that adapts and extends ideas from Copycat. In particular, Sifter reduces analogy-making to searching for a sequence of update rule applications. Sifter uses a novel representation for mathematical structures capable of effectively representing the wide variety of information embedded in software.","static analysis, program understanding, software maintenance, fluid analogies","","Onward! 2020"
"Conference Paper","Zaytsev V","Software Language Engineers’ Worst Nightmare","","2020","","","72–85","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering","Virtual, USA","2020","9781450381765","","https://doi.org/10.1145/3426425.3426933;http://dx.doi.org/10.1145/3426425.3426933","10.1145/3426425.3426933","Many techniques in software language engineering get their first validation by being prototyped to work on one particular language such as Java, Scala, Scheme, or ML, or a subset of such a language. Claims of their generalisability, as well as discussion on potential threats to their external validity, are often based on authors' ad hoc understanding of the world outside their usual comfort zone. To facilitate and simplify such discussions by providing a solid measurable ground, we propose a language called BabyCobol, which was specifically designed to contain features that turn processing legacy programming languages such as COBOL, FORTRAN, PL/I, REXX, CLIST, and 4GLs (fourth generation languages), into such a challenge. The language is minimal by design so that it can help to quickly find weaknesses in frameworks making them inapplicable to dealing with legacy software. However, applying new techniques of software language engineering and reverse engineering to such a small language will not be too tedious and overwhelming. BabyCobol was designed in collaboration with industrial compiler developers by systematically traversing features of several second, third and fourth generation languages to identify the core culprits in making development of compiler for legacy languages difficult.","legacy software, teaching SLE, language engineering, domain-specific languages, software migration","","SLE 2020"
"Journal Article","Sui Y,Cheng X,Zhang G,Wang H","Flow2Vec: Value-Flow-Based Precise Code Embedding","Proc. ACM Program. Lang.","2020","4","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3428301;http://dx.doi.org/10.1145/3428301","10.1145/3428301","Code embedding, as an emerging paradigm for source code analysis, has attracted much attention over the past few years. It aims to represent code semantics through distributed vector representations, which can be used to support a variety of program analysis tasks (e.g., code summarization and semantic labeling). However, existing code embedding approaches are intraprocedural, alias-unaware and ignoring the asymmetric transitivity of directed graphs abstracted from source code, thus they are still ineffective in preserving the structural information of code. This paper presents Flow2Vec, a new code embedding approach that precisely preserves interprocedural program dependence (a.k.a value-flows). By approximating the high-order proximity, i.e., the asymmetric transitivity of value-flows, Flow2Vec embeds control-flows and alias-aware data-flows of a program in a low-dimensional vector space. Our value-flow embedding is formulated as matrix multiplication to preserve context-sensitive transitivity through CFL reachability by filtering out infeasible value-flow paths. We have evaluated Flow2Vec using 32 popular open-source projects. Results from our experiments show that Flow2Vec successfully boosts the performance of two recent code embedding approaches codevec and codeseq for two client applications, i.e., code classification and code summarization. For code classification, Flow2Vec improves codevec with an average increase of 21.2%, 20.1% and 20.7% in precision, recall and F1, respectively. For code summarization, Flow2Vec outperforms codeseq by an average of 13.2%, 18.8% and 16.0% in precision, recall and F1, respectively.","asymmetric transitivity, Flow2Vec, value-flows, code embedding","",""
"Journal Article","Devore-McDonald B,Berger ED","Mossad: Defeating Software Plagiarism Detection","Proc. ACM Program. Lang.","2020","4","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3428206;http://dx.doi.org/10.1145/3428206","10.1145/3428206","Automatic software plagiarism detection tools are widely used in educational settings to ensure that submitted work was not copied. These tools have grown in use together with the rise in enrollments in computer science programs and the widespread availability of code on-line. Educators rely on the robustness of plagiarism detection tools; the working assumption is that the effort required to evade detection is as high as that required to actually do the assigned work. This paper shows this is not the case. It presents an entirely automatic program transformation approach, MOSSAD, that defeats popular software plagiarism detection tools. MOSSAD comprises a framework that couples techniques inspired by genetic programming with domain-specific knowledge to effectively undermine plagiarism detectors. MOSSAD is effective at defeating four plagiarism detectors, including Moss and JPlag. MOSSAD is both fast and effective: it can, in minutes, generate modified versions of programs that are likely to escape detection. More insidiously, because of its non-deterministic approach, MOSSAD can, from a single program, generate dozens of variants, which are classified as no more suspicious than legitimate assignments. A detailed study of MOSSAD across a corpus of real student assignments demonstrates its efficacy at evading detection. A user study shows that graduate student assistants consistently rate MOSSAD-generated code as just as readable as authentic student code. This work motivates the need for both research on more robust plagiarism detection tools and greater integration of naturally plagiarism-resistant methodologies like code review into computer science education.","neural and evolutionary computing, computers and society, cryptography and security, programming languages","",""
"Journal Article","Wang Y,Wang K,Gao F,Wang L","Learning Semantic Program Embeddings with Graph Interval Neural Network","Proc. ACM Program. Lang.","2020","4","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3428205;http://dx.doi.org/10.1145/3428205","10.1145/3428205","Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed (fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic program embeddings.","Control-flow graphs, Null pointer dereference detection, Program embeddings, Graph neural networks, Intervals","",""
"Journal Article","Gao X,Barke S,Radhakrishna A,Soares G,Gulwani S,Leung A,Nagappan N,Tiwari A","Feedback-Driven Semi-Supervised Synthesis of Program Transformations","Proc. ACM Program. Lang.","2020","4","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3428287;http://dx.doi.org/10.1145/3428287","10.1145/3428287","While editing code, it is common for developers to make multiple related repeated edits that are all instances of a more general program transformation. Since this process can be tedious and error-prone, we study the problem of automatically learning program transformations from past edits, which can then be used to predict future edits. We take a novel view of the problem as a semi-supervised learning problem: apart from the concrete edits that are instances of the general transformation, the learning procedure also exploits access to additional inputs (program subtrees) that are marked as positive or negative depending on whether the transformation applies on those inputs. We present a procedure to solve the semi-supervised transformation learning problem using anti-unification and programming-by-example synthesis technology. To eliminate reliance on access to marked additional inputs, we generalize the semi-supervised learning procedure to a feedback-driven procedure that also generates the marked additional inputs in an iterative loop. We apply these ideas to build and evaluate three applications that use different mechanisms for generating feedback. Compared to existing tools that learn program transformations from edits, our feedback-driven semi-supervised approach is vastly more effective in successfully predicting edits with significantly lesser amounts of past edit data.","Program synthesis, Refactoring, Program transformation, Programming by Example","",""
"Conference Paper","Stucki N,Biboudis A,Doeraene S,Odersky M","Semantics-Preserving Inlining for Metaprogramming","","2020","","","14–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM SIGPLAN International Symposium on Scala","Virtual, USA","2020","9781450381772","","https://doi.org/10.1145/3426426.3428486;http://dx.doi.org/10.1145/3426426.3428486","10.1145/3426426.3428486","Inlining is used in many different ways in programming languages: some languages use it as a compiler-directive solely for optimization, some use it as a metaprogramming feature, and others lay their design in-between. This paper presents inlining through the lens of metaprogramming and we describe a powerful set of metaprogramming constructs that help programmers to unfold domain-specific decisions at compile-time. In a multi-paradigm language like Scala, the concern for generality of inlining poses several interesting questions and the challenge we tackle is to offer inlining without changing the model seen by the programmer. In this paper, we explore these questions by explaining the rationale behind the design of Scala-3's inlining capability and how it relates to its metaprogramming architecture.","OO, Macros, Inlining, Metaprogramming","","SCALA 2020"
"Journal Article","Bartell S,Dietz W,Adve VS","Guided Linking: Dynamic Linking without the Costs","Proc. ACM Program. Lang.","2020","4","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3428213;http://dx.doi.org/10.1145/3428213","10.1145/3428213","Dynamic linking is extremely common in modern software systems, thanks to the flexibility and space savings it offers. However, this flexibility comes at a cost: it’s impossible to perform interprocedural optimizations that involve calls to a dynamic library. The basic problem is that the run-time behavior of the dynamic linker can’t be predicted at compile time, so the compiler can make no assumptions about how such calls will behave. This paper introduces guided linking, a technique for optimizing dynamically linked software when some information about the dynamic linker’s behavior is known in advance. The developer provides an arbitrary set of programs, libraries, and plugins to our tool, along with constraints that limit the possible dynamic linking behavior of the software. By taking advantage of the constraints, our tool enables any existing optimization to be applied across dynamic linking boundaries. For example, the NoOverride constraint can be applied to a function when the developer knows it will never be overridden with a different definition at run time; guided linking then enables the function to be inlined into its callers in other libraries. We also introduce a novel code size optimization that deduplicates identical functions even across different parts of the software set. By applying guided linking to the Python interpreter and its dynamically loaded modules, supplying the constraint that no other programs or modules will be used, we increase speed by an average of 9%. By applying guided linking to a dynamically linked distribution of Clang and LLVM, and using the constraint that no other software will use the LLVM libraries, we can increase speed by 5% and reduce file size by 13%. If we relax the constraint to allow other software to use the LLVM libraries, we can still increase speed by 5% and reduce file size by 5%. If we use guided linking to combine 11 different versions of the Boost library, using minimal constraints, we can reduce the total library size by 57%.","Plugins, LTO, Link-Time Optimization, IR, LLVM, Code deduplication, Dynamic Linking, Shared Libraries","",""
"Conference Paper","Laddad S,Sen K","ScalaPy: Seamless Python Interoperability for Cross-Platform Scala Programs","","2020","","","2–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM SIGPLAN International Symposium on Scala","Virtual, USA","2020","9781450381772","","https://doi.org/10.1145/3426426.3428485;http://dx.doi.org/10.1145/3426426.3428485","10.1145/3426426.3428485","In recent years, Python has become the language of choice for data scientists with its many high-quality scientific libraries and Scala has become the go-to language for big data systems. In this paper, we bridge these languages with ScalaPy, a system for interoperability between Scala and Python. With ScalaPy, developers can use Python libraries in Scala by treating Python values as Scala objects and exposing Scala values to Python. ScalaPy supports both Scala on the JVM and Scala Native, enabling its usage from data experiments in interactive notebook environments to performance-critical production systems. In this paper, we explore the challenges involved with mixing the semantics and implementations of these two disparate languages.","language interoperability, Scala, Python","","SCALA 2020"
"Conference Paper","Rabin MR,Mukherjee A,Gnawali O,Alipour MA","Towards Demystifying Dimensions of Source Code Embeddings","","2020","","","29–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages","Virtual, USA","2020","9781450381253","","https://doi.org/10.1145/3416506.3423580;http://dx.doi.org/10.1145/3416506.3423580","10.1145/3416506.3423580","Source code representations are key in applying machine learning techniques for processing and analyzing programs. A popular approach in representing source code is neural source code embeddings that represents programs with high-dimensional vectors computed by training deep neural networks on a large volume of programs. Although successful, there is little known about the contents of these vectors and their characteristics. In this paper, we present our preliminary results towards better understanding the contents of code2vec neural source code embeddings. In particular, in a small case study, we use the code2vec embeddings to create binary SVM classifiers and compare their performance with the handcrafted features. Our results suggest that the handcrafted features can perform very close to the highly-dimensional code2vec embeddings, and the information gains are more evenly distributed in the code2vec embeddings compared to the handcrafted features. We also find that the code2vec embeddings are more resilient to the removal of dimensions with low information gains than the handcrafted features. We hope our results serve a stepping stone toward principled analysis and evaluation of these code representations.","Models of Code, Interpretability, Source Code Representation, Source Code Embeddings","","RL+SE&PL 2020"
"Conference Paper","Mahajan S,Abolhassani N,Prasad MR","Recommending Stack Overflow Posts for Fixing Runtime Exceptions Using Failure Scenario Matching","","2020","","","1052–1064","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3409764;http://dx.doi.org/10.1145/3368089.3409764","10.1145/3368089.3409764","Using online Q&A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice. Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO. In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code. MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match. To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations. The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences. We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques. We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances. In some cases the post is judged to be even better than the one manually found by the participant.","crowd intelligence, code search, runtime exceptions, static analysis","","ESEC/FSE 2020"
"Conference Paper","Miloudi C,Cheikhi L,Idri A","A Review of Open Source Software Maintenance Effort Estimation","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications","Rabat, Morocco","2020","9781450377331","","https://doi.org/10.1145/3419604.3419809;http://dx.doi.org/10.1145/3419604.3419809","10.1145/3419604.3419809","Open Source Software (OSS) is gaining interests of software engineering community as well as practitioners from industry with the growth of the internet. Studies in estimating maintenance effort (MEE) of such software product have been published in the literature in order to provide better estimation. The aim of this study is to provide a review of studies related to maintenance effort estimation for open source software (OSSMEE). To this end, a set of 60 primary empirical studies are selected from six electronic databases and a discussion is provided according to eight research questions (RQs) related to: publication year, publication source, datasets (OSS projects), metrics (independent variables), techniques, maintenance effort (dependent variable), validation methods, and accuracy criteria used in the empirical validation. This study has found that popular OSS projects have been used, Linear Regression, Naïve Bayes and k Nearest Neighbors were frequently used, and bug resolution was the most used regarding the estimation of maintenance effort for the future releases. A set of gaps are identified and recommendations for researchers are also provided.","Review, Maintenance effort estimation, Empirical, metrics, Datasets, techniques, Open source software","","SITA'20"
"Conference Paper","Li B,Yan M,Xia X,Hu X,Li G,Lo D","DeepCommenter: A Deep Code Comment Generation Tool with Hybrid Lexical and Syntactical Information","","2020","","","1571–1575","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3417926;http://dx.doi.org/10.1145/3368089.3417926","10.1145/3368089.3417926","As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client’s request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin.","Program Comprehension, Comment Generation, Deep Learning","","ESEC/FSE 2020"
"Conference Paper","Lee J,Nie P,Li JJ,Gligoric M","On the Naturalness of Hardware Descriptions","","2020","","","530–542","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3409692;http://dx.doi.org/10.1145/3368089.3409692","10.1145/3368089.3409692","Mining software repositories (MSR) has been shown effective for extracting data used to improve various software engineering tasks, including code completion, code repair, code search, and code summarization. Despite a large body of work on MSR, researchers have focused almost exclusively on repositories that contain code written in imperative programming languages, such as Java and C/C++. Unlike prior work, in this paper, we focus on mining publicly available hardware descriptions (HDs) written in hardware description languages (HDLs), such as VHDL. HDLs have unique syntax and semantics compared to popular imperative languages, and learning-based tools available to hardware designers are well behind those used in other application domains. We assembled large HD corpora consisting of source code written in several HDLs and report on their characteristics. Our language model evaluation reveals that HDs possess a high level of naturalness similar to software written in imperative languages. Further, by utilizing our corpora, we built several deep learning models for automated code completion in VHDL; our models take into account unique characteristics of HDLs, including similarities of nearby concurrent signal assignment statements, in-built concurrency, and the frequently used signal types. These characteristics led to more effective neural models, achieving a BLEU score of 37.3, an 8-14-point improvement over rule-based and neural baselines.","VHDL, Sys- temVerilog, Hardware Description Languages, Code Completion, Naturalness, Verilog, Natural Language Processing","","ESEC/FSE 2020"
"Conference Paper","Escobar-Velásquez C,Riveros D,Linares-Vásquez M","MutAPK 2.0: A Tool for Reducing Mutation Testing Effort of Android Apps","","2020","","","1611–1615","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3417942;http://dx.doi.org/10.1145/3368089.3417942","10.1145/3368089.3417942","Mutation testing is a time consuming process because large sets of fault-injected-versions of an original app are generated and executed with the purpose of evaluating the quality of a given test suite. In the case of Android apps, recent studies even suggest that mutant generation and mutation testing effort could be greater when the mutants are generated at the APK level. To reduce that effort, useless (e.g., equivalent) mutants should be avoided and mutant selection techniques could be used to reduce the set of mutants used with mutation testing. However, despite the existence of mutation testing tools, none of those tools provides features for removing useless mutants and sampling mutant sets. In this paper, we present MutAPK 2.0, an improved version of our open source mutant generation tool (MutAPK) for Android apps at APK level. To the best of our knowledge, MutAPK 2.0 is the first tool that enables the removal of dead-code mutants, provides a set of mutant selection strategies, and removes automatically equivalent and duplicate mutants. MutAPK 2.0 is publicly available at GitHub: https://thesoftwaredesignlab.github.io/MutAPK/ VIDEO: https://thesoftwaredesignlab.github.io/MutAPK/video.html","Mutation Testing, Mutant Selection, Equivalent, Duplicate, Dead code","","ESEC/FSE 2020"
"Conference Paper","Yan M,Xia X,Fan Y,Lo D,Hassan AE,Zhang X","Effort-Aware Just-in-Time Defect Identification in Practice: A Case Study at Alibaba","","2020","","","1308–1319","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3417048;http://dx.doi.org/10.1145/3368089.3417048","10.1145/3368089.3417048","Effort-aware Just-in-Time (JIT) defect identification aims at identifying defect-introducing changes just-in-time with limited code inspection effort. Such identification has two benefits compared with traditional module-level defect identification, i.e., identifying defects in a more cost-effective and efficient manner. Recently, researchers have proposed various effort-aware JIT defect identification approaches, including supervised (e.g., CBS+, OneWay) and unsupervised approaches (e.g., LT and Code Churn). The comparison of the effectiveness between such supervised and unsupervised approaches has attracted a large amount of research interest. However, the effectiveness of the recently proposed approaches and the comparison among them have never been investigated in an industrial setting. In this paper, we investigate the effectiveness of state-of-the-art effort-aware JIT defect identification approaches in an industrial setting. To that end, we conduct a case study on 14 Alibaba projects with 196,790 changes. In our case study, we investigate three aspects: (1) The effectiveness of state-of-the-art supervised (i.e., CBS+,OneWay, EALR) and unsupervised (i.e., LT and Code Churn) effortaware JIT defect identification approaches on Alibaba projects, (2) the importance of the features used in the effort-aware JIT defect identification approach, and (3) the association between projectspecific factors and the likelihood of a defective change. Moreover, we develop a tool based on the best performing approach and investigate the tool's effectiveness in a real-life setting at Alibaba.","Industrial study, Effort-aware, Just-in-Time defect identification","","ESEC/FSE 2020"
"Conference Paper","Gopstein D,Fayard AL,Apel S,Cappos J","Thinking Aloud about Confusing Code: A Qualitative Investigation of Program Comprehension and Atoms of Confusion","","2020","","","605–616","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3409714;http://dx.doi.org/10.1145/3368089.3409714","10.1145/3368089.3409714","Atoms of confusion are small patterns of code that have been empirically validated to be difficult to hand-evaluate by programmers. Previous research focused on defining and quantifying this phenomenon, but not on explaining or critiquing it. In this work, we address core omissions to the body of work on atoms of confusion, focusing on the ‘how’ and ‘why’ of programmer misunderstanding. We performed a think-aloud study in which we observed programmers, both professionals and students, as they hand-evaluated confusing code. We performed a qualitative analysis of the data and found several surprising results, which explain previous results, outline avenues of further research, and suggest improvements of the research methodology. A notable observation is that correct hand-evaluations do not imply understanding, and incorrect evaluations not misunderstanding. We believe this and other observations may be used to improve future studies and models of program comprehension. We argue that thinking of confusion as an atomic construct may pose challenges to formulating new candidates for atoms of confusion. Ultimately, we question whether hand-evaluation correctness is, itself, a sufficient instrument to study program comprehension.","Program Understanding, Think-Aloud Study, Atoms of Confusion","","ESEC/FSE 2020"
"Conference Paper","Barnaby C,Sen K,Zhang T,Glassman E,Chandra S","Exempla Gratis (E.G.): Code Examples for Free","","2020","","","1353–1364","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3417052;http://dx.doi.org/10.1145/3368089.3417052","10.1145/3368089.3417052","Modern software engineering often involves using many existing APIs, both open source and – in industrial coding environments– proprietary. Programmers reference documentation and code search tools to remind themselves of proper common usage patterns of APIs. However, high-quality API usage examples are computationally expensive to curate and maintain, and API usage examples retrieved from company-wide code search can be tedious to review. We present a tool, EG, that mines codebases and shows the common, idiomatic us-age examples for API methods. EG was integrated into Facebook’s internal code search tool for the Hack language and evaluated on open-source GitHub projects written in Python. EG was also compared against code search results and hand-written examples from a popular programming website called ProgramCreek. Compared with these two baselines, examples generated by EG are more succinct and representative with less extraneous statements. In addition, a survey with Facebook developers shows that EG examples are preferred in 97% of cases.","API examples, big code, software tools","","ESEC/FSE 2020"
"Conference Paper","Li L,Li Z,Zhang W,Zhou J,Wang P,Wu J,He G,Zeng X,Deng Y,Xie T","Clustering Test Steps in Natural Language toward Automating Test Automation","","2020","","","1285–1295","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Virtual Event, USA","2020","9781450370431","","https://doi.org/10.1145/3368089.3417067;http://dx.doi.org/10.1145/3368089.3417067","10.1145/3368089.3417067","For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover’sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8% improvement on cluster quality, reducing 65.9% number of clusters, i.e., the number of test API methods to be implemented.","Clustering, software testing, natural language processing","","ESEC/FSE 2020"
"Journal Article","Samuelsson SG,Book M","Eliciting Sketched Expressions of Command Intentions in an IDE","Proc.  ACM Hum. -Comput.  Interact.","2020","4","ISS","","Association for Computing Machinery","New York, NY, USA","","","2020-11","","","https://doi.org/10.1145/3427328;http://dx.doi.org/10.1145/3427328","10.1145/3427328","Software engineers routinely use sketches (informal, ad-hoc drawings) to visualize and communicate complex ideas for colleagues or themselves. We hypothesize that sketching could also be used as a novel interaction modality in integrated software development environments (IDEs), allowing developers to express desired source code manipulations by sketching right on top of the IDE, rather than remembering keyboard shortcuts or using a mouse to navigate menus and dialogs. For an initial assessment of the viability of this idea, we conducted an elicitation study that prompted software developers to express a number of common IDE commands through sketches. For many of our task prompts, we observed considerable agreement in how developers would express the respective commands through sketches, suggesting that further research on a more formal sketch-based visual command language for IDEs would be worthwhile.","elicitation study, user interfaces, software development environments, sketching","",""
"Conference Paper","Cremers C,Fairoze J,Kiesl B,Naska A","Clone Detection in Secure Messaging: Improving Post-Compromise Security in Practice","","2020","","","1481–1495","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3423354;http://dx.doi.org/10.1145/3372297.3423354","10.1145/3372297.3423354","We investigate whether modern messaging apps achieve the strong post-compromise security guarantees offered by their underlying protocols. In particular, we perform a black-box experiment in which a user becomes the victim of a clone attack; in this attack, the user's full state (including identity keys) is compromised by an attacker who clones their device and then later attempts to impersonate them, using the app through its user interface.Our attack should be prevented by protocols that offer post-compromise security, and thus, by all apps that are based on Signal's double-ratchet algorithm (for instance, the Signal app, WhatsApp, and Facebook Secret Conversations). Our experiments reveal that this is not the case: most deployed messaging apps fall far short of the security that their underlying mechanisms suggest.We conjecture that this security gap is a result of many apps trading security for usability, by tolerating certain forms of desynchronization. We show that the tolerance of desynchronization necessarily leads to loss of post-compromise security in the strict sense, but we also show that more security can be retained than is currently offered in practice. Concretely, we present a modified version of the double-ratchet algorithm that tolerates forms of desynchronization while still being able to detect cloning activity. Moreover, we formally analyze our algorithm using the Tamarin prover to show that it achieves the desired security properties.","post-compromise security, security protocols, forward secrecy, clone detection, secure messaging, tamarin prover, formal verification, double ratchet","","CCS '20"
"Conference Paper","Zhao L,Zhu Y,Ming J,Zhang Y,Zhang H,Yin H","PatchScope: Memory Object Centric Patch Diffing","","2020","","","149–165","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3423342;http://dx.doi.org/10.1145/3372297.3423342","10.1145/3372297.3423342","Software patching is one of the most significant mechanisms to combat vulnerabilities. To demystify underlying patch details, the techniques of patch differential analysis (a.k.a. patch diffing) are proposed to find differences between patched and unpatched programs' binary code. Considering the sophisticated security patches, patch diffing is expected to not only correctly locate patch changes but also provide sufficient explanation for understanding patch details and the fixed vulnerabilities. Unfortunately, none of the existing patch diffing techniques can meet these requirements. In this study, we first perform a large-scale study on code changes of security patches for better understanding their patterns. We then point out several challenges and design principles for patch diffing. To address the above challenges, we design a dynamic patch diffing technique PatchScope. Our technique is motivated by two key observations: 1) the way that a program processes its input reveals a wealth of semantic information, and 2) most memory corruption patches regulate the handling of malformed inputs via updating the manipulations of input-related data structures. The core of PatchScope is a new semantics-aware program representation, memory object access sequence, which characterizes how a program references data structures to manipulate inputs. The representation can not only deliver succinct patch differences but also offer rich patch context information such as input-patch correlations. Such information can interpret patch differences and further help security analysts understand patch details, locate vulnerability root causes, and even detect buggy patches.","patch diffing, software security, vulnerability analysis","","CCS '20"
"Conference Paper","Sebastian S,Caballero J","Towards Attribution in Mobile Markets: Identifying Developer Account Polymorphism","","2020","","","771–785","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3417281;http://dx.doi.org/10.1145/3372297.3417281","10.1145/3372297.3417281","Malicious developers may succeed at publishing their apps in mobile markets, including the official ones. If reported, the apps will be taken down and the developer accounts possibly be banned. Unfortunately, such take-downs do not prevent the attackers to use other developer accounts to publish variations of their malicious apps. This work presents a novel approach for identifying developer accounts, and other indicators of compromise (IOCs) in mobile markets, that belong to the same operation, i.e., to the same owners. Given a set of seed IOCs, our approach explores app and version metadata to identify new IOCs that belong to the same operation. It outputs an attribution graph, which details the attribution inferences, so that they can be reviewed. We have implemented our approach into Retriever, a tool that supports multiple mobile markets including the official GooglePlay and AppleStore. We have evaluated Retriever on 17 rogueware and adware operations. In 94% of the operations, Retriever discovers at least one previously unknown developer account. Furthermore, Retriever reveals that operations that look dead still have active developer accounts.","developer polymorphism, adware, attribution, mobile markets","","CCS '20"
"Conference Paper","Pantelaios N,Nikiforakis N,Kapravelos A","You've Changed: Detecting Malicious Browser Extensions through Their Update Deltas","","2020","","","477–491","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3423343;http://dx.doi.org/10.1145/3372297.3423343","10.1145/3372297.3423343","In this paper, we conduct the largest to-date analysis of browser extensions, by investigating 922,684 different extension versions collected in the past six years, and using this data to discover malicious versions of extensions. We propose a two-stage system that first identifies malicious extensions based on anomalous extension ratings and locates the code that was added to a benign extension in order to make it malicious. We encode these code deltas according to the APIs that they abuse and search our historical dataset for other similar deltas of extensions which have not yet been flagged, neither by users nor by Chrome's Web Store. We were able to discover 143 malicious extensions belonging to 21 malicious clusters, exhibiting a wide range of abuse, from history stealing and ad injection, to the hijacking of new tabs and search engines. Our results show that our proposed techniques operate in an abuse-agnostic way and can identify malicious extensions that are evading detection.","web, machine learning, malicious, extensions, security, browser","","CCS '20"
"Conference Paper","Jiang Z,Zhang Y,Xu J,Wen Q,Wang Z,Zhang X,Xing X,Yang M,Yang Z","PDiff: Semantic-Based Patch Presence Testing for Downstream Kernels","","2020","","","1149–1163","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3417240;http://dx.doi.org/10.1145/3372297.3417240","10.1145/3372297.3417240","Open-source kernels have been adopted by massive downstream vendors on billions of devices. However, these vendors often omit or delay the adoption of patches released in the mainstream version. Even worse, many vendors are not publicizing the patching progress or even disclosing misleading information. However, patching status is critical for groups (e.g., governments and enterprise users) that are keen to security threats. Such a practice motivates the need for reliable patch presence testing for downstream kernels. Currently, the best means of patch presence testing is to examine the existence of a patch in the target kernel by using the code signature match. However, such an approach cannot address the key challenges in practice. Specifically, downstream vendors widely customize the mainstream code and use non-standard building configurations, which often change the code around the patching sites such that the code signatures are ineffective.In this work, we propose PDiff, a system to perform highly reliable patch presence testing with downstream kernel images. Technically speaking, PDiff generates summaries carrying the semantics related to a target patch. Based on the semantic summaries, PDiff compares the target kernel with its mainstream version before and after the adoption of the patch, preferring the closer reference version to determine the patching status. Unlike previous research on patch presence testing, our approach examines similarity based on the semantics of patches and therefore, provides high tolerance to code-level variations. Our test with 398 kernel images corresponding to 51 patches shows that PDiff can achieve high accuracy with an extremely low rate of false negatives and zero false positives. This significantly outperforms the state-of-the-art tool. More importantly, PDiff demonstrates consistently high effectiveness when code customization and non-standard building configurations occur.","patch presence test, linux kernel security, patch semantics","","CCS '20"
"Conference Paper","López-Morales E,Rubio-Medrano C,Doupé A,Shoshitaishvili Y,Wang R,Bao T,Ahn GJ","HoneyPLC: A Next-Generation Honeypot for Industrial Control Systems","","2020","","","279–291","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security","Virtual Event, USA","2020","9781450370899","","https://doi.org/10.1145/3372297.3423356;http://dx.doi.org/10.1145/3372297.3423356","10.1145/3372297.3423356","Industrial Control Systems (ICS) provide management and control capabilities for mission-critical utilities such as the nuclear, power, water, and transportation grids. Within ICS, Programmable Logic Controllers (PLCs) play a key role as they serve as a convenient bridge between the cyber and the physical worlds, e.g., controlling centrifuge machines in nuclear power plants. The critical roles that ICS and PLCs play have made them the target of sophisticated cyberattacks that are designed to disrupt their operation, which creates both social unrest and financial losses. In this context, honeypots have been shown to be highly valuable tools for collecting real data, e.g., malware payload, to better understand the many different methods and strategies that attackers use. However, existing state-of-the-art honeypots for PLCs lack sophisticated service simulations that are required to obtain valuable data. Worse, they cannot adapt while ICS malware keeps evolving, and attack patterns become more sophisticated. To overcome these shortcomings, we present HoneyPLC, a high-interaction, extensible, and malware collecting honeypot supporting a broad spectrum of PLCs models and vendors. Results from our experiments show that HoneyPLC exhibits a high level of camouflaging: it is identified as real devices by multiple widely used reconnaissance tools, including Nmap, Shodan's Honeyscore, the Siemens Step7 Manager, PLCinject, and PLCScan, with a high level of confidence. We deployed HoneyPLC on Amazon AWS and recorded a large amount of interesting interactions over the Internet, showing not only that attackers are in fact targeting ICS systems, but also that HoneyPLC can effectively engage and deceive them while collecting data samples for future analysis.","honeypot, industrial control systems, programmable logic controllers","","CCS '20"
"Conference Paper","Silva LP,Vilain P","LCCSS: A Similarity Metric for Identifying Similar Test Code","","2020","","","91–100","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse","Natal, Brazil","2020","9781450387545","","https://doi.org/10.1145/3425269.3425283;http://dx.doi.org/10.1145/3425269.3425283","10.1145/3425269.3425283","Test code maintainability is a common concern in software testing. In order to achieve good maintainability, test methods should be clearly structured, well named, small in size, and, mainly, test code duplication should be avoided. Several strategies exist to avoid test code duplication, such as implicit setup and delegated setup. However, prior to applying these strategies, first it is necessary to identify the duplicate code, which can be a time-consuming task. To address this problem, we automate the identification of duplicate test code through the application of code similarity metrics. We propose a novel similarity metric, called Longest Common Contiguous Start Sub-Sequence (LCCSS), to identify refactoring candidates. LCCSS is a metric used to measure similarity between pairs of tests. The most similar pairs are reported as strong candidates to be refactored through the implicit setup strategy. We also develop a framework, called Róża, that can use different similarity metrics to identify test code duplication. An experiment shows that LCCSS and Simian, a clone detection tool, have both identified pairs of tests to be refactored through the implicit setup strategy with maximum precision in all the eleven standard recall levels. But, unlike Simian, LCCSS does not need to be calibrated for each project.","implicit setup, similarity, measure, metric, refactoring, testing","","SBCARS '20"
"Conference Paper","Talbot M,Geldreich K,Sommer J,Hubwieser P","Re-Use of Programming Patterns or Problem Solving? Representation of Scratch Programs by TGraphs to Support Static Code Analysis","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th Workshop on Primary and Secondary Computing Education","Virtual Event, Germany","2020","9781450387590","","https://doi.org/10.1145/3421590.3421604;http://dx.doi.org/10.1145/3421590.3421604","10.1145/3421590.3421604","Novice programmers seem to learn basic programming skills amazingly fast by using visual programming environments like Scratch or Snap. Yet at a second glance, in many cases, the students' programming projects make use of pre-learned solution patterns like collision detection. Aiming to investigate how far such pre-learned patterns are used and adapted, we have to analyze the program structure of a substantially large number of Scratch projects, e.g. from the Scratch repository, in a very detailed way. To automate the static code analysis of these projects, we developed a scheme to transform Scratch projects into a common graph format (TGraph), which was used up to now to analyze programs in Java and Haskell as well as UML diagrams and mathematical solutions. In a second step, this representation enabled us to apply a SQL-like query language for graphs (GReQL) to detect programming patterns in students' Scratch projects. This paper describes the design of our TGraph scheme for Scratch as well as how to query patterns in Scratch code using GReQL, in order to stimulate the use of this methodology by other researchers. As a feasibility study, we report its application on the outcomes of one of our Scratch courses attended by 143 children aged 8-12 years. The study showed that with the presented methodology any code structure can be detected in Scratch projects. To check the validity of the methodology, the programs were additionally checked manually for the occurrence of two patterns - the results were consistent.","computer science education, programming, assessment, patterns, code analysis, scratch","","WiPSCE '20"
"Conference Paper","Michelon GK","Evolving System Families in Space and Time","","2020","","","104–111","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B","Montreal, QC, Canada","2020","9781450375702","","https://doi.org/10.1145/3382026.3431252;http://dx.doi.org/10.1145/3382026.3431252","10.1145/3382026.3431252","Managing the evolution of system families in space and time, i.e., system variants and their revisions is still an open challenge. The software product line (SPL) approach can support the management of product variants in space by reusing a common set of features. However, feature changes over time are often necessary due to adaptations and/or bug fixes, leading to different product versions. Such changes are commonly tracked in version control systems (VCSs). However, VCSs only deal with the change history of source code, and, even though their branching mechanisms allow to develop features in isolation, VCS does not allow propagating changes across variants. Variation control systems have been developed to support more fine-grained management of variants and to allow tracking of changes at the level of files or features. However, these systems are also limited regarding the types and granularity of artifacts. Also, they are cognitively very demanding with increasing numbers of revisions and variants. Furthermore, propagating specific changes over variants of a system is still a complex task that also depends on the variability-aware change impacts. Based on these existing limitations, the goal of this doctoral work is to investigate and define a flexible and unified approach to allow an easy and scalable evolution of SPLs in space and time. The expected contributions will aid the management of SPL products and support engineers to reason about the potential impact of changes during SPL evolution. To evaluate the approach, we plan to conduct case studies with real-world SPLs.","feature-oriented software development, software evolution, version control systems, software product lines","","SPLC '20"
"Conference Paper","Rost W","Mining of DSLs and Generator Templates from Reference Applications","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings","Virtual Event, Canada","2020","9781450381352","","https://doi.org/10.1145/3417990.3419492;http://dx.doi.org/10.1145/3417990.3419492","10.1145/3417990.3419492","Domain-Specific Languages (DSLs) found application in different domains. The development of Model-Driven Development (MDD) components is facilitated by a wealth of frameworks like EMF, Xtext, and Xtend. However, the development of the necessary IDE components still can take up to several weeks or even months until it can be used in a production environment. The first step during the development of such an MDD infrastructure is to analyse a set of reference applications to deduce the DSL used by the domain experts and the templates used in the generator. The analysis requires technical expertise and is usually performed by MDD infrastructure developers, who have to adhere to a close communication with domain experts and are exposed to high cognitive load and time-consuming tasks.The objective of this PhD project is to reduce the initial effort during the creation of new MDD infrastructure facilities for either a new domain or newly discovered platforms within a known domain. This should be made possible by the (semi-)automatic analysis of multiple codebases using Code Clone Detection (CCD) tools in a defined process flow. Code clones represent schematically redundant and generic code fragments which were found in the provided codebase. In the process, the key steps include (i) choosing appropriate reference applications (ii) distinguishing the codebase by clustering the files, (iii) reviewing the quality of the clusters, (iv) analysing the cluster by tailored CCD, and (v) transforming of the code clones, depending on the code clone type, to extract a DSL and the corresponding generator templates.","information extraction, MDD component creation, model-driven software engineering, clustering and classification, code clone detection","","MODELS '20"
"Conference Paper","Nikoo MS,Babur Ö,van den Brand M","A Survey on Service Composition Languages","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings","Virtual Event, Canada","2020","9781450381352","","https://doi.org/10.1145/3417990.3421402;http://dx.doi.org/10.1145/3417990.3421402","10.1145/3417990.3421402","In recent years, service-oriented architecture (SOA) has been adopted by industry in developing enterprise systems. Web service composition has been one of the challenging topics in SOA. Numerous approaches have been proposed to tackle this problem. In industry big companies such as Amazon, Netflix, and Uber have developed their own web service composition languages and tools. In academia, on the other hand, there have also been attempts to resolve some of the complexities in web service composition. In this survey we identify and evaluate current prominent service composition languages, and discuss our key findings. After a scan of dozens of service composition systems, 14 systems that used a language-based approach were included in this study. We believe that our findings will help people from industry and academia to learn about some of the major active composition languages and get an overall idea about their commonalities and differences.","service oriented architecture, domain-specific language, service composition, choreography, orchestration","","MODELS '20"
"Conference Paper","Lano K,Fang S,Kolahdouz-Rahimi S","TL: An Abstract Specification Language for Bidirectional Transformations","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings","Virtual Event, Canada","2020","9781450381352","","https://doi.org/10.1145/3417990.3419223;http://dx.doi.org/10.1145/3417990.3419223","10.1145/3417990.3419223","Model transformation verification has been hindered by the complex language mechanisms and semantics of mainstream transformation languages. In this paper we describe an abstract formalism, TL, for the definition of bidirectional and unidirectional transformations in a purely declarative manner. In contrast to model transformation languages such as ATL or QVT-R, there is no implicit or explicit sequencing of rules in TL specifications. Reasoning about TL specifications is therefore facilitated. We show that semantics-preserving translations can be defined from TL to subsets of the mainstream transformation languages.","transformation correctness, transformation specification, QVT-R, bidirectional transformations","","MODELS '20"
"Conference Paper","Nair A,Roy A,Meinke K","FuncGNN: A Graph Neural Network Approach to Program Similarity","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","Bari, Italy","2020","9781450375801","","https://doi.org/10.1145/3382494.3410675;http://dx.doi.org/10.1145/3382494.3410675","10.1145/3382494.3410675","Background: Program similarity is a fundamental concept, central to the solution of software engineering tasks such as software plagiarism, clone identification, code refactoring and code search. Accurate similarity estimation between programs requires an in-depth understanding of their structure, semantics and flow. A control flow graph (CFG), is a graphical representation of a program which captures its logical control flow and hence its semantics. A common approach is to estimate program similarity by analysing CFGs using graph similarity measures, e.g. graph edit distance (GED). However, graph edit distance is an NP-hard problem and computationally expensive, making the application of graph similarity techniques to complex software programs impractical. Aim: This study intends to examine the effectiveness of graph neural networks to estimate program similarity, by analysing the associated control flow graphs. Method: We introduce funcGNN1, which is a graph neural network trained on labeled CFG pairs to predict the GED between unseen program pairs by utilizing an effective embedding vector. To our knowledge, this is the first time graph neural networks have been applied on labeled CFGs for estimating the similarity between highlevel language programs. Results: We demonstrate the effectiveness of funcGNN to estimate the GED between programs and our experimental analysis demonstrates how it achieves a lower error rate (1.94 x10-3), with faster (23 times faster than the quickest traditional GED approximation method) and better scalability compared with state of the art methods. Conclusion: funcGNN posses the inductive learning ability to infer program structure and generalise to unseen programs. The graph embedding of a program proposed by our methodology could be applied to several related software engineering problems (such as code plagiarism and clone identification) thus opening multiple research directions.","Control Flow Graph, Software Engineering, Attention Mechanism, Graph Similarity, Graph Neural Network, ProgramSimilarity, Machine Learning, Graph Embedding, Graph Edit Distance","","ESEM '20"
"Journal Article","Marijan D,Sen S","Good Practices in Aligning Software Engineering Research and Industry Practice","SIGSOFT Softw. Eng. Notes","2020","44","3","65–67","Association for Computing Machinery","New York, NY, USA","","","2020-10","","0163-5948","https://doi.org/10.1145/3356773.3356812;http://dx.doi.org/10.1145/3356773.3356812","10.1145/3356773.3356812","There is a long-standing challenge to narrow the gap between software engineering research and industry practice, to align their interests and realize true synergies between the two communities. Some difficulties to this challenge include mismatched agendas, priorities and expectations from the research collaboration on both sides. To overcome these difficulties, an initial step is to gain a clearer understanding of collaboration challenges from both perspectives. With this goal in mind, we organized the 5th International Workshop on Software Engineering Research and Industrial Practice, collocated with the International Conference on Software Engineering 2018. The workshop featured two keynote talks, one from industry and one from academia, followed by paper presentations and a round-table discussion session. Here we summarize experiences shared by the keynotes from industry and academia, along with findings from paper presentations and overall discussions by workshop participants on the ways of aligning software engineering research and industry practice.","technology transfer, innovation, software engineering, industry-academia collaboration, research collaboration","",""
"Journal Article","Siegmund J,Begel A,Peitek N","Summary of the Sixth Edition of the International Workshop on Eye Movements in Programming","SIGSOFT Softw. Eng. Notes","2020","44","3","54–55","Association for Computing Machinery","New York, NY, USA","","","2020-10","","0163-5948","https://doi.org/10.1145/3356773.3356809;http://dx.doi.org/10.1145/3356773.3356809","10.1145/3356773.3356809","The study of eye gaze data has great potential for research in computer programming, computing education, and software engineering practice. To highlight its role for the software engineering community, the Sixth Edition of the International Workshop on Eye Movements in Programming (EMIP 2019) was co-located with the 41st International Conference on Software Engineering. The goal of the workshop was to advance the methodology of using eye tracking for programming, both theoretically and in applications.","programming, computing education, software engineering, eye tracking","",""
"Conference Paper","Soares E,Ribeiro M,Amaral G,Gheyi R,Fernandes L,Garcia A,Fonseca B,Santos A","Refactoring Test Smells: A Perspective from Open-Source Developers","","2020","","","50–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing","Natal, Brazil","2020","9781450387552","","https://doi.org/10.1145/3425174.3425212;http://dx.doi.org/10.1145/3425174.3425212","10.1145/3425174.3425212","Test smells are symptoms in the test code that indicate possible design or implementation problems. Their presence, along with their harmfulness, has already been demonstrated by previous researches. However, we do not know to what extent developers acknowledge the presence of test smells and how to refactor existing code to eliminate them in practice. This study aims to assess open-source developers' awareness about the existence of test smells and their refactoring strategies. We conducted a mixed-method study with two parts: (i) a survey with 73 experienced open-source developers to assess their preference and motivation to choose between 10 different smelly test code samples, found in 272 open-source projects, and their refactored versions; and (ii) the submission of 50 pull requests to assess developers' acceptance of the proposed refactorings. As a result, most surveyed developers preferred the refactored proposal for 78% of the investigated test smells, and the pull requests had an average acceptance of 75% among respondents. Additionally, we were able to provide empiric validation for literature-proposed refactoring strategies. This study demonstrates that although not always using the academic terminology, developers acknowledge both the negative impact of test smells presence and most of the literature's proposals for their removal.","","","SAST 20"
"Conference Paper","Zhang T,Lowmanstone L,Wang X,Glassman EL","Interactive Program Synthesis by Augmented Examples","","2020","","","627–648","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology","Virtual Event, USA","2020","9781450375146","","https://doi.org/10.1145/3379337.3415900;http://dx.doi.org/10.1145/3379337.3415900","10.1145/3379337.3415900","Programming-by-example (PBE) has become an increasingly popular component in software development tools, human-robot interaction, and end-user programming. A long-standing challenge in PBE is the inherent ambiguity in user-provided examples. This paper presents an interaction model to disambiguate user intent and reduce the cognitive load of understanding and validating synthesized programs. Our model provides two types of augmentations to user-given examples: 1) semantic augmentation where a user can specify how different aspects of an example should be treated by a synthesizer via light-weight annotations, and 2) data augmentation where the synthesizer generates additional examples to help the user understand and validate synthesized programs. We implement and demonstrate this interaction model in the domain of regular expressions, which is a popular mechanism for text processing and data wrangling and is often considered hard to master even for experienced programmers. A within-subjects user study with twelve participants shows that, compared with only inspecting and annotating synthesized programs, interacting with augmented examples significantly increases the success rate of finishing a programming task with less time and increases users? confidence of synthesized programs.","example augmentation, program synthesis, disambiguation","","UIST '20"
"Conference Paper","Makedonski P,Grabowski J","Facilitating the Co-Evolution of Semantic Descriptions in Standards and Models","","2020","","","75–84","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th System Analysis and Modelling Conference","Virtual Event, Canada","2020","9781450381406","","https://doi.org/10.1145/3419804.3421449;http://dx.doi.org/10.1145/3419804.3421449","10.1145/3419804.3421449","One of the main activities of the Interfaces and Architecture (IFA) working group within the Network Function Virtualisation (NFV) Industry Specification Group (ISG) at the European Telecommunications Standards Institute (ETSI) is the specification of requirements and information modelling of NFV descriptors and artefacts. The information elements are spread across 11 different NFV-IFA specifications covering the scope of functionality of the individual functional blocks and reference points.Part of the work of the NFV-IFA working group is dedicated to producing a unified Information Model (IM) for NFV providing a consolidated view based on the different specifications. The IM has been helpful for the identification of gaps and inconsistencies in the specifications and in implementations of the specifications.With the growing size of the IM and the corresponding specifications and the rapid release cycles resulting from the intense work within the ISG, the IM plays an important role in helping to ensure that the specifications are consistent.Previous work outlined the foundations for facilitating the co-evolution of models and standards based on the NFV IM and models extracted from the related standardised specifications. It defined a methodology for consistency checking and alignment of the IM and the related specifications focusing on structural aspects.This article refines the methodology to address semantic descriptions as well as further aspects related to the co-evolution of standards and models. We also report on our experiences with the application of a prototypical implementation of the methodology during the continued alignment and maintenance of the information model and the related standardised specifications with the NFV-IFA working group and how the feedback from the working group provided insights on how to refine the methodology even further.","validation, standards, duplication, maintenance, model evolution, semantics, traceability","","SAM '20"
"Conference Paper","Jongeling R,Cicchetti A,Ciccozzi F,Carlson J","Co-Evolution of Simulink Models in a Model-Based Product Line","","2020","","","263–273","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems","Virtual Event, Canada","2020","9781450370196","","https://doi.org/10.1145/3365438.3410989;http://dx.doi.org/10.1145/3365438.3410989","10.1145/3365438.3410989","Co-evolution of metamodels and conforming models is a known challenge in model-driven engineering. A variation of co-evolution occurs in model-based software product line engineering, where it is needed to efficiently co-evolve various products together with the single common platform from which they are derived. In this paper, we aim to alleviate manual efforts during this co-evolution process in an industrial setting where Simulink models are partially reused across various products. We propose and implement an approach providing support for the co-evolution of reusable model fragments. A demonstration on a realistic example model shows that our approach yields a correct co-evolution result and is feasible in practice, although practical application challenges remain. Furthermore, we discuss insights from applying the approach within the studied industrial setting.","model-driven engineering, clone management, change propagation, software product line engineering, co-evolution","","MODELS '20"
"Conference Paper","López JA,Cuadrado JS","MAR: A Structure-Based Search Engine for Models","","2020","","","57–67","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems","Virtual Event, Canada","2020","9781450370196","","https://doi.org/10.1145/3365438.3410947;http://dx.doi.org/10.1145/3365438.3410947","10.1145/3365438.3410947","The availability of shared software models provides opportunities for reusing, adapting and learning from them. Public models are typically stored in a variety of locations, including model repositories, regular source code repositories, web pages, etc. To profit from them developers need effective search mechanisms to locate the models relevant for their tasks. However, to date, there has been little success in creating a generic and efficient search engine specially tailored to the modelling domain.In this paper we present MAR, a search engine for models. MAR is generic in the sense that it can index any type of model if its meta-model is known. MAR uses a query-by-example approach, that is, it uses example models as queries. The search takes the model structure into account using the notion of bag of paths, which encodes the structure of a model using paths between model elements and is a representation amenable for indexing. MAR is built over HBase using a specific design to deal with large repositories. Our benchmarks show that the engine is efficient and has fast response times in most cases. We have also evaluated the precision of the search engine by creating model mutants which simulate user queries. A REST API is available to perform queries and an Eclipse plug-in allows end users to connect to the search engine from model editors. We have currently indexed more than 50.000 models of different kinds, including Ecore meta-models, BPMN diagrams and UML models. MAR is available at http://mar-search.org.","meta-model classification, search engine, model repositories","","MODELS '20"
"Conference Paper","Luo J,Lu F,Wang T","A Multi-Dimensional Assessment Model and Its Application in E-Learning Courses of Computer Science","","2020","","","187–193","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st Annual Conference on Information Technology Education","Virtual Event, USA","2020","9781450370455","","https://doi.org/10.1145/3368308.3415388;http://dx.doi.org/10.1145/3368308.3415388","10.1145/3368308.3415388","Computer science is a practical discipline. It is always a great challenge to evaluate students' computer practice using computer-aided means for large scale students. We always need to address problems such as suspected plagiarism and deviation of the overall difficulty factor. In this paper, a multi-dimensional assessment model is designed for CS courses based on the detailed practice processing data in an E-learning system. The model comprehensively evaluates the students' learning process and results in three aspects of correctness, originality, and quality detection. Besides, the teacher can easily participate in the assessment according to their needs. The correctness is an essential requirement, and the originality is based on the clustering results of students' behaviors after clone detection to curb homework plagiarism. SonarQube is used to detect code quality and put forward higher requirements for codes. Manual participation intelligence has improved the flexibility and applicability of the model to a certain extent. We applied this model on the EduCoder online education platform and carried out a comprehensive analysis of 485 students in the Parallel Programming Principles and Practice Class of Huazhong University of Science and Technology. Experiment results confirm the distinction, rationality, and fairness of the model in assessing student performance. It not only gives students a credible, comprehensive score in large-scale online practical programming courses but also gives teachers and students corresponding suggestions based on the evaluation results. Furthermore, the model can be extended to other online education platforms.","student behavior analysis, multi-dimensional intelligent scoring model, massive online open practice, student assessment","","SIGITE '20"
"Conference Paper","Hendler D,Kels S,Rubin A","AMSI-Based Detection of Malicious PowerShell Code Using Contextual Embeddings","","2020","","","679–693","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM Asia Conference on Computer and Communications Security","Taipei, Taiwan","2020","9781450367509","","https://doi.org/10.1145/3320269.3384742;http://dx.doi.org/10.1145/3320269.3384742","10.1145/3320269.3384742","PowerShell is a command-line shell, supporting a scripting language. It is widely used in organizations for configuration management and task automation but is also increasingly used for launching cyber attacks against organizations, mainly because it is pre-installed on Windows machines and exposes strong functionality that may be leveraged by attackers. This makes the problem of detecting malicious PowerShell code both urgent and challenging. Microsoft's Antimalware Scan Interface (AMSI), built into Windows 10, allows defending systems to scan all the code passed to scripting engines such as PowerShell prior to its execution. In this work, we conduct the first study of malicious PowerShell code detection using the information made available by AMSI. We present several novel deep-learning based detectors of malicious PowerShell code that employ pretrained contextual embeddings of words from the PowerShell ""language"". A contextual word embedding is able to project semantically-similar words to proximate vectors in the embedding space. A known problem in the cybersecurity domain is that labeled data is relatively scarce, in comparison with unlabeled data, making it difficult to devise effective supervised detection of malicious activity of many types. This is also the case with PowerShell code. Our work shows that this problem can be mitigated by learning a pretrained contextual embedding based on unlabeled data. We trained and evaluated our models using real-world data, collected using AMSI. The contextual embedding was learnt using a large corpus of unlabeled PowerShell scripts and modules collected from public repositories. Our performance analysis establishes that the use of unlabeled data for the embedding significantly improved the performance of our detectors. Our best-performing model uses an architecture that enables the processing of textual signals from both the character and token levels and obtains a true-positive rate of nearly 90% while maintaining a low false-positive rate of less than 0.1%.","powershell, contextual embedding, neural networks, cybersecurity","","ASIA CCS '20"
"Conference Paper","Gussoni A,Di Federico A,Fezzardi P,Agosta G","A Comb for Decompiled C Code","","2020","","","637–651","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM Asia Conference on Computer and Communications Security","Taipei, Taiwan","2020","9781450367509","","https://doi.org/10.1145/3320269.3384766;http://dx.doi.org/10.1145/3320269.3384766","10.1145/3320269.3384766","Decompilers are fundamental tools to perform security assessments of third-party software. The quality of decompiled code can be a game changer in order to reduce the time and effort required for analysis. This paper proposes a novel approach to restructure the control flow graph recovered from binary programs in a semantics-preserving fashion. The algorithm is designed from the ground up with the goal of producing C code that is both goto-free and drastically reducing the mental load required for an analyst to understand it. As a result, the code generated with this technique is well-structured, idiomatic, readable, easy to understand and fully exploits the expressiveness of C language. The algorithm has been implemented on top of the revng static binary analysis framework. The resulting decompiler, revngc, is compared on real-world binaries with state-of-the-art commercial and open source tools. The results show that our decompilation process introduces between 40% and 50% less extra cyclomatic complexity.","goto, reverse engineering, decompilation, control flow restructuring","","ASIA CCS '20"
"Journal Article","Margulieux LE,Morrison BB,Franke B,Ramilison H","Effect of Implementing Subgoals in Code.Org's Intro to Programming Unit in Computer Science Principles","ACM Trans. Comput. Educ.","2020","20","4","","Association for Computing Machinery","New York, NY, USA","","","2020-10","","","https://doi.org/10.1145/3415594;http://dx.doi.org/10.1145/3415594","10.1145/3415594","The subgoal learning framework has improved performance for novice programmers in higher education, but it has only started to be applied and studied in K-12 (primary/secondary). Programming education in K-12 is growing, and many international initiatives are attempting to increase participation, including curricular initiatives like Computer Science Principles and non-profit organizations like Code.org. Given that subgoal learning is designed to help students with no prior knowledge, we designed and implemented subgoals in the introduction to programming unit in Code.org's Computer Science Principles course. The redesigned unit includes subgoal-oriented instruction and subgoal-themed pre-written comments that students could add to their programming activities. To evaluate efficacy, we compared behaviors and performance of students who received the redesigned subgoal unit to those receiving the original unit. We found that students who learned with subgoals performed better on problem-solving questions but not knowledge-based questions and wrote more in open-ended response questions, including a practice Performance Task for the AP exam. Moreover, at least one-third of subgoal students continued to use the subgoal comments after the subgoal-oriented instruction had been faded, suggesting that they found them useful. Survey data from the teachers suggested that students who struggled with the concepts found the subgoals most useful. Implications for future designs are discussed.","Code.org, Subgoal learning, Computer Science Principles, instructional design, K-12","",""
"Conference Paper","Zhang J,Wang X,Zhang H,Sun H,Liu X","Retrieval-Based Neural Source Code Summarization","","2020","","","1385–1397","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380383;http://dx.doi.org/10.1145/3377811.3380383","10.1145/3377811.3380383","Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code. Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets. Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks. The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones. In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set. Our approach can take advantages of both neural and retrieval-based techniques. Specifically, we first train an attentional encoder-decoder model based on the code snippets and the summaries in the training set; Second, given one input code snippet for testing, we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics, respectively; Third, we encode the input and two retrieved code snippets, and predict the summary by fusing them during decoding. We conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state-of-the-art methods.","source code summarization, information retrieval, deep neural network","","ICSE '20"
"Conference Paper","Mathew G,Parnin C,Stolee KT","SLACC: Simion-Based Language Agnostic Code Clones","","2020","","","210–221","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380407;http://dx.doi.org/10.1145/3377811.3380407","10.1145/3377811.3380407","Successful cross-language clone detection could enable researchers and developers to create robust language migration tools, facilitate learning additional programming languages once one is mastered, and promote reuse of code snippets over a broader codebase. However, identifying cross-language clones presents special challenges to the clone detection problem. A lack of common underlying representation between arbitrary languages means detecting clones requires one of the following solutions: 1) a static analysis framework replicated across each targeted language with annotations matching language features across all languages, or 2) a dynamic analysis framework that detects clones based on runtime behavior.In this work, we demonstrate the feasibility of the latter solution, a dynamic analysis approach called SLACC for cross-language clone detection. Like prior clone detection techniques, we use input/output behavior to match clones, though we overcome limitations of prior work by amplifying the number of inputs and covering more data types; and as a result, achieve better clusters than prior attempts. Since clusters are generated based on input/output behavior, SLACC supports cross-language clone detection. As an added challenge, we target a static typed language, Java, and a dynamic typed language, Python. Compared to HitoshiIO, a recent clone detection tool for Java, SLACC retrieves 6 times as many clusters and has higher precision (86.7% vs. 30.7%).This is the first work to perform clone detection for dynamic typed languages (precision = 87.3%) and the first to perform clone detection across languages that lack a common underlying representation (precision = 94.1%). It provides a first step towards the larger goal of scalable language migration tools.","semantic code clone detection, cross-language analysis","","ICSE '20"
"Conference Paper","Zhai J,Xu X,Shi Y,Tao G,Pan M,Ma S,Xu L,Zhang W,Tan L,Zhang X","CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis","","2020","","","1359–1371","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380427;http://dx.doi.org/10.1145/3377811.3380427","10.1145/3377811.3380427","Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, specification inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on different aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more effectively utilize comments, a fine-grained and elaborated taxonomy of comments and a reliable classifier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been confirmed and fixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the effectiveness of our approach. Our user study confirms propagated comments align well with existing comments in terms of quality.","","","ICSE '20"
"Conference Paper","Aghajani E,Nagy C,Linares-Vásquez M,Moreno L,Bavota G,Lanza M,Shepherd DC","Software Documentation: The Practitioners' Perspective","","2020","","","590–601","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380405;http://dx.doi.org/10.1145/3377811.3380405","10.1145/3377811.3380405","In theory, (good) documentation is an invaluable asset to any software project, as it helps stakeholders to use, understand, maintain, and evolve a system. In practice, however, documentation is generally affected by numerous shortcomings and issues, such as insufficient and inadequate content and obsolete, ambiguous information. To counter this, researchers are investigating the development of advanced recommender systems that automatically suggest high-quality documentation, useful for a given task. A crucial first step is to understand what quality means for practitioners and what information is actually needed for specific tasks.We present two surveys performed with 146 practitioners to investigate (i) the documentation issues they perceive as more relevant together with solutions they apply when these issues arise; and (ii) the types of documentation considered as important in different tasks. Our findings can help researchers in designing the next generation of documentation recommender systems.","documentation, empirical study","","ICSE '20"
"Conference Paper","Halepmollasi R","A Composed Technical Debt Identification Methodology to Predict Software Vulnerabilities","","2020","","","186–189","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings","Seoul, South Korea","2020","9781450371223","","https://doi.org/10.1145/3377812.3381396;http://dx.doi.org/10.1145/3377812.3381396","10.1145/3377812.3381396","Technical debt (TD), its impact on development and its consequences such as defects and vulnerabilities, are of common interest and great importance to software researchers and practitioners. Although there exist many studies investigating TD, the majority of them focuses on identifying and detecting TD from a single stage of development. There are also studies that analyze vulnerabilities focusing on some phases of the life cycle. Moreover, several approaches have investigated the relationship between TD and vulnerabilities, however, the generalizability and validity of findings are limited due to small dataset. In this study, we aim to identify TD through multiple phases of development, and to automatically measure it through data and text mining techniques to form a comprehensive feature model. We plan to utilize neural network based classifiers that will incorporate evolutionary changes on TD measures into predicting vulnerabilities. Our approach will be empirically assessed on open source and industrial projects.","machine learning, software security, feature engineering, technical debt","","ICSE '20"
"Conference Paper","Liu K,Wang S,Koyuncu A,Kim K,Bissyandé TF,Kim D,Wu P,Klein J,Mao X,Traon YL","On the Efficiency of Test Suite Based Program Repair: A Systematic Assessment of 16 Automated Repair Systems for Java Programs","","2020","","","615–627","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380338;http://dx.doi.org/10.1145/3377811.3380338","10.1145/3377811.3380338","Test-based automated program repair has been a prolific field of research in software engineering in the last decade. Many approaches have indeed been proposed, which leverage test suites as a weak, but affordable, approximation to program specifications. Although the literature regularly sets new records on the number of benchmark bugs that can be fixed, several studies increasingly raise concerns about the limitations and biases of state-of-the-art approaches. For example, the correctness of generated patches has been questioned in a number of studies, while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results. Nevertheless, there is little work addressing the efficiency of patch generation, with regard to the practicality of program repair. In this paper, we fill this gap in the literature, by providing an extensive review on the efficiency of test suite based program repair. Our objective is to assess the number of generated patch candidates, since this information is correlated to (1) the strategy to traverse the search space efficiently in order to select sensical repair attempts, (2) the strategy to minimize the test effort for identifying a plausible patch, (3) as well as the strategy to prioritize the generation of a correct patch. To that end, we perform a large-scale empirical study on the efficiency, in terms of quantity of generated patch candidates of the 16 open-source repair tools for Java programs. The experiments are carefully conducted under the same fault localization configurations to limit biases. Eventually, among other findings, we note that: (1) many irrelevant patch candidates are generated by changing wrong code locations; (2) however, if the search space is carefully triaged, fault localization noise has little impact on patch generation efficiency; (3) yet, current template-based repair systems, which are known to be most effective in fixing a large number of bugs, are actually least efficient as they tend to generate majoritarily irrelevant patch candidates.","efficiency, program repair, patch generation, empirical assessment","","ICSE '20"
"Conference Paper","Yandrapally R,Stocco A,Mesbah A","Near-Duplicate Detection in Web App Model Inference","","2020","","","186–197","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380416;http://dx.doi.org/10.1145/3377811.3380416","10.1145/3377811.3380416","Automated web testing techniques infer models from a given web app, which are used for test generation. From a testing viewpoint, such an inferred model should contain the minimal set of states that are distinct, yet, adequately cover the app's main functionalities. In practice, models inferred automatically are affected by near-duplicates, i.e., replicas of the same functional webpage differing only by small insignificant changes. We present the first study of near-duplicate detection algorithms used in within app model inference. We first characterize functional near-duplicates by classifying a random sample of state-pairs, from 493k pairs of webpages obtained from over 6,000 websites, into three categories, namely clone, near-duplicate, and distinct. We systematically compute thresholds that define the boundaries of these categories for each detection technique. We then use these thresholds to evaluate 10 near-duplicate detection techniques from three different domains, namely, information retrieval, web testing, and computer vision on nine open-source web apps. Our study highlights the challenges posed in automatically inferring a model for any given web app. Our findings show that even with the best thresholds, no algorithm is able to accurately detect all functional near-duplicates within apps, without sacrificing coverage.","near-duplicate detection, model-based testing, reverse engineering","","ICSE '20"
"Conference Paper","Liu B,Meng G,Zou W,Gong Q,Li F,Lin M,Sun D,Huo W,Zhang C","A Large-Scale Empirical Study on Vulnerability Distribution within Projects and the Lessons Learned","","2020","","","1547–1559","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380923;http://dx.doi.org/10.1145/3377811.3380923","10.1145/3377811.3380923","The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.","vulnerability distribution, empirical study","","ICSE '20"
"Conference Paper","Lamothe M,Shang W","When APIs Are Intentionally Bypassed: An Exploratory Study of API Workarounds","","2020","","","912–924","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380433;http://dx.doi.org/10.1145/3377811.3380433","10.1145/3377811.3380433","Application programming interfaces (APIs) have become ubiquitous in software development. However, external APIs are not guaranteed to contain every desirable feature, nor are they immune to software defects. Therefore, API users will sometimes be faced with situations where a current API does not satisfy all of their requirements, but migrating to another API is costly. In these cases, due to the lack of communication channels between API developers and users, API users may intentionally bypass an existing API after inquiring into workarounds for their API problems with online communities. This mechanism takes the API developer out of the conversation, potentially leaving API defects unreported and desirable API features undiscovered. In this paper we explore API workaround inquiries from API users on Stack Overflow. We uncover general reasons why API users inquire about API workarounds, and general solutions to API workaround requests. Furthermore, using workaround implementations in Stack Overflow answers, we develop three API workaround implementation patterns. We identify instances of these patterns in real-life open source projects and determine their value for API developers from their responses to feature requests based on the identified API workarounds.","","","ICSE '20"
"Conference Paper","Chowdhury SA,Shrestha SL,Johnson TT,Csallner C","SLEMI: Equivalence modulo Input (EMI) Based Mutation of CPS Models for Finding Compiler Bugs in Simulink","","2020","","","335–346","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380381;http://dx.doi.org/10.1145/3377811.3380381","10.1145/3377811.3380381","Finding bugs in commercial cyber-physical system development tools (or ""model-based design"" tools) such as MathWorks's Simulink is important in practice, as these tools are widely used to generate embedded code that gets deployed in safety-critical applications such as cars and planes. Equivalence Modulo Input (EMI) based mutation is a new twist on differential testing that promises lower use of computational resources and has already been successful at finding bugs in compilers for procedural languages. To provide EMI-based mutation for differential testing of cyber-physical system (CPS) development tools, this paper develops several novel mutation techniques. These techniques deal with CPS language features that are not found in procedural languages, such as an explicit notion of execution time and zombie code, which combines properties of live and dead procedural code. In our experiments the most closely related work (SLforge) found two bugs in the Simulink tool. In comparison, SLEMI found a super-set of issues, including 9 confirmed as bugs by MathWorks Support.","equivalence modulo input, model mutation, cyber-physical systems, differential testing, simulink","","ICSE '20"
"Conference Paper","Hough K,Welearegai G,Hammer C,Bell J","Revealing Injection Vulnerabilities by Leveraging Existing Tests","","2020","","","284–296","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380326;http://dx.doi.org/10.1145/3377811.3380326","10.1145/3377811.3380326","Code injection attacks, like the one used in the high-profile 2017 Equifax breach, have become increasingly common, now ranking #1 on OWASP's list of critical web application vulnerabilities. Static analyses for detecting these vulnerabilities can overwhelm developers with false positive reports. Meanwhile, most dynamic analyses rely on detecting vulnerabilities as they occur in the field, which can introduce a high performance overhead in production code. This paper describes a new approach for detecting injection vulnerabilities in applications by harnessing the combined power of human developers' test suites and automated dynamic analysis. Our new approach, Rivulet, monitors the execution of developer-written functional tests in order to detect information flows that may be vulnerable to attack. Then, Rivulet uses a white-box test generation technique to repurpose those functional tests to check if any vulnerable flow could be exploited. When applied to the version of Apache Struts exploited in the 2017 Equifax attack, Rivulet quickly identifies the vulnerability, leveraging only the tests that existed in Struts at that time. We compared Rivulet to the state-of-the-art static vulnerability detector Julia on benchmarks, finding that Rivulet outperformed Julia in both false positives and false negatives. We also used Rivulet to detect new vulnerabilities.","vulnerability testing, taint tracking, injection attacks","","ICSE '20"
"Conference Paper","Chen S,Fan L,Meng G,Su T,Xue M,Xue Y,Liu Y,Xu L","An Empirical Assessment of Security Risks of Global Android Banking Apps","","2020","","","1310–1322","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380417;http://dx.doi.org/10.1145/3377811.3380417","10.1145/3377811.3380417","Mobile banking apps, belonging to the most security-critical app category, render massive and dynamic transactions susceptible to security risks. Given huge potential financial loss caused by vulnerabilities, existing research lacks a comprehensive empirical study on the security risks of global banking apps to provide useful insights and improve the security of banking apps.Since data-related weaknesses in banking apps are critical and may directly cause serious financial loss, this paper first revisits the state-of-the-art available tools and finds that they have limited capability in identifying data-related security weaknesses of banking apps. To complement the capability of existing tools in data-related weakness detection, we propose a three-phase automated security risk assessment system, named Ausera, which leverages static program analysis techniques and sensitive keyword identification. By leveraging Ausera, we collect 2,157 weaknesses in 693 real-world banking apps across 83 countries, which we use as a basis to conduct a comprehensive empirical study from different aspects, such as global distribution and weakness evolution during version updates. We find that apps owned by subsidiary banks are always less secure than or equivalent to those owned by parent banks. In addition, we also track the patching of weaknesses and receive much positive feedback from banking entities so as to improve the security of banking apps in practice. We further find that weaknesses derived from outdated versions of banking apps or third-party libraries are highly prone to being exploited by attackers. To date, we highlight that 21 banks have confirmed the weaknesses we reported (including 126 weaknesses in total). We also exchange insights with 7 banks, such as HSBC in UK and OCBC in Singapore, via in-person or online meetings to help them improve their apps. We hope that the insights developed in this paper will inform the communities about the gaps among multiple stakeholders, including banks, academic researchers, and third-party security companies.","empirical study, vulnerability, mobile banking apps, weakness","","ICSE '20"
"Conference Paper","Kellogg M,Ran M,Sridharan M,Schäf M,Ernst MD","Verifying Object Construction","","2020","","","1447–1458","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380341;http://dx.doi.org/10.1145/3377811.3380341","10.1145/3377811.3380341","In object-oriented languages, constructors often have a combination of required and optional formal parameters. It is tedious and inconvenient for programmers to write a constructor by hand for each combination. The multitude of constructors is error-prone for clients, and client code is difficult to read due to the large number of constructor arguments. Therefore, programmers often use design patterns that enable more flexible object construction---the builder pattern, dependency injection, or factory methods.However, these design patterns can be too flexible: not all combinations of logical parameters lead to the construction of well-formed objects. When a client uses the builder pattern to construct an object, the compiler does not check that a valid set of values was provided. Incorrect use of builders can lead to security vulnerabilities, run-time crashes, and other problems.This work shows how to statically verify uses of object construction, such as the builder pattern. Using a simple specification language, programmers specify which combinations of logical arguments are permitted. Our compile-time analysis detects client code that may construct objects unsafely. Our analysis is based on a novel special case of typestate checking, accumulation analysis, that modularly reasons about accumulations of method calls. Because accumulation analysis does not require precise aliasing information for soundness, our analysis scales to industrial programs. We evaluated it on over 9 million lines of code, discovering defects which included previously-unknown security vulnerabilities and potential null-pointer violations in heavily-used open-source codebases. Our analysis has a low false positive rate and low annotation burden.Our implementation and experimental data are publicly available.","autovalue, lombok, lightweight verification, pluggable type systems, AMI sniping, builder pattern","","ICSE '20"
"Conference Paper","Wang H,Xie X,Li Y,Wen C,Li Y,Liu Y,Qin S,Chen H,Sui Y","Typestate-Guided Fuzzer for Discovering Use-after-Free Vulnerabilities","","2020","","","999–1010","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380386;http://dx.doi.org/10.1145/3377811.3380386","10.1145/3377811.3380386","Existing coverage-based fuzzers usually use the individual control flow graph (CFG) edge coverage to guide the fuzzing process, which has shown great potential in finding vulnerabilities. However, CFG edge coverage is not effective in discovering vulnerabilities such as use-after-free (UaF). This is because, to trigger UaF vulnerabilities, one needs not only to cover individual edges, but also to traverse some (long) sequence of edges in a particular order, which is challenging for existing fuzzers. To this end, we propose to model UaF vulnerabilities as typestate properties, and develop a typestate-guided fuzzer, named UAFL, for discovering vulnerabilities violating typestate properties. Given a typestate property, we first perform a static typestate analysis to find operation sequences potentially violating the property. Our fuzzing process is then guided by the operation sequences in order to progressively generate test cases triggering property violations. In addition, we also employ an information flow analysis to improve the efficiency of the fuzzing process. We have performed a thorough evaluation of UAFL on 14 widely-used real-world programs. The experiment results show that UAFL substantially outperforms the state-of-the-art fuzzers, including AFL, AFLFast, FairFuzz, MOpt, Angora and QSYM, in terms of the time taken to discover vulnerabilities. We have discovered 10 previously unknown vulnerabilities, and received 5 new CVEs.","fuzzing, use-after-free vulnerabilities, typestate-guided fuzzing","","ICSE '20"
"Conference Paper","Alshayban A,Ahmed I,Malek S","Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward","","2020","","","1323–1334","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","Seoul, South Korea","2020","9781450371216","","https://doi.org/10.1145/3377811.3380392;http://dx.doi.org/10.1145/3377811.3380392","10.1145/3377811.3380392","Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.","","","ICSE '20"
"Journal Article","Wang W,Li G,Shen S,Xia X,Jin Z","Modular Tree Network for Source Code Representation Learning","ACM Trans. Softw. Eng. Methodol.","2020","29","4","","Association for Computing Machinery","New York, NY, USA","","","2020-09","","1049-331X","https://doi.org/10.1145/3409331;http://dx.doi.org/10.1145/3409331","10.1145/3409331","Learning representation for source code is a foundation of many program analysis tasks. In recent years, neural networks have already shown success in this area, but most existing models did not make full use of the unique structural information of programs. Although abstract syntax tree (AST)-based neural models can handle the tree structure in the source code, they cannot capture the richness of different types of substructure in programs. In this article, we propose a modular tree network that dynamically composes different neural network units into tree structures based on the input AST. Different from previous tree-structural neural network models, a modular tree network can capture the semantic differences between types of AST substructures. We evaluate our model on two tasks: program classification and code clone detection. Our model achieves the best performance compared with state-of-the-art approaches in both tasks, showing the advantage of leveraging more elaborate structure information of the source code.","program classification, neural networks, code clone detection, Deep learning","",""
"Journal Article","Pantiuchina J,Zampetti F,Scalabrino S,Piantadosi V,Oliveto R,Bavota G,Penta MD","Why Developers Refactor Source Code: A Mining-Based Study","ACM Trans. Softw. Eng. Methodol.","2020","29","4","","Association for Computing Machinery","New York, NY, USA","","","2020-09","","1049-331X","https://doi.org/10.1145/3408302;http://dx.doi.org/10.1145/3408302","10.1145/3408302","Refactoring aims at improving code non-functional attributes without modifying its external behavior. Previous studies investigated the motivations behind refactoring by surveying developers. With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects. First, we mine 287,813 refactoring operations performed in the history of 150 systems. Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics. Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication). Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.","empirical software engineering, Refactoring","",""
"Journal Article","Gao Z,Xia X,Grundy J,Lo D,Li YF","Generating Question Titles for Stack Overflow from Mined Code Snippets","ACM Trans. Softw. Eng. Methodol.","2020","29","4","","Association for Computing Machinery","New York, NY, USA","","","2020-09","","1049-331X","https://doi.org/10.1145/3401026;http://dx.doi.org/10.1145/3401026","10.1145/3401026","Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.","sequence-to-sequence, question quality, question generation, Stack overflow","",""
"Conference Paper","Kurbatova Z,Veselov I,Golubev Y,Bryksin T","Recommendation of Move Method Refactoring Using Path-Based Representation of Code","","2020","","","315–322","Association for Computing Machinery","New York, NY, USA","Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops","Seoul, Republic of Korea","2020","9781450379632","","https://doi.org/10.1145/3387940.3392191;http://dx.doi.org/10.1145/3387940.3392191","10.1145/3387940.3392191","Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field.","Move Method Refactoring, Feature Envy, Code Smells, Automatic Refactoring Recommendation, Path-based Representation","","ICSEW'20"
"Conference Paper","Omri S,Sinz C","Deep Learning for Software Defect Prediction: A Survey","","2020","","","209–214","Association for Computing Machinery","New York, NY, USA","Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops","Seoul, Republic of Korea","2020","9781450379632","","https://doi.org/10.1145/3387940.3391463;http://dx.doi.org/10.1145/3387940.3391463","10.1145/3387940.3391463","Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.","software testing, deep learning, software defect prediction, software quality assurance, machine learning","","ICSEW'20"
"Conference Paper","Pérez B,Castellanos C,Correal D,Rios N,Freire S,Spínola R,Seaman C","What Are the Practices Used by Software Practitioners on Technical Debt Payment: Results from an International Family of Surveys","","2020","","","103–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Conference on Technical Debt","Seoul, Republic of Korea","2020","9781450379601","","https://doi.org/10.1145/3387906.3388632;http://dx.doi.org/10.1145/3387906.3388632","10.1145/3387906.3388632","Context: Technical debt (TD) is a metaphor used to describe technical decisions that can give the company a benefit in the short term but possibly hurting the overall quality of the software in the long term. Objective: This study aims to characterize the current state of practices related to TD payment from the point of view of software practitioners. Method: We used a survey research method to collect and analyze - both quantitatively and qualitatively - a corpus of responses from a survey of 432 software practitioners from Colombia, Chile, Brazil, and the United States, as a part of the InsighTD project. Results: We were able to identify that refactoring (24.3%) was the main practice related to TD payment, along with improving testing (6.2%) and improve design (5.8%). Also, we identify that small-sized systems and big-sized systems, along with young systems (less than one year) tend to use more refactoring. As a part of these results, we also could identify that some practices do not eliminate the debt by itself, but support a favorable scenario for TD payment or prevention. Additionally, after comparing the three major TD types cited (code debt, test debt and design debt) we could discover an important similarity of TD payment practices between code debt and design debt. Lastly, we identified that no matter the cause leading to TD occurrence, refactoring remained the most common practice. Conclusion: Definition of practices related to TD payment is an essential activity for software development teams. Developing healthy software systems that can be maintained in the future requires that companies find the right approaches for TD payment.","InsighTD, family of surveys, technical debt causes, technical debt management, payment practices, technical debt","","TechDebt '20"
"Conference Paper","Connolly Bree D,Cinnéide MÓ","Inheritance versus Delegation: Which is More Energy Efficient?","","2020","","","323–329","Association for Computing Machinery","New York, NY, USA","Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops","Seoul, Republic of Korea","2020","9781450379632","","https://doi.org/10.1145/3387940.3392192;http://dx.doi.org/10.1145/3387940.3392192","10.1145/3387940.3392192","Energy consumption of software is receiving more attention as concerns regarding climate change increase. One factor that significantly impacts how much energy is expended by a software application is the design of the software itself. Existing studies find few consistent results regarding the impact of common refactorings on energy consumption, nor do they define a concrete set of metrics that measure the energy efficiency of software. In this paper, we present the results of preliminary experiments that explore the Replace Inheritance with Delegation refactoring, and its inverse, to assess the impact these design-level refactorings have on energy consumption in the Java programming language. In the tested programs, inheritance proved to be more energy efficient than delegation, with a reduction in run time of 77% and a reduction in average power consumption of 4%. We subsequently propose a research plan to further explore this problem and observe a number of specific challenges in this area. The primary goals of this research are threefold: (i) to investigate how redundancy in an object-oriented design can contribute to unnecessary energy consumption, (ii) to determine how refactoring of the software can remove this redundancy, and (iii) to develop a general-purpose automated tool to perform this refactoring.","Software Refactoring, Design Patterns, Electricity Consumption, Code Smells, Energy-Aware Software","","ICSEW'20"
"Conference Paper","Bogart A,AlOmar EA,Mkaouer MW,Ouni A","Increasing the Trust In Refactoring Through Visualization","","2020","","","334–341","Association for Computing Machinery","New York, NY, USA","Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops","Seoul, Republic of Korea","2020","9781450379632","","https://doi.org/10.1145/3387940.3392190;http://dx.doi.org/10.1145/3387940.3392190","10.1145/3387940.3392190","In software development, maintaining good design is essential. The process of refactoring enables developers to improve this design during development without altering the program's existing behavior. However, this process can be time-consuming, introduce semantic errors, and be difficult for developers inexperienced with refactoring or unfamiliar with a given code base. Automated refactoring tools can help not only by applying these changes, but by identifying opportunities for refactoring. Yet, developers have not been quick to adopt these tools due to a lack of trust between the developer and the tool. We propose an approach in the form of a visualization to aid developers in understanding these suggested operations and increasing familiarity with automated refactoring tools. We also provide a manual validation of this approach and identify options to continue experimentation.","Visualization, Refactoring, Software maintenance and evolution","","ICSEW'20"
"Conference Paper","Dominic J,Houser J,Steinmacher I,Ritter C,Rodeghero P","Conversational Bot for Newcomers Onboarding to Open Source Projects","","2020","","","46–50","Association for Computing Machinery","New York, NY, USA","Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops","Seoul, Republic of Korea","2020","9781450379632","","https://doi.org/10.1145/3387940.3391534;http://dx.doi.org/10.1145/3387940.3391534","10.1145/3387940.3391534","This paper targets the problems newcomers face when onboarding to open source projects and the low retention rate of newcomers. Open source software projects are becoming increasingly more popular. Many major companies have started building open source software. Unfortunately, many newcomers only commit once to an open source project before moving on to another project. Even worse, many novices struggle with joining open source communities and end up leaving quickly, sometimes before their first successful contribution. In this paper, we propose a conversational bot that would recommend projects to newcomers and assist in the onboarding to the open source community. The bot would be able to provide helpful resources, such as Stack Overflow related content. It would also be able to recommend human mentors. We believe that this bot would improve newcomers' experience by providing support not only during their first contribution, but by acting as an agent to engage them to the project.","open source software, newcomer, onboarding, bot","","ICSEW'20"
"Conference Paper","Tan J,Feitosa D,Avgeriou P","An Empirical Study on Self-Fixed Technical Debt","","2020","","","11–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Conference on Technical Debt","Seoul, Republic of Korea","2020","9781450379601","","https://doi.org/10.1145/3387906.3388621;http://dx.doi.org/10.1145/3387906.3388621","10.1145/3387906.3388621","Technical Debt (TD) can be paid back either by those that incurred it or by others. We call the former self-fixed TD, and it is particularly effective, as developers are experts in their own code and are best-suited to fix the corresponding TD issues. To what extent is TD self-fixed, which types of TD are more likely to be self-fixed and is the remediation time of self-fixed TD shorter than non-self-fixed TD? This paper attempts to answer these questions. It reports on an empirical study that analyzes the self-fixed issues of five types of TD (i.e., Code, Defect, Design, Documentation and Test), captured via static analysis, in more than 17,000 commits from 20 Python projects of the Apache Software Foundation. The results show that more than two thirds of the issues are self-fixed and that the self-fixing rate is negatively correlated with the number of commits, developers and project size. Furthermore, the survival time of self-fixed issues is generally shorter than non-self-fixed issues. Moreover, the majority of Defect Debt tends to be self-fixed and has a shorter survival time, while Test Debt and Design Debt are likely to be fixed by other developers. These results can benefit both researchers and practitioners by aiding the prioritization of TD remediation activities within development teams, and by informing the development of TD management tools.","self-fixed issues, static analysis, technical debt, python","","TechDebt '20"
"Conference Paper","Bryksin T,Petukhov V,Alexin I,Prikhodko S,Shpilman A,Kovalenko V,Povarov N","Using Large-Scale Anomaly Detection on Code to Improve Kotlin Compiler","","2020","","","455–465","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387447;http://dx.doi.org/10.1145/3379597.3387447","10.1145/3379597.3387447","In this work, we apply anomaly detection to source code and byte-code to facilitate the development of a programming language and its compiler. We define anomaly as a code fragment that is different from typical code written in a particular programming language. Identifying such code fragments is beneficial to both language developers and end users, since anomalies may indicate potential issues with the compiler or with runtime performance. Moreover, anomalies could correspond to problems in language design. For this study, we choose Kotlin as the target programming language. We outline and discuss approaches to obtaining vector representations of source code and bytecode and to the detection of anomalies across vectorized code snippets. The paper presents a method that aims to detect two types of anomalies: syntax tree anomalies and so-called compiler-induced anomalies that arise only in the compiled bytecode. We describe several experiments that employ different combinations of vectorization and anomaly detection techniques and discuss types of detected anomalies and their usefulness for language developers. We demonstrate that the extracted anomalies and the underlying extraction technique provide additional value for language development.","","","MSR '20"
"Conference Paper","Xia Q,Zhou Z,Li Z,Xu B,Zou W,Chen Z,Ma H,Liang G,Lu H,Guo S,Xiong T,Deng Y,Xie T","JSidentify: A Hybrid Framework for Detecting Plagiarism among JavaScript Code in Online Mini Games","","2020","","","211–220","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice","Seoul, South Korea","2020","9781450371230","","https://doi.org/10.1145/3377813.3381352;http://dx.doi.org/10.1145/3377813.3381352","10.1145/3377813.3381352","Online mini games are lightweight game apps, typically implemented in JavaScript (JS), that run inside another host mobile app (such as WeChat, Baidu, and Alipay). These mini games do not need to be downloaded or upgraded through an app store, making it possible for one host mobile app to perform the aggregated services of many apps. Hundreds of millions of users play tens of thousands of mini games, which make a great profit, and consequently are popular targets of plagiarism. In cases of plagiarism, deeply obfuscated code cloned from the original code often embodies malicious code segments and copyright infringements, posing great challenges for existing plagiarism detection tools. To address these challenges, in this paper, we design and implement JSidentify, a hybrid framework to detect plagiarism among online mini games. JSidentify includes three techniques based on different levels of code abstraction. JSidentify applies the included techniques in the constructed priority list one by one to reduce overall detection time. Our evaluation results show that JSidentify outperforms other existing related state-of-the-art approaches and achieves the best precision and recall with affordable detection time when detecting plagiarism among online mini games and clones among general JS programs. Our deployment experience of JSidentify also shows that JSidentify is indispensable in the daily operations of online mini games in WeChat.","clone detection, online mini games, plagiarism detection, JavaScript","","ICSE-SEIP '20"
"Conference Paper","Compton R,Frank E,Patros P,Koay A","Embedding Java Classes with Code2vec: Improvements from Variable Obfuscation","","2020","","","243–253","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387445;http://dx.doi.org/10.1145/3379597.3387445","10.1145/3379597.3387445","Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared1 for further ML research on source code.","code2vec, code obfuscation, machine learning, source code, neural networks","","MSR '20"
"Conference Paper","Pietri A,Rousseau G,Zacchiroli S","Forking Without Clicking: On How to Identify Software Repository Forks","","2020","","","277–287","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387450;http://dx.doi.org/10.1145/3379597.3387450","10.1145/3379597.3387450","The notion of software ""fork"" has been shifting over time from the (negative) phenomenon of community disagreements that result in the creation of separate development lines and ultimately software products, to the (positive) practice of using distributed version control system (VCS) repositories to collaboratively improve a single product without stepping on each others toes. In both cases the VCS repositories participating in a fork share parts of a common development history.Studies of software forks generally rely on hosting platform metadata, such as GitHub, as the source of truth for what constitutes a fork. These ""forge forks"" however can only identify as forks repositories that have been created on the platform, e.g., by clicking a ""fork"" button on the platform user interface. The increased diversity in code hosting platforms (e.g., GitLab) and the habits of significant development communities (e.g., the Linux kernel, which is not primarily hosted on any single platform) call into question the reliability of trusting code hosting platforms to identify forks. Doing so might introduce selection and methodological biases in empirical studies.In this article we explore various definitions of ""software forks"", trying to capture forking workflows that exist in the real world. We quantify the differences in how many repositories would be identified as forks on GitHub according to the various definitions, confirming that a significant number could be overlooked by only considering forge forks. We study the structure and size of fork networks, observing how they are affected by the proposed definitions and discuss the potential impact on empirical research.","open source, software evolution, free software, source code, software fork, version control system","","MSR '20"
"Conference Paper","Jia A,Fan M,Xu X,Cui D,Wei W,Yang Z,Ye K,Liu T","From Innovations to Prospects: What Is Hidden Behind Cryptocurrencies?","","2020","","","288–299","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387439;http://dx.doi.org/10.1145/3379597.3387439","10.1145/3379597.3387439","The great influence of Bitcoin has promoted the rapid development of blockchain-based digital currencies, especially the altcoins, since 2013. However, most altcoins share similar source codes, resulting in concerns about code innovations. In this paper, an empirical study on existing altcoins is carried out to offer a thorough understanding of various aspects associated with altcoin innovations. Firstly, we construct the dataset of altcoins, including source code repository, GitHub fork relation, and market capitalization (cap). Then, we analyze the altcoin innovations from the perspective of source code similarities. The results demonstrate that more than 85% of altcoin repositories present high code similarities. Next, a temporal clustering algorithm is proposed to mine the inheritance relationship among various altcoins. The family pedigrees of altcoin are constructed, in which the altcoin presents similar evolution features as biology, such as power-law in family size, variety in family evolution, etc. Finally, we investigate the correlation between code innovations and market capitalization. Although we fail to predict the price of altcoins based on their code similarities, the results show that altcoins with higher innovations reflect better market prospects.","Innovations, Altcoins, Relation, Prospects","","MSR '20"
"Conference Paper","Golubev Y,Eliseeva M,Povarov N,Bryksin T","A Study of Potential Code Borrowing and License Violations in Java Projects on GitHub","","2020","","","54–64","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387455;http://dx.doi.org/10.1145/3379597.3387455","10.1145/3379597.3387455","With an ever-increasing amount of open-source software, the popularity of services like GitHub that facilitate code reuse, and common misconceptions about the licensing of open-source software, the problem of license violations in the code is getting more and more prominent. In this study, we compile an extensive corpus of popular Java projects from GitHub, search it for code clones, and perform an original analysis of possible code borrowing and license violations on the level of code fragments. We chose Java as a language because of its popularity in industry, where the plagiarism problem is especially relevant because of possible legal action. We analyze and discuss distribution of 94 different discovered and manually evaluated licenses in files and projects, differences in the licensing of files, distribution of potential code borrowing between licenses, various types of possible license violations, most violated licenses, etc. Studying possible license violations in specific blocks of code, we have discovered that 29.6% of them might be involved in potential code borrowing and 9.4% of them could potentially violate original licenses.","","","MSR '20"
"Conference Paper","Singh SS,Sarangi SR","SoftMon: A Tool to Compare Similar Open-Source Software from a Performance Perspective","","2020","","","397–408","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387444;http://dx.doi.org/10.1145/3379597.3387444","10.1145/3379597.3387444","Over the past two decades, a rich ecosystem of open-source software has evolved. For every type of application, there are a wide variety of alternatives. We observed that even if different applications that perform similar tasks and compiled with the same versions of the compiler and the libraries, they perform very differently while running on the same system. Sadly prior work in this area that compares two code bases for similarities does not help us in finding the reasons for the differences in performance.In this paper, we develop a tool, SoftMon, that can compare the codebases of two separate applications and pinpoint the exact set of functions that are disproportionately responsible for differences in performance. Our tool uses machine learning and NLP techniques to analyze why a given open-source application has a lower performance as compared to its peers, design bespoke applications that can incorporate specific innovations (identified by SoftMon) in competing applications, and diagnose performance bugs.In this paper, we compare a wide variety of large open-source programs such as image editors, audio players, text editors, PDF readers, mail clients and even full-fledged operating systems (OSs). In all cases, our tool was able to pinpoint a set of at the most 10-15 functions that are responsible for the differences within 200 seconds. A subsequent manual analysis assisted by our graph visualization engine helps us find the reasons. We were able to validate most of the reasons by correlating them with subsequent observations made by developers or from existing technical literature. The manual phase of our analysis is limited to 30 minutes (tested with human subjects).","NLP based matching, Software comparison, Performance debugging","","MSR '20"
"Conference Paper","Xavier L,Ferreira F,Brito R,Valente MT","Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems","","2020","","","137–146","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387459;http://dx.doi.org/10.1145/3379597.3387459","10.1145/3379597.3387459","Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where developers explicitly acknowledge their sub-optimal implementation decisions. Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study a sample of 286 SATD-I instances collected from five open source projects, including Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the studied SATD-I instances can be tracked to source code comments. We also show that SATD-I issues take more time to be closed, compared to other issues, although they are not more complex in terms of code churn. Besides, in 45% of the studied issues TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally, we report that most developers pay SATD-I to reduce its costs or interests (66%). Our findings suggest that there is space for designing novel tools to support technical debt management, particularly tools that encourage developers to create and label issues containing TD concerns.","","","MSR '20"
"Conference Paper","Liu J,Huang Q,Xia X,Shihab E,Lo D,Li S","Is Using Deep Learning Frameworks Free? Characterizing Technical Debt in Deep Learning Frameworks","","2020","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Society","Seoul, South Korea","2020","9781450371254","","https://doi.org/10.1145/3377815.3381377;http://dx.doi.org/10.1145/3377815.3381377","10.1145/3377815.3381377","Developers of deep learning applications (shortened as application developers) commonly use deep learning frameworks in their projects. However, due to time pressure, market competition, and cost reduction, developers of deep learning frameworks (shortened as framework developers) often have to sacrifice software quality to satisfy a shorter completion time. This practice leads to technical debt in deep learning frameworks, which results in the increasing burden to both the application developers and the framework developers in future development.In this paper, we analyze the comments indicating technical debt (self-admitted technical debt) in 7 of the most popular open-source deep learning frameworks. Although framework developers are aware of such technical debt, typically the application developers are not. We find that: 1) there is a significant number of technical debt in all the studied deep learning frameworks. 2) there is design debt, defect debt, documentation debt, test debt, requirement debt, compatibility debt, and algorithm debt in deep learning frameworks. 3) the majority of the technical debt in deep learning framework is design debt (24.07% - 65.27%), followed by requirement debt (7.09% - 31.48%) and algorithm debt (5.62% - 20.67%). In some projects, compatibility debt accounts for more than 10%. These findings illustrate that technical debt is common in deep learning frameworks, and many types of technical debt also impact the deep learning applications. Based on our findings, we highlight future research directions and provide recommendations for practitioners.","empirical study, deep learning, self-admitted technical debt, categorization","","ICSE-SEIS '20"
"Conference Paper","Muse BA,Rahman MM,Nagy C,Cleve A,Khomh F,Antoniol G","On the Prevalence, Impact, and Evolution of SQL Code Smells in Data-Intensive Systems","","2020","","","327–338","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387467;http://dx.doi.org/10.1145/3379597.3387467","10.1145/3379597.3387467","Code smells indicate software design problems that harm software quality. Data-intensive systems that frequently access databases often suffer from SQL code smells besides the traditional smells. While there have been extensive studies on traditional code smells, recently, there has been a growing interest in SQL code smells. In this paper, we conduct an empirical study to investigate the prevalence and evolution of SQL code smells in open-source, data-intensive systems. We collected 150 projects and examined both traditional and SQL code smells in these projects. Our investigation delivers several important findings. First, SQL code smells are indeed prevalent in data-intensive software systems. Second, SQL code smells have a weak co-occurrence with traditional code smells. Third, SQL code smells have a weaker association with bugs than that of traditional code smells. Fourth, SQL code smells are more likely to be introduced at the beginning of the project lifetime and likely to be left in the code without a fix, compared to traditional code smells. Overall, our results show that SQL code smells are indeed prevalent and persistent in the studied data-intensive software systems. Developers should be aware of these smells and consider detecting and refactoring SQL code smells and traditional code smells separately, using dedicated tools.","Code smells, data-intensive systems, SQL code smells, database access","","MSR '20"
"Conference Paper","Le TH,Hin D,Croft R,Babar MA","PUMiner: Mining Security Posts from Developer Question and Answer Websites with PU Learning","","2020","","","350–361","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387443;http://dx.doi.org/10.1145/3379597.3387443","10.1145/3379597.3387443","Security is an increasing concern in software development. Developer Question and Answer (Q&A) websites provide a large amount of security discussion. Existing studies have used human-defined rules to mine security discussions, but these works still miss many posts, which may lead to an incomplete analysis of the security practices reported on Q&A websites. Traditional supervised Machine Learning methods can automate the mining process; however, the required negative (non-security) class is too expensive to obtain. We propose a novel learning framework, PUMiner, to automatically mine security posts from Q&A websites. PUMiner builds a context-aware embedding model to extract features of the posts, and then develops a two-stage PU model to identify security content using the labelled Positive and Un-labelled posts. We evaluate PUMiner on more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange. We show that PUMiner is effective with the validation performance of at least 0.85 across all model configurations. Moreover, Matthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner also performs well with an MCC of 0.745 for scenarios where string matching totally fails. Even when the ratio of the labelled positive posts to the un-labelled ones is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security content on Q&A websites for practitioners and researchers.","Machine Learning, Natural Language Processing, Software Security, Positive Unlabelled Learning, Mining Software Repositories","","MSR '20"
"Conference Paper","Gonzalez D,Rath M,Mirakhorli M","Did You Remember To Test Your Tokens?","","2020","","","232–242","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387471;http://dx.doi.org/10.1145/3379597.3387471","10.1145/3379597.3387471","Authentication is a critical security feature for confirming the identity of a system's users, typically implemented with help from frameworks like Spring Security. It is a complex feature which should be robustly tested at all stages of development. Unit testing is an effective technique for fine-grained verification of feature behaviors that is not widely-used to test authentication. Part of the problem is that resources to help developers unit test security features are limited. Most security testing guides recommend test cases in a ""black box"" or penetration testing perspective. These resources are not easily applicable to developers writing new unit tests, or who want a security-focused perspective on coverage.In this paper, we address these issues by applying a grounded theory-based approach to identify common (unit) test cases for token authentication through analysis of 481 JUnit tests exercising Spring Security-based authentication implementations from 53 open source Java projects. The outcome of this study is a developer-friendly unit testing guide organized as a catalog of 53 test cases for token authentication, representing unique combinations of 17 scenarios, 40 conditions, and 30 expected outcomes learned from the data set in our analysis. We supplement the test guide with common test smells to avoid. To verify the accuracy and usefulness of our testing guide, we sought feedback from selected developers, some of whom authored unit tests in our dataset.","Java, Authentication, Unit Test, Security Test, Repository Mining","","MSR '20"
"Conference Paper","Zhang X,Zhu C,Li Y,Guo J,Liu L,Gu H","Precfix: Large-Scale Patch Recommendation by Mining Defect-Patch Pairs","","2020","","","41–50","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice","Seoul, South Korea","2020","9781450371230","","https://doi.org/10.1145/3377813.3381356;http://dx.doi.org/10.1145/3377813.3381356","10.1145/3377813.3381356","Patch recommendation is the process of identifying errors in software systems and suggesting suitable fixes for them. Patch recommendation can significantly improve developer productivity by reducing both the debugging and repairing time. Existing techniques usually rely on complete test suites and detailed debugging reports, which are often absent in practical industrial settings. In this paper, we propose Precfix, a pragmatic approach targeting large-scale industrial codebase and making recommendations based on previously observed debugging activities. Precfix collects defect-patch pairs from development histories, performs clustering, and extracts generic reusable patching patterns as recommendations. We conducted experimental study on an industrial codebase with 10K projects involving diverse defect patterns. We managed to extract 3K templates of defect-patch pairs, which have been successfully applied to the entire codebase. Our approach is able to make recommendations within milliseconds and achieves a false positive rate of 22% confirmed by manual review. The majority (10/12) of the interviewed developers appreciated Precfix, which has been rolled out to Alibaba to support various critical businesses.","patch recommendation, defect detection, patch generation","","ICSE-SEIP '20"
"Conference Paper","Strand A,Gunnarson M,Britto R,Usman M","Using a Context-Aware Approach to Recommend Code Reviewers: Findings from an Industrial Case Study","","2020","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice","Seoul, South Korea","2020","9781450371230","","https://doi.org/10.1145/3377813.3381365;http://dx.doi.org/10.1145/3377813.3381365","10.1145/3377813.3381365","Code review is a commonly used practice in software development. It refers to the process of reviewing new code changes before they are merged with the code base. However, to perform the review, developers are mostly assigned manually to code changes. This may lead to problems such as: a time-consuming selection process, limited pool of known candidates and risk of over-allocation of a few reviewers. To address the above problems, we developed Carrot, a machine learning-based tool to recommend code reviewers. We conducted an improvement case study at Ericsson. We evaluated Carrot using a mixed approach. we evaluated the prediction accuracy using historical data and the metrical Mean Reciprocal Rank (MRR). Furthermore, we deployed the tool in one Ericsson project and evaluated how adequate the recommendations were from the point of view of the tool users and the recommended reviewers. We also asked the opinion of senior developers about the usefulness of the tool. The results show that Carrot can help identify relevant non-obvious reviewers and be of great assistance to new developers. However, there were mixed opinions on Carrot's ability to assist with workload balancing and the decrease code review lead time.","","","ICSE-SEIP '20"
"Conference Paper","Sousa L,Cedrim D,Garcia A,Oizumi W,Bibiano AC,Oliveira D,Kim M,Oliveira A","Characterizing and Identifying Composite Refactorings: Concepts, Heuristics and Patterns","","2020","","","186–197","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Mining Software Repositories","Seoul, Republic of Korea","2020","9781450375177","","https://doi.org/10.1145/3379597.3387477;http://dx.doi.org/10.1145/3379597.3387477","10.1145/3379597.3387477","Refactoring consists of a transformation applied to improve the program internal structure, for instance, by contributing to remove code smells. Developers often apply multiple interrelated refactorings called composite refactoring. Even though composite refactoring is a common practice, an investigation from different points of view on how composite refactoring manifests in practice is missing. Previous empirical studies also neglect how different kinds of composite refactorings affect the removal, prevalence or introduction of smells. To address these matters, we provide a conceptual framework and two heuristics to respectively characterize and identify composite refactorings within and across commits. Then, we mined the commit history of 48 GitHub software projects. We identified and analyzed 24,911 composite refactorings involving 104,505 single refactorings. Amongst several findings, we observed that most composite refactorings occur in the same commit and have the same refactoring type. We found that several refactorings are semantically related to each other, which occur in different parts of the system but are still related to the same task. Our study is the first to reveal that many smells are introduced in a program due to ""incomplete"" composite refactorings. Our study is also the first to reveal 111 patterns of composite refactorings that frequently introduce or remove certain smell types. These patterns can be used as guidelines for developers to improve their refactoring practices as well as for designers of recommender systems.","","","MSR '20"
"Conference Paper","Azeem MI,Panichella S,Di Sorbo A,Serebrenik A,Wang Q","Action-Based Recommendation in Pull-Request Development","","2020","","","115–124","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Software and System Processes","Seoul, Republic of Korea","2020","9781450375122","","https://doi.org/10.1145/3379177.3388904;http://dx.doi.org/10.1145/3379177.3388904","10.1145/3379177.3388904","Pull requests (PRs) selection is a challenging task faced by integrators in pull-based development (PbD), with hundreds of PRs submitted on a daily basis to large open-source projects. Managing these PRs manually consumes integrators' time and resources and may lead to delays in the acceptance, response, or rejection of PRs that can propose bug fixes or feature enhancements. On the one hand, well-known platforms for performing PbD, like GitHub, do not provide built-in recommendation mechanisms for facilitating the management of PRs. On the other hand, prior research on PRs recommendation has focused on the likelihood of either a PR being accepted or receive a response by the integrator. In this paper, we consider both those likelihoods, this to help integrators in the PRs selection process by suggesting to them the appropriate actions to undertake on each specific PR. To this aim, we propose an approach, called CARTESIAN (aCceptance And Response classificaTion-based requESt IdentificAtioN) modeling the PRs recommendation according to PR actions. In particular, CARTESIAN is able to recommend three types of PR actions: accept, respond, and reject. We evaluated CARTESIAN on the PRs of 19 popular GitHub projects. The results of our study demonstrate that our approach can identify PR actions with an average precision and recall of about 86%. Moreover, our findings also highlight that CARTESIAN outperforms the results of two baseline approaches in the task of PRs selection.","Machine learning, Pull Requests recommendation, Software maintenance and evolution","","ICSSP '20"
"Conference Paper","Gilson F,Galster M,Georis F","Generating Use Case Scenarios from User Stories","","2020","","","31–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Software and System Processes","Seoul, Republic of Korea","2020","9781450375122","","https://doi.org/10.1145/3379177.3388895;http://dx.doi.org/10.1145/3379177.3388895","10.1145/3379177.3388895","Textual user stories capture interactions of users with the system as high-level requirements. However, user stories are typically rather short and backlogs can include many stories. This makes it hard to (a) maintain user stories and backlogs, (b) fully understand the scope of a software project without a detailed analysis of the backlog, and (c) analyse how user stories impact design decisions during sprint planning and implementation. This paper proposes a technique to automatically transform textual user stories into visual use case scenarios in the form of robustness diagrams (a semi-formal scenario-based visualisation of workflows). In addition to creating diagrams for individual stories, the technique allows combining diagrams of multiple stories into one diagram to visualise workflows within sets of stories (e.g., a backlog). Moreover, the technique supports ""viewpoint-based"" diagrams, i.e., diagrams that show relationships between actors, domain entities and user interfaces starting from a diagram element (e.g., an actor) selected by the analyst. The technique utilises natural language processing and rule-based transformations. We evaluated the technique with more than 1,400 user stories from 22 backlogs and show that (a) the technique generates syntactically valid robustness diagrams, and (b) the quality of automatically generated robustness diagrams compares to the quality of diagrams created by human experts, but depends on the quality of the textual user stories.","model-driven software development, Agile software development, natural language processing, textual user stories","","ICSSP '20"
"Conference Paper","Keirsgieter W,Visser W","Graft: Static Analysis of Java Bytecode with Graph Databases","","2020","","","217–226","Association for Computing Machinery","New York, NY, USA","Conference of the South African Institute of Computer Scientists and Information Technologists 2020","Cape Town, South Africa","2020","9781450388474","","https://doi.org/10.1145/3410886.3410901;http://dx.doi.org/10.1145/3410886.3410901","10.1145/3410886.3410901","This paper proposes a static analysis tool for finding security vulnerabilities in Java programs. Security vulnerabilities are an ever-present concern for developers and researchers alike. Even a large-scale system can be breached or even rendered nonoperational if a security vulnerability is found and exploited by an attacker. As computer systems grow ever larger and more complex, the need for robust and precise tools for finding such vulnerabilities only grows as well. Most security vulnerabilities stem from untrusted input making its way to sensitive portions of the system, which can have serious consequences ranging from exposure of sensitive data to memory corruption and denial of service. The methods described in this paper allow the security analyst to model the program under analysis as a graph structure, which can then be traversed to find specific patterns that correspond to security vulnerabilities in the program. Techniques are described to build this structure and update it incrementally as the program itself changes during development, as well as perform various analyses to expose potential vulnerabilities. The methods are evaluated against a benchmark suite and shown to be effective in finding vulnerabilities arising from unchecked inputs.","","","SAICSIT '20"
"Conference Paper","Pham TM,Yang J","The Secret Life of Commented-Out Source Code","","2020","","","308–318","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389259;http://dx.doi.org/10.1145/3387904.3389259","10.1145/3387904.3389259","Source code commenting is a common practice to improve code comprehension in software development. While comments often consist of descriptive natural language, surprisingly, there exists a non-trivial portion of comments that are actually code statements, i.e., commented-out code (CO code), even in well-maintained software systems. Commented-out code practice is rarely studied and often excluded in prior studies on comments due to its irrelevance to natural language. When being openly discussed, CO practice is generally considered a bad practice. However, there is no prior work to assess the nature (prevalence, evolution, motivation, and necessity of utilization) of CO code practice.In this paper, we perform the first study to understand CO code practice. Inspired by prior works in comment analysis, we develop automated solutions to identify CO code and track its evolution in development history. Through analyzing six open-source projects of different sizes and from diverse domains, we find that CO code practice is non-trivial in software development, especially in the early phase of development history, e.g., up to 20% of the commits involve CO code practice. We observe common evolution patterns of CO code and find that developers may uncomment and comment code more frequently than expected, e.g., 10% of the CO code practices have been uncommented at least once. Through a manual analysis, we identify the common reasons that developers adopt CO code practices and reveal maintenance challenges associated with CO code practices.","commented-out code, comment analysis, comment/code evolution","","ICPC '20"
"Conference Paper","Alomari HW,Stephan M","SrcClone: Detecting Code Clones via Decompositional Slicing","","2020","","","274–284","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389271;http://dx.doi.org/10.1145/3387904.3389271","10.1145/3387904.3389271","Detecting code clones is an established method for comprehending and maintaining systems. One important but challenging form of code clone detection involves detecting semantic clones, which are those that are semantically similar code segments that differ syntactically. Existing approaches to semantic clone detection do not scale well to large code bases and have room for improvement in their precision and recall. In this paper, we present a scalable slicing-based approach for detecting code clones, including semantic clones. We determine code segment similarity based on their corresponding program slices. We take advantage of a lightweight, publicly available, and scalable program slicing approach to compute the necessary information. Our approach uses dependency analysis to find and measure cloned elements, and provides insights into elements of the code that are affected by an entire clone set/class. We have implemented our approach as a tool called srcClone. We evaluate it by comparing it to two semantic clone detectors in terms of clones, performance, and scalability; and perform recall and precision analysis using established benchmark scenarios. In our evaluation, we illustrate our approach is both relatively scalable and accurate. srcClone can also be used by program analysts to run on non-compilable and incomplete source code, which serves comprehension and maintenance tasks very well. We believe our approach is an important advancement in program comprehension that can help improve clone detection practices and provide developers greater insights into their software.","Program slicing, Clone detection, Semantic clones, Code clone","","ICPC '20"
"Conference Paper","Wu X,Qin L,Yu B,Xie X,Ma L,Xue Y,Liu Y,Zhao J","How Are Deep Learning Models Similar? An Empirical Study on Clone Analysis of Deep Learning Software","","2020","","","172–183","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389254;http://dx.doi.org/10.1145/3387904.3389254","10.1145/3387904.3389254","Deep learning (DL) has been successfully applied to many cutting-edge applications, e.g., image processing, speech recognition, and natural language processing. As more and more DL software is made open-sourced, publicly available, and organized in model repositories and stores (Model Zoo, ModelDepot), there comes a need to understand the relationships of these DL models regarding their maintenance and evolution tasks. Although clone analysis has been extensively studied for traditional software, up to the present, clone analysis has not been investigated for DL software. Since DL software adopts the data-driven development paradigm, it is still not clear whether and to what extent the clone analysis techniques of traditional software could be adapted to DL software.In this paper, we initiate the first step on the clone analysis of DL software at three different levels, i.e., source code level, model structural level, and input/output (I/0)-semantic level, which would be a key in DL software management, maintenance and evolution. We intend to investigate the similarity between these DL models from clone analysis perspective. Several tools and metrics are selected to conduct clone analysis of DL software at three different levels. Our study on two popular datasets (i.e., MNIST and CIFAR-10) and eight DL models of five architectural families (i.e., LeNet, ResNet, DenseNet, AlexNet, and VGG) shows that: 1). the three levels of similarity analysis are generally adequate to find clones between DL models ranging from structural to semantic; 2). different measures for clone analysis used at each level yield similar results; 3) clone analysis of one single level may not render a complete picture of the similarity of DL models. Our findings open up several research opportunities worth further exploration towards better understanding and more effective clone analysis of DL software.","model similarity, Code clone detection, deep learning","","ICPC '20"
"Conference Paper","Hebig R,Ho-Quang T,Jolak R,Schröder J,Linero H,\rAgren M,Maro SH","How Do Students Experience and Judge Software Comprehension Techniques?","","2020","","","425–435","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389283;http://dx.doi.org/10.1145/3387904.3389283","10.1145/3387904.3389283","Today, there is a wide range of techniques to support software comprehension. However, we do not fully understand yet what techniques really help novices, to comprehend a software system. In this paper, we present a master level project course on software evolution, which has a large focus on software comprehension. We collected data about student's experience with diverse comprehension techniques during focus group discussions over the course of two years. Our results indicate that systematic code reading can be supported by additional techniques to guiding reading efforts. Most techniques are considered valuable for gaining an overview and some techniques are judged to be helpful only in later stages of software comprehension efforts.","education, Program comprehension","","ICPC '20"
"Conference Paper","Wen F,Nagy C,Lanza M,Bavota G","An Empirical Study of Quick Remedy Commits","","2020","","","60–71","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389266;http://dx.doi.org/10.1145/3387904.3389266","10.1145/3387904.3389266","Software systems are continuously modified to implement new features, to fix bugs, and to improve quality attributes. Most of these activities are not atomic changes, but rather the result of several related changes affecting different parts of the code. For this reason, it may happen that developers omit some of the needed changes and, as a consequence, leave a task partially unfinished, introduce technical debt or, in the worst case scenario, inject bugs. Knowing the changes that are mistakenly omitted by developers can help in designing recommender systems able to automatically identify risky situations in which, for example, the developer is likely to be pushing an incomplete change to the software repository.We present a qualitative study investigating ""quick remedy commits"" performed by developers with the goal of implementing changes omitted in previous commits. With quick remedy commits we refer to commits that (i) quickly follow a commit performed by the same developer in the same repository, and (ii) aim at remedying issues introduced as the result of code changes omitted in the previous commit (e.g., fix references to code components that have been broken as a consequence of a rename refactoring). Through a manual analysis of 500 quick remedy commits, we define a taxonomy categorizing the types of changes that developers tend to omit. The defined taxonomy can guide the development of tools aimed at detecting omitted changes, and possibly autocomplete them.","Mining Software Repositories, Fixing Commits, Empirical Software Engineering","","ICPC '20"
"Conference Paper","Sousa L,Oizumi W,Garcia A,Oliveira A,Cedrim D,Lucena C","When Are Smells Indicators of Architectural Refactoring Opportunities: A Study of 50 Software Projects","","2020","","","354–365","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389276;http://dx.doi.org/10.1145/3387904.3389276","10.1145/3387904.3389276","Refactoring is a widely adopted practice for improving code comprehension and for removing severe structural problems in a project. When refactorings affect the system architecture, they are called architectural refactorings. Unfortunately, developers usually do not know when and how they should apply refactorings to remove architectural problems. Nevertheless, they might be more susceptible to applying architectural refactoring if they rely on code smells and code refactoring -- two concepts that they usually deal with through their routine programming activities. To investigate if smells can serve as indicators of architectural refactoring opportunities, we conducted a retrospective study over the commit history of 50 software projects. We analyzed 52,667 refactored elements to investigate if they had architectural problems that could have been indicated by automatically-detected smells. We considered purely structural refactorings to identify elements that were likely to have architectural problems. We found that the proportion of refactored elements without smells is much lower than those refactored with smells. By analyzing the latter, we concluded that smells can be used as indicators of architectural refactoring opportunities when the affected source code is deteriorated, i.e., the code hosting two or more smells. For example, when God Class or Complex Class appear together with other smells, they are indicators of architectural refactoring opportunities. In general, smells that often co-occurred with other smells (67.53%) are indicators of architectural refactoring opportunities in most cases (88.53% of refactored elements). Our study also enables us to derive a catalog with patterns of smells that indicate refactoring opportunities to remove specific types of architectural problems. These patterns can guide developers and make them more susceptible to apply architectural refactorings.","","","ICPC '20"
"Conference Paper","Liu F,Li G,Wei B,Xia X,Fu Z,Jin Z","A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning","","2020","","","37–47","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389261;http://dx.doi.org/10.1145/3387904.3389261","10.1145/3387904.3389261","Code completion, one of the most useful features in the Integrated Development Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a self-attentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning (MTL) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.","Multi-task learning, Self-attention, Code completion, Hierarchical structure","","ICPC '20"
"Conference Paper","Shuai J,Xu L,Liu C,Yan M,Xia X,Lei Y","Improving Code Search with Co-Attentive Representation Learning","","2020","","","196–207","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th International Conference on Program Comprehension","Seoul, Republic of Korea","2020","9781450379588","","https://doi.org/10.1145/3387904.3389269;http://dx.doi.org/10.1145/3387904.3389269","10.1145/3387904.3389269","Searching and reusing existing code from a large-scale codebase, e.g, GitHub, can help developers complete a programming task efficiently. Recently, Gu et al. proposed a deep learning-based model (i.e., DeepCS), which significantly outperformed prior models. The DeepCS embedded codebase and natural language queries into vectors by two LSTM (long and short-term memory) models separately, and returned developers the code with higher similarity to a code search query. However, such embedding method learned two isolated representations for code and query but ignored their internal semantic correlations. As a result, the learned isolated representations of code and query may limit the effectiveness of code search.To address the aforementioned issue, we propose a co-attentive representation learning model, i.e., Co-Attentive Representation Learning Code Search-CNN (CARLCS-CNN). CARLCS-CNN learns interdependent representations for the embedded code and query with a co-attention mechanism. Generally, such mechanism learns a correlation matrix between embedded code and query, and co-attends their semantic relationship via row/column-wise max-pooling. In this way, the semantic correlation between code and query can directly affect their individual representations. We evaluate the effectiveness of CARLCS-CNN on Gu et al.'s dataset with 10k queries. Experimental results show that the proposed CARLCS-CNN model significantly outperforms DeepCS by 26.72% in terms of MRR (mean reciprocal rank). Additionally, CARLCS-CNN is five times faster than DeepCS in model training and four times in testing.","co-attention mechanism, representation learning, code search","","ICPC '20"
"Conference Paper","Ouairy L,Bouder HL,Lanet JL","Confiance: Detecting Vulnerabilities in Java Card Applets","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Availability, Reliability and Security","Virtual Event, Ireland","2020","9781450388337","","https://doi.org/10.1145/3407023.3407031;http://dx.doi.org/10.1145/3407023.3407031","10.1145/3407023.3407031","This study focuses on automatically detecting wrong implementations of specifications in Java Card programs, without any knowledge on the source code or the specification itself. To achieve this, an approach based on Natural Language Processing and machine-learning is proposed. First, an oracle gathering methods with similar semantics in groups, is created. This focuses on evaluating our approach performances during the neighborhood discovery. Based on the groups of similar methods automatically retrieved, the anomaly detection relies on the Control Flow Graph of programs of these groups. In order to benchmark our approach's ability to detect vulnerabilities, an oracle of anomaly is created. This oracle knows every anomaly the approach should automatically retrieve. Both the neighborhood discovery and the anomaly detection steps are benchmarked. This approach is implemented in a tool: Confiance, and it is compared to another machine-learning tool for automatic vulnerability detection. The results expose the better performances of Confiance to detect vulnerabilities in open-source programs available online.","","","ARES '20"
"Conference Paper","Franklin D,Coenraad M,Palmer J,Eatinger D,Zipp A,Anaya M,White M,Pham H,Gökdemir O,Weintrop D","An Analysis of Use-Modify-Create Pedagogical Approach's Success in Balancing Structure and Student Agency","","2020","","","14–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM Conference on International Computing Education Research","Virtual Event, New Zealand","2020","9781450370929","","https://doi.org/10.1145/3372782.3406256;http://dx.doi.org/10.1145/3372782.3406256","10.1145/3372782.3406256","As computer science instruction gets offered to more young learners, transitioning from elective to requirement, it is important to explore the relationship between pedagogical approach and student behavior. While different pedagogical approaches have particular motivations and intended goals, little is known about to what degree they satisfy those goals.In this paper, we present analysis of 536 students' (age 9-14, grades 4-8) work within a Scratch-based, Use-Modify-Create (UMC) curriculum, Scratch Encore. We investigate to what degree the UMC progression encourages students to engage with the content of the lesson while providing the flexibility for creativity and exploration.Our findings show that this approach does balance structure with flexibility and creativity, allowing teachers wide variation in the degree to which they adhere to the structured tasks. Many students utilized recently-learned blocks in open-ended activities, yet they also explored blocks not formally taught. In addition, they took advantage of open-ended projects to change sprites, backgrounds, and integrate narratives into their projects.","scratch, computational thinking, k-12 education","","ICER '20"
"Journal Article","Stump A,Jenkins C,Spahn S,McDonald C","Strong Functional Pearl: Harper’s Regular-Expression Matcher in Cedille","Proc. ACM Program. Lang.","2020","4","ICFP","","Association for Computing Machinery","New York, NY, USA","","","2020-08","","","https://doi.org/10.1145/3409004;http://dx.doi.org/10.1145/3409004","10.1145/3409004","This paper describes an implementation of Harper's continuation-based regular-expression matcher as a strong functional program in Cedille; i.e., Cedille statically confirms termination of the program on all inputs. The approach uses neither dependent types nor termination proofs. Instead, a particular interface dubbed a recursion universe is provided by Cedille, and the language ensures that all programs written against this interface terminate. Standard polymorphic typing is all that is needed to check the code against the interface. This answers a challenge posed by Bove, Krauss, and Sozeau.","regular-expression matcher, strong functional programming, programming with continuations, recursion schemes","",""
"Conference Paper","Han L,Chen T,Demartini G,Indulska M,Sadiq S","On Understanding Data Worker Interaction Behaviors","","2020","","","269–278","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval","Virtual Event, China","2020","9781450380164","","https://doi.org/10.1145/3397271.3401059;http://dx.doi.org/10.1145/3397271.3401059","10.1145/3397271.3401059","Understanding how data workers interact with data and various pieces of information (e.g., code snippet examples) is key to design systems that can better support them in exploring a given dataset. To date, however, there is a paucity of research studying information seeking patterns and the strategies adopted by data workers as they carry out data curation activities. In this work, we aim at understanding the behaviors of data workers in discovering data quality issues, and how these behavioral observations relate to their performance. Specifically, we investigate how data workers use information resources and tools to support their task completion. To this end, we collect a multi-modal dataset through a data-driven experiment that relies on the use of eye-tracking technology with a purpose-designed platform built on top of iPython Notebook. The collected data reveals that: (i) searching in external resources is a prevalent action that can be leveraged to achieve better performance; (ii) 'copy-paste-modify' is a typical strategy for writing code to complete tasks; (iii) providing sample code within the system could help data workers to get started with their task; and (iv) surfacing underlying data is an effective way to support exploration. By investigating the behaviors prior to each search action, we also find that the most common reasons that trigger external search actions are the need to seek assistance in writing or debugging code and to search for relevant code to reuse. Our findings provide insights into patterns of interactions with various system components and information resources to perform data curation tasks. This bears implications on the design of domain-specific IR systems for data workers like code-base search.","data curation, interaction behavior, search pattern","","SIGIR '20"
"Conference Paper","Fang C,Liu Z,Shi Y,Huang J,Shi Q","Functional Code Clone Detection with Syntax and Semantics Fusion Learning","","2020","","","516–527","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual Event, USA","2020","9781450380089","","https://doi.org/10.1145/3395363.3397362;http://dx.doi.org/10.1145/3395363.3397362","10.1145/3395363.3397362","Clone detection of source code is among the most fundamental software engineering techniques. Despite intensive research in the past decade, existing techniques are still unsatisfactory in detecting ""functional"" code clones. In particular, existing techniques cannot efficiently extract syntax and semantics information from source code. In this paper, we propose a novel joint code representation that applies fusion embedding techniques to learn hidden syntactic and semantic features of source codes. Besides, we introduce a new granularity for functional code clone detection. Our approach regards the connected methods with caller-callee relationships as a functionality and the method without any caller-callee relationship with other methods represents a single functionality. Then we train a supervised deep learning model to detect functional code clones. We conduct evaluations on a large dataset of C++ programs and the experimental results show that fusion learning can significantly outperform the state-of-the-art techniques in detecting functional code clones.","Code clone detection, syntax and semantics fusion learning, functional clone detection, code representation","","ISSTA 2020"
"Conference Paper","Zhang Y,Dou W,Zhu J,Xu L,Zhou Z,Wei J,Ye D,Yang B","Learning to Detect Table Clones in Spreadsheets","","2020","","","528–540","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual Event, USA","2020","9781450380089","","https://doi.org/10.1145/3395363.3397384;http://dx.doi.org/10.1145/3395363.3397384","10.1145/3395363.3397384","In order to speed up spreadsheet development productivity, end users can create a spreadsheet table by copying and modifying an existing one. These two tables share the similar computational semantics, and form a table clone. End users may modify the tables in a table clone, e.g., adding new rows and deleting columns, thus introducing structure changes into the table clone. Our empirical study on real-world spreadsheets shows that about 58.5% of table clones involve structure changes. However, existing table clone detection approaches in spreadsheets can only detect table clones with the same structures. Therefore, many table clones with structure changes cannot be detected. We observe that, although the tables in a table clone may be modified, they usually share the similar structures and formats, e.g., headers, formulas and background colors. Based on this observation, we propose LTC (Learning to detect Table Clones), to automatically detect table clones with or without structure changes. LTC utilizes the structure and format information from labeled table clones and non table clones to train a binary classifier. LTC first identifies tables in spreadsheets, and then uses the trained binary classifier to judge whether every two tables can form a table clone. Our experiments on real-world spreadsheets from the EUSES and Enron corpora show that, LTC can achieve a precision of 97.8% and recall of 92.1% in table clone detection, significantly outperforming the state-of-the-art technique (a precision of 37.5% and recall of 11.1%).","table clone, format, structure, Spreadsheet","","ISSTA 2020"
"Conference Paper","Lutellier T,Pham HV,Pang L,Li Y,Wei M,Tan L","CoCoNuT: Combining Context-Aware Neural Translation Models Using Ensemble for Program Repair","","2020","","","101–114","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual Event, USA","2020","9781450380089","","https://doi.org/10.1145/3395363.3397369;http://dx.doi.org/10.1145/3395363.3397369","10.1145/3395363.3397369","Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.","Automated program repair, Deep Learning, AI and Software Engineering, Neural Machine Translation","","ISSTA 2020"
"Conference Paper","Liu Z,Wang S","How Far We Have Come: Testing Decompilation Correctness of C Decompilers","","2020","","","475–487","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual Event, USA","2020","9781450380089","","https://doi.org/10.1145/3395363.3397370;http://dx.doi.org/10.1145/3395363.3397370","10.1145/3395363.3397370","A C decompiler converts an executable (the output from a C compiler) into source code. The recovered C source code, once recompiled, will produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications, including legacy software migration, security retrofitting, software comprehension, and to act as the first step in launching adversarial software exploitations. As the paramount component and the trust base in numerous cybersecurity tasks, C decompilers have enabled the analysis of malware, ransomware, and promoted cybersecurity professionals’ understanding of vulnerabilities in real-world systems. In contrast to this flourishing market, our observation is that in academia, outputs of C decompilers (i.e., recovered C source code) are still not extensively used. Instead, the intermediate representations are often more desired for usage when developing applications such as binary security retrofitting. We acknowledge that such conservative approaches in academia are a result of widespread and pessimistic views on the decompilation correctness. However, in conventional software engineering and security research, how much of a problem is, for instance, reusing a piece of simple legacy code by taking the output of modern C decompilers? In this work, we test decompilation correctness to present an up-to-date understanding regarding modern C decompilers. We detected a total of 1,423 inputs that can trigger decompilation errors from four popular decompilers, and with extensive manual effort, we identified 13 bugs in two open-source decompilers. Our findings show that the overly pessimistic view of decompilation correctness leads researchers to underestimate the potential of modern decompilers; the state-of-the-art decompilers certainly care about the functional correctness, and they are making promising progress. However, some tasks that have been studied for years in academia, such as type inference and optimization, still impede C decompilers from generating quality outputs more than is reflected in the literature. These issues rarely receive enough attention and can lead to great confusion that misleads users.","Reverse Engineering, Decompiler, Software Testing","","ISSTA 2020"
"Conference Paper","Xu Y,Xu Z,Chen B,Song F,Liu Y,Liu T","Patch Based Vulnerability Matching for Binary Programs","","2020","","","376–387","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual Event, USA","2020","9781450380089","","https://doi.org/10.1145/3395363.3397361;http://dx.doi.org/10.1145/3395363.3397361","10.1145/3395363.3397361","The binary-level function matching has been widely used to detect whether there are 1-day vulnerabilities in released programs. However, the high false positive is a challenge for current function matching solutions, since the vulnerable function is highly similar to its corresponding patched version. In this paper, the Binary X-Ray (BinXray), a patch based vulnerability matching approach, is proposed to identify the specific 1-day vulnerabilities in target programs accurately and effectively. In the preparing step, a basic block mapping algorithm is designed to extract the signature of a patch, by comparing the given vulnerable and patched programs. The signature is represented as a set of basic block traces. In the detection step, the patching semantics is applied to reduce irrelevant basic block traces to speed up the signature searching. The trace similarity is also designed to identify whether a target program is patched. In experiments, 12 real software projects related to 479 CVEs are collected. BinXray achieves 93.31% accuracy and the analysis time cost is only 296.17ms per function, outperforming the state-of-the-art works.","Binary Analysis, Vulnerability Matching, Patch Presence Identification, Security","","ISSTA 2020"
"Journal Article","Kwak J,Lee S,Park K,Jeong J,Song YH","Cosmos+ OpenSSD: Rapid Prototype for Flash Storage Systems","ACM Trans. Storage","2020","16","3","","Association for Computing Machinery","New York, NY, USA","","","2020-07","","1553-3077","https://doi.org/10.1145/3385073;http://dx.doi.org/10.1145/3385073","10.1145/3385073","As semiconductor technology has advanced, many storage systems have begun to use non-volatile memories as storage media. The organization and architecture of storage controllers have become more complex to meet various design requirements in terms of performance, response time, quality of service (QoS), and so on. In addition, due to the evolution of memory technology and the emergence of new applications, storage controllers employ new firmware algorithms and hardware modules. When designing storage controllers, engineers often evaluate the performance impact of using new software and hardware components using software simulators. However, this technique often yields limited evaluation accuracy because of the difficulty of modeling complex operations of components and the interactions among them. In this article, we present a reconfigurable flash storage controller design that serves as a rapid prototype. This design can be synthesized into a field-programmable gate array device and used in a realistic performance evaluation environment. We show the usefulness of our design by demonstrating the performance impact of design parameters.","storage system, Flash memory, solid state drive (SSD), flash translation layer (FTL)","",""
"Conference Paper","Tan J,Jiao S,Chabbi M,Liu X","What Every Scientific Programmer Should Know about Compiler Optimizations?","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 34th ACM International Conference on Supercomputing","Barcelona, Spain","2020","9781450379830","","https://doi.org/10.1145/3392717.3392754;http://dx.doi.org/10.1145/3392717.3392754","10.1145/3392717.3392754","Compilers are an indispensable component in the software stack. Besides generating machine code, compilers perform multiple optimizations to improve code performance. Typically, scientific programmers treat compilers as a blackbox and expect them to optimize code thoroughly. However, optimizing compilers are not performance panacea. They can miss optimization opportunities or even introduce inefficiencies that are not in the source code. There is a lack of tool infrastructures and datasets that can provide such a study to help understand compiler optimizations.In this paper, we investigate an important compiler optimization---dead and redundant operation elimination. We first develop a tool CIDetector to analyze a large number of programs. In our analysis, we select 12 representative programs from different domains to form a dataset called CIBench. We utilize five compilers to optimize CIBench with the highest optimization options available and leverage CIDetector to study each generated binary. We provide insights into two aspects. First, we show that modern compilers miss several optimization opportunities, in fact they even introduce some inefficiencies, which require programmers to refactor the source code. Second, we show how compilers have advanced in a vertical evolution (the same compiler of different release versions) and a horizontal comparison (different compilers of the most recent releases). With empirical studies, we provide insights for software engineers, compiler writers, and tool developers.","binary analysis, redundancy, compiler inefficiencies, profiling","","ICS '20"
"Conference Paper","Cereda S,Palermo G,Cremonesi P,Doni S","A Collaborative Filtering Approach for the Automatic Tuning of Compiler Optimisations","","2020","","","15–25","Association for Computing Machinery","New York, NY, USA","The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems","London, United Kingdom","2020","9781450370943","","https://doi.org/10.1145/3372799.3394361;http://dx.doi.org/10.1145/3372799.3394361","10.1145/3372799.3394361","Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.","characterization, flag, compiler, embedded, reaction, autotuning, recommender systems, collaborative filtering, selection, optimization, tuning, performance","","LCTES '20"
"Conference Paper","Bharosa N,Meijer K,van der Voort H","Innovation in Public Service Design: Developing a Co-Creation Tool for Public Service Innovation Journeys","","2020","","","275–284","Association for Computing Machinery","New York, NY, USA","The 21st Annual International Conference on Digital Government Research","Seoul, Republic of Korea","2020","9781450387910","","https://doi.org/10.1145/3396956.3396981;http://dx.doi.org/10.1145/3396956.3396981","10.1145/3396956.3396981","Outpaced by the speed of digital innovation in the private sector, governments are looking for new approaches to public service innovation. Drawing on three complementary innovation theories – open innovation, recombinant innovation and co-creation – this paper presents a prototype that is designed to enhance the online innovation journey for public services. The main strategy explored is that of online public-service co-creation, allowing innovators to combine online and offline efforts. The outcome of this research is a prototype of an online co-creation tool. The tool is consumed via a web-portal that includes an overview of ongoing experiments, tools, labs data sets and digital building blocks. This paper contributes by presenting the requirements and lessons learned when developing a co-creation tool for innovation in public service design. While the proposed co-creation tool is expected to enhance and speed up online cocreation efforts, findings indicate that innovators from the public and private sector still need to learn how to combine online and offline co-creation efforts. The added value expected from the online tool is that it should provide an up to date oversight of digital building blocks, innovation methods and labs. Interviews with prospective users suggest that this oversight is needed to jumpstart the first step of the innovation journey. Development of a digital sandbox – a shared online experimentation environment – is considered to be an important next step for innovation in public service design.","recombinant innovation, e-government, co-creation, open innovation, Public services","","dg.o '20"
"Conference Paper","Ainsworth S,Jones TM","Prefetching in Functional Languages","","2020","","","16–29","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGPLAN International Symposium on Memory Management","London, UK","2020","9781450375665","","https://doi.org/10.1145/3381898.3397209;http://dx.doi.org/10.1145/3381898.3397209","10.1145/3381898.3397209","Functional programming languages contain a number of runtime and language features, such as garbage collection, indirect memory accesses, linked data structures and immutability, that interact with a processor’s memory system. These conspire to cause a variety of unintuitive memory-performance effects. For example, it is slower to traverse through linked lists and arrays of data that have been sorted than to traverse the same data accessed in the order it was allocated. We seek to understand these issues and mitigate them in a manner consistent with functional languages, taking advantage of the features themselves where possible. For example, immutability and garbage collection force linked lists to be allocated roughly sequentially in memory, even when the data pointed to within each node is not. We add language primitives for software-prefetching to the OCaml language to exploit this, and observe significant performance improvements a variety of micro- and macro-benchmarks, resulting in speedups of up to 2× on the out-of-order superscalar Intel Haswell and Xeon Phi Knights Landing systems, and up to 3× on the in-order Arm Cortex-A53.","Functional Programming, Hardware Prefetching, Software Prefetching, OCaml","","ISMM 2020"
"Conference Paper","Frädrich C,Obermüller F,Körber N,Heuer U,Fraser G","Common Bugs in Scratch Programs","","2020","","","89–95","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education","Trondheim, Norway","2020","9781450368742","","https://doi.org/10.1145/3341525.3387389;http://dx.doi.org/10.1145/3341525.3387389","10.1145/3341525.3387389","Bugs in SCRATCH programs can spoil the fun and inhibit learning success. Many common bugs are the result of recurring patterns of bad code. In this paper we present a collection of common code patterns that typically hint at bugs in SCRATCH programs, and the LitterBox tool which can automatically detect them. We empirically evaluate how frequently these patterns occur, and how severe their consequences usually are. While fixing bugs inevitably is part of learning, the possibility to identify the bugs automatically provides the potential to support learners.","block-based programming, scratch, code quality","","ITiCSE '20"
"Conference Paper","Funke H,Mühlig J,Teubner J","Efficient Generation of Machine Code for Query Compilers","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Workshop on Data Management on New Hardware","Portland, Oregon","2020","9781450380249","","https://doi.org/10.1145/3399666.3399925;http://dx.doi.org/10.1145/3399666.3399925","10.1145/3399666.3399925","Query compilation can make query execution extremely efficient, but it introduces additional compilation time. The compilation time causes a relatively high overhead especially for short-running and high-complexity queries.We propose Flounder IR as a lightweight intermediate representation for query compilation to reduce compilation times. Flounder IR is close to machine assembly and adds just that set of features that is necessary for efficient query compilation: virtual registers and function calls ease the construction of the compiler front-end; database-specific extensions enable efficient pipelining in query plans; more elaborate IR features are intentionally left out to maximize compilation speed.In this paper, we present the Flounder IR language and motivate its design; we show how the language makes query compilation intuitive and efficient; and we demonstrate with benchmarks how our Flounder library can significantly reduce query compilation times.","","","DaMoN '20"
"Journal Article","King P","A History of the Groovy Programming Language","Proc. ACM Program. Lang.","2020","4","HOPL","","Association for Computing Machinery","New York, NY, USA","","","2020-06","","","https://doi.org/10.1145/3386326;http://dx.doi.org/10.1145/3386326","10.1145/3386326","This paper describes the history of the Groovy programming language. At the time of Groovy’s inception, Java was a dominant programming language with a wealth of useful libraries. Despite this, it was perceived by some to be evolving slowing and to have shortcomings for scripting, rapid prototyping and when trying to write minimalistic code. Other languages seemed to be innovating faster than Java and, while overcoming some of Java’s shortcomings, used syntax that was less familiar to Java developers. Integration with Java libraries was also non-optimal. Groovy was created as a complementary language to Java—its dynamic counterpart. It would look and feel like Java but focus on extensibility and rapid innovation. Groovy would borrow ideas from dynamic languages like Ruby, Python and Smalltalk where needed to provide compelling JVM solutions for some of Java’s shortcomings. Groovy supported innovation through its runtime and compile-time metaprogramming capabilities. It supported simple operator overloading, had a flexible grammar and was extensible. These characteristics made it suitable for growing the language to have new commands (verbs) and properties (nouns) specific to a particular domain, a so called Domain Specific Language (DSL). While still intrinsically linked with Java, over time Groovy has evolved from a niche dynamic scripting language into a compelling mainstream language. After many years as a principally dynamically-typed language, a static nature was added to Groovy. Code could be statically type checked or when dynamic features weren’t needed, they could be turned off entirely for Java-like performance. A number of nuances to the static nature came about to support the style of coding used by Groovy developers. Many choices made by Groovy in its design, later appeared in other languages (Swift, C#, Kotlin, Ceylon, PHP, Ruby, Coffeescript, Scala, Frege, TypeScript and Java itself). This includes Groovy’s dangling closure, Groovy builders, null-safe navigation, the Elvis operator, ranges, the spaceship operator, and flow typing. For most languages, we don’t know to what extent Groovy played a part in their choices. We do know that Kotlin took inspiration from Groovy’s dangling closures, builder concept, default it parameter for closures, templates and interpolated strings, null-safe navigation and the Elvis operator. The leadership, governance and sponsorship arrangements of Groovy have evolved over time, but Groovy has always been a successful highly collaborative open source project driven more by the needs of the community than by a vision of a particular company or person.","Domain Specific Languages, Extensibility, Closure, Static typing, Dynamic typing, Scripting, Object-oriented, Metaprogramming, Functional programming","",""
"Conference Paper","Premtoon V,Koppel J,Solar-Lezama A","Semantic Code Search via Equational Reasoning","","2020","","","1066–1082","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386001;http://dx.doi.org/10.1145/3385412.3386001","10.1145/3385412.3386001","We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle’s Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.","code search, equational reasoning","","PLDI 2020"
"Conference Paper","Rocha RC,Petoumenos P,Wang Z,Cole M,Leather H","Effective Function Merging in the SSA Form","","2020","","","854–868","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386030;http://dx.doi.org/10.1145/3385412.3386030","10.1145/3385412.3386030","Function merging is an important optimization for reducing code size. This technique eliminates redundant code across functions by merging them into a single function. While initially limited to identical or trivially similar functions, the most recent approach can identify all merging opportunities in arbitrary pairs of functions. However, this approach has a serious limitation which prevents it from reaching its full potential. Because it cannot handle phi-nodes, the state-of-the-art applies register demotion to eliminate them before applying its core algorithm. While a superficially minor workaround, this has a three-fold negative effect: by artificially lengthening the instruction sequences to be aligned, it hinders the identification of mergeable instruction; it prevents a vast number of functions from being profitably merged; it increases compilation overheads, both in terms of compile-time and memory usage. We present SalSSA, a novel approach that fully supports the SSA form, removing any need for register demotion. By doing so, we notably increase the number of profitably merged functions. We implement SalSSA in LLVM and apply it to the SPEC 2006 and 2017 suites. Experimental results show that our approach delivers on average, 7.9% to 9.7% reduction on the final size of the compiled code. This translates to around 2x more code size reduction over the state-of-the-art. Moreover, as a result of aligning shorter sequences of instructions and reducing the number of wasteful merge operations, our new approach incurs an average compile-time overhead of only 5%, 3x less than the state-of-the-art, while also reducing memory usage by over 2x.","Code Size Reduction, Function Merging, LTO","","PLDI 2020"
"Conference Paper","Dasgupta S,Dinesh S,Venkatesh D,Adve VS,Fletcher CW","Scalable Validation of Binary Lifters","","2020","","","655–671","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3385964;http://dx.doi.org/10.1145/3385412.3385964","10.1145/3385412.3385964","Validating the correctness of binary lifters is pivotal to gain trust in binary analysis, especially when used in scenarios where correctness is important. Existing approaches focus on validating the correctness of lifting instructions or basic blocks in isolation and do not scale to full programs. In this work, we show that formal translation validation of single instructions for a complex ISA like x86-64 is not only practical, but can be used as a building block for scalable full-program validation. Our work is the first to do translation validation of single instructions on an architecture as extensive as x86-64, uses the most precise formal semantics available, and has the widest coverage in terms of the number of instructions tested for correctness. Next, we develop a novel technique that uses validated instructions to enable program-level validation, without resorting to performance-heavy semantic equivalence checking. Specifically, we compose the validated IR sequences using a tool we develop called Compositional Lifter to create a reference standard. The semantic equivalence check between the reference and the lifter output is then reduced to a graph-isomorphism check through the use of semantic preserving transformations. The translation validation of instructions in isolation revealed 29 new bugs in McSema – a mature open-source lifter from x86-64 to LLVM IR. Towards the validation of full programs, our approach was able to prove the translational correctness of 2254/2348 functions taken from LLVM’s single-source benchmark test-suite.","Compiler Optimizations, Formal Semantics, Graph Isomorphism, LLVM IR, Translation Validation, x86-64","","PLDI 2020"
"Conference Paper","Qin B,Chen Y,Yu Z,Song L,Zhang Y","Understanding Memory and Thread Safety Practices and Issues in Real-World Rust Programs","","2020","","","763–779","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386036;http://dx.doi.org/10.1145/3385412.3386036","10.1145/3385412.3386036","Rust is a young programming language designed for systems software development. It aims to provide safety guarantees like high-level languages and performance efficiency like low-level languages. The core design of Rust is a set of strict safety rules enforced by compile-time checking. To support more low-level controls, Rust allows programmers to bypass these compiler checks to write unsafe code. It is important to understand what safety issues exist in real Rust programs and how Rust safety mechanisms impact programming practices. We performed the first empirical study of Rust by close, manual inspection of 850 unsafe code usages and 170 bugs in five open-source Rust projects, five widely-used Rust libraries, two online security databases, and the Rust standard library. Our study answers three important questions: how and why do programmers write unsafe code, what memory-safety issues real Rust programs have, and what concurrency bugs Rust programmers make. Our study reveals interesting real-world Rust program behaviors and new issues Rust programmers make. Based on our study results, we propose several directions of building Rust bug detectors and built two static bug detectors, both of which revealed previously unknown bugs.","Memory Bug, Rust, Bug Study, Concurrency Bug","","PLDI 2020"
"Conference Paper","Allamanis M,Barr ET,Ducousso S,Gao Z","Typilus: Neural Type Hints","","2020","","","91–105","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3385997;http://dx.doi.org/10.1145/3385412.3385997","10.1145/3385412.3385997","Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program’s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace — a continuous relaxation of the discrete space of types — and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.","meta-learning, graph neural networks, type inference, structured learning, deep learning","","PLDI 2020"
"Conference Paper","Muller SK,Singer K,Goldstein N,Acar UA,Agrawal K,Lee IT","Responsive Parallelism with Futures and State","","2020","","","577–591","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386013;http://dx.doi.org/10.1145/3385412.3386013","10.1145/3385412.3386013","Motivated by the increasing shift to multicore computers, recent work has developed language support for responsive parallel applications that mix compute-intensive tasks with latency-sensitive, usually interactive, tasks. These developments include calculi that allow assigning priorities to threads, type systems that can rule out priority inversions, and accompanying cost models for predicting responsiveness. These advances share one important limitation: all of this work assumes purely functional programming. This is a significant restriction, because many realistic interactive applications, from games to robots to web servers, use mutable state, e.g., for communication between threads. In this paper, we lift the restriction concerning the use of state. We present λi4, a calculus with implicit parallelism in the form of prioritized futures and mutable state in the form of references. Because both futures and references are first-class values, λi4 programs can exhibit complex dependencies, including interaction between threads and with the external world (users, network, etc). To reason about the responsiveness of λi4 programs, we extend traditional graph-based cost models for parallelism to account for dependencies created via mutable state, and we present a type system to outlaw priority inversions that can lead to unbounded blocking. We show that these techniques are practical by implementing them in C++ and present an empirical evaluation.","type systems, Cilk, concurrency, futures, parallelism, responsiveness, shared memory","","PLDI 2020"
"Journal Article","Bao L,Xing Z,Xia X,Lo D,Wu M,Yang X","Psc2code: Denoising Code Extraction from Programming Screencasts","ACM Trans. Softw. Eng. Methodol.","2020","29","3","","Association for Computing Machinery","New York, NY, USA","","","2020-06","","1049-331X","https://doi.org/10.1145/3392093;http://dx.doi.org/10.1145/3392093","10.1145/3392093","Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts.In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code.We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.","Programming videos, code search, deep learning","",""
"Journal Article","Mukherjee R,Chaudhuri S,Jermaine C","Searching a Database of Source Codes Using Contextualized Code Search","Proc. VLDB Endow.","2020","13","10","1765–1778","VLDB Endowment","","","","2020-06","","2150-8097","https://doi.org/10.14778/3401960.3401972;http://dx.doi.org/10.14778/3401960.3401972","10.14778/3401960.3401972","Consider the case where a programmer has written some part of a program, but has left part of the program (such as a method or a function body) incomplete. The goal is to use the context surrounding the missing code to automatically ""figure out"" which of the codes in the database would be useful to the programmer in order to help complete the missing code. The search is ""contextualized"" in the sense that the search engine should use clues in the partially-completed code to figure out which database code is most useful. The user should not be required to formulate an explicit query.We cast contextualized code search as a learning problem, where the goal is to learn a distribution function computing the likelihood that each database code completes the program, and propose a neural model for predicting which database code is likely to be most useful. Because it will be prohibitively expensive to apply a neural model to each code in a database of millions or billions of codes at search time, one of our key technical concerns is ensuring a speedy search. We address this by learning a ""reverse encoder"" that can be used to reduce the problem of evaluating each database code to computing a convolution of two normal distributions.","","",""
"Conference Paper","Fariha A,Nath S,Meliou A","Causality-Guided Adaptive Interventional Debugging","","2020","","","431–446","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data","Portland, OR, USA","2020","9781450367356","","https://doi.org/10.1145/3318464.3389694;http://dx.doi.org/10.1145/3318464.3389694","10.1145/3318464.3389694","Runtime nondeterminism is a fact of life in modern database applications. Previous research has shown that nondeterminism can cause applications to intermittently crash, become unresponsive, or experience data corruption. We propose Adaptive Interventional Debugging (AID) for debugging such intermittent failures. AID combines existing statistical debugging, causal analysis, fault injection, and group testing techniques in a novel way to (1) pinpoint the root cause of an application's intermittent failure and (2) generate an explanation of how the root cause triggers the failure. AID works by first identifying a set of runtime behaviors (called predicates) that are strongly correlated to the failure. It then utilizes temporal properties of the predicates to (over)-approximate their causal relationships. Finally, it uses fault injection to execute a sequence of interventions on the predicates and discover their true causal relationships. This enables AID to identify the true root cause and its causal relationship to the failure. We theoretically analyze how fast AID can converge to the identification. We evaluate AID with six real-world applications that intermittently fail under specific inputs. In each case, AID was able to identify the root cause and explain how the root cause triggered the failure, much faster than group testing and more precisely than statistical debugging. We also evaluate AID with many synthetically generated applications with known root causes and confirm that the benefits also hold for them.","root-causing, trace analysis, concurrency bug, group testing","","SIGMOD '20"
"Journal Article","Gajrani J,Tripathi M,Laxmi V,Somani G,Zemmari A,Gaur MS","Vulvet: Vetting of Vulnerabilities in Android Apps to Thwart Exploitation","Digital Threats","2020","1","2","","Association for Computing Machinery","New York, NY, USA","","","2020-05","","2692-1626","https://doi.org/10.1145/3376121;http://dx.doi.org/10.1145/3376121","10.1145/3376121","Data security and privacy of Android users is one of the challenging security problems addressed by the security research community. A major source of the security vulnerabilities in Android apps is attributed to bugs within source code, insecure APIs, and unvalidated code before performing sensitive operations. Specifically, the major class of app vulnerabilities is related to the categories such as inter-component communication (ICC), networking, web, cryptographic APIs, storage, and runtime-permission validation. A major portion of current contributions focus on identifying a smaller subset of vulnerabilities. In addition, these methods do not discuss how to remove detected vulnerabilities from the affected code.In this work, we propose a novel vulnerability detection and patching framework, Vulvet, which employs static analysis approaches from different domains of program analysis for detection of a wide range of vulnerabilities in Android apps. We propose an additional light-weight technique, FP-Validation, to mitigate false positives in comparison to existing solutions owing to over-approximation. In addition to improved detection, Vulvet provides an automated patching of apps with safe code for each of the identified vulnerability using bytecode instrumentation. We implement Vulvet as an extension of Soot. To demonstrate the efficiency of our proposed framework, we analyzed 3,700 apps collected from various stores and benchmarks consisting of various weak implementations. Our results indicate that Vulvet is able to achieve vulnerability detection with 95.23% precision and 0.975 F-measure on benchmark apps; a significant improvement in comparison to recent works along with successful patching of identified vulnerabilities.","protection, Android, vulnerabilities, static analysis, security","",""
"Conference Paper","Lee Y,Ko U,Aitkazin I,Park S,Tak HS,Cho HG","A Fast Detecting Method for Clone Functions Using Global Alignment of Token Sequences","","2020","","","17–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 12th International Conference on Machine Learning and Computing","Shenzhen, China","2020","9781450376426","","https://doi.org/10.1145/3383972.3384014;http://dx.doi.org/10.1145/3383972.3384014","10.1145/3383972.3384014","In large software projects, proper source code reuse can make development more efficient, but a lot of duplicate code and error code reuse can be a major cause of difficult system maintenance. Efficient clone code detection for large project can help manage the project. However, most of the clone detection methods are difficult to perform on adaptive analysis that adjusts specificity or sensitivity according to the type of clone to be detected. Therefore, when a user wants to find a particular type of clone in a large project, they must analyze it repeatedly using various tools to adjust the options. In this study, we propose a clone detection system based on the global sequence alignment. Lex based token analysis models and global alignment algorithm-based clone detection models were able to detect not only exact matches but also various types of clones by setting lower bound scores. Using features of the global alignment score calculation method to eliminate functions that cannot be clone candidates in advance, alignment analysis was possible even for large projects, and the execution time was predicted. For clone functions, we visualized the matching area, which is the result of alignment analysis, to represent clone information more efficiently.","clone function, code analysis, Clone detection, global alignment","","ICMLC 2020"
"Conference Paper","Zhao F,Zhao J,Bai Y","A Survey of Automatic Generation of Code Comments","","2020","","","21–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2020","9781450376419","","https://doi.org/10.1145/3380625.3380649;http://dx.doi.org/10.1145/3380625.3380649","10.1145/3380625.3380649","Code comments are a valuable form of documentation attached to code that is the most intuitive and efficient way for programmers to understand software code. Good code comments can help programmers quickly understand the role of source code and facilitate understanding of programs and software maintenance tasks. However, in practice, most programmers only pay attention to the code and ignore the comments and documents, which makes the program's readability and maintainability greatly reduced. Based on the meaning of code comments, this paper discusses the current progress in the field of code comments research, adopts the comparative analysis method, focuses on the classification research of the methods and tools for automatic generation of code comments, expounds its advantages and disadvantages, and reveals the issues that need further study.","Code Comments, Comments method, Automatic Generation, Comments tool","","ICMSS 2020"
"Journal Article","Smeets H,Ceriotti M,Marrón PJ","Adapting Recursive Sinusoidal Software Oscillators for Low-Power Fixed-Point Processors","ACM Trans. Embed. Comput. Syst.","2020","19","3","","Association for Computing Machinery","New York, NY, USA","","","2020-05","","1539-9087","https://doi.org/10.1145/3378559;http://dx.doi.org/10.1145/3378559","10.1145/3378559","The growing field of the Internet of Things relies at the bottom on components with very scarce computing resources that currently do not allow complex processing of sensed data. Any computation involving Fast Fourier Transforms (FFT), Wavelet Transforms (WT), or simple sines and cosines is considered impractical on low-end devices due to the lack of floating point and math libraries. This article presents new techniques that make it possible to use these functions also on severely constrained target platforms.Current literature abounds with schemes to compute sine and cosine functions, with focus on speed, hardware footprint, software size, target type, or precision. Even so, there is no practical exploration of the design space available for embedded devices with limited resources, in particular when only integer operations are possible. We select an efficient set of recursive sine and cosine generators and measure the frequency, amplitude, and phase error over a wide parameter range. We show that their simplicity allows them to be implemented on the most bare targets with good precision, reducing power consumption and size while being the fastest on integer-only processors. We also introduce specially tailored FFT and WT algorithms and show that they are usable in practice while having an extremely small code footprint, good precision, and high speed.","fourier transform, intermediate euler, cosine, IoT, wavelet transform, sine","",""
"Conference Paper","Lu Y,Li J","Generative Adversarial Network for Improving Deep Learning Based Malware Classification","","2020","","","584–593","IEEE Press","National Harbor, Maryland","Proceedings of the Winter Simulation Conference","","2020","9781728132839","","","","The generative adversarial network (GAN) had been successfully applied in many domains in the past, the GAN network provides a new approach for solving computer vision, object detection and classification problems by learning, mimicking and generating any distribution of data. One of the difficulties in deep learning-based malware detection and classification tasks is lacking of training malware samples. With insufficient training data the classification performance of the deep model could be compromised significantly. To solve this issue, in this paper, we propose a method which uses the Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic malware samples. Our experiment results show that by using the DCGAN generated adversarial synthetic malware samples, the classification accuracy of the classifier --- a 18-layer deep residual network is significantly improved by approximately 6%.","","","WSC '19"
"Journal Article","Kapur R,Sodhi B","A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics","ACM Trans. Softw. Eng. Methodol.","2020","29","2","","Association for Computing Machinery","New York, NY, USA","","","2020-04","","1049-331X","https://doi.org/10.1145/3384517;http://dx.doi.org/10.1145/3384517","10.1145/3384517","An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.","source code mining, Maintaining software, automated software engineering, AI in software engineering, software defect prediction, software metrics, software faults and failures","",""
"Conference Paper","Head A,Jiang J,Smith J,Hearst MA,Hartmann B","Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs","","2020","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","Honolulu, HI, USA","2020","9781450367080","","https://doi.org/10.1145/3313831.3376798;http://dx.doi.org/10.1145/3313831.3376798","10.1145/3313831.3376798","Programming tutorials are a pervasive, versatile medium for teaching programming. In this paper, we report on the content and structure of programming tutorials, the pain points authors experience in writing them, and a design for a tool to help improve this process. An interview study with 12 experienced tutorial authors found that they construct documents by interleaving code snippets with text and illustrative outputs. It also revealed that authors must often keep related artifacts of source programs, snippets, and outputs consistent as a program evolves. A content analysis of 200 frequently-referenced tutorials on the web also found that most tutorials contain related artifacts—duplicate code and outputs generated from snippets—that an author would need to keep consistent with each other. To address these needs, we designed a tool called Torii with novel authoring capabilities. An in-lab study showed that tutorial authors can successfully use the tool for the unique affordances identified, and provides guidance for designing future tools for tutorial authoring.","literate programming, authoring, programming tutorials, code evolution, code editors, consistency","","CHI '20"
"Conference Paper","Han HL,Renom MA,Mackay WE,Beaudouin-Lafon M","Textlets: Supporting Constraints and Consistency in Text Documents","","2020","","","1–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","Honolulu, HI, USA","2020","9781450367080","","https://doi.org/10.1145/3313831.3376804;http://dx.doi.org/10.1145/3313831.3376804","10.1145/3313831.3376804","Writing technical documents frequently requires following constraints and consistently using domain-specific terms. We interviewed 12 legal professionals and found that they all use a standard word processor, but must rely on their memory to manage dependencies and maintain consistent vocabulary within their documents. We introduce Textlets, interactive objects that reify text selections into persistent items. We show how Textlets help manage consistency and constraints within the document, including selective search and replace, word count, and alternative wording. Eight participants tested a search-and-replace Textlet as a technology probe. All successfully interacted directly with the Textlet to perform advanced tasks; and most (6/8) spontaneously generated a novel replace-all-then-correct strategy. Participants suggested additional ideas, such as supporting collaborative editing over time by embedding a Textlet into the document to flag forbidden words. We argue that Textlets serve as a generative concept for creating powerful new tools for document editing.","reification, document processing, text editing","","CHI '20"
"Conference Paper","An K,Tilevich E","Client Insourcing: Bringing Ops In-House for Seamless Re-Engineering of Full-Stack JavaScript Applications","","2020","","","179–189","Association for Computing Machinery","New York, NY, USA","Proceedings of The Web Conference 2020","Taipei, Taiwan","2020","9781450370233","","https://doi.org/10.1145/3366423.3380105;http://dx.doi.org/10.1145/3366423.3380105","10.1145/3366423.3380105","Modern web applications are distributed across a browser-based client and a cloud-based server. Distribution provides access to remote resources, accessed over the web and shared by clients. Much of the complexity of inspecting and evolving web applications lies in their distributed nature. Also, the majority of mature program analysis and transformation tools works only with centralized software. Inspired by business process re-engineering, in which remote operations can be insourced back in house to restructure and outsource anew, we bring an analogous approach to the re-engineering of web applications. Our target domain are full-stack JavaScript applications that implement both the client and server code in this language. Our approach is enabled by Client Insourcing, a novel automatic refactoring that creates a semantically equivalent centralized version of a distributed application. This centralized version is then inspected, modified, and redistributed to meet new requirements. After describing the design and implementation of Client Insourcing, we demonstrate its utility and value in addressing changes in security, reliability, and performance requirements. By reducing the complexity of the non-trivial program inspection and evolution tasks performed to meet these requirements, our approach can become a helpful aid in the re-engineering of web applications in this domain.","Web Applications, Re-Engineering, Software Engineering, Program Analysis & Transformation, Mobile Apps, Middleware, JavaScript","","WWW '20"
"Conference Paper","Zhao Y,Xiao L,Wang X,Sun L,Chen B,Liu Y,Bondi AB","How Are Performance Issues Caused and Resolved?-An Empirical Study from a Design Perspective","","2020","","","181–192","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/SPEC International Conference on Performance Engineering","Edmonton AB, Canada","2020","9781450369916","","https://doi.org/10.1145/3358960.3379130;http://dx.doi.org/10.1145/3358960.3379130","10.1145/3358960.3379130","Empirical experience regarding how real-life performance issues are caused and resolved can provide valuable insights for practitioners to effectively and efficiently prevent, detect, and fix performance issues. Prior work shows that most performance issues have their roots in poor architectural decisions. This paper contributes a large scale empirical study of 192 real-life performance issues, with an emphasis on software design. First, this paper contributes a holistic view of eight common root causes and typical resolutions that recur in different projects, and surveyed existing literature, in particular, tools, that can detect and fix each type of performance issue. Second, this study is first-of-its-kind to investigate performance issues from a design perspective. In the 192 issues, 33% required design-level optimization, i.e. simultaneously revising a group of related source files for resolving the issues. We reveal four design-level optimization patterns, which have shown different prevalence in resolving different root causes. Finally, this study investigated the Return on Investment for addressing performance issues, to help practitioners choose between localized or design-level optimization resolutions, and to prioritize issues due to different root causes.","software design structure, design patterns, software performance","","ICPE '20"
"Conference Paper","Wu H","A Systematical Study for Deep Learning Based Android Malware Detection","","2020","","","177–182","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 9th International Conference on Software and Computer Applications","Langkawi, Malaysia","2020","9781450376655","","https://doi.org/10.1145/3384544.3384546;http://dx.doi.org/10.1145/3384544.3384546","10.1145/3384544.3384546","Nowadays with the development of smartphone and its operating system, such as Android, the amount of mobile malware is correspondingly increasing. To protect the privacy security of both users and manufacturers, Android malware detection has received a lot of research focuses. Traditional methods mainly rely on static analysis or dynamic monitoring, which is either software sensitive or time-consuming. Recently, with the development of machine learning and deep learning techniques, many efforts introduced such learning-based techniques into the Android malware detection and achieved promising detection results as well as substantially reduced time costing. Nevertheless, there is still a lack of a comprehensive summary at the deep learning technical level for these learning-based malware detection works. As a result, it is limited to improve the detection technique referring to existing works from a global map. To address the challenge, in this paper, we systematically study existing deep learning-based malware detection works and classify them from the technique perspective. We also conclude the advantages and threats within each category of detection technique and provide a concrete technical reference for future improvement work.","estimate, deep learning, malware classification, malware detection","","ICSCA 2020"
"Journal Article","Walker A,Cerny T","On Cloud Computing Infrastructure for Existing Code-Clone Detection Algorithms","SIGAPP Appl. Comput. Rev.","2020","20","1","5–14","Association for Computing Machinery","New York, NY, USA","","","2020-04","","1559-6915","https://doi.org/10.1145/3392350.3392351;http://dx.doi.org/10.1145/3392350.3392351","10.1145/3392350.3392351","Microservice Architecture (MSA) is becoming a design standard for modern cloud-based software systems. However, even though cloud-based applications have been thoroughly explored with regards to networking, scalability, and decomposition of existing monolithic applications into MSA based applications, not much research has been done showing the viability of MSA in new problem domains. In this paper, we explore the application of MSA to the code-clone detection problem domain to identify any improvements that can be made over existing local code-clone detection applications. A fragment of source code that is identical or similar to another is a code-clone. Code-clones make it difficult to maintain applications as they create multiple points within the code that bugs must be fixed, new rules enforced, or design decisions imposed. As applications grow larger and larger, the pervasiveness of code-clones likewise grows. To face the code-clone related issues, many tools and algorithms have been proposed to find and document code-clones within an application. In this paper, we show that many improvements can be made by utilizing emerging cloud-based technologies.","clone detection, microservices, scalable code clone detection, cloud computing, software as a service, code clone","",""
"Conference Paper","Bergsten L,Johansson BJ,Berggren P,van Laere J,Ibrahim O,Larsson A,Olsson L","Designing Engaging Computer Based Simulation Games for Increasing Societal Resilience to Payment System Disruptions","","2020","","","166–172","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business","Tokyo, Japan","2020","9781450376778","","https://doi.org/10.1145/3383845.3383859;http://dx.doi.org/10.1145/3383845.3383859","10.1145/3383845.3383859","Large or lengthy disruptions to the card payment system are threats that can cause crisis in society, especially in countries where other payment options are scarce. This paper presents a study that provides suggestions on how to improve a simulation game used to increase societal resilience to payment system disruptions. Questionnaires and interviews have been used to investigate how 16 participant in crisis exercises experience realism, relevance and validity in such exercises. Suggestions on how to improve the simulation game are provided, such as improvements to the graphical interface and introducing supporting roles from the exercise management.","Simulator design, Critical infrastructure, Resilience, Crisis response, Payment system","","ICCMB 2020"
"Conference Paper","Gholamian S,Ward PA","Logging Statements' Prediction Based on Source Code Clones","","2020","","","82–91","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM Symposium on Applied Computing","Brno, Czech Republic","2020","9781450368667","","https://doi.org/10.1145/3341105.3373845;http://dx.doi.org/10.1145/3341105.3373845","10.1145/3341105.3373845","Log files are widely used to record runtime information of software systems, such as the time-stamp of an event, the unique ID of the source of the log, and a part of the state of task execution. The rich information of logs enables system operators to monitor the runtime behaviors of their systems and further track down system problems in production settings. Although logs are useful, there exists a trade-off between their benefit and cost, and it is a crucial problem to optimize the location and content of log messages in the source code, i.e., ""where and what to log?""Prior research has analyzed logging statements in the source code and proposed ways to predict and suggest the location of log statements in order to partially automate log statement addition to the source code. However, there are gaps and unsolved problems in the literature to fully automate the logging process. Thus, in this research, we perform an experimental study on open-source Java projects and apply code-clone detection methods for log statements' prediction. Our work demonstrates the feasibility of logging automation by predicting the location of a log point in a code snippet based on the existence of a logging statement in its corresponding code clone pair. We propose a Log-Aware Code-Clone Detector (LACC) which achieves a higher accuracy of log prediction when compared to state-of-the-art general-purpose clone detectors. Our analysis shows that 98% of clone snippets match in their logging behavior, and LACC can predict the location of logging statements by the accuracy of 90+% for Apache Java projects.","source code, code clones, software engineering, automation, logging statement","","SAC '20"
"Conference Paper","Corradini F,Marcelletti A,Morichetta A,Polini A,Re B,Tiezzi F","Engineering Trustable Choreography-Based Systems Using Blockchain","","2020","","","1470–1479","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM Symposium on Applied Computing","Brno, Czech Republic","2020","9781450368667","","https://doi.org/10.1145/3341105.3373988;http://dx.doi.org/10.1145/3341105.3373988","10.1145/3341105.3373988","The adoption of model-driven engineering methodologies contributes to reduce the complexity of developing distributed systems. A key point to master such complexity is the use of modelling languages, such as the BPMN standard. This permits to specify choreography diagrams describing, from a global point of view, the interactions that should occur among distributed components in order to reach given goals. Even though BPMN choreographies are promising to increase business possibilities, their concrete adoption has been challenging and faced complex hurdles. On the one hand, there is a lack of concrete support to the different phases of the choreography life-cycle, especially in relation to the choreography execution. Another obstacle consists in the lack of distributed infrastructures allowing the participants involved in the cooperation to trust each other, and in particular to get enough guarantees that all of them will behave as prescribed by the choreography model.In this paper, we face such challenges by proposing a methodology and a related model-driven framework, named ChorChain, that are based on the blockchain technology. We provide support to the whole life-cycle of choreographies, from their modelling to their distributed execution. More specifically, ChorChain takes as input a BPMN choreography model and automatically translates it in a Solidity smart contract. Such a contract will permit to enforce the interactions among the cooperating participants, so to satisfy the prescriptions reported in the starting model. The methodology and the framework have been evaluated through experiments conducted on the Rinkeby Ethereum Testnet.","","","SAC '20"
"Conference Paper","Dashevskyi S,Zhauniarovich Y,Gadyatskaya O,Pilgun A,Ouhssain H","Dissecting Android Cryptocurrency Miners","","2020","","","191–202","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy","New Orleans, LA, USA","2020","9781450371070","","https://doi.org/10.1145/3374664.3375724;http://dx.doi.org/10.1145/3374664.3375724","10.1145/3374664.3375724","Cryptojacking applications pose a serious threat to mobile devices. Due to the extensive computations, they deplete the battery fast and can even damage the device. In this work we make a step towards combating this threat. We collected and manually verified a large dataset of Android mining apps. In this paper, we analyze the gathered miners and identify how they work, what are the most popular libraries and APIs used to facilitate their development, and what static features are typical for this class of applications. Further, we analyzed our dataset using VirusTotal. The majority of our samples is considered malicious by at least one VirusTotal scanner, but 16 apps are not detected by any engine; and at least 5 apks were not seen previously by the service. Mining code could be obfuscated or fetched at runtime, and there are many confusing miner-related apps that actually do not mine. Thus, static features alone are not sufficient for miner detection. We have collected a feature set of dynamic metrics both for miners and unrelated benign apps, and built a machine learning-based tool for dynamic detection. Our BrenntDroid tool is able to detect miners with 95% of accuracy on our dataset.","android, cpu mining, malware, cryptojacking, cryptominer","","CODASPY '20"
"Conference Paper","Cheers H,Lin Y","A Novel Graph-Based Program Representation for Java Code Plagiarism Detection","","2020","","","115–122","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Conference on Software Engineering and Information Management","Sydney, NSW, Australia","2020","9781450376907","","https://doi.org/10.1145/3378936.3378960;http://dx.doi.org/10.1145/3378936.3378960","10.1145/3378936.3378960","Source code plagiarism is a long-standing issue in undergraduate computer science education. Identifying instances of source code plagiarism is a difficult and time-consuming task. To aid in its identification, many automated tools have been proposed to find indications of plagiarism. However, prior works have shown that common source code plagiarism detection tools are susceptible to plagiarism-hiding transformations. In this paper a novel graph-based representation of Java programs is presented which is resilient to plagiarism-hiding transformations. This graph is titled the Program Interaction Dependency Graph (PIDG) and represents the interaction and transformation of data within a program, and how this data interacts with the system. To show the effectiveness of this graph, it is evaluated on a data set of simulated source code plagiarism. The results of this evaluation indicate the PIDG is a promising means of representing programs in a form that is resilient to plagiarism.","Program similarity, Source code plagiarism detection, Program representation, Dependency graph","","ICSIM '20"
"Conference Paper","Stuurman S,Passier HJ,Geven F,Barendsen E","Autism: Implications for Inclusive Education with Respect to Software Engineering","","2020","","","15–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Computer Science Education Research Conference","Larnaca, Cyprus","2020","9781450377171","","https://doi.org/10.1145/3375258.3375261;http://dx.doi.org/10.1145/3375258.3375261","10.1145/3375258.3375261","Within Computer science and Software engineering, the prevalence of students with a diagnosis of autism spectrum disorder is relatively high. Ideally, education should be inclusive, with which we mean that education must be given in such a way that additional support is needed as little as possible.In this paper, we present an overview on what is known about the cognitive style of autistic individuals and compare that cognitive thinking style with computational thinking, thinking as an engineer, and with academic thinking. We illustrate the cognitive style of autistic students with anecdotes from our students.From the comparison, we derive a set of guidelines for inclusive education, and we present ideas for future work.","Inclusive education, Autism, Cognitive thinking style","","CSERC '19"
"Conference Paper","Power JF,Waldron J","Calibration and Analysis of Source Code Similarity Measures for Verilog Hardware Description Language Projects","","2020","","","420–426","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st ACM Technical Symposium on Computer Science Education","Portland, OR, USA","2020","9781450367936","","https://doi.org/10.1145/3328778.3366928;http://dx.doi.org/10.1145/3328778.3366928","10.1145/3328778.3366928","In this paper we report on our experiences during a first-year course on digital logic design using the Verilog hardware description language. As part of the course the students were given a series of take-home assignments, which were then marked using an automated assessment system developed by the authors. During the course the instructor was made aware that a set of solutions had been circulated to the students, and was asked to assess the impact that this had on the assessment regime. In order to answer this question, we examined and implemented a number of approaches to calculating similarity between Verilog programs, and we present the results of that study in this paper. An important feature of this work was ensuring that the measurements used were well-understood, properly calibrated and defensible. We report on the results of this study, applied to a class of 115 students who completed up to 11 projects each.","program similarity, automated assessment, hardware description language","","SIGCSE '20"
"Conference Paper","Ericson BJ,Miller BN","Runestone: A Platform for Free, On-Line, and Interactive Ebooks","","2020","","","1012–1018","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st ACM Technical Symposium on Computer Science Education","Portland, OR, USA","2020","9781450367936","","https://doi.org/10.1145/3328778.3366950;http://dx.doi.org/10.1145/3328778.3366950","10.1145/3328778.3366950","The Runestone platform is open-source, extensible, and serves free ebooks to over 25,000 learners a day from around the world. The site hosts 18 ebooks for computing courses. Some of these ebook have been translated into several languages. There are ebooks for secondary computer science (AP CSP and AP CSA), CS1, CS2, data science, and web programming courses. The platform currently supports executable and editable examples in Python, Java, C, C++, HTML, JavaScript, Processing, and SQL. Runestone provides features for instructors, learners, authors, and researchers. Instructors can create a custom course from any of the existing ebooks and their students can register for that course. Instructors can create assignments from the existing material or author new problems, grade assignments, and visualize student progress. Learners can execute and modify examples and answer practice questions with immediate feedback. Runestone includes common practice types, such as multiple-choice questions, as well as some unique types, such as adaptive Parsons problems. Authors can modify the existing ebooks or write new ebooks using restructuredText: a markup language. Researchers can create and test new interactive features, run experiments, and analyze log file data. This paper describes the architecture of the platform, highlights some of the unique features, provides an overview of how instructors use the platform, summarizes the research studies conducted on the platform, and describes plans for future development.","adaptive learning, intelligent ebooks, ebooks, on-line learning, parsons problems, practice tools","","SIGCSE '20"
"Conference Paper","Thakur M,Nandivada VK","Mix Your Contexts Well: Opportunities Unleashed by Recent Advances in Scaling Context-Sensitivity","","2020","","","27–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th International Conference on Compiler Construction","San Diego, CA, USA","2020","9781450371209","","https://doi.org/10.1145/3377555.3377902;http://dx.doi.org/10.1145/3377555.3377902","10.1145/3377555.3377902","Existing precise context-sensitive heap analyses do not scale well for large OO programs. Further, identifying the right context abstraction becomes quite intriguing as two of the most popular categories of context abstractions (call-site- and object-sensitive) lead to theoretically incomparable precision. In this paper, we address this problem by first doing a detailed comparative study (in terms of precision and efficiency) of the existing approaches, both with and without heap cloning. In addition, we propose novel context abstractions that lead to a new sweet-spot in the arena. We first enhance the precision of level-summarized relevant value (LSRV) contexts (a highly scalable abstraction with precision matching that of call-site-sensitivity) using heap cloning. Then, motivated by the resultant scalability, we propose the idea of mixing various context abstractions, and add the advantages of k-object-sensitive analyses to LSRV contexts, in an efficient manner. The resultant context abstraction, which we call lsrvkobjH, also leads to a novel connection between the two broad variants of otherwise incomparable context-sensitive analyses. Our evaluation shows that the newer proposals not only enhance the precision of both LSRV contexts and object-sensitive analyses (to perform control-flow analysis of Java programs), but also scale well to large programs.","Java, Context-sensitivity, Static analysis","","CC 2020"
"Conference Paper","Nafi KW,Kar TS,Roy B,Roy CK,Schneider KA","CLCDSA: Cross Language Code Clone Detection Using Syntactical Features and API Documentation","","2020","","","1026–1037","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00099;http://dx.doi.org/10.1109/ASE.2019.00099","10.1109/ASE.2019.00099","Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is twofold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.","API documentation, code clone, Word2Vector, source code syntax","","ASE '19"
"Conference Paper","Wan Y,Shu J,Sui Y,Xu G,Zhao Z,Wu J,Yu PS","Multi-Modal Attention Network Learning for Semantic Source Code Retrieval","","2020","","","13–25","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00012;http://dx.doi.org/10.1109/ASE.2019.00012","10.1109/ASE.2019.00012","Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.","multi-modal network, deep learning, code retrieval, attention mechanism","","ASE '19"
"Conference Paper","Liu X,Huang L,Ge J,Ng V","Predicting Licenses for Changed Source Code","","2020","","","686–697","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00070;http://dx.doi.org/10.1109/ASE.2019.00070","10.1109/ASE.2019.00070","Open source software licenses regulate the circumstances under which software can be redistributed, reused and modified. Ensuring license compatibility and preventing license restriction conflicts among source code during software changes are the key to protect their commercial use. However, selecting the appropriate licenses for software changes requires lots of experience and manual effort that involve examining, assimilating and comparing various licenses as well as understanding their relationships with software changes. Worse still, there is no state-of-the-art methodology to provide this capability. Motivated by this observation, we propose in this paper Automatic License Prediction (ALP), a novel learning-based method and tool for predicting licenses as software changes. An extensive evaluation of ALP on predicting licenses in 700 open source projects demonstrate its effectiveness: ALP can achieve not only a high overall prediction accuracy (92.5% in micro F1 score) but also high accuracies across all license types.","mining software repository, software license prediction","","ASE '19"
"Conference Paper","Tokumoto S,Takayama K","PHANTA: Diversified Test Code Quality Measurement for Modern Software Development","","2020","","","1206–1207","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00138;http://dx.doi.org/10.1109/ASE.2019.00138","10.1109/ASE.2019.00138","Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study.","","","ASE '19"
"Conference Paper","Wei B","Retrieve and Refine: Exemplar-Based Neural Comment Generation","","2020","","","1250–1252","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00152;http://dx.doi.org/10.1109/ASE.2019.00152","10.1109/ASE.2019.00152","Code comment generation is a crucial task in the field of automatic software development. Most previous neural comment generation systems used an encoder-decoder neural network and encoded only information from source code as input. Software reuse is common in software development. However, this feature has not been introduced to existing systems. Inspired by the traditional IR-based approaches, we propose to use the existing comments of similar source code as exemplars to guide the comment generation process. Based on an open source search engine, we first retrieve a similar code and treat its comment as an exemplar. Then we applied a seq2seq neural network to conduct an exemplar-based comment generation. We evaluate our approach on a large-scale Java corpus, and experimental results demonstrate that our model significantly outperforms the state-of-the-art methods.","program comprehension, comment generation, deep learning","","ASE '19"
"Conference Paper","Gu X,Zhang H,Kim S","CodeKernel: A Graph Kernel Based Approach to the Selection of API Usage Examples","","2020","","","590–601","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00061;http://dx.doi.org/10.1109/ASE.2019.00061","10.1109/ASE.2019.00061","Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples.We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative graph from each cluster using designed ranking metrics. Our empirical evaluation shows that CodeKernel selects more accurate code examples than the related work (MUSE and eXoaDocs). A user study involving 25 developers in a multinational company also confirms the usefulness of CodeKernel in selecting API usage examples.","","","ASE '19"
"Conference Paper","Kang HJ,Bissyandé TF,Lo D","Assessing the Generalizability of Code2vec Token Embeddings","","2020","","","1–12","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00011;http://dx.doi.org/10.1109/ASE.2019.00011","10.1109/ASE.2019.00011","Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for.In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.","distributed representations, code embeddings, big code","","ASE '19"
"Conference Paper","Feng M,Yuan Z,Li F,Ban G,Xiao Y,Wang S,Tang Q,Su H,Yu C,Xu J,Piao A,Xue J,Huo W","B2SFinder: Detecting Open-Source Software Reuse in COTS Software","","2020","","","1038–1049","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00100;http://dx.doi.org/10.1109/ASE.2019.00100","10.1109/ASE.2019.00100","COTS software products are developed extensively on top of OSS projects, resulting in OSS reuse vulnerabilities. To detect such vulnerabilities, finding OSS reuses in COTS software has become imperative. While scalable to tens of thousands of OSS projects, existing binary-to-source matching approaches are severely imprecise in analyzing COTS software products, since they support only a limited number of code features, compute matching scores only approximately in measuring OSS reuses, and neglect the code structures in OSS projects.We introduce a novel binary-to-source matching approach, called B2SFinder1, to address these limitations. First of all, B2SFinder can reason about seven kinds of code features that are traceable in both binary and source code. In order to compute matching scores precisely, B2SFinder employs a weighted feature matching algorithm that combines three matching methods (for dealing with different code features) with two importance-weighting methods (for computing the weight of an instance of a code feature in a given COTS software application based on its specificity and occurrence frequency). Finally, B2SFinder identifies different types of code reuses based on matching scores and code structures of OSS projects. We have implemented B2SFinder using an optimized data structure. We have evaluated B2SFinder using 21991 binaries from 1000 popular COTS software products and 2189 candidate OSS projects. Our experimental results show that B2SFinder is not only precise but also scalable. Compared with the state of the art, B2SFinder has successfully found up to 2.15x as many reuse cases in 53.85 seconds per binary file on average. We also discuss how B2SFinder can be leveraged in detecting OSS reuse vulnerabilities in practice.","binary-to-source matching, OSS, code feature, one-day vulnerability, code reuse, COTS software","","ASE '19"
"Conference Paper","Nam D,Horvath A,Macvean A,Myers B,Vasilescu B","MARBLE: Mining for Boilerplate Code to Identify API Usability Problems","","2020","","","615–627","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00063;http://dx.doi.org/10.1109/ASE.2019.00063","10.1109/ASE.2019.00063","Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: ""boilerplate code."" We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances.","","","ASE '19"
"Conference Paper","Zhou S,Shen B,Zhong H","Lancer: Your Code Tell Me What You Need","","2020","","","1202–1205","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00137;http://dx.doi.org/10.1109/ASE.2019.00137","10.1109/ASE.2019.00137","Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.","language model, code reuse, code recommendation","","ASE '19"
"Conference Paper","Lacomis J,Yin P,Schwartz EJ,Allamanis M,Goues CL,Neubig G,Vasilescu B","DIRE: A Neural Approach to Decompiled Identifier Naming","","2020","","","628–639","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00064;http://dx.doi.org/10.1109/ASE.2019.00064","10.1109/ASE.2019.00064","The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.","","","ASE '19"
"Conference Paper","Wang M,Lin Z,Zou Y,Xie B","CoRA: Decomposing and Describing Tangled Code Changes for Reviewer","","2020","","","1050–1061","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00101;http://dx.doi.org/10.1109/ASE.2019.00101","10.1109/ASE.2019.00101","Code review is an important mechanism for code quality assurance both in open source software and industrial software. Reviewers usually suffer from numerous, tangled and loosely related code changes that are bundled in a single commit, which makes code review very difficult. In this paper, we propose CoRA (Code Review Assistant), an automatic approach to decompose a commit into different parts and generate concise descriptions for reviewers. More specifically, CoRA can decompose a commit into independent parts (e.g., bug fixing, new feature adding, or refactoring) by code dependency analysis and tree-based similar-code detection, then identify the most important code changes in each part based on the PageRank algorithm and heuristic rules. As a result, CoRA can generate a concise description for each part of the commit. We evaluate our approach in seven open source software projects and 50 code commits. The results indicate that CoRA can improve the accuracy of decomposing code changes by 6.3% over the state-of-art practice. At the same time, CoRA can identify the important part from the fine-grained code changes with a mean average precision (MAP) of 87.7%. We also conduct a human study with eight participants to evaluate the performance and usefulness of CoRA, the user feedback indicates that CoRA can effectively help reviewers.","program comprehension, code review, code changes description, code changes decomposition","","ASE '19"
"Conference Paper","Jiang L,Liu H,Jiang H","Machine Learning Based Recommendation of Method Names: How Far Are We","","2020","","","602–614","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00062;http://dx.doi.org/10.1109/ASE.2019.00062","10.1109/ASE.2019.00062","High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go.","machine learning, code recommendation","","ASE '19"
"Conference Paper","Hu Y,Ahmed UZ,Mechtaev S,Leong B,Roychoudhury A","Re-Factoring Based Program Repair Applied to Programming Assignments","","2020","","","388–398","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00044;http://dx.doi.org/10.1109/ASE.2019.00044","10.1109/ASE.2019.00044","Automated program repair has been used to provide feedback for incorrect student programming assignments, since program repair captures the code modification needed to make a given buggy program pass a given test-suite. Existing student feedback generation techniques are limited because they either require manual effort in the form of providing an error model, or require a large number of correct student submissions to learn from, or suffer from lack of scalability and accuracy.In this work, we propose a fully automated approach for generating student program repairs in real-time. This is achieved by first re-factoring all available correct solutions to semantically equivalent solutions. Given an incorrect program, we match the program with the closest matching refactored program based on its control flow structure. Subsequently, we infer the input-output specifications of the incorrect program's basic blocks from the executions of the correct program's aligned basic blocks. Finally, these specifications are used to modify the blocks of the incorrect program via search-based synthesis.Our dataset consists of almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. Our experimental results suggest that our method is more effective and efficient than recently proposed feedback generation approaches. About 30% of the patches produced by our tool Refactory are smaller than those produced by the state-of-art tool Clara, and can be produced given fewer correct solutions (often a single correct solution) and in a shorter time. We opine that our method is applicable not only to programming assignments, and could be seen as a general-purpose program repair method that can achieve good results with just a single correct reference solution.","programming education, software refactoring, program repair","","ASE '19"
"Conference Paper","Jiang J,Ren L,Xiong Y,Zhang L","Inferring Program Transformations from Singular Examples via Big Code","","2020","","","255–266","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00033;http://dx.doi.org/10.1109/ASE.2019.00033","10.1109/ASE.2019.00033","Inferring program transformations from concrete program changes has many potential uses, such as applying systematic program edits, refactoring, and automated program repair. Existing work for inferring program transformations usually rely on statistical information over a potentially large set of program-change examples. However, in many practical scenarios we do not have such a large set of program-change examples.In this paper, we address the challenge of inferring a program transformation from one single example. Our core insight is that ""big code"" can provide effective guide for the generalization of a concrete change into a program transformation, i.e., code elements appearing in many files are general and should not be abstracted away. We first propose a framework for transformation inference, where programs are represented as hypergraphs to enable fine-grained generalization of transformations. We then design a transformation inference approach, GenPat, that infers a program transformation based on code context and statistics from a big code corpus.We have evaluated GenPat under two distinct application scenarios, systematic editing and program repair. The evaluation on systematic editing shows that GenPat significantly outperforms a state-of-the-art approach, Sydit, with up to 5.5x correctly transformed cases. The evaluation on program repair suggests that GenPat has the potential to be integrated in advanced program repair tools - GenPat successfully repaired 19 real-world bugs in the Defects4J benchmark by simply applying transformations inferred from existing patches, where 4 bugs have never been repaired by any existing technique. Overall, the evaluation results suggest that GenPat is effective for transformation inference and can potentially be adopted for many different applications.","code abstraction, pattern generation, program adaptation","","ASE '19"
"Conference Paper","Feng Q,Cai Y,Kazman R,Cui D,Liu T,Fang H","Active Hotspot: An Issue-Oriented Model to Monitor Software Evolution and Degradation","","2020","","","986–997","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00095;http://dx.doi.org/10.1109/ASE.2019.00095","10.1109/ASE.2019.00095","Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely.","software evolution, architecture debt","","ASE '19"
"Conference Paper","Nejadgholi M,Yang J","A Study of Oracle Approximations in Testing Deep Learning Libraries","","2020","","","785–796","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00078;http://dx.doi.org/10.1109/ASE.2019.00078","10.1109/ASE.2019.00078","Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature.In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in testing DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries.","software quality assurance, software testing, test oracle, testing deep learning libraries","","ASE '19"
"Conference Paper","Michael LG,Donohue J,Davis JC,Lee D,Servant F","Regexes Are Hard: Decision-Making, Difficulties, and Risks in Programming Regular Expressions","","2020","","","415–426","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00047;http://dx.doi.org/10.1109/ASE.2019.00047","10.1109/ASE.2019.00047","Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face.In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer.In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.","developer process, regular expressions, qualitative research","","ASE '19"
"Conference Paper","Luo L,Bodden E,Späth J","A Qualitative Analysis of Android Taint-Analysis Results","","2020","","","102–114","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00020;http://dx.doi.org/10.1109/ASE.2019.00020","10.1109/ASE.2019.00020","In the past, researchers have developed a number of popular taint-analysis approaches, particularly in the context of Android applications. Numerous studies have shown that automated code analyses are adopted by developers only if they yield a good ""signal to noise ratio"", i.e., high precision. Many previous studies have reported analysis precision quantitatively, but this gives little insight into what can and should be done to increase precision further.To guide future research on increasing precision, we present a comprehensive study that evaluates static Android taint-analysis results on a qualitative level. To unravel the exact nature of taint flows, we have designed COVA, an analysis tool to compute partial path constraints that inform about the circumstances under which taint flows may actually occur in practice.We have conducted a qualitative study on the taint flows reported by FlowDroid in 1,022 real-world Android applications. Our results reveal several key findings: Many taint flows occur only under specific conditions, e.g., environment settings, user interaction, I/O. Taint analyses should consider the application context to discern such situations. COVA shows that few taint flows are guarded by multiple different kinds of conditions simultaneously, so tools that seek to confirm true positives dynamically can concentrate on one kind at a time, e.g., only simulating user interactions. Lastly, many false positives arise due to a too liberal source/sink configuration. Taint analyses must be more carefully configured, and their configuration could benefit from better tool assistance.","taint analysis, path conditions, android","","ASE '19"
"Conference Paper","Schlie A,Schulze S,Schaefer I","Recovering Variability Information from Source Code of Clone-and-Own Software Systems","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems","Magdeburg, Germany","2020","9781450375016","","https://doi.org/10.1145/3377024.3377034;http://dx.doi.org/10.1145/3377024.3377034","10.1145/3377024.3377034","Clone-and-own prevails as an ad-hoc reuse strategy that addresses changing requirements by copying and modifying existing system variants. Proper documentation is typically not cherished and knowledge about common and varying parts between individual variants, denoted their variability information, is lost with a growing system family. With overall maintainability impaired in the longrun, software product lines (SPLs) or concepts thereof, can be a remedy. However, migrating a system family towards structured reuse requires a prior recovery of the systems' variability information. For software systems resulting from clone-and-own, this information is not explicitly available and recovering it remains an open challenge.We aim to bridge this gap and propose a fine-grained metric and analysis procedure, which compares software systems to the extent of individual statements including their nesting. By that, we recover variability information from software systems written in imperative programming languages. Moreover, we create a software family representation of all analyzed systems, called a 150% model, which contains implementation artifacts and their identified variability information. We demonstrate the feasibility of our approach using two case studies implemented in Java and show our approach to exhibit a good performance and the 150% model to precisely capture variability information of the analyzed systems.","variability, clone-and-own, recovering, 150% model, source code","","VaMoS '20"
"Conference Paper","Bordis T,Runge T,Knüppel A,Thüm T,Schaefer I","Variational Correctness-by-Construction","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems","Magdeburg, Germany","2020","9781450375016","","https://doi.org/10.1145/3377024.3377038;http://dx.doi.org/10.1145/3377024.3377038","10.1145/3377024.3377038","Nowadays, the requirements for software and therefore also the required complexity is increasing steadily. Consequently, various techniques to handle the growing demand for software variants in one specific domain are used. These techniques often rely on variable code structures to implement a whole product family more efficiently. Variational software is also increasingly used for safety-critical systems, which need to be verified to guarantee their functionality in-field. However, usual verification techniques can not directly be applied to the variable code structures of most techniques. In this paper, we propose variational correctness-by-construction as a methodology to implement variational software extending the correctness-by-construction approach. Correctness-by-construction is an incremental approach to create and verify programs using small tractable refinement steps guided by a specification following the design-by-contract paradigm. Our contribution is threefold. First, we extend the list of refinement rules to enable variability in programs developed with correctness-by-construction. Second, we motivate the need for contract composition of refined method contracts and illustrate how this can be achieved. Third, we implement variational correctness-by-construction in a tool called VarCorC. We successfully conducted two case studies showing the applicability of VarCorC and were able to assess reduced verification costs compared to post-hoc verification as well.","variational software, formal methods, correctness-by-construction, design-by-contract, deductive verification","","VaMoS '20"
"Journal Article","Ahmed SS,Roy S,Kalita J","Assessing the Effectiveness of Causality Inference Methods for Gene Regulatory Networks","IEEE/ACM Trans. Comput. Biol. Bioinformatics","2020","17","1","56–70","IEEE Computer Society Press","Washington, DC, USA","","","2020-02","","1545-5963","https://doi.org/10.1109/TCBB.2018.2853728;http://dx.doi.org/10.1109/TCBB.2018.2853728","10.1109/TCBB.2018.2853728","Causality inference is the use of computational techniques to predict possible causal relationships for a set of variables, thereby forming a directed network. Causality inference in Gene Regulatory Networks (GRNs) is an important, yet challenging task due to the limits of available data and lack of efficiency in existing causality inference techniques. A number of techniques have been proposed and applied to infer causal relationships in various domains, although they are not specific to regulatory network inference. In this paper, we assess the effectiveness of methods for inferring causal GRNs. We introduce seven different inference methods and apply them to infer directed edges in GRNs. We use time-series expression data from the DREAM challenges to assess the methods in terms of quality of inference and rank them based on performance. The best method is applied to Breast Cancer data to infer a causal network. Experimental results show that Causation Entropy is best, however, highly time-consuming and not feasible to use in a relatively large network. We infer Breast Cancer GRN with the second-best method, Transfer Entropy. The topological analysis of the network reveals that top out-degree genes such as SLC39A5 which are considered central genes, play important role in cancer progression.","","",""
"Conference Paper","Cheers H,Lin Y,Smith SP","Detecting Pervasive Source Code Plagiarism through Dynamic Program Behaviours","","2020","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373168;http://dx.doi.org/10.1145/3373165.3373168","10.1145/3373165.3373168","Source code plagiarism is a persistent problem in undergraduate computer science education. Unfortunately, it is a widespread phenomena with many students plagiarising either because they are unwilling or incapable of completing their own work. Many source code plagiarism detection tools have been proposed to identify suspected cases of source code plagiarism. However, these tools are not resilient to pervasive plagiarism-hiding transformations that significantly change the structure of source code. In this paper, two case studies are presented that explore how resilient current source code plagiarism detection tools are to plagiarism-hiding transformations. Furthermore, an evaluation of a new advanced technique for source code plagiarism detection is presented to show that is it possible to identify pervasive cases of source code plagiarism. The results of this evaluation indicate the technique is robust in its ability to identify the same program after it has been transformed.","Program similarity, Source code plagiarism detection, Source code obfuscation","","ACE'20"
"Conference Paper","Karnalim O,Simon","Syntax Trees and Information Retrieval to Improve Code Similarity Detection","","2020","","","48–55","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373171;http://dx.doi.org/10.1145/3373165.3373171","10.1145/3373165.3373171","In dealing with source code plagiarism and collusion, automated code similarity detection can be used to filter student submissions and draw attention to pairs of programs that appear unduly similar. The effectiveness of the detection process can be improved by considering more structural information about each program, but the ensuing computation can increase the processing time. This paper proposes a similarity detection technique that uses richer structural information than normal while maintaining a reasonable execution time. The technique generates the syntax trees of program code files, extracts directly connected n-gram structure tokens from them, and performs the subsequent comparisons using an algorithm from information retrieval, cosine correlation in the vector space model. Evaluation of the approach shows that consideration of the program structure (i.e., syntax tree) increases the recall and f-score (measures of effectiveness) at the expense of execution time (a measure of efficiency). However, the use of an information retrieval comparison process goes some way to offsetting this loss of efficiency.","plagiarism and collusion in programming, information retrieval, syntax tree, source code similarity detection, computing education","","ACE'20"
"Conference Paper","Pelchen T,Mathieson L,Lister R","On the Evidence for a Learning Hierarchy in Data Structures Exams","","2020","","","122–131","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373179;http://dx.doi.org/10.1145/3373165.3373179","10.1145/3373165.3373179","Several previous research studies have found a relationship between the ability of novices to trace and explain code, and the ability to write code. Harrington and Cheng refer to that relationship as the Learning Hierarchy. However, almost all of those studies examined students at the end of their first semester of learning to program (i.e. CS1). This paper is only the third paper to describe a study of explain in plain English questions on students at the end of an introductory data structures course. The preceding two papers reached contradictory conclusions. Corney et al. presented results consistent with the Learning Hierarchy identified in the CS1 studies. However, Harrington and Cheng presented results for data structures students suggesting that the hierarchy reversed by the time students had progressed to the level of learning about data structures; that is, tracing and explaining were skills that followed writing. In our study of data structures students, we present results that are consistent with the Learning Hierarchy derived from the CS1 students. We believe that the reversal identified by Harrington and Cheng can occur, but only as a consequence of a mismatch in the relative difficulty of tracing, explaining and writing questions.","explain in plain English, programming, data structures","","ACE'20"
"Journal Article","Haas R,Niedermayr R,Roehm T,Apel S","Is Static Analysis Able to Identify Unnecessary Source Code?","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3368267;http://dx.doi.org/10.1145/3368267","10.1145/3368267","Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way.We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closed-source software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects. Our study shows that recommendations generated from stability and centrality information point to unnecessary code that cannot be identified by dead code detectors. Developers confirmed that 34% of recommendations were indeed unnecessary and deleted 20% of the recommendations shortly after our interviews. Overall, our results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice.","Unnecessary code, code stability, code centrality","",""
"Journal Article","Yuan Y,Banzhaf W","Toward Better Evolutionary Program Repair: An Integrated Approach","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3360004;http://dx.doi.org/10.1145/3360004","10.1145/3360004","Bug repair is a major component of software maintenance, which requires a huge amount of manpower. Evolutionary computation, particularly genetic programming (GP), is a class of promising techniques for automating this time-consuming and expensive process. Although recent research in evolutionary program repair has made significant progress, major challenges still remain. In this article, we propose ARJA-e, a new evolutionary repair system for Java code that aims to address challenges for the search space, search algorithm, and patch overfitting. To determine a search space that is more likely to contain correct patches, ARJA-e combines two sources of fix ingredients (i.e., the statement-level redundancy assumption and repair templates) with contextual analysis-based search space reduction, thereby leveraging their complementary strengths. To encode patches in GP more properly, ARJA-e unifies the edits at different granularities into statement-level edits and then uses a lower-granularity patch representation that is characterized by the decoupling of statements for replacement and statements for insertion. ARJA-e also uses a finer-grained fitness function that can make full use of semantic information contained in the test suite, which is expected to better guide the search of GP. To alleviate patch overfitting, ARJA-e further includes a postprocessing tool that can serve the purposes of overfit detection and patch ranking. We evaluate ARJA-e on 224 real Java bugs from Defects4J and compare it with the state-of-the-art repair techniques. The evaluation results show that ARJA-e can correctly fix 39 bugs in terms of the patches ranked first, achieving substantial performance improvements over the state of the art. In addition, we analyze the effect of the components of ARJA-e qualitatively and quantitatively to demonstrate their effectiveness and advantages.","genetic improvement, genetic programming, program repair, Evolutionary computation","",""
"Journal Article","Chen L,Wu D,Ma W,Zhou Y,Xu B,Leung H","How C++ Templates Are Used for Generic Programming: An Empirical Study on 50 Open Source Systems","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3356579;http://dx.doi.org/10.1145/3356579","10.1145/3356579","Generic programming is a key paradigm for developing reusable software components. The inherent support for generic constructs is therefore important in programming languages. As for C++, the generic construct, templates, has been supported since the language was first released. However, little is currently known about how C++ templates are actually used in developing real software. In this study, we conduct an experiment to investigate the use of templates in practice. We analyze 1,267 historical revisions of 50 open source systems, consisting of 566 million lines of C++ code, to collect the data of the practical use of templates. We perform statistical analyses on the collected data and produce many interesting results. We uncover the following important findings: (1) templates are practically used to prevent code duplication, but this benefit is largely confined to a few highly used templates; (2) function templates do not effectively replace C-style generics, and developers with a C background do not show significant preference between the two language constructs; (3) developers seldom convert dynamic polymorphism to static polymorphism by using CRTP (Curiously Recursive Template Pattern); (4) the use of templates follows a power-law distribution in most cases, and C++ developers who prefer using templates are those without other language background; (5) C developer background seems to override C++ project guidelines. These findings are helpful not only for researchers to understand the tendency of template use but also for tool builders to implement better tools to support generic programming.","empirical study, Programming language, template, C++, generic programming","",""
"Journal Article","Walker A,Cerny T,Song E","Open-Source Tools and Benchmarks for Code-Clone Detection: Past, Present, and Future Trends","SIGAPP Appl. Comput. Rev.","2020","19","4","28–39","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1559-6915","https://doi.org/10.1145/3381307.3381310;http://dx.doi.org/10.1145/3381307.3381310","10.1145/3381307.3381310","A fragment of source code that is identical or similar to another is a code-clone. Code-clones make it difficult to maintain applications as they create multiple points within the code that bugs must be fixed, new rules enforced, or design decisions imposed. As applications grow larger and larger, the pervasiveness of code-clones likewise grows. To face the code-clone related issues, many tools and algorithms have been proposed to find and document code-clones within an application. In this paper, we present the historical trends in code-clone detection tools to show how we arrived at the current implementations. We then present our results from a systematic mapping study on current (2009-2019) code-clone detection tools with regards to technique, open-source nature, and language coverage. Lastly, we propose future directions for code-clone detection tools. This paper provides the essentials to understanding the code-clone detection process and the current state-of-art solutions.","code clone, mapping study, survey, clone detection","",""
"Conference Paper","Panigutti C,Perotti A,Pedreschi D","Doctor XAI: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations","","2020","","","629–639","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency","Barcelona, Spain","2020","9781450369367","","https://doi.org/10.1145/3351095.3372855;http://dx.doi.org/10.1145/3351095.3372855","10.1145/3351095.3372855","Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","explainable artificial intelligence, healthcare data, machine learning","","FAT* '20"
"Conference Paper","Sato Y,Kameyama Y,Watanabe T","Module Generation without Regret","","2020","","","1–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation","New Orleans, LA, USA","2020","9781450370967","","https://doi.org/10.1145/3372884.3373160;http://dx.doi.org/10.1145/3372884.3373160","10.1145/3372884.3373160","Modules are an indispensable mechanism for providing abstraction to programming languages. To reduce the abstraction overhead in the usage of modules, Watanabe et al. proposed a language for generating and manipulating code of modules, and implemented it via a translation to plain MetaOCaml. Unfortunately, their solution has a serious problem of code explosion if functors are repeatedly applied to modules. Another problem in their solution is that it does not allow nested modules. This paper proposes a refined translation for a two-stage typed language with module generation where nested modules are allowed. Our translation does not suffer from the code-duplication problem. The key idea is to use the genlet operator in latest MetaOCaml, which performs let insertion at the code-generation time to allow sharing of code fragments. To our knowledge, our work is the first to apply genlet to code generation for modules. We conduct an experiment using a microbenchmark, and the result shows that our method is effective to reduce the size of generated code that would have been exponentially large.","Program Transformation, Modules, Type Safety, Program Generation","","PEPM 2020"
"Conference Paper","Vasileiadis L,Ceccato M,Corradini D","Revealing Malicious Remote Engineering Attempts on Android Apps with Magic Numbers","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Workshop on Software Security, Protection, and Reverse Engineering","San Juan, Puerto Rico, USA","2019","9781450377461","","https://doi.org/10.1145/3371307.3371312;http://dx.doi.org/10.1145/3371307.3371312","10.1145/3371307.3371312","Malicious reverse engineering is a prominent activity conducted by attackers to plan their code tampering attacks. Android apps are particularly exposed to malicious reverse engineering, because their code can be easily analyzed and decompiled, or monitored using debugging tools, that were originally meant to be used by developers.In this paper, we propose a solution to identify attempts of malicious reverse engineering on Android apps. Our approach is based on a series of periodic checks on the execution environment (i.e., Android components) and on the app itself. The check outcome is encoded into a Magic Number and send to a sever for validation. The owner of the app is then supposed to take countermeasures and react, by disconnecting or banning the apps under attack.Our empirical validation suggests that the execution overhead caused by our periodic checks is acceptable, because its resource consumption is compatible with the resources commonly available in smartphones.","remote attestation, code tampering, malicious reverse engineering","","SSPREW9 '19"
"Journal Article","Albluwi I","Plagiarism in Programming Assessments: A Systematic Review","ACM Trans. Comput. Educ.","2019","20","1","","Association for Computing Machinery","New York, NY, USA","","","2019-12","","","https://doi.org/10.1145/3371156;http://dx.doi.org/10.1145/3371156","10.1145/3371156","This article is a systematic review of work in the computing education literature on plagiarism. The goal of the review is to summarize the main results found in the literature and highlight areas that need further work. Despite the the large body of work on plagiarism, no systematic reviews have been published so far.The reviewed papers were categorized and analyzed using a theoretical framework from the field of Fraud Deterrence named the Fraud Triangle. According to this framework, fraudulent behavior occurs when the person is under pressure, perceives the availability of an opportunity to commit fraud, and rationalizes the fraudulent behavior in a way that makes it seem not unethical to him or her.The review found the largest amount of the reviewed papers to discuss ways for reducing the opportunity to plagiarize, as well as tools for detecting plagiarism. However, there is a clear lack of empirical work evaluating the deterrent efficacy of these strategies and tools. The reviewed papers also included mentions of a wide range of rationalizations used by computing students when justifying plagiarism, the most important of which are rationalizations that stem from confusion about what constitutes plagiarism. Finally, work on the relationship between pressure in computing courses and plagiarism was found to be very scarce and incommensurate with the significant contribution of this factor to plagiarism.","Introductory programming, academic integrity, cheating, plagiarism","",""
"Conference Paper","Ericson B,McCall A,Cunningham K","Investigating the Affect and Effect of Adaptive Parsons Problems","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th Koli Calling International Conference on Computing Education Research","Koli, Finland","2019","9781450377157","","https://doi.org/10.1145/3364510.3364524;http://dx.doi.org/10.1145/3364510.3364524","10.1145/3364510.3364524","In a Parsons problem the learner places mixed-up code blocks in the correct order to solve a problem. Parsons problems can be used for both practice and assessment in programming courses. While most students correctly solve Parsons problems, some do not. Unsuccessful practice is not conducive to learning, leads to frustration, and lowers self-efficacy. Ericson invented two types of adaptation for Parsons problems, intra-problem and inter-problem, in order to decrease frustration and maximize learning gains. In intra-problem adaptation, if the learner is struggling, the problem can dynamically be made easier. In inter-problem adaptation, the next problem's difficulty is modified based on the learner's performance on the last problem. This paper reports on the first observational studies of five undergraduate students and 11 secondary teachers solving both intra-problem adaptive and non-adaptive Parsons problems. It also reports on a log file analysis with data from over 8,000 users solving non-adaptive and adaptive Parsons problems. The paper reports on teachers' understanding of the intra-problem adaptation process, their preference for adaptive or non-adaptive Parsons problems, their perception of the usefulness of solving Parsons problems in helping them learn to fix and write similar code, and the effect of adaptation (both intra-problem and inter-problem) on problem correctness. Teachers understood most of the intra-problem adaptation process, but not all. Most teachers preferred adaptive Parsons problems and felt that solving Parsons problems helped them learn to fix and write similar code. Analysis of the log file data provided evidence that learners are nearly twice as likely to correctly solve adaptive Parsons problems than non-adaptive ones.","Parsons problems, self-efficacy, Parson's problems, adaptation","","Koli Calling '19"
"Conference Paper","Bauer M,Garland M","Legate NumPy: Accelerated and Distributed Array Computing","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","Denver, Colorado","2019","9781450362290","","https://doi.org/10.1145/3295500.3356175;http://dx.doi.org/10.1145/3295500.3356175","10.1145/3295500.3356175","NumPy is a popular Python library used for performing array-based numerical computations. The canonical implementation of NumPy used by most programmers runs on a single CPU core and is parallelized to use multiple cores for some operations. This restriction to a single-node CPU-only execution limits both the size of data that can be handled and the potential speed of NumPy code. In this work we introduce Legate, a drop-in replacement for NumPy that requires only a single-line code change and can scale up to an arbitrary number of GPU accelerated nodes. Legate works by translating NumPy programs to the Legion programming model and then leverages the scalability of the Legion runtime system to distribute data and computations across an arbitrary sized machine. Compared to similar programs written in the distributed Dask array library in Python, Legate achieves speed-ups of up to 10X on 1280 CPUs and 100X on 256 GPUs.","logical regions, control replication, NumPy, task-based runtimes, legion, GPU, Python, distributed execution, HPC, legate","","SC '19"
"Journal Article","Zheng Lnico,Albano CM,Vora NM,Mai F,Nickerson JV","The Roles Bots Play in Wikipedia","Proc.  ACM Hum. -Comput.  Interact.","2019","3","CSCW","","Association for Computing Machinery","New York, NY, USA","","","2019-11","","","https://doi.org/10.1145/3359317;http://dx.doi.org/10.1145/3359317","10.1145/3359317","Bots are playing an increasingly important role in the creation of knowledge in Wikipedia. In many cases, editors and bots form tightly knit teams. Humans develop bots, argue for their approval, and maintain them, performing tasks such as monitoring activity, merging similar bots, splitting complex bots, and turning off malfunctioning bots. Yet this is not the entire picture. Bots are designed to perform certain functions and can acquire new functionality over time. They play particular roles in the editing process. Understanding these roles is an important step towards understanding the ecosystem, and designing better bots and interfaces between bots and humans. This is important for understanding Wikipedia along with other kinds of work in which autonomous machines affect tasks performed by humans. In this study, we use unsupervised learning to build a nine category taxonomy of bots based on their functions in English Wikipedia. We then build a multi-class classifier to classify 1,601 bots based on labeled data. We discuss different bot activities, including their edit frequency, their working spaces, and their software evolution. We use a model to investigate how bots playing certain roles will have differential effects on human editors. In particular, we build on previous research on newcomers by studying the relationship between the roles bots play, the interactions they have with newcomers, and the ensuing survival rate of the newcomers.","taxonomy, online communities, roles, wikipedia, governance, bots","",""
"Conference Paper","Li Z,Chen QA,Xiong C,Chen Y,Zhu T,Yang H","Effective and Light-Weight Deobfuscation and Semantic-Aware Attack Detection for PowerShell Scripts","","2019","","","1831–1847","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3363187;http://dx.doi.org/10.1145/3319535.3363187","10.1145/3319535.3363187","In recent years, PowerShell is increasingly reported to appear in a variety of cyber attacks ranging from advanced persistent threat, ransomware, phishing emails, cryptojacking, financial threats, to fileless attacks. However, since the PowerShell language is dynamic by design and can construct script pieces at different levels, state-of-the-art static analysis based PowerShell attack detection approaches are inherently vulnerable to obfuscations. To overcome this challenge, in this paper we design the first effective and light-weight deobfuscation approach for PowerShell scripts. To address the challenge in precisely identifying the recoverable script pieces, we design a novel subtree-based deobfuscation method that performs obfuscation detection and emulation-based recovery at the level of subtrees in the abstract syntax tree of PowerShell scripts. Building upon the new deobfuscation method, we are able to further design the first semantic-aware PowerShell attack detection system. To enable semantic-based detection, we leverage the classic objective-oriented association mining algorithm and newly identify 31 semantic signatures for PowerShell attacks. We perform an evaluation on a collection of 2342 benign samples and 4141 malicious samples, and find that our deobfuscation method takes less than 0.5 seconds on average and meanwhile increases the similarity between the obfuscated and original scripts from only 0.5% to around 80%, which is thus both effective and light-weight. In addition, with our deobfuscation applied, the attack detection rates for Windows Defender and VirusTotal increase substantially from 0.3% and 2.65% to 75.0% and 90.0%, respectively. Furthermore, when our deobfuscation is applied, our semantic-aware attack detection system outperforms both Windows Defender and VirusTotal with a 92.3% true positive rate and a 0% false positive rate on average.","abstract syntax tree, powershell, semantic-aware, deobfuscation","","CCS '19"
"Conference Paper","Fass A,Backes M,Stock B","HideNoSeek: Camouflaging Malicious JavaScript in Benign ASTs","","2019","","","1899–1913","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3345656;http://dx.doi.org/10.1145/3319535.3345656","10.1145/3319535.3345656","In the malware field, learning-based systems have become popular to detect new malicious variants. Nevertheless, attackers with specific and internal knowledge of a target system may be able to produce input samples which are misclassified. In practice, the assumption of strong attackers is not realistic as it implies access to insider information. We instead propose HideNoSeek, a novel and generic camouflage attack, which evades the entire class of detectors based on syntactic features, without needing any information about the system it is trying to evade. Our attack consists of changing the constructs of malicious JavaScript samples to reproduce a benign syntax. For this purpose, we automatically rewrite the Abstract Syntax Trees (ASTs) of malicious JavaScript inputs into existing benign ones. In particular, HideNoSeek uses malicious seeds and searches for isomorphic subgraphs between the seeds and traditional benign scripts. Specifically, it replaces benign sub-ASTs by their malicious equivalents (same syntactic structure) and adjusts the benign data dependencies--without changing the AST--so that the malicious semantics is kept. In practice, we leveraged 23 malicious seeds to generate 91,020 malicious scripts, which perfectly reproduce ASTs of Alexa top 10,000 web pages. Also, we can produce on average 14 different malicious samples with the same AST as each Alexa top 10. Overall, a standard trained classifier has 99.98% false negatives with HideNoSeek inputs, while a classifier trained on such samples has over 88.74% false positives, rendering the targeted static detectors unreliable.","web security, AST, malicious JavaScript, adversarial attacks","","CCS '19"
"Conference Paper","He J,Balunović M,Ambroladze N,Tsankov P,Vechev M","Learning to Fuzz from Symbolic Execution with Application to Smart Contracts","","2019","","","531–548","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3363230;http://dx.doi.org/10.1145/3319535.3363230","10.1145/3319535.3363230","Fuzzing and symbolic execution are two complementary techniques for discovering software vulnerabilities. Fuzzing is fast and scalable, but can be ineffective when it fails to randomly select the right inputs. Symbolic execution is thorough but slow and often does not scale to deep program paths with complex path conditions. In this work, we propose to learn an effective and fast fuzzer from symbolic execution, by phrasing the learning task in the framework of imitation learning. During learning, a symbolic execution expert generates a large number of quality inputs improving coverage on thousands of programs. Then, a fuzzing policy, represented with a suitable architecture of neural networks, is trained on the generated dataset. The learned policy can then be used to fuzz new programs. We instantiate our approach to the problem of fuzzing smart contracts, a domain where contracts often implement similar functionality (facilitating learning) and security is of utmost importance. We present an end-to-end system, ILF (for Imitation Learning based Fuzzer), and an extensive evaluation over >18K contracts. Our results show that ILF is effective: (i) it is fast, generating 148 transactions per second, (ii) it outperforms existing fuzzers (e.g., achieving 33% more coverage), and (iii) it detects more vulnerabilities than existing fuzzing and symbolic execution tools for Ethereum.","imitation learning, smart contracts, fuzzing, symbolic execution","","CCS '19"
"Conference Paper","Gruss D,Kraft E,Tiwari T,Schwarz M,Trachtenberg A,Hennessey J,Ionescu A,Fogh A","Page Cache Attacks","","2019","","","167–180","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3339809;http://dx.doi.org/10.1145/3319535.3339809","10.1145/3319535.3339809","We present a new side-channel attack that targets one of the most fundamental software caches in modern computer systems: the operating system page cache. The page cache is a pure software cache that contains all disk-backed pages, including program binaries, shared libraries, and other files. On Windows, dynamic pages are also part of this cache and can be attacked as well, e.g., data, heap, and stacks. Our side channel permits unprivileged monitoring of accesses to these pages of other processes, with a spatial resolution of 4kB and a temporal resolution of 2µs on Linux (≤6.7 measurements per second), and 466ns on Windows 10 (≤223 measurements per second). We systematically analyze the side channel by demonstrating different hardware-agnostic local attacks, including a sandbox-bypassing high-speed covert channel, an ASLR break on Windows 10, and various information leakages that can be used for targeted extortion, spam campaigns, and more directly for UI redressing attacks. We also show that, as with hardware cache attacks, we can attack the generation of temporary passwords on vulnerable cryptographic implementations. Our hardware-agnostic attacks can be mitigated with our proposed security patches, but the basic side channel remains exploitable via timing measurements. We demonstrate this with a remote covert channel exfiltrating information from a colluding process through innocuous server requests.","operating systems, cache attacks, software-based attacks","","CCS '19"
"Conference Paper","Mondal M,Roy B,Roy CK,Schneider KA","Ranking Co-Change Candidates of Micro-Clones","","2019","","","244–253","IBM Corp.","USA","Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering","Toronto, Ontario, Canada","2019","","","","","Identical or nearly similar code fragments in a software system's code-base are known as code clones. Code clones from the same clone class have a tendency of co-changing (changing together) consistently during evolution. Focusing on this co-change tendency, existing studies have investigated prediction and ranking co-change candidates of regular clones. However, a recent study shows that micro-clones which are smaller than the minimum size threshold of regular clones might also need to be co-changed consistently during evolution. Thus, identifying and ranking co-change candidates of micro-clones is also important. In this paper, we investigate factors that influenc the co-change tendency of the co-change candidates of a target micro-clone fragment.We mine fil level evolutionary coupling from thousands of revisions of our subject systems through mining association rules and analyze this coupling for the purpose of ranking. According to our finding on six open-source subject systems written in Java and C, consistent co-change tendency of micro-clones is influenc d by fil proximity of the micro-clone fragments as well as evolutionary coupling of the file containing those micro-clone fragments. On the basis of our finding we propose a composite ranking mechanism by incorporating both fil proximity and file coupling for ranking co-change candidates for micro-clones and fin that our proposed mechanism performs significantl better than File Proximity Ranking mechanism. We believe that our proposed ranking mechanism has the potential to help programmers in updating micro-clones consistently with less effort","","","CASCON '19"
"Conference Paper","Medeiros H,Vilain P,Mylopoulos J,Jacobsen HA","SolUnit: A Framework for Reducing Execution Time of Smart Contract Unit Tests","","2019","","","264–273","IBM Corp.","USA","Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering","Toronto, Ontario, Canada","2019","","","","","Smart contracts are software programs implemented on a blockchain platform that monitor and automate the execution of contracts to ensure compliance with the terms and conditions of a contract. As such, smart contracts represent a new kind of software that poses its own engineering challenges and requires novel software engineering techniques. In particular, smart contracts require thorough testing before they are deployed because they can't be changed after deployment. This paper proposes a novel approach for executing unit tests for smart contracts intended to reduce test execution time. This reduction is achieved through the reuse of the deployment execution of the smart contract in each test and also the reuse of the setup execution of each test. We implemented the framework SolUnit that uses this approach to execute tests written in Java for Ethereum Solidity smart contracts. We also evaluated the framework SolUnit in five projects. The results show that our approach achieves a meaningful reduction of the time to execute the tests, without breaking the principle of independent tests. The experiments were performed in two environments: an in-memory simulated blockchain and a private Ethereum-based blockchain. Overall, our approach was able to reduce the test execution time by up to 70%.","smart contract, Ethereum, software testing, SolUnit, testing framework, blockchain, unit testing, test automation, solidity","","CASCON '19"
"Conference Paper","Marin VJ,Rivero CR","Clustering Recurrent and Semantically Cohesive Program Statements in Introductory Programming Assignments","","2019","","","911–920","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM International Conference on Information and Knowledge Management","Beijing, China","2019","9781450369763","","https://doi.org/10.1145/3357384.3357960;http://dx.doi.org/10.1145/3357384.3357960","10.1145/3357384.3357960","Students taking introductory programming courses are typically required to complete assignments and expect timely feedback to advance their learning. With the current popularity of these courses in both traditional and online versions, graders are seeing themselves overwhelmed by the sheer amount of student programs they have to handle, and the quality of the educational experience provided is often compromised for promptness. Thus, there is a need for automated approaches to effectively increase grading productivity. Existing approaches in this context fail to support flexible grading schemes and customization based on the assignment at hand. This paper presents a data-driven approach for clustering recurrent program statements performing similar but not exact semantics across student programs, which we refer to as core statements. We rely on structural graph clustering over the program dependence graph representations of student programs. Such clustering is performed over the graph resulting from the pairwise approximate graph alignments of programs. Core statements help graders understand solution variations at a glance and, since they group program statements present in individual student programs, can be used to propagate feedback, thus increasing grading productivity. Our experimental results show that, on average, we discover core statements covering more than 50% of individual student programs, and that program statements grouped by core statements are semantically cohesive, which ensures effective grading.","structural graph clustering, program dependence graph, approximate graph alignment","","CIKM '19"
"Conference Paper","Guo R,Gu T,Yao Y,Xu F,Ma X","Speedup Automatic Program Repair Using Dynamic Software Updating: An Empirical Study","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Asia-Pacific Symposium on Internetware","Fukuoka, Japan","2019","9781450377010","","https://doi.org/10.1145/3361242.3361245;http://dx.doi.org/10.1145/3361242.3361245","10.1145/3361242.3361245","A typical generate-and-validate automatic program repair (APR) tool needs to repeatedly run the same test suite to validate each generated patch. This procedure is expensive when the number of patches is huge. Additionally, to scale to large programs, a program repair tool has to consider a small patch space in practice and thus may sacrifice the capability to find potential correct repairs. In this work, we propose to speed up automatic program repair to mitigate the above issues. One the one hand, we found that restarting processes to load patched code consumes the majority of total validation time. This problem is even severe when the program is running in a managed runtime such as Java virtual machine (JVM). On the other hand, dynamic software updating (DSU) can load and execute new code without restarting. To this end, we propose to use DSU techniques to speed up automatic program repair and present an empirical study in this paper. Within our study, DSU can bring up to 66.3 times speedup in comparison with the traditional restart approach. However, DSU may not be able to handle all patches and can also incur unknown side effects that lead to inconsistent validation results. We then further study the feasibility and consistency of applying DSU to speed up APR. Our results show that 1) less than 1% patches cannot be dynamically updated using the builtin DSU ability of JVM, and 2) DSU based validation leads to potentially harmful inconsistency in only 16 of 1,897,518 patches.","patch validation speedup, Automatic program repair, dynamic software update","","Internetware '19"
"Conference Paper","Cao Y,Zou Y,Xie B","Extracting Code-Relevant Description Sentences Based on Structural Similarity","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Asia-Pacific Symposium on Internetware","Fukuoka, Japan","2019","9781450377010","","https://doi.org/10.1145/3361242.3362699;http://dx.doi.org/10.1145/3361242.3362699","10.1145/3361242.3362699","Software developers often need to read code snippets that are dispersed among different documentation, e.g., Q&A posts, to reuse APIs to complete certain tasks. These code snippets are often surrounded by lengthy context text which are used to describe the functions of code snippets. It will be helpful for code comprehension if we can align a code snippet with its description. In this paper, we propose an approach to extracting code-relevant sentences from its context text. To quantify the relevance between code line and natural language sentence, we represent them with structure trees and calculate their structural similarity. We conduct two experiments to evaluate our approach. In Experiment I, the results show that our approach achieves 83.5% precision and 80.1% recall in aligning Lucene code snippets and corresponding comments. Our approach achieves 27.6% 40.2% improvement in precision compared with existing method, and 33.8% 39.7% improvement in recall. In Experiment II, the results show that our approach achieves 66.4% 93.9% precision to extract code-relevant sentences.","code and text alignment, code comprehension, code relevant description, structural similarity","","Internetware '19"
"Conference Paper","Santos IM,Hauswirth M,Nystrom N","Experiences in Bridging from Functional to Object-Oriented Programming","","2019","","","36–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN Symposium on SPLASH-E","Athens, Greece","2019","9781450369893","","https://doi.org/10.1145/3358711.3361628;http://dx.doi.org/10.1145/3358711.3361628","10.1145/3358711.3361628","Understanding how students' prior knowledge affects their learning of new concepts is essential for effective teaching. The same learning activity, or the same explanation, may have very different effects on students with different prior knowledge. In the context of teaching programming, prior knowledge includes the programming languages students studied in prior courses. In this experience report we describe our observations in teaching object-oriented programming in Java to students who previously learned functional programming in Racket's student languages. We highlight four concrete problems we encountered in teaching the second course in this sequence. We detected and addressed these problems primarily thanks to a teaching assistant who assisted in both of the courses. This experience made us realize the importance of explicitly bridging between languages in introductory programming course sequences. It also showed that the sharing of teaching staff across courses can be an effective way to detect aspects that need bridging.","functional programming, object-oriented programming, prior knowledge","","SPLASH-E 2019"
"Conference Paper","Allamanis M","The Adverse Effects of Code Duplication in Machine Learning Models of Code","","2019","","","143–153","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Athens, Greece","2019","9781450369954","","https://doi.org/10.1145/3359591.3359735;http://dx.doi.org/10.1145/3359591.3359735","10.1145/3359591.3359735","The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.","duplication, machine learning, code naturalness, big code, dataset collection","","Onward! 2019"
"Conference Paper","Kesselbacher M,Bollin A","Discriminating Programming Strategies in Scratch: Making the Difference between Novice and Experienced Programmers","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Workshop in Primary and Secondary Computing Education","Glasgow, Scotland, Uk","2019","9781450377041","","https://doi.org/10.1145/3361721.3361727;http://dx.doi.org/10.1145/3361721.3361727","10.1145/3361721.3361727","Nowadays, block-based programming environments are often used to offer a gentle introduction to learning a programming language. However, an assessment of students' programming skills based on the results of a programming task is not sufficient to determine all areas students are struggling with. We therefore introduce a learning analytics approach of measuring and evaluating the programming sequences of students that program with Scratch 3. With our measurement framework, it is possible to record, store and analyze programming sequences done on a publicly-available, instrumented Scratch 3 environment. Changes in the programming sequence are categorized regarding the used block types and types of program change. We conducted an exploratory programming trial with lower and upper secondary school students to investigate small-scale programming strategies in the recorded programming sequences. Our goals are to identify students in need of support and to identify recurring patterns used by students successful in the trial. Clustering with k-means makes it possible to identify struggling students based on both interacted block types and types of program changes. Recurring patterns in the programming sequences of successful students show that small-scale programming strategies are very diverse.","block-based programming, programming patterns, learning analytics","","WiPSCE'19"
"Conference Paper","Baniassad E,Beschastnikh I,Holmes R,Kiczales G,Allen M","Learning to Listen for Design","","2019","","","179–186","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Athens, Greece","2019","9781450369954","","https://doi.org/10.1145/3359591.3359738;http://dx.doi.org/10.1145/3359591.3359738","10.1145/3359591.3359738","In his essay, Designed as Designer, Richard Gabriel suggests that artifacts are agents of their own design. Building on Gabriel’s position, this essay makes three observations (1) Code “speaks” to the programmer through code smells, and it talks about the shape it wants to take by signalling design principle violations. By “listening” to code, even a novice programmer can let the code itself signal its own emergent natural structure. (2) Seasoned programmers listen for code smells, but they hear in the language of design principles (3) Design patterns are emergent structures that naturally arise from designers listening to what the code is signaling and then responding to these signals through refactoring transformations. Rather than seeing design patterns as an educational destination, we see them as a vehicle for teaching the skill of listening. By showing novices the stories of listening to code and unfolding design patterns (starting from code smells, through refactorings, to arrive at principled structure), we can open up the possibility of listening for emergent design.","Education, Design, Patterns","","Onward! 2019"
"Conference Paper","Cabrera Arteaga J,Monperrus M,Baudry B","Scalable Comparison of JavaScript V8 Bytecode Traces","","2019","","","22–31","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages","Athens, Greece","2019","9781450369879","","https://doi.org/10.1145/3358504.3361228;http://dx.doi.org/10.1145/3358504.3361228","10.1145/3358504.3361228","The comparison and alignment of runtime traces are essential, e.g., for semantic analysis or debugging. However, naive sequence alignment algorithms cannot address the needs of the modern web: (i) the bytecode generation process of V8 is not deterministic; (ii) bytecode traces are large. We present STRAC, a scalable and extensible tool tailored to compare bytecode traces generated by the V8 JavaScript engine. Given two V8 bytecode traces and a distance function between trace events, STRAC computes and provides the best alignment. The key insight is to split access between memory and disk. STRAC can identify semantically equivalent web pages and is capable of processing huge V8 bytecode traces whose order of magnitude matches today's web like https://2019.splashcon.org, which generates approx. 150k of V8 bytecode instructions.","Bytecode, JavaScript, V8, Similarity measurement, Sequence alignment","","VMIL 2019"
"Conference Paper","Cronburg K,Guyer SZ","Floorplan: Spatial Layout in Memory Management Systems","","2019","","","81–93","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","Athens, Greece","2019","9781450369800","","https://doi.org/10.1145/3357765.3359519;http://dx.doi.org/10.1145/3357765.3359519","10.1145/3357765.3359519","In modern runtime systems, memory layout calculations are hand-coded in systems languages. Primitives in these languages are not powerful enough to describe a rich set of layouts, leading to reliance on ad-hoc macros, numerous interrelated static constants, and other boilerplate code. Memory management policies must also carefully orchestrate their application of address calculations in order to modify memory cooperatively, a task ill-suited to low-level systems languages at hand which lack proper safety mechanisms. In this paper we introduce Floorplan, a declarative language for specifying high level memory layouts. Constraints formerly implemented by describing how to compute locations are, in Floorplan, defined declaratively using explicit layout constructs. The challenge here was to discover constructs capable of sufficiently enabling the automatic generation of address calculations. Floorplan is implemented as a compiler for generating a Rust library. In a case study of an existing implementation of the immix garbage collection algorithm, Floorplan eliminates 55 out of the 63 unsafe lines of code: 100% of unsafe lines pertaining to memory safety.","Runtime Systems, Memory Management","","GPCE 2019"
"Conference Paper","Mattis T,Rein P,Hirschfeld R","Ambiguous, Informal, and Unsound: Metaprogramming for Naturalness","","2019","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection","Athens, Greece","2019","9781450369855","","https://doi.org/10.1145/3358502.3361270;http://dx.doi.org/10.1145/3358502.3361270","10.1145/3358502.3361270","Program code needs to be understood by both machines and programmers. While the goal of executing programs requires the unambiguity of a formal language, programmers use natural language within these formal constraints to explain implemented concepts to each other. This so called naturalness – the property of programs to resemble human communication – motivated many statistical and machine learning (ML) approaches with the goal to improve software engineering activities. The metaprogramming facilities of most programming environments model the formal elements of a program (meta-objects). If ML is used to support engineering or analysis tasks, complex infrastructure needs to bridge the gap between meta-objects and ML models, changes are not reflected in the ML model, and the mapping from an ML output back into the program’s meta-object domain is laborious. In the scope of this work, we propose to extend metaprogramming facilities to give tool developers access to the representations of program elements within an exchangeable ML model. We demonstrate the usefulness of this abstraction in two case studies on test prioritization and refactoring. We conclude that aligning ML representations with the program’s formal structure lowers the entry barrier to exploit statistical properties in tool development.","metaprogramming, naturalness, machine learning, meta-objects","","META 2019"
"Conference Paper","Koscina M,Lombard-Platet M,Cluchet P","PlasticCoin: An ERC20 Implementation on Hyperledger Fabric for Circular Economy and Plastic Reuse","","2019","","","223–230","Association for Computing Machinery","New York, NY, USA","IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume","Thessaloniki, Greece","2019","9781450369886","","https://doi.org/10.1145/3358695.3361107;http://dx.doi.org/10.1145/3358695.3361107","10.1145/3358695.3361107","Cryptocurrencies have gained popularity in the last few years, thanks to the democratization of Bitcoin. However, most of these currencies have a very general purpose, namely of allowing people to pay their products with an electronic, decentralised currency. Moreover, the lack of trust of these open-network blockchains comes at a significant economic cost. In this paper, we present PlasticCoin, a cryptocurrency empowering plastic reuse in a circular economical model, that anyone can join, and that respects the ERC20 specifications on a consortium blockchain. We also explore the economical ecosystem revolving around PlasticCoin, and also introduce a way to print tickets that can temporarily hold the role of physical banknotes. Finally, we show that our system is flexible, and it can be adapted to many other business purposes.","Blockchain, Hyperledger Fabric, Circular economy, ERC20","","WI '19 Companion"
"Journal Article","Luan S,Yang D,Barnaby C,Sen K,Chandra S","Aroma: Code Recommendation via Structural Code Search","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360578;http://dx.doi.org/10.1145/3360578","10.1145/3360578","Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.","structural code search, code recommendation, clustering, feature-based code representation, clone detection","",""
"Journal Article","Li Y,Wang S,Nguyen TN,Van Nguyen S","Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360588;http://dx.doi.org/10.1145/3360588","10.1145/3360588","Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph (PDG) and Data Flow Graph (DFG) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the AST built from the method’s body. The use of PDG and DFG enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206% in accuracy.","Deep Learning, Network Embedding, Code Representation Learning, Bug Detection, Program Graphs, Attention Neural Networks","",""
"Journal Article","Wu B,Campora III JP,He Y,Schlecht A,Chen S","Generating Precise Error Specifications for C: A Zero Shot Learning Approach","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360586;http://dx.doi.org/10.1145/3360586","10.1145/3360586","In C programs, error specifications, which specify the value range that each function returns to indicate failures, are widely used to check and propagate errors for the sake of reliability and security. Various kinds of C analyzers employ error specifications for different purposes, e.g., to detect error handling bugs, yet a general approach for generating precise specifications is still missing. This limits the applicability of those tools. In this paper, we solve this problem by developing a machine learning-based approach named MLPEx. It generates error specifications by analyzing only the source code, and is thus general. We propose a novel machine learning paradigm based on transfer learning, enabling MLPEx to require only one-time minimal data labeling from us (as the tool developers) and zero manual labeling efforts from users. To improve the accuracy of generated error specifications, MLPEx extracts and exploits project-specific information. We evaluate MLPEx on 10 projects, including 6 libraries and 4 applications. An investigation of 3,443 functions and 17,750 paths reveals that MLPEx generates error specifications with a precision of 91% and a recall of 94%, significantly higher than those of state-of-the-art approaches. To further demonstrate the usefulness of the generated error specifications, we use them to detect 57 bugs in 5 tested projects.","machine learning, Error specification generation, project-specific features","",""
"Journal Article","Miltner A,Gulwani S,Le V,Leung A,Radhakrishna A,Soares G,Tiwari A,Udupa A","On the Fly Synthesis of Edit Suggestions","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360569;http://dx.doi.org/10.1145/3360569","10.1145/3360569","When working with a document, users often perform context-specific repetitive edits – changes to the document that are similar but specific to the contexts at their locations. Programming by demonstration/examples (PBD/PBE) systems automate these tasks by learning programs to perform the repetitive edits from demonstration or examples. However, PBD/PBE systems are not widely adopted, mainly because they require modal UIs – users must enter a special mode to give the demonstration/examples. This paper presents Blue-Pencil, a modeless system for synthesizing edit suggestions on the fly. Blue-Pencil observes users as they make changes to the document, silently identifies repetitive changes, and automatically suggests transformations that can apply at other locations. Blue-Pencil is parameterized – it allows the ”plug-and-play” of different PBE engines to support different document types and different kinds of transformations. We demonstrate this parameterization by instantiating Blue-Pencil to several domains – C# and SQL code, markdown documents, and spreadsheets – using various existing PBE engines. Our evaluation on 37 code editing sessions shows that Blue-Pencil synthesized edit suggestions with a precision of 0.89 and a recall of 1.0, and took 199 ms to return suggestions on average. Finally, we report on several improvements based on feedback gleaned from a field study with professional programmers to investigate the use of Blue-Pencil during long code editing sessions. Blue-Pencil has been integrated with Visual Studio IntelliCode to power the IntelliCode refactorings feature.","Programming by example, Program transformation, Refactoring, Program synthesis","",""
"Journal Article","Ziegler A,Geus J,Heinloth B,Hönig T,Lohmann D","Honey, I Shrunk the ELFs: Lightweight Binary Tailoring of Shared Libraries","ACM Trans. Embed. Comput. Syst.","2019","18","5s","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","1539-9087","https://doi.org/10.1145/3358222;http://dx.doi.org/10.1145/3358222","10.1145/3358222","In the embedded domain, industrial sectors (i.e., automotive industry, avionics) are undergoing radical changes. They broadly adopt commodity hardware and move away from special-purpose control units. During this transition, heterogeneous software components are consolidated to run on commodity operating systems.To efficiently consolidate such components, a modular encapsulation of common functionality into reusable binary files (i.e., shared libraries) is essential. However, shared libraries are often unnecessarily large as they entail a lot of generic functionality that is not required in a narrowly defined scenario. As the source code of proprietary components is often unavailable and the industry is heading towards binary-only distribution, we propose an approach towards lightweight binary tailoring.As demonstrated in the evaluation, lightweight binary tailoring effectively reduces the amount of code in all shared libraries on a Linux-based system by 63 percent and shrinks their files by 17 percent. The reduction in size is beneficial to cut down costs (e.g., lower storage and memory footprint) and eases code analyses that are necessary for code audits.","Shared libraries, binary tailoring, Linux","",""
"Conference Paper","Elkhail AA,Svacina J,Cerny T","Intelligent Token-Based Code Clone Detection System for Large Scale Source Code","","2019","","","256–260","Association for Computing Machinery","New York, NY, USA","Proceedings of the Conference on Research in Adaptive and Convergent Systems","Chongqing, China","2019","9781450368438","","https://doi.org/10.1145/3338840.3355654;http://dx.doi.org/10.1145/3338840.3355654","10.1145/3338840.3355654","A code clone refers to code fragments in the source code that are identical or similar to each other. Code clones lead difficulties in software maintenance, bug fixing, present poor design and increase the system size. Code clone detection techniques and tools have been proposed by many researchers, however, there is a lack of clone detection techniques especially for large scale repositories. In this paper, we present a token-based clone detector called Intelligent Clone Detection Tool (ICDT) that can detect both exact and near-miss clones from large repositories using a standard workstation environment. In order to evaluate the scalability and the efficiency of ICDT, we use the most recent benchmark which is a big benchmark of real clones, BigCloneBench. In addition, we compare ICDT to four publicly available and state-of-the-art tools.","case study, BigCloneBench, code clone, clone detection","","RACS '19"
"Conference Paper","Souza IS,Machado I,Seaman C,Gomes G,Chavez C,de Almeida ES,Masiero P","Investigating Variability-Aware Smells in SPLs: An Exploratory Study","","2019","","","367–376","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIII Brazilian Symposium on Software Engineering","Salvador, Brazil","2019","9781450376518","","https://doi.org/10.1145/3350768.3350774;http://dx.doi.org/10.1145/3350768.3350774","10.1145/3350768.3350774","Variability-aware smell is a concept referring to artifact shortcomings in the context of highly-configurable systems that can degrade aspects such as program comprehension, maintainability, and evolvability. To the best of our knowledge, there is very little evidence that variability-aware smells exist in Software Product Lines (SPLs). This work presents an exploratory study that investigated (I) evidence that variability-aware smells exist in SPLs and (II) new types of variability-aware smell not yet documented in the literature based on a quantitative study with open source SPL projects. We collected quantitative data to generate reliable research evidence, by performing feature model and source code inspections on eleven open-source SPL projects. Our findings revealed that (1) instances of variability-aware smells exist in open-source SPL projects and (2) feature information presented significant associations with variability-aware smells. Furthermore, (3) the study presented six new types of variability-aware smells.","Exploratory Study, Variability-Aware Smells, Software Product Lines, Empirical Study","","SBES '19"
"Conference Paper","Assunção E,Souza R","Incidence of Code Smells in the Application of Design Patterns: A Method-Level Analysis","","2019","","","73–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse","Salvador, Brazil","2019","9781450376372","","https://doi.org/10.1145/3357141.3357143;http://dx.doi.org/10.1145/3357141.3357143","10.1145/3357141.3357143","Design patterns are reusable solutions that can be applied to solve specific problems in software design. Such patterns can be misapplied, though, and give rise to code smells, i.e., fragments in the code that indicate possible design flaws. In this study, we aim to understand how often code smells co-occur with design patterns, as well as to determine the most common co-occurrences. To this end, we identified instances of code smells and design patterns in methods of 25 open source Java projects, by using automated detection tools. We also manually inspected fragments of the projects' source code to gather insight on the relationship between specific pairs of smells and patterns. Among other findings, we found that methods that are part of the Adapter pattern are more likely to contain code smells, especially the Feature Envy smell, although it can be argued that the detection of this smell in this context is a false positive.","Design Patterns, Software Design, Code Smells","","SBCARS '19"
"Conference Paper","Lucas W,Bonifácio R,Canedo ED,Marcílio D,Lima F","Does the Introduction of Lambda Expressions Improve the Comprehension of Java Programs?","","2019","","","187–196","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIII Brazilian Symposium on Software Engineering","Salvador, Brazil","2019","9781450376518","","https://doi.org/10.1145/3350768.3350791;http://dx.doi.org/10.1145/3350768.3350791","10.1145/3350768.3350791","Background: The Java programming language version eighth introduced a number of features that encourage the functional style of programming, including the support for lambda expressions and the Stream API. Currently, there is a common wisdom that refactoring a legacy code to introduce lambda expressions, besides other potential benefits, simplifies the code and improves program comprehension. Aims: The purpose of this paper is to investigate this belief, conducting an in depth study to evaluate the effect of introducing lambda expressions on program comprehension. Method: We conduct this research using a mixed-method study. First, we quantitatively analyze 66 pairs of real code snippets, where each pair corresponds to the body of a method before and after the introduction of lambda expressions. We computed two metrics related to source code complexity (number of lines of code and cyclomatic complexity) and two metrics that estimate the readability of the source code. Second, we conduct a survey with practitioners to collect their perceptions about the benefits on program comprehension, with the introduction of lambda expressions. The practitioners evaluate a number between three and six pairs of code snippets, to answer questions about possible improvements. Results: We found contradictory results in our research. Based on the quantitative assessment, we could not find evidences that the introduction of lambda expressions improves software readability--one of the components of program comprehension. Differently, our findings of the qualitative assessment suggest that the introduction of lambda expression improves program comprehension. Implications: We argue in this paper that one can improve program comprehension when she applies particular transformations to introduce lambda expressions (e.g., replacing anonymous inner classes by lambda expressions). In addition, the opinion of the participants shine the opportunities in which a transformation for introducing lambda might be advantageous. This might support the implementation of effective tools for automatic program transformations. Finally, our results suggest that state-of-the-art models for estimating program readability are not helpful to capture the benefits of a program transformation to introduce lambda expressions.","Program Comprehension, Empirical Studies, Java Lambda Expressions","","SBES '19"
"Conference Paper","Mendonça WD,Assunção WK,Vergilio SR","Reusing Test Cases on Graph Product Line Variants: Results from a State-of-the-Practice Test Data Generation Tool","","2019","","","52–61","Association for Computing Machinery","New York, NY, USA","Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing","Salvador, Brazil","2019","9781450376488","","https://doi.org/10.1145/3356317.3356318;http://dx.doi.org/10.1145/3356317.3356318","10.1145/3356317.3356318","Software testing is an essential activity for quality assurance, but, it is an error-prone and effort consuming task when conducted manually. Because of this, the use of automated tools is fundamental, as well as, the evaluation of these tools in practice. However, there is not so much evidence on how such tools perform on highly-configurable systems. Highly-configurable systems are commonly observed in industry as an approach to develop families of products, where products have different configuration options to meet customer needs. To fulfill such a gap, this paper reports results on the use of the tool Randoop, which is widely used in industry, to test variants of the Graph Product Line (GPL) family of products. Our goal is to evaluate reusability of a test data set generated by Randoop for one product when reused for testing other GPL products. Besides, we also investigate the impact of using different values of runtime, the main Randoop parameter, on the number of reused test data. The results show that the used value for runtime in general does not contribute to increase the coverage of test data reused in different products. Furthermore, similarity among products does not ensure a greater reusability.","Test Data Generation, Software Reuse, Highly-configurable systems, Family of Products, Test Coverage","","SAST 2019"
"Conference Paper","Tan J,Jiao S,Chabbi M,Liu X","What Every Scientific Programmer Should Know about Compiler Optimizations?","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 34th ACM International Conference on Supercomputing","Barcelona, Spain","2020","9781450379830","","https://doi.org/10.1145/3392717.3392754;http://dx.doi.org/10.1145/3392717.3392754","10.1145/3392717.3392754","Compilers are an indispensable component in the software stack. Besides generating machine code, compilers perform multiple optimizations to improve code performance. Typically, scientific programmers treat compilers as a blackbox and expect them to optimize code thoroughly. However, optimizing compilers are not performance panacea. They can miss optimization opportunities or even introduce inefficiencies that are not in the source code. There is a lack of tool infrastructures and datasets that can provide such a study to help understand compiler optimizations.In this paper, we investigate an important compiler optimization---dead and redundant operation elimination. We first develop a tool CIDetector to analyze a large number of programs. In our analysis, we select 12 representative programs from different domains to form a dataset called CIBench. We utilize five compilers to optimize CIBench with the highest optimization options available and leverage CIDetector to study each generated binary. We provide insights into two aspects. First, we show that modern compilers miss several optimization opportunities, in fact they even introduce some inefficiencies, which require programmers to refactor the source code. Second, we show how compilers have advanced in a vertical evolution (the same compiler of different release versions) and a horizontal comparison (different compilers of the most recent releases). With empirical studies, we provide insights for software engineers, compiler writers, and tool developers.","binary analysis, redundancy, compiler inefficiencies, profiling","","ICS '20"
"Conference Paper","Cereda S,Palermo G,Cremonesi P,Doni S","A Collaborative Filtering Approach for the Automatic Tuning of Compiler Optimisations","","2020","","","15–25","Association for Computing Machinery","New York, NY, USA","The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems","London, United Kingdom","2020","9781450370943","","https://doi.org/10.1145/3372799.3394361;http://dx.doi.org/10.1145/3372799.3394361","10.1145/3372799.3394361","Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.","characterization, flag, compiler, embedded, reaction, autotuning, recommender systems, collaborative filtering, selection, optimization, tuning, performance","","LCTES '20"
"Conference Paper","Bharosa N,Meijer K,van der Voort H","Innovation in Public Service Design: Developing a Co-Creation Tool for Public Service Innovation Journeys","","2020","","","275–284","Association for Computing Machinery","New York, NY, USA","The 21st Annual International Conference on Digital Government Research","Seoul, Republic of Korea","2020","9781450387910","","https://doi.org/10.1145/3396956.3396981;http://dx.doi.org/10.1145/3396956.3396981","10.1145/3396956.3396981","Outpaced by the speed of digital innovation in the private sector, governments are looking for new approaches to public service innovation. Drawing on three complementary innovation theories – open innovation, recombinant innovation and co-creation – this paper presents a prototype that is designed to enhance the online innovation journey for public services. The main strategy explored is that of online public-service co-creation, allowing innovators to combine online and offline efforts. The outcome of this research is a prototype of an online co-creation tool. The tool is consumed via a web-portal that includes an overview of ongoing experiments, tools, labs data sets and digital building blocks. This paper contributes by presenting the requirements and lessons learned when developing a co-creation tool for innovation in public service design. While the proposed co-creation tool is expected to enhance and speed up online cocreation efforts, findings indicate that innovators from the public and private sector still need to learn how to combine online and offline co-creation efforts. The added value expected from the online tool is that it should provide an up to date oversight of digital building blocks, innovation methods and labs. Interviews with prospective users suggest that this oversight is needed to jumpstart the first step of the innovation journey. Development of a digital sandbox – a shared online experimentation environment – is considered to be an important next step for innovation in public service design.","recombinant innovation, e-government, co-creation, open innovation, Public services","","dg.o '20"
"Conference Paper","Ainsworth S,Jones TM","Prefetching in Functional Languages","","2020","","","16–29","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGPLAN International Symposium on Memory Management","London, UK","2020","9781450375665","","https://doi.org/10.1145/3381898.3397209;http://dx.doi.org/10.1145/3381898.3397209","10.1145/3381898.3397209","Functional programming languages contain a number of runtime and language features, such as garbage collection, indirect memory accesses, linked data structures and immutability, that interact with a processor’s memory system. These conspire to cause a variety of unintuitive memory-performance effects. For example, it is slower to traverse through linked lists and arrays of data that have been sorted than to traverse the same data accessed in the order it was allocated. We seek to understand these issues and mitigate them in a manner consistent with functional languages, taking advantage of the features themselves where possible. For example, immutability and garbage collection force linked lists to be allocated roughly sequentially in memory, even when the data pointed to within each node is not. We add language primitives for software-prefetching to the OCaml language to exploit this, and observe significant performance improvements a variety of micro- and macro-benchmarks, resulting in speedups of up to 2× on the out-of-order superscalar Intel Haswell and Xeon Phi Knights Landing systems, and up to 3× on the in-order Arm Cortex-A53.","Functional Programming, Hardware Prefetching, Software Prefetching, OCaml","","ISMM 2020"
"Conference Paper","Frädrich C,Obermüller F,Körber N,Heuer U,Fraser G","Common Bugs in Scratch Programs","","2020","","","89–95","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM Conference on Innovation and Technology in Computer Science Education","Trondheim, Norway","2020","9781450368742","","https://doi.org/10.1145/3341525.3387389;http://dx.doi.org/10.1145/3341525.3387389","10.1145/3341525.3387389","Bugs in SCRATCH programs can spoil the fun and inhibit learning success. Many common bugs are the result of recurring patterns of bad code. In this paper we present a collection of common code patterns that typically hint at bugs in SCRATCH programs, and the LitterBox tool which can automatically detect them. We empirically evaluate how frequently these patterns occur, and how severe their consequences usually are. While fixing bugs inevitably is part of learning, the possibility to identify the bugs automatically provides the potential to support learners.","block-based programming, scratch, code quality","","ITiCSE '20"
"Conference Paper","Funke H,Mühlig J,Teubner J","Efficient Generation of Machine Code for Query Compilers","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Workshop on Data Management on New Hardware","Portland, Oregon","2020","9781450380249","","https://doi.org/10.1145/3399666.3399925;http://dx.doi.org/10.1145/3399666.3399925","10.1145/3399666.3399925","Query compilation can make query execution extremely efficient, but it introduces additional compilation time. The compilation time causes a relatively high overhead especially for short-running and high-complexity queries.We propose Flounder IR as a lightweight intermediate representation for query compilation to reduce compilation times. Flounder IR is close to machine assembly and adds just that set of features that is necessary for efficient query compilation: virtual registers and function calls ease the construction of the compiler front-end; database-specific extensions enable efficient pipelining in query plans; more elaborate IR features are intentionally left out to maximize compilation speed.In this paper, we present the Flounder IR language and motivate its design; we show how the language makes query compilation intuitive and efficient; and we demonstrate with benchmarks how our Flounder library can significantly reduce query compilation times.","","","DaMoN '20"
"Journal Article","King P","A History of the Groovy Programming Language","Proc. ACM Program. Lang.","2020","4","HOPL","","Association for Computing Machinery","New York, NY, USA","","","2020-06","","","https://doi.org/10.1145/3386326;http://dx.doi.org/10.1145/3386326","10.1145/3386326","This paper describes the history of the Groovy programming language. At the time of Groovy’s inception, Java was a dominant programming language with a wealth of useful libraries. Despite this, it was perceived by some to be evolving slowing and to have shortcomings for scripting, rapid prototyping and when trying to write minimalistic code. Other languages seemed to be innovating faster than Java and, while overcoming some of Java’s shortcomings, used syntax that was less familiar to Java developers. Integration with Java libraries was also non-optimal. Groovy was created as a complementary language to Java—its dynamic counterpart. It would look and feel like Java but focus on extensibility and rapid innovation. Groovy would borrow ideas from dynamic languages like Ruby, Python and Smalltalk where needed to provide compelling JVM solutions for some of Java’s shortcomings. Groovy supported innovation through its runtime and compile-time metaprogramming capabilities. It supported simple operator overloading, had a flexible grammar and was extensible. These characteristics made it suitable for growing the language to have new commands (verbs) and properties (nouns) specific to a particular domain, a so called Domain Specific Language (DSL). While still intrinsically linked with Java, over time Groovy has evolved from a niche dynamic scripting language into a compelling mainstream language. After many years as a principally dynamically-typed language, a static nature was added to Groovy. Code could be statically type checked or when dynamic features weren’t needed, they could be turned off entirely for Java-like performance. A number of nuances to the static nature came about to support the style of coding used by Groovy developers. Many choices made by Groovy in its design, later appeared in other languages (Swift, C#, Kotlin, Ceylon, PHP, Ruby, Coffeescript, Scala, Frege, TypeScript and Java itself). This includes Groovy’s dangling closure, Groovy builders, null-safe navigation, the Elvis operator, ranges, the spaceship operator, and flow typing. For most languages, we don’t know to what extent Groovy played a part in their choices. We do know that Kotlin took inspiration from Groovy’s dangling closures, builder concept, default it parameter for closures, templates and interpolated strings, null-safe navigation and the Elvis operator. The leadership, governance and sponsorship arrangements of Groovy have evolved over time, but Groovy has always been a successful highly collaborative open source project driven more by the needs of the community than by a vision of a particular company or person.","Domain Specific Languages, Extensibility, Closure, Static typing, Dynamic typing, Scripting, Object-oriented, Metaprogramming, Functional programming","",""
"Conference Paper","Premtoon V,Koppel J,Solar-Lezama A","Semantic Code Search via Equational Reasoning","","2020","","","1066–1082","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386001;http://dx.doi.org/10.1145/3385412.3386001","10.1145/3385412.3386001","We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle’s Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.","code search, equational reasoning","","PLDI 2020"
"Conference Paper","Rocha RC,Petoumenos P,Wang Z,Cole M,Leather H","Effective Function Merging in the SSA Form","","2020","","","854–868","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386030;http://dx.doi.org/10.1145/3385412.3386030","10.1145/3385412.3386030","Function merging is an important optimization for reducing code size. This technique eliminates redundant code across functions by merging them into a single function. While initially limited to identical or trivially similar functions, the most recent approach can identify all merging opportunities in arbitrary pairs of functions. However, this approach has a serious limitation which prevents it from reaching its full potential. Because it cannot handle phi-nodes, the state-of-the-art applies register demotion to eliminate them before applying its core algorithm. While a superficially minor workaround, this has a three-fold negative effect: by artificially lengthening the instruction sequences to be aligned, it hinders the identification of mergeable instruction; it prevents a vast number of functions from being profitably merged; it increases compilation overheads, both in terms of compile-time and memory usage. We present SalSSA, a novel approach that fully supports the SSA form, removing any need for register demotion. By doing so, we notably increase the number of profitably merged functions. We implement SalSSA in LLVM and apply it to the SPEC 2006 and 2017 suites. Experimental results show that our approach delivers on average, 7.9% to 9.7% reduction on the final size of the compiled code. This translates to around 2x more code size reduction over the state-of-the-art. Moreover, as a result of aligning shorter sequences of instructions and reducing the number of wasteful merge operations, our new approach incurs an average compile-time overhead of only 5%, 3x less than the state-of-the-art, while also reducing memory usage by over 2x.","Code Size Reduction, Function Merging, LTO","","PLDI 2020"
"Conference Paper","Dasgupta S,Dinesh S,Venkatesh D,Adve VS,Fletcher CW","Scalable Validation of Binary Lifters","","2020","","","655–671","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3385964;http://dx.doi.org/10.1145/3385412.3385964","10.1145/3385412.3385964","Validating the correctness of binary lifters is pivotal to gain trust in binary analysis, especially when used in scenarios where correctness is important. Existing approaches focus on validating the correctness of lifting instructions or basic blocks in isolation and do not scale to full programs. In this work, we show that formal translation validation of single instructions for a complex ISA like x86-64 is not only practical, but can be used as a building block for scalable full-program validation. Our work is the first to do translation validation of single instructions on an architecture as extensive as x86-64, uses the most precise formal semantics available, and has the widest coverage in terms of the number of instructions tested for correctness. Next, we develop a novel technique that uses validated instructions to enable program-level validation, without resorting to performance-heavy semantic equivalence checking. Specifically, we compose the validated IR sequences using a tool we develop called Compositional Lifter to create a reference standard. The semantic equivalence check between the reference and the lifter output is then reduced to a graph-isomorphism check through the use of semantic preserving transformations. The translation validation of instructions in isolation revealed 29 new bugs in McSema – a mature open-source lifter from x86-64 to LLVM IR. Towards the validation of full programs, our approach was able to prove the translational correctness of 2254/2348 functions taken from LLVM’s single-source benchmark test-suite.","Compiler Optimizations, Formal Semantics, Graph Isomorphism, LLVM IR, Translation Validation, x86-64","","PLDI 2020"
"Conference Paper","Qin B,Chen Y,Yu Z,Song L,Zhang Y","Understanding Memory and Thread Safety Practices and Issues in Real-World Rust Programs","","2020","","","763–779","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386036;http://dx.doi.org/10.1145/3385412.3386036","10.1145/3385412.3386036","Rust is a young programming language designed for systems software development. It aims to provide safety guarantees like high-level languages and performance efficiency like low-level languages. The core design of Rust is a set of strict safety rules enforced by compile-time checking. To support more low-level controls, Rust allows programmers to bypass these compiler checks to write unsafe code. It is important to understand what safety issues exist in real Rust programs and how Rust safety mechanisms impact programming practices. We performed the first empirical study of Rust by close, manual inspection of 850 unsafe code usages and 170 bugs in five open-source Rust projects, five widely-used Rust libraries, two online security databases, and the Rust standard library. Our study answers three important questions: how and why do programmers write unsafe code, what memory-safety issues real Rust programs have, and what concurrency bugs Rust programmers make. Our study reveals interesting real-world Rust program behaviors and new issues Rust programmers make. Based on our study results, we propose several directions of building Rust bug detectors and built two static bug detectors, both of which revealed previously unknown bugs.","Memory Bug, Rust, Bug Study, Concurrency Bug","","PLDI 2020"
"Conference Paper","Allamanis M,Barr ET,Ducousso S,Gao Z","Typilus: Neural Type Hints","","2020","","","91–105","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3385997;http://dx.doi.org/10.1145/3385412.3385997","10.1145/3385412.3385997","Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program’s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace — a continuous relaxation of the discrete space of types — and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.","meta-learning, graph neural networks, type inference, structured learning, deep learning","","PLDI 2020"
"Conference Paper","Muller SK,Singer K,Goldstein N,Acar UA,Agrawal K,Lee IT","Responsive Parallelism with Futures and State","","2020","","","577–591","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation","London, UK","2020","9781450376136","","https://doi.org/10.1145/3385412.3386013;http://dx.doi.org/10.1145/3385412.3386013","10.1145/3385412.3386013","Motivated by the increasing shift to multicore computers, recent work has developed language support for responsive parallel applications that mix compute-intensive tasks with latency-sensitive, usually interactive, tasks. These developments include calculi that allow assigning priorities to threads, type systems that can rule out priority inversions, and accompanying cost models for predicting responsiveness. These advances share one important limitation: all of this work assumes purely functional programming. This is a significant restriction, because many realistic interactive applications, from games to robots to web servers, use mutable state, e.g., for communication between threads. In this paper, we lift the restriction concerning the use of state. We present λi4, a calculus with implicit parallelism in the form of prioritized futures and mutable state in the form of references. Because both futures and references are first-class values, λi4 programs can exhibit complex dependencies, including interaction between threads and with the external world (users, network, etc). To reason about the responsiveness of λi4 programs, we extend traditional graph-based cost models for parallelism to account for dependencies created via mutable state, and we present a type system to outlaw priority inversions that can lead to unbounded blocking. We show that these techniques are practical by implementing them in C++ and present an empirical evaluation.","type systems, Cilk, concurrency, futures, parallelism, responsiveness, shared memory","","PLDI 2020"
"Journal Article","Bao L,Xing Z,Xia X,Lo D,Wu M,Yang X","Psc2code: Denoising Code Extraction from Programming Screencasts","ACM Trans. Softw. Eng. Methodol.","2020","29","3","","Association for Computing Machinery","New York, NY, USA","","","2020-06","","1049-331X","https://doi.org/10.1145/3392093;http://dx.doi.org/10.1145/3392093","10.1145/3392093","Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts.In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code.We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.","Programming videos, code search, deep learning","",""
"Journal Article","Mukherjee R,Chaudhuri S,Jermaine C","Searching a Database of Source Codes Using Contextualized Code Search","Proc. VLDB Endow.","2020","13","10","1765–1778","VLDB Endowment","","","","2020-06","","2150-8097","https://doi.org/10.14778/3401960.3401972;http://dx.doi.org/10.14778/3401960.3401972","10.14778/3401960.3401972","Consider the case where a programmer has written some part of a program, but has left part of the program (such as a method or a function body) incomplete. The goal is to use the context surrounding the missing code to automatically ""figure out"" which of the codes in the database would be useful to the programmer in order to help complete the missing code. The search is ""contextualized"" in the sense that the search engine should use clues in the partially-completed code to figure out which database code is most useful. The user should not be required to formulate an explicit query.We cast contextualized code search as a learning problem, where the goal is to learn a distribution function computing the likelihood that each database code completes the program, and propose a neural model for predicting which database code is likely to be most useful. Because it will be prohibitively expensive to apply a neural model to each code in a database of millions or billions of codes at search time, one of our key technical concerns is ensuring a speedy search. We address this by learning a ""reverse encoder"" that can be used to reduce the problem of evaluating each database code to computing a convolution of two normal distributions.","","",""
"Conference Paper","Fariha A,Nath S,Meliou A","Causality-Guided Adaptive Interventional Debugging","","2020","","","431–446","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data","Portland, OR, USA","2020","9781450367356","","https://doi.org/10.1145/3318464.3389694;http://dx.doi.org/10.1145/3318464.3389694","10.1145/3318464.3389694","Runtime nondeterminism is a fact of life in modern database applications. Previous research has shown that nondeterminism can cause applications to intermittently crash, become unresponsive, or experience data corruption. We propose Adaptive Interventional Debugging (AID) for debugging such intermittent failures. AID combines existing statistical debugging, causal analysis, fault injection, and group testing techniques in a novel way to (1) pinpoint the root cause of an application's intermittent failure and (2) generate an explanation of how the root cause triggers the failure. AID works by first identifying a set of runtime behaviors (called predicates) that are strongly correlated to the failure. It then utilizes temporal properties of the predicates to (over)-approximate their causal relationships. Finally, it uses fault injection to execute a sequence of interventions on the predicates and discover their true causal relationships. This enables AID to identify the true root cause and its causal relationship to the failure. We theoretically analyze how fast AID can converge to the identification. We evaluate AID with six real-world applications that intermittently fail under specific inputs. In each case, AID was able to identify the root cause and explain how the root cause triggered the failure, much faster than group testing and more precisely than statistical debugging. We also evaluate AID with many synthetically generated applications with known root causes and confirm that the benefits also hold for them.","root-causing, trace analysis, concurrency bug, group testing","","SIGMOD '20"
"Journal Article","Gajrani J,Tripathi M,Laxmi V,Somani G,Zemmari A,Gaur MS","Vulvet: Vetting of Vulnerabilities in Android Apps to Thwart Exploitation","Digital Threats","2020","1","2","","Association for Computing Machinery","New York, NY, USA","","","2020-05","","2692-1626","https://doi.org/10.1145/3376121;http://dx.doi.org/10.1145/3376121","10.1145/3376121","Data security and privacy of Android users is one of the challenging security problems addressed by the security research community. A major source of the security vulnerabilities in Android apps is attributed to bugs within source code, insecure APIs, and unvalidated code before performing sensitive operations. Specifically, the major class of app vulnerabilities is related to the categories such as inter-component communication (ICC), networking, web, cryptographic APIs, storage, and runtime-permission validation. A major portion of current contributions focus on identifying a smaller subset of vulnerabilities. In addition, these methods do not discuss how to remove detected vulnerabilities from the affected code.In this work, we propose a novel vulnerability detection and patching framework, Vulvet, which employs static analysis approaches from different domains of program analysis for detection of a wide range of vulnerabilities in Android apps. We propose an additional light-weight technique, FP-Validation, to mitigate false positives in comparison to existing solutions owing to over-approximation. In addition to improved detection, Vulvet provides an automated patching of apps with safe code for each of the identified vulnerability using bytecode instrumentation. We implement Vulvet as an extension of Soot. To demonstrate the efficiency of our proposed framework, we analyzed 3,700 apps collected from various stores and benchmarks consisting of various weak implementations. Our results indicate that Vulvet is able to achieve vulnerability detection with 95.23% precision and 0.975 F-measure on benchmark apps; a significant improvement in comparison to recent works along with successful patching of identified vulnerabilities.","protection, Android, vulnerabilities, static analysis, security","",""
"Conference Paper","Lee Y,Ko U,Aitkazin I,Park S,Tak HS,Cho HG","A Fast Detecting Method for Clone Functions Using Global Alignment of Token Sequences","","2020","","","17–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 12th International Conference on Machine Learning and Computing","Shenzhen, China","2020","9781450376426","","https://doi.org/10.1145/3383972.3384014;http://dx.doi.org/10.1145/3383972.3384014","10.1145/3383972.3384014","In large software projects, proper source code reuse can make development more efficient, but a lot of duplicate code and error code reuse can be a major cause of difficult system maintenance. Efficient clone code detection for large project can help manage the project. However, most of the clone detection methods are difficult to perform on adaptive analysis that adjusts specificity or sensitivity according to the type of clone to be detected. Therefore, when a user wants to find a particular type of clone in a large project, they must analyze it repeatedly using various tools to adjust the options. In this study, we propose a clone detection system based on the global sequence alignment. Lex based token analysis models and global alignment algorithm-based clone detection models were able to detect not only exact matches but also various types of clones by setting lower bound scores. Using features of the global alignment score calculation method to eliminate functions that cannot be clone candidates in advance, alignment analysis was possible even for large projects, and the execution time was predicted. For clone functions, we visualized the matching area, which is the result of alignment analysis, to represent clone information more efficiently.","clone function, code analysis, Clone detection, global alignment","","ICMLC 2020"
"Conference Paper","Zhao F,Zhao J,Bai Y","A Survey of Automatic Generation of Code Comments","","2020","","","21–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2020","9781450376419","","https://doi.org/10.1145/3380625.3380649;http://dx.doi.org/10.1145/3380625.3380649","10.1145/3380625.3380649","Code comments are a valuable form of documentation attached to code that is the most intuitive and efficient way for programmers to understand software code. Good code comments can help programmers quickly understand the role of source code and facilitate understanding of programs and software maintenance tasks. However, in practice, most programmers only pay attention to the code and ignore the comments and documents, which makes the program's readability and maintainability greatly reduced. Based on the meaning of code comments, this paper discusses the current progress in the field of code comments research, adopts the comparative analysis method, focuses on the classification research of the methods and tools for automatic generation of code comments, expounds its advantages and disadvantages, and reveals the issues that need further study.","Code Comments, Comments method, Automatic Generation, Comments tool","","ICMSS 2020"
"Journal Article","Smeets H,Ceriotti M,Marrón PJ","Adapting Recursive Sinusoidal Software Oscillators for Low-Power Fixed-Point Processors","ACM Trans. Embed. Comput. Syst.","2020","19","3","","Association for Computing Machinery","New York, NY, USA","","","2020-05","","1539-9087","https://doi.org/10.1145/3378559;http://dx.doi.org/10.1145/3378559","10.1145/3378559","The growing field of the Internet of Things relies at the bottom on components with very scarce computing resources that currently do not allow complex processing of sensed data. Any computation involving Fast Fourier Transforms (FFT), Wavelet Transforms (WT), or simple sines and cosines is considered impractical on low-end devices due to the lack of floating point and math libraries. This article presents new techniques that make it possible to use these functions also on severely constrained target platforms.Current literature abounds with schemes to compute sine and cosine functions, with focus on speed, hardware footprint, software size, target type, or precision. Even so, there is no practical exploration of the design space available for embedded devices with limited resources, in particular when only integer operations are possible. We select an efficient set of recursive sine and cosine generators and measure the frequency, amplitude, and phase error over a wide parameter range. We show that their simplicity allows them to be implemented on the most bare targets with good precision, reducing power consumption and size while being the fastest on integer-only processors. We also introduce specially tailored FFT and WT algorithms and show that they are usable in practice while having an extremely small code footprint, good precision, and high speed.","fourier transform, intermediate euler, cosine, IoT, wavelet transform, sine","",""
"Conference Paper","Lu Y,Li J","Generative Adversarial Network for Improving Deep Learning Based Malware Classification","","2020","","","584–593","IEEE Press","National Harbor, Maryland","Proceedings of the Winter Simulation Conference","","2020","9781728132839","","","","The generative adversarial network (GAN) had been successfully applied in many domains in the past, the GAN network provides a new approach for solving computer vision, object detection and classification problems by learning, mimicking and generating any distribution of data. One of the difficulties in deep learning-based malware detection and classification tasks is lacking of training malware samples. With insufficient training data the classification performance of the deep model could be compromised significantly. To solve this issue, in this paper, we propose a method which uses the Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic malware samples. Our experiment results show that by using the DCGAN generated adversarial synthetic malware samples, the classification accuracy of the classifier --- a 18-layer deep residual network is significantly improved by approximately 6%.","","","WSC '19"
"Journal Article","Kapur R,Sodhi B","A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics","ACM Trans. Softw. Eng. Methodol.","2020","29","2","","Association for Computing Machinery","New York, NY, USA","","","2020-04","","1049-331X","https://doi.org/10.1145/3384517;http://dx.doi.org/10.1145/3384517","10.1145/3384517","An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.","source code mining, Maintaining software, automated software engineering, AI in software engineering, software defect prediction, software metrics, software faults and failures","",""
"Conference Paper","Head A,Jiang J,Smith J,Hearst MA,Hartmann B","Composing Flexibly-Organized Step-by-Step Tutorials from Linked Source Code, Snippets, and Outputs","","2020","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","Honolulu, HI, USA","2020","9781450367080","","https://doi.org/10.1145/3313831.3376798;http://dx.doi.org/10.1145/3313831.3376798","10.1145/3313831.3376798","Programming tutorials are a pervasive, versatile medium for teaching programming. In this paper, we report on the content and structure of programming tutorials, the pain points authors experience in writing them, and a design for a tool to help improve this process. An interview study with 12 experienced tutorial authors found that they construct documents by interleaving code snippets with text and illustrative outputs. It also revealed that authors must often keep related artifacts of source programs, snippets, and outputs consistent as a program evolves. A content analysis of 200 frequently-referenced tutorials on the web also found that most tutorials contain related artifacts—duplicate code and outputs generated from snippets—that an author would need to keep consistent with each other. To address these needs, we designed a tool called Torii with novel authoring capabilities. An in-lab study showed that tutorial authors can successfully use the tool for the unique affordances identified, and provides guidance for designing future tools for tutorial authoring.","literate programming, authoring, programming tutorials, code evolution, code editors, consistency","","CHI '20"
"Conference Paper","Han HL,Renom MA,Mackay WE,Beaudouin-Lafon M","Textlets: Supporting Constraints and Consistency in Text Documents","","2020","","","1–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","Honolulu, HI, USA","2020","9781450367080","","https://doi.org/10.1145/3313831.3376804;http://dx.doi.org/10.1145/3313831.3376804","10.1145/3313831.3376804","Writing technical documents frequently requires following constraints and consistently using domain-specific terms. We interviewed 12 legal professionals and found that they all use a standard word processor, but must rely on their memory to manage dependencies and maintain consistent vocabulary within their documents. We introduce Textlets, interactive objects that reify text selections into persistent items. We show how Textlets help manage consistency and constraints within the document, including selective search and replace, word count, and alternative wording. Eight participants tested a search-and-replace Textlet as a technology probe. All successfully interacted directly with the Textlet to perform advanced tasks; and most (6/8) spontaneously generated a novel replace-all-then-correct strategy. Participants suggested additional ideas, such as supporting collaborative editing over time by embedding a Textlet into the document to flag forbidden words. We argue that Textlets serve as a generative concept for creating powerful new tools for document editing.","reification, document processing, text editing","","CHI '20"
"Conference Paper","An K,Tilevich E","Client Insourcing: Bringing Ops In-House for Seamless Re-Engineering of Full-Stack JavaScript Applications","","2020","","","179–189","Association for Computing Machinery","New York, NY, USA","Proceedings of The Web Conference 2020","Taipei, Taiwan","2020","9781450370233","","https://doi.org/10.1145/3366423.3380105;http://dx.doi.org/10.1145/3366423.3380105","10.1145/3366423.3380105","Modern web applications are distributed across a browser-based client and a cloud-based server. Distribution provides access to remote resources, accessed over the web and shared by clients. Much of the complexity of inspecting and evolving web applications lies in their distributed nature. Also, the majority of mature program analysis and transformation tools works only with centralized software. Inspired by business process re-engineering, in which remote operations can be insourced back in house to restructure and outsource anew, we bring an analogous approach to the re-engineering of web applications. Our target domain are full-stack JavaScript applications that implement both the client and server code in this language. Our approach is enabled by Client Insourcing, a novel automatic refactoring that creates a semantically equivalent centralized version of a distributed application. This centralized version is then inspected, modified, and redistributed to meet new requirements. After describing the design and implementation of Client Insourcing, we demonstrate its utility and value in addressing changes in security, reliability, and performance requirements. By reducing the complexity of the non-trivial program inspection and evolution tasks performed to meet these requirements, our approach can become a helpful aid in the re-engineering of web applications in this domain.","Web Applications, Re-Engineering, Software Engineering, Program Analysis & Transformation, Mobile Apps, Middleware, JavaScript","","WWW '20"
"Conference Paper","Zhao Y,Xiao L,Wang X,Sun L,Chen B,Liu Y,Bondi AB","How Are Performance Issues Caused and Resolved?-An Empirical Study from a Design Perspective","","2020","","","181–192","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/SPEC International Conference on Performance Engineering","Edmonton AB, Canada","2020","9781450369916","","https://doi.org/10.1145/3358960.3379130;http://dx.doi.org/10.1145/3358960.3379130","10.1145/3358960.3379130","Empirical experience regarding how real-life performance issues are caused and resolved can provide valuable insights for practitioners to effectively and efficiently prevent, detect, and fix performance issues. Prior work shows that most performance issues have their roots in poor architectural decisions. This paper contributes a large scale empirical study of 192 real-life performance issues, with an emphasis on software design. First, this paper contributes a holistic view of eight common root causes and typical resolutions that recur in different projects, and surveyed existing literature, in particular, tools, that can detect and fix each type of performance issue. Second, this study is first-of-its-kind to investigate performance issues from a design perspective. In the 192 issues, 33% required design-level optimization, i.e. simultaneously revising a group of related source files for resolving the issues. We reveal four design-level optimization patterns, which have shown different prevalence in resolving different root causes. Finally, this study investigated the Return on Investment for addressing performance issues, to help practitioners choose between localized or design-level optimization resolutions, and to prioritize issues due to different root causes.","software design structure, design patterns, software performance","","ICPE '20"
"Conference Paper","Wu H","A Systematical Study for Deep Learning Based Android Malware Detection","","2020","","","177–182","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 9th International Conference on Software and Computer Applications","Langkawi, Malaysia","2020","9781450376655","","https://doi.org/10.1145/3384544.3384546;http://dx.doi.org/10.1145/3384544.3384546","10.1145/3384544.3384546","Nowadays with the development of smartphone and its operating system, such as Android, the amount of mobile malware is correspondingly increasing. To protect the privacy security of both users and manufacturers, Android malware detection has received a lot of research focuses. Traditional methods mainly rely on static analysis or dynamic monitoring, which is either software sensitive or time-consuming. Recently, with the development of machine learning and deep learning techniques, many efforts introduced such learning-based techniques into the Android malware detection and achieved promising detection results as well as substantially reduced time costing. Nevertheless, there is still a lack of a comprehensive summary at the deep learning technical level for these learning-based malware detection works. As a result, it is limited to improve the detection technique referring to existing works from a global map. To address the challenge, in this paper, we systematically study existing deep learning-based malware detection works and classify them from the technique perspective. We also conclude the advantages and threats within each category of detection technique and provide a concrete technical reference for future improvement work.","estimate, deep learning, malware classification, malware detection","","ICSCA 2020"
"Journal Article","Walker A,Cerny T","On Cloud Computing Infrastructure for Existing Code-Clone Detection Algorithms","SIGAPP Appl. Comput. Rev.","2020","20","1","5–14","Association for Computing Machinery","New York, NY, USA","","","2020-04","","1559-6915","https://doi.org/10.1145/3392350.3392351;http://dx.doi.org/10.1145/3392350.3392351","10.1145/3392350.3392351","Microservice Architecture (MSA) is becoming a design standard for modern cloud-based software systems. However, even though cloud-based applications have been thoroughly explored with regards to networking, scalability, and decomposition of existing monolithic applications into MSA based applications, not much research has been done showing the viability of MSA in new problem domains. In this paper, we explore the application of MSA to the code-clone detection problem domain to identify any improvements that can be made over existing local code-clone detection applications. A fragment of source code that is identical or similar to another is a code-clone. Code-clones make it difficult to maintain applications as they create multiple points within the code that bugs must be fixed, new rules enforced, or design decisions imposed. As applications grow larger and larger, the pervasiveness of code-clones likewise grows. To face the code-clone related issues, many tools and algorithms have been proposed to find and document code-clones within an application. In this paper, we show that many improvements can be made by utilizing emerging cloud-based technologies.","clone detection, microservices, scalable code clone detection, cloud computing, software as a service, code clone","",""
"Conference Paper","Bergsten L,Johansson BJ,Berggren P,van Laere J,Ibrahim O,Larsson A,Olsson L","Designing Engaging Computer Based Simulation Games for Increasing Societal Resilience to Payment System Disruptions","","2020","","","166–172","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business","Tokyo, Japan","2020","9781450376778","","https://doi.org/10.1145/3383845.3383859;http://dx.doi.org/10.1145/3383845.3383859","10.1145/3383845.3383859","Large or lengthy disruptions to the card payment system are threats that can cause crisis in society, especially in countries where other payment options are scarce. This paper presents a study that provides suggestions on how to improve a simulation game used to increase societal resilience to payment system disruptions. Questionnaires and interviews have been used to investigate how 16 participant in crisis exercises experience realism, relevance and validity in such exercises. Suggestions on how to improve the simulation game are provided, such as improvements to the graphical interface and introducing supporting roles from the exercise management.","Simulator design, Critical infrastructure, Resilience, Crisis response, Payment system","","ICCMB 2020"
"Conference Paper","Gholamian S,Ward PA","Logging Statements' Prediction Based on Source Code Clones","","2020","","","82–91","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM Symposium on Applied Computing","Brno, Czech Republic","2020","9781450368667","","https://doi.org/10.1145/3341105.3373845;http://dx.doi.org/10.1145/3341105.3373845","10.1145/3341105.3373845","Log files are widely used to record runtime information of software systems, such as the time-stamp of an event, the unique ID of the source of the log, and a part of the state of task execution. The rich information of logs enables system operators to monitor the runtime behaviors of their systems and further track down system problems in production settings. Although logs are useful, there exists a trade-off between their benefit and cost, and it is a crucial problem to optimize the location and content of log messages in the source code, i.e., ""where and what to log?""Prior research has analyzed logging statements in the source code and proposed ways to predict and suggest the location of log statements in order to partially automate log statement addition to the source code. However, there are gaps and unsolved problems in the literature to fully automate the logging process. Thus, in this research, we perform an experimental study on open-source Java projects and apply code-clone detection methods for log statements' prediction. Our work demonstrates the feasibility of logging automation by predicting the location of a log point in a code snippet based on the existence of a logging statement in its corresponding code clone pair. We propose a Log-Aware Code-Clone Detector (LACC) which achieves a higher accuracy of log prediction when compared to state-of-the-art general-purpose clone detectors. Our analysis shows that 98% of clone snippets match in their logging behavior, and LACC can predict the location of logging statements by the accuracy of 90+% for Apache Java projects.","source code, code clones, software engineering, automation, logging statement","","SAC '20"
"Conference Paper","Corradini F,Marcelletti A,Morichetta A,Polini A,Re B,Tiezzi F","Engineering Trustable Choreography-Based Systems Using Blockchain","","2020","","","1470–1479","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM Symposium on Applied Computing","Brno, Czech Republic","2020","9781450368667","","https://doi.org/10.1145/3341105.3373988;http://dx.doi.org/10.1145/3341105.3373988","10.1145/3341105.3373988","The adoption of model-driven engineering methodologies contributes to reduce the complexity of developing distributed systems. A key point to master such complexity is the use of modelling languages, such as the BPMN standard. This permits to specify choreography diagrams describing, from a global point of view, the interactions that should occur among distributed components in order to reach given goals. Even though BPMN choreographies are promising to increase business possibilities, their concrete adoption has been challenging and faced complex hurdles. On the one hand, there is a lack of concrete support to the different phases of the choreography life-cycle, especially in relation to the choreography execution. Another obstacle consists in the lack of distributed infrastructures allowing the participants involved in the cooperation to trust each other, and in particular to get enough guarantees that all of them will behave as prescribed by the choreography model.In this paper, we face such challenges by proposing a methodology and a related model-driven framework, named ChorChain, that are based on the blockchain technology. We provide support to the whole life-cycle of choreographies, from their modelling to their distributed execution. More specifically, ChorChain takes as input a BPMN choreography model and automatically translates it in a Solidity smart contract. Such a contract will permit to enforce the interactions among the cooperating participants, so to satisfy the prescriptions reported in the starting model. The methodology and the framework have been evaluated through experiments conducted on the Rinkeby Ethereum Testnet.","","","SAC '20"
"Conference Paper","Dashevskyi S,Zhauniarovich Y,Gadyatskaya O,Pilgun A,Ouhssain H","Dissecting Android Cryptocurrency Miners","","2020","","","191–202","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy","New Orleans, LA, USA","2020","9781450371070","","https://doi.org/10.1145/3374664.3375724;http://dx.doi.org/10.1145/3374664.3375724","10.1145/3374664.3375724","Cryptojacking applications pose a serious threat to mobile devices. Due to the extensive computations, they deplete the battery fast and can even damage the device. In this work we make a step towards combating this threat. We collected and manually verified a large dataset of Android mining apps. In this paper, we analyze the gathered miners and identify how they work, what are the most popular libraries and APIs used to facilitate their development, and what static features are typical for this class of applications. Further, we analyzed our dataset using VirusTotal. The majority of our samples is considered malicious by at least one VirusTotal scanner, but 16 apps are not detected by any engine; and at least 5 apks were not seen previously by the service. Mining code could be obfuscated or fetched at runtime, and there are many confusing miner-related apps that actually do not mine. Thus, static features alone are not sufficient for miner detection. We have collected a feature set of dynamic metrics both for miners and unrelated benign apps, and built a machine learning-based tool for dynamic detection. Our BrenntDroid tool is able to detect miners with 95% of accuracy on our dataset.","android, cpu mining, malware, cryptojacking, cryptominer","","CODASPY '20"
"Conference Paper","Cheers H,Lin Y","A Novel Graph-Based Program Representation for Java Code Plagiarism Detection","","2020","","","115–122","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Conference on Software Engineering and Information Management","Sydney, NSW, Australia","2020","9781450376907","","https://doi.org/10.1145/3378936.3378960;http://dx.doi.org/10.1145/3378936.3378960","10.1145/3378936.3378960","Source code plagiarism is a long-standing issue in undergraduate computer science education. Identifying instances of source code plagiarism is a difficult and time-consuming task. To aid in its identification, many automated tools have been proposed to find indications of plagiarism. However, prior works have shown that common source code plagiarism detection tools are susceptible to plagiarism-hiding transformations. In this paper a novel graph-based representation of Java programs is presented which is resilient to plagiarism-hiding transformations. This graph is titled the Program Interaction Dependency Graph (PIDG) and represents the interaction and transformation of data within a program, and how this data interacts with the system. To show the effectiveness of this graph, it is evaluated on a data set of simulated source code plagiarism. The results of this evaluation indicate the PIDG is a promising means of representing programs in a form that is resilient to plagiarism.","Program similarity, Source code plagiarism detection, Program representation, Dependency graph","","ICSIM '20"
"Conference Paper","Stuurman S,Passier HJ,Geven F,Barendsen E","Autism: Implications for Inclusive Education with Respect to Software Engineering","","2020","","","15–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Computer Science Education Research Conference","Larnaca, Cyprus","2020","9781450377171","","https://doi.org/10.1145/3375258.3375261;http://dx.doi.org/10.1145/3375258.3375261","10.1145/3375258.3375261","Within Computer science and Software engineering, the prevalence of students with a diagnosis of autism spectrum disorder is relatively high. Ideally, education should be inclusive, with which we mean that education must be given in such a way that additional support is needed as little as possible.In this paper, we present an overview on what is known about the cognitive style of autistic individuals and compare that cognitive thinking style with computational thinking, thinking as an engineer, and with academic thinking. We illustrate the cognitive style of autistic students with anecdotes from our students.From the comparison, we derive a set of guidelines for inclusive education, and we present ideas for future work.","Inclusive education, Autism, Cognitive thinking style","","CSERC '19"
"Conference Paper","Power JF,Waldron J","Calibration and Analysis of Source Code Similarity Measures for Verilog Hardware Description Language Projects","","2020","","","420–426","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st ACM Technical Symposium on Computer Science Education","Portland, OR, USA","2020","9781450367936","","https://doi.org/10.1145/3328778.3366928;http://dx.doi.org/10.1145/3328778.3366928","10.1145/3328778.3366928","In this paper we report on our experiences during a first-year course on digital logic design using the Verilog hardware description language. As part of the course the students were given a series of take-home assignments, which were then marked using an automated assessment system developed by the authors. During the course the instructor was made aware that a set of solutions had been circulated to the students, and was asked to assess the impact that this had on the assessment regime. In order to answer this question, we examined and implemented a number of approaches to calculating similarity between Verilog programs, and we present the results of that study in this paper. An important feature of this work was ensuring that the measurements used were well-understood, properly calibrated and defensible. We report on the results of this study, applied to a class of 115 students who completed up to 11 projects each.","program similarity, automated assessment, hardware description language","","SIGCSE '20"
"Conference Paper","Ericson BJ,Miller BN","Runestone: A Platform for Free, On-Line, and Interactive Ebooks","","2020","","","1012–1018","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st ACM Technical Symposium on Computer Science Education","Portland, OR, USA","2020","9781450367936","","https://doi.org/10.1145/3328778.3366950;http://dx.doi.org/10.1145/3328778.3366950","10.1145/3328778.3366950","The Runestone platform is open-source, extensible, and serves free ebooks to over 25,000 learners a day from around the world. The site hosts 18 ebooks for computing courses. Some of these ebook have been translated into several languages. There are ebooks for secondary computer science (AP CSP and AP CSA), CS1, CS2, data science, and web programming courses. The platform currently supports executable and editable examples in Python, Java, C, C++, HTML, JavaScript, Processing, and SQL. Runestone provides features for instructors, learners, authors, and researchers. Instructors can create a custom course from any of the existing ebooks and their students can register for that course. Instructors can create assignments from the existing material or author new problems, grade assignments, and visualize student progress. Learners can execute and modify examples and answer practice questions with immediate feedback. Runestone includes common practice types, such as multiple-choice questions, as well as some unique types, such as adaptive Parsons problems. Authors can modify the existing ebooks or write new ebooks using restructuredText: a markup language. Researchers can create and test new interactive features, run experiments, and analyze log file data. This paper describes the architecture of the platform, highlights some of the unique features, provides an overview of how instructors use the platform, summarizes the research studies conducted on the platform, and describes plans for future development.","adaptive learning, intelligent ebooks, ebooks, on-line learning, parsons problems, practice tools","","SIGCSE '20"
"Conference Paper","Thakur M,Nandivada VK","Mix Your Contexts Well: Opportunities Unleashed by Recent Advances in Scaling Context-Sensitivity","","2020","","","27–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th International Conference on Compiler Construction","San Diego, CA, USA","2020","9781450371209","","https://doi.org/10.1145/3377555.3377902;http://dx.doi.org/10.1145/3377555.3377902","10.1145/3377555.3377902","Existing precise context-sensitive heap analyses do not scale well for large OO programs. Further, identifying the right context abstraction becomes quite intriguing as two of the most popular categories of context abstractions (call-site- and object-sensitive) lead to theoretically incomparable precision. In this paper, we address this problem by first doing a detailed comparative study (in terms of precision and efficiency) of the existing approaches, both with and without heap cloning. In addition, we propose novel context abstractions that lead to a new sweet-spot in the arena. We first enhance the precision of level-summarized relevant value (LSRV) contexts (a highly scalable abstraction with precision matching that of call-site-sensitivity) using heap cloning. Then, motivated by the resultant scalability, we propose the idea of mixing various context abstractions, and add the advantages of k-object-sensitive analyses to LSRV contexts, in an efficient manner. The resultant context abstraction, which we call lsrvkobjH, also leads to a novel connection between the two broad variants of otherwise incomparable context-sensitive analyses. Our evaluation shows that the newer proposals not only enhance the precision of both LSRV contexts and object-sensitive analyses (to perform control-flow analysis of Java programs), but also scale well to large programs.","Java, Context-sensitivity, Static analysis","","CC 2020"
"Conference Paper","Nafi KW,Kar TS,Roy B,Roy CK,Schneider KA","CLCDSA: Cross Language Code Clone Detection Using Syntactical Features and API Documentation","","2020","","","1026–1037","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00099;http://dx.doi.org/10.1109/ASE.2019.00099","10.1109/ASE.2019.00099","Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is twofold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.","API documentation, code clone, Word2Vector, source code syntax","","ASE '19"
"Conference Paper","Wan Y,Shu J,Sui Y,Xu G,Zhao Z,Wu J,Yu PS","Multi-Modal Attention Network Learning for Semantic Source Code Retrieval","","2020","","","13–25","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00012;http://dx.doi.org/10.1109/ASE.2019.00012","10.1109/ASE.2019.00012","Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.","multi-modal network, deep learning, code retrieval, attention mechanism","","ASE '19"
"Conference Paper","Liu X,Huang L,Ge J,Ng V","Predicting Licenses for Changed Source Code","","2020","","","686–697","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00070;http://dx.doi.org/10.1109/ASE.2019.00070","10.1109/ASE.2019.00070","Open source software licenses regulate the circumstances under which software can be redistributed, reused and modified. Ensuring license compatibility and preventing license restriction conflicts among source code during software changes are the key to protect their commercial use. However, selecting the appropriate licenses for software changes requires lots of experience and manual effort that involve examining, assimilating and comparing various licenses as well as understanding their relationships with software changes. Worse still, there is no state-of-the-art methodology to provide this capability. Motivated by this observation, we propose in this paper Automatic License Prediction (ALP), a novel learning-based method and tool for predicting licenses as software changes. An extensive evaluation of ALP on predicting licenses in 700 open source projects demonstrate its effectiveness: ALP can achieve not only a high overall prediction accuracy (92.5% in micro F1 score) but also high accuracies across all license types.","mining software repository, software license prediction","","ASE '19"
"Conference Paper","Tokumoto S,Takayama K","PHANTA: Diversified Test Code Quality Measurement for Modern Software Development","","2020","","","1206–1207","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00138;http://dx.doi.org/10.1109/ASE.2019.00138","10.1109/ASE.2019.00138","Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study.","","","ASE '19"
"Conference Paper","Wei B","Retrieve and Refine: Exemplar-Based Neural Comment Generation","","2020","","","1250–1252","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00152;http://dx.doi.org/10.1109/ASE.2019.00152","10.1109/ASE.2019.00152","Code comment generation is a crucial task in the field of automatic software development. Most previous neural comment generation systems used an encoder-decoder neural network and encoded only information from source code as input. Software reuse is common in software development. However, this feature has not been introduced to existing systems. Inspired by the traditional IR-based approaches, we propose to use the existing comments of similar source code as exemplars to guide the comment generation process. Based on an open source search engine, we first retrieve a similar code and treat its comment as an exemplar. Then we applied a seq2seq neural network to conduct an exemplar-based comment generation. We evaluate our approach on a large-scale Java corpus, and experimental results demonstrate that our model significantly outperforms the state-of-the-art methods.","program comprehension, comment generation, deep learning","","ASE '19"
"Conference Paper","Gu X,Zhang H,Kim S","CodeKernel: A Graph Kernel Based Approach to the Selection of API Usage Examples","","2020","","","590–601","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00061;http://dx.doi.org/10.1109/ASE.2019.00061","10.1109/ASE.2019.00061","Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples.We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative graph from each cluster using designed ranking metrics. Our empirical evaluation shows that CodeKernel selects more accurate code examples than the related work (MUSE and eXoaDocs). A user study involving 25 developers in a multinational company also confirms the usefulness of CodeKernel in selecting API usage examples.","","","ASE '19"
"Conference Paper","Kang HJ,Bissyandé TF,Lo D","Assessing the Generalizability of Code2vec Token Embeddings","","2020","","","1–12","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00011;http://dx.doi.org/10.1109/ASE.2019.00011","10.1109/ASE.2019.00011","Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for.In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.","distributed representations, code embeddings, big code","","ASE '19"
"Conference Paper","Feng M,Yuan Z,Li F,Ban G,Xiao Y,Wang S,Tang Q,Su H,Yu C,Xu J,Piao A,Xue J,Huo W","B2SFinder: Detecting Open-Source Software Reuse in COTS Software","","2020","","","1038–1049","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00100;http://dx.doi.org/10.1109/ASE.2019.00100","10.1109/ASE.2019.00100","COTS software products are developed extensively on top of OSS projects, resulting in OSS reuse vulnerabilities. To detect such vulnerabilities, finding OSS reuses in COTS software has become imperative. While scalable to tens of thousands of OSS projects, existing binary-to-source matching approaches are severely imprecise in analyzing COTS software products, since they support only a limited number of code features, compute matching scores only approximately in measuring OSS reuses, and neglect the code structures in OSS projects.We introduce a novel binary-to-source matching approach, called B2SFinder1, to address these limitations. First of all, B2SFinder can reason about seven kinds of code features that are traceable in both binary and source code. In order to compute matching scores precisely, B2SFinder employs a weighted feature matching algorithm that combines three matching methods (for dealing with different code features) with two importance-weighting methods (for computing the weight of an instance of a code feature in a given COTS software application based on its specificity and occurrence frequency). Finally, B2SFinder identifies different types of code reuses based on matching scores and code structures of OSS projects. We have implemented B2SFinder using an optimized data structure. We have evaluated B2SFinder using 21991 binaries from 1000 popular COTS software products and 2189 candidate OSS projects. Our experimental results show that B2SFinder is not only precise but also scalable. Compared with the state of the art, B2SFinder has successfully found up to 2.15x as many reuse cases in 53.85 seconds per binary file on average. We also discuss how B2SFinder can be leveraged in detecting OSS reuse vulnerabilities in practice.","binary-to-source matching, OSS, code feature, one-day vulnerability, code reuse, COTS software","","ASE '19"
"Conference Paper","Nam D,Horvath A,Macvean A,Myers B,Vasilescu B","MARBLE: Mining for Boilerplate Code to Identify API Usability Problems","","2020","","","615–627","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00063;http://dx.doi.org/10.1109/ASE.2019.00063","10.1109/ASE.2019.00063","Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: ""boilerplate code."" We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances.","","","ASE '19"
"Conference Paper","Zhou S,Shen B,Zhong H","Lancer: Your Code Tell Me What You Need","","2020","","","1202–1205","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00137;http://dx.doi.org/10.1109/ASE.2019.00137","10.1109/ASE.2019.00137","Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.","language model, code reuse, code recommendation","","ASE '19"
"Conference Paper","Lacomis J,Yin P,Schwartz EJ,Allamanis M,Goues CL,Neubig G,Vasilescu B","DIRE: A Neural Approach to Decompiled Identifier Naming","","2020","","","628–639","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00064;http://dx.doi.org/10.1109/ASE.2019.00064","10.1109/ASE.2019.00064","The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.","","","ASE '19"
"Conference Paper","Wang M,Lin Z,Zou Y,Xie B","CoRA: Decomposing and Describing Tangled Code Changes for Reviewer","","2020","","","1050–1061","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00101;http://dx.doi.org/10.1109/ASE.2019.00101","10.1109/ASE.2019.00101","Code review is an important mechanism for code quality assurance both in open source software and industrial software. Reviewers usually suffer from numerous, tangled and loosely related code changes that are bundled in a single commit, which makes code review very difficult. In this paper, we propose CoRA (Code Review Assistant), an automatic approach to decompose a commit into different parts and generate concise descriptions for reviewers. More specifically, CoRA can decompose a commit into independent parts (e.g., bug fixing, new feature adding, or refactoring) by code dependency analysis and tree-based similar-code detection, then identify the most important code changes in each part based on the PageRank algorithm and heuristic rules. As a result, CoRA can generate a concise description for each part of the commit. We evaluate our approach in seven open source software projects and 50 code commits. The results indicate that CoRA can improve the accuracy of decomposing code changes by 6.3% over the state-of-art practice. At the same time, CoRA can identify the important part from the fine-grained code changes with a mean average precision (MAP) of 87.7%. We also conduct a human study with eight participants to evaluate the performance and usefulness of CoRA, the user feedback indicates that CoRA can effectively help reviewers.","program comprehension, code review, code changes description, code changes decomposition","","ASE '19"
"Conference Paper","Jiang L,Liu H,Jiang H","Machine Learning Based Recommendation of Method Names: How Far Are We","","2020","","","602–614","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00062;http://dx.doi.org/10.1109/ASE.2019.00062","10.1109/ASE.2019.00062","High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go.","machine learning, code recommendation","","ASE '19"
"Conference Paper","Hu Y,Ahmed UZ,Mechtaev S,Leong B,Roychoudhury A","Re-Factoring Based Program Repair Applied to Programming Assignments","","2020","","","388–398","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00044;http://dx.doi.org/10.1109/ASE.2019.00044","10.1109/ASE.2019.00044","Automated program repair has been used to provide feedback for incorrect student programming assignments, since program repair captures the code modification needed to make a given buggy program pass a given test-suite. Existing student feedback generation techniques are limited because they either require manual effort in the form of providing an error model, or require a large number of correct student submissions to learn from, or suffer from lack of scalability and accuracy.In this work, we propose a fully automated approach for generating student program repairs in real-time. This is achieved by first re-factoring all available correct solutions to semantically equivalent solutions. Given an incorrect program, we match the program with the closest matching refactored program based on its control flow structure. Subsequently, we infer the input-output specifications of the incorrect program's basic blocks from the executions of the correct program's aligned basic blocks. Finally, these specifications are used to modify the blocks of the incorrect program via search-based synthesis.Our dataset consists of almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. Our experimental results suggest that our method is more effective and efficient than recently proposed feedback generation approaches. About 30% of the patches produced by our tool Refactory are smaller than those produced by the state-of-art tool Clara, and can be produced given fewer correct solutions (often a single correct solution) and in a shorter time. We opine that our method is applicable not only to programming assignments, and could be seen as a general-purpose program repair method that can achieve good results with just a single correct reference solution.","programming education, software refactoring, program repair","","ASE '19"
"Conference Paper","Jiang J,Ren L,Xiong Y,Zhang L","Inferring Program Transformations from Singular Examples via Big Code","","2020","","","255–266","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00033;http://dx.doi.org/10.1109/ASE.2019.00033","10.1109/ASE.2019.00033","Inferring program transformations from concrete program changes has many potential uses, such as applying systematic program edits, refactoring, and automated program repair. Existing work for inferring program transformations usually rely on statistical information over a potentially large set of program-change examples. However, in many practical scenarios we do not have such a large set of program-change examples.In this paper, we address the challenge of inferring a program transformation from one single example. Our core insight is that ""big code"" can provide effective guide for the generalization of a concrete change into a program transformation, i.e., code elements appearing in many files are general and should not be abstracted away. We first propose a framework for transformation inference, where programs are represented as hypergraphs to enable fine-grained generalization of transformations. We then design a transformation inference approach, GenPat, that infers a program transformation based on code context and statistics from a big code corpus.We have evaluated GenPat under two distinct application scenarios, systematic editing and program repair. The evaluation on systematic editing shows that GenPat significantly outperforms a state-of-the-art approach, Sydit, with up to 5.5x correctly transformed cases. The evaluation on program repair suggests that GenPat has the potential to be integrated in advanced program repair tools - GenPat successfully repaired 19 real-world bugs in the Defects4J benchmark by simply applying transformations inferred from existing patches, where 4 bugs have never been repaired by any existing technique. Overall, the evaluation results suggest that GenPat is effective for transformation inference and can potentially be adopted for many different applications.","code abstraction, pattern generation, program adaptation","","ASE '19"
"Conference Paper","Feng Q,Cai Y,Kazman R,Cui D,Liu T,Fang H","Active Hotspot: An Issue-Oriented Model to Monitor Software Evolution and Degradation","","2020","","","986–997","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00095;http://dx.doi.org/10.1109/ASE.2019.00095","10.1109/ASE.2019.00095","Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely.","software evolution, architecture debt","","ASE '19"
"Conference Paper","Nejadgholi M,Yang J","A Study of Oracle Approximations in Testing Deep Learning Libraries","","2020","","","785–796","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00078;http://dx.doi.org/10.1109/ASE.2019.00078","10.1109/ASE.2019.00078","Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature.In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in testing DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries.","software quality assurance, software testing, test oracle, testing deep learning libraries","","ASE '19"
"Conference Paper","Michael LG,Donohue J,Davis JC,Lee D,Servant F","Regexes Are Hard: Decision-Making, Difficulties, and Risks in Programming Regular Expressions","","2020","","","415–426","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00047;http://dx.doi.org/10.1109/ASE.2019.00047","10.1109/ASE.2019.00047","Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face.In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer.In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.","developer process, regular expressions, qualitative research","","ASE '19"
"Conference Paper","Luo L,Bodden E,Späth J","A Qualitative Analysis of Android Taint-Analysis Results","","2020","","","102–114","IEEE Press","San Diego, California","Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering","","2020","9781728125084","","https://doi.org/10.1109/ASE.2019.00020;http://dx.doi.org/10.1109/ASE.2019.00020","10.1109/ASE.2019.00020","In the past, researchers have developed a number of popular taint-analysis approaches, particularly in the context of Android applications. Numerous studies have shown that automated code analyses are adopted by developers only if they yield a good ""signal to noise ratio"", i.e., high precision. Many previous studies have reported analysis precision quantitatively, but this gives little insight into what can and should be done to increase precision further.To guide future research on increasing precision, we present a comprehensive study that evaluates static Android taint-analysis results on a qualitative level. To unravel the exact nature of taint flows, we have designed COVA, an analysis tool to compute partial path constraints that inform about the circumstances under which taint flows may actually occur in practice.We have conducted a qualitative study on the taint flows reported by FlowDroid in 1,022 real-world Android applications. Our results reveal several key findings: Many taint flows occur only under specific conditions, e.g., environment settings, user interaction, I/O. Taint analyses should consider the application context to discern such situations. COVA shows that few taint flows are guarded by multiple different kinds of conditions simultaneously, so tools that seek to confirm true positives dynamically can concentrate on one kind at a time, e.g., only simulating user interactions. Lastly, many false positives arise due to a too liberal source/sink configuration. Taint analyses must be more carefully configured, and their configuration could benefit from better tool assistance.","taint analysis, path conditions, android","","ASE '19"
"Conference Paper","Schlie A,Schulze S,Schaefer I","Recovering Variability Information from Source Code of Clone-and-Own Software Systems","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems","Magdeburg, Germany","2020","9781450375016","","https://doi.org/10.1145/3377024.3377034;http://dx.doi.org/10.1145/3377024.3377034","10.1145/3377024.3377034","Clone-and-own prevails as an ad-hoc reuse strategy that addresses changing requirements by copying and modifying existing system variants. Proper documentation is typically not cherished and knowledge about common and varying parts between individual variants, denoted their variability information, is lost with a growing system family. With overall maintainability impaired in the longrun, software product lines (SPLs) or concepts thereof, can be a remedy. However, migrating a system family towards structured reuse requires a prior recovery of the systems' variability information. For software systems resulting from clone-and-own, this information is not explicitly available and recovering it remains an open challenge.We aim to bridge this gap and propose a fine-grained metric and analysis procedure, which compares software systems to the extent of individual statements including their nesting. By that, we recover variability information from software systems written in imperative programming languages. Moreover, we create a software family representation of all analyzed systems, called a 150% model, which contains implementation artifacts and their identified variability information. We demonstrate the feasibility of our approach using two case studies implemented in Java and show our approach to exhibit a good performance and the 150% model to precisely capture variability information of the analyzed systems.","variability, clone-and-own, recovering, 150% model, source code","","VaMoS '20"
"Conference Paper","Bordis T,Runge T,Knüppel A,Thüm T,Schaefer I","Variational Correctness-by-Construction","","2020","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems","Magdeburg, Germany","2020","9781450375016","","https://doi.org/10.1145/3377024.3377038;http://dx.doi.org/10.1145/3377024.3377038","10.1145/3377024.3377038","Nowadays, the requirements for software and therefore also the required complexity is increasing steadily. Consequently, various techniques to handle the growing demand for software variants in one specific domain are used. These techniques often rely on variable code structures to implement a whole product family more efficiently. Variational software is also increasingly used for safety-critical systems, which need to be verified to guarantee their functionality in-field. However, usual verification techniques can not directly be applied to the variable code structures of most techniques. In this paper, we propose variational correctness-by-construction as a methodology to implement variational software extending the correctness-by-construction approach. Correctness-by-construction is an incremental approach to create and verify programs using small tractable refinement steps guided by a specification following the design-by-contract paradigm. Our contribution is threefold. First, we extend the list of refinement rules to enable variability in programs developed with correctness-by-construction. Second, we motivate the need for contract composition of refined method contracts and illustrate how this can be achieved. Third, we implement variational correctness-by-construction in a tool called VarCorC. We successfully conducted two case studies showing the applicability of VarCorC and were able to assess reduced verification costs compared to post-hoc verification as well.","variational software, formal methods, correctness-by-construction, design-by-contract, deductive verification","","VaMoS '20"
"Journal Article","Ahmed SS,Roy S,Kalita J","Assessing the Effectiveness of Causality Inference Methods for Gene Regulatory Networks","IEEE/ACM Trans. Comput. Biol. Bioinformatics","2020","17","1","56–70","IEEE Computer Society Press","Washington, DC, USA","","","2020-02","","1545-5963","https://doi.org/10.1109/TCBB.2018.2853728;http://dx.doi.org/10.1109/TCBB.2018.2853728","10.1109/TCBB.2018.2853728","Causality inference is the use of computational techniques to predict possible causal relationships for a set of variables, thereby forming a directed network. Causality inference in Gene Regulatory Networks (GRNs) is an important, yet challenging task due to the limits of available data and lack of efficiency in existing causality inference techniques. A number of techniques have been proposed and applied to infer causal relationships in various domains, although they are not specific to regulatory network inference. In this paper, we assess the effectiveness of methods for inferring causal GRNs. We introduce seven different inference methods and apply them to infer directed edges in GRNs. We use time-series expression data from the DREAM challenges to assess the methods in terms of quality of inference and rank them based on performance. The best method is applied to Breast Cancer data to infer a causal network. Experimental results show that Causation Entropy is best, however, highly time-consuming and not feasible to use in a relatively large network. We infer Breast Cancer GRN with the second-best method, Transfer Entropy. The topological analysis of the network reveals that top out-degree genes such as SLC39A5 which are considered central genes, play important role in cancer progression.","","",""
"Conference Paper","Cheers H,Lin Y,Smith SP","Detecting Pervasive Source Code Plagiarism through Dynamic Program Behaviours","","2020","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373168;http://dx.doi.org/10.1145/3373165.3373168","10.1145/3373165.3373168","Source code plagiarism is a persistent problem in undergraduate computer science education. Unfortunately, it is a widespread phenomena with many students plagiarising either because they are unwilling or incapable of completing their own work. Many source code plagiarism detection tools have been proposed to identify suspected cases of source code plagiarism. However, these tools are not resilient to pervasive plagiarism-hiding transformations that significantly change the structure of source code. In this paper, two case studies are presented that explore how resilient current source code plagiarism detection tools are to plagiarism-hiding transformations. Furthermore, an evaluation of a new advanced technique for source code plagiarism detection is presented to show that is it possible to identify pervasive cases of source code plagiarism. The results of this evaluation indicate the technique is robust in its ability to identify the same program after it has been transformed.","Program similarity, Source code plagiarism detection, Source code obfuscation","","ACE'20"
"Conference Paper","Karnalim O,Simon","Syntax Trees and Information Retrieval to Improve Code Similarity Detection","","2020","","","48–55","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373171;http://dx.doi.org/10.1145/3373165.3373171","10.1145/3373165.3373171","In dealing with source code plagiarism and collusion, automated code similarity detection can be used to filter student submissions and draw attention to pairs of programs that appear unduly similar. The effectiveness of the detection process can be improved by considering more structural information about each program, but the ensuing computation can increase the processing time. This paper proposes a similarity detection technique that uses richer structural information than normal while maintaining a reasonable execution time. The technique generates the syntax trees of program code files, extracts directly connected n-gram structure tokens from them, and performs the subsequent comparisons using an algorithm from information retrieval, cosine correlation in the vector space model. Evaluation of the approach shows that consideration of the program structure (i.e., syntax tree) increases the recall and f-score (measures of effectiveness) at the expense of execution time (a measure of efficiency). However, the use of an information retrieval comparison process goes some way to offsetting this loss of efficiency.","plagiarism and collusion in programming, information retrieval, syntax tree, source code similarity detection, computing education","","ACE'20"
"Conference Paper","Pelchen T,Mathieson L,Lister R","On the Evidence for a Learning Hierarchy in Data Structures Exams","","2020","","","122–131","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Second Australasian Computing Education Conference","Melbourne, VIC, Australia","2020","9781450376860","","https://doi.org/10.1145/3373165.3373179;http://dx.doi.org/10.1145/3373165.3373179","10.1145/3373165.3373179","Several previous research studies have found a relationship between the ability of novices to trace and explain code, and the ability to write code. Harrington and Cheng refer to that relationship as the Learning Hierarchy. However, almost all of those studies examined students at the end of their first semester of learning to program (i.e. CS1). This paper is only the third paper to describe a study of explain in plain English questions on students at the end of an introductory data structures course. The preceding two papers reached contradictory conclusions. Corney et al. presented results consistent with the Learning Hierarchy identified in the CS1 studies. However, Harrington and Cheng presented results for data structures students suggesting that the hierarchy reversed by the time students had progressed to the level of learning about data structures; that is, tracing and explaining were skills that followed writing. In our study of data structures students, we present results that are consistent with the Learning Hierarchy derived from the CS1 students. We believe that the reversal identified by Harrington and Cheng can occur, but only as a consequence of a mismatch in the relative difficulty of tracing, explaining and writing questions.","explain in plain English, programming, data structures","","ACE'20"
"Journal Article","Haas R,Niedermayr R,Roehm T,Apel S","Is Static Analysis Able to Identify Unnecessary Source Code?","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3368267;http://dx.doi.org/10.1145/3368267","10.1145/3368267","Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way.We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closed-source software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects. Our study shows that recommendations generated from stability and centrality information point to unnecessary code that cannot be identified by dead code detectors. Developers confirmed that 34% of recommendations were indeed unnecessary and deleted 20% of the recommendations shortly after our interviews. Overall, our results suggest that static analysis can provide quick feedback on unnecessary code and is useful in practice.","Unnecessary code, code stability, code centrality","",""
"Journal Article","Yuan Y,Banzhaf W","Toward Better Evolutionary Program Repair: An Integrated Approach","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3360004;http://dx.doi.org/10.1145/3360004","10.1145/3360004","Bug repair is a major component of software maintenance, which requires a huge amount of manpower. Evolutionary computation, particularly genetic programming (GP), is a class of promising techniques for automating this time-consuming and expensive process. Although recent research in evolutionary program repair has made significant progress, major challenges still remain. In this article, we propose ARJA-e, a new evolutionary repair system for Java code that aims to address challenges for the search space, search algorithm, and patch overfitting. To determine a search space that is more likely to contain correct patches, ARJA-e combines two sources of fix ingredients (i.e., the statement-level redundancy assumption and repair templates) with contextual analysis-based search space reduction, thereby leveraging their complementary strengths. To encode patches in GP more properly, ARJA-e unifies the edits at different granularities into statement-level edits and then uses a lower-granularity patch representation that is characterized by the decoupling of statements for replacement and statements for insertion. ARJA-e also uses a finer-grained fitness function that can make full use of semantic information contained in the test suite, which is expected to better guide the search of GP. To alleviate patch overfitting, ARJA-e further includes a postprocessing tool that can serve the purposes of overfit detection and patch ranking. We evaluate ARJA-e on 224 real Java bugs from Defects4J and compare it with the state-of-the-art repair techniques. The evaluation results show that ARJA-e can correctly fix 39 bugs in terms of the patches ranked first, achieving substantial performance improvements over the state of the art. In addition, we analyze the effect of the components of ARJA-e qualitatively and quantitatively to demonstrate their effectiveness and advantages.","genetic improvement, genetic programming, program repair, Evolutionary computation","",""
"Journal Article","Chen L,Wu D,Ma W,Zhou Y,Xu B,Leung H","How C++ Templates Are Used for Generic Programming: An Empirical Study on 50 Open Source Systems","ACM Trans. Softw. Eng. Methodol.","2020","29","1","","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1049-331X","https://doi.org/10.1145/3356579;http://dx.doi.org/10.1145/3356579","10.1145/3356579","Generic programming is a key paradigm for developing reusable software components. The inherent support for generic constructs is therefore important in programming languages. As for C++, the generic construct, templates, has been supported since the language was first released. However, little is currently known about how C++ templates are actually used in developing real software. In this study, we conduct an experiment to investigate the use of templates in practice. We analyze 1,267 historical revisions of 50 open source systems, consisting of 566 million lines of C++ code, to collect the data of the practical use of templates. We perform statistical analyses on the collected data and produce many interesting results. We uncover the following important findings: (1) templates are practically used to prevent code duplication, but this benefit is largely confined to a few highly used templates; (2) function templates do not effectively replace C-style generics, and developers with a C background do not show significant preference between the two language constructs; (3) developers seldom convert dynamic polymorphism to static polymorphism by using CRTP (Curiously Recursive Template Pattern); (4) the use of templates follows a power-law distribution in most cases, and C++ developers who prefer using templates are those without other language background; (5) C developer background seems to override C++ project guidelines. These findings are helpful not only for researchers to understand the tendency of template use but also for tool builders to implement better tools to support generic programming.","empirical study, Programming language, template, C++, generic programming","",""
"Journal Article","Walker A,Cerny T,Song E","Open-Source Tools and Benchmarks for Code-Clone Detection: Past, Present, and Future Trends","SIGAPP Appl. Comput. Rev.","2020","19","4","28–39","Association for Computing Machinery","New York, NY, USA","","","2020-01","","1559-6915","https://doi.org/10.1145/3381307.3381310;http://dx.doi.org/10.1145/3381307.3381310","10.1145/3381307.3381310","A fragment of source code that is identical or similar to another is a code-clone. Code-clones make it difficult to maintain applications as they create multiple points within the code that bugs must be fixed, new rules enforced, or design decisions imposed. As applications grow larger and larger, the pervasiveness of code-clones likewise grows. To face the code-clone related issues, many tools and algorithms have been proposed to find and document code-clones within an application. In this paper, we present the historical trends in code-clone detection tools to show how we arrived at the current implementations. We then present our results from a systematic mapping study on current (2009-2019) code-clone detection tools with regards to technique, open-source nature, and language coverage. Lastly, we propose future directions for code-clone detection tools. This paper provides the essentials to understanding the code-clone detection process and the current state-of-art solutions.","code clone, mapping study, survey, clone detection","",""
"Conference Paper","Panigutti C,Perotti A,Pedreschi D","Doctor XAI: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations","","2020","","","629–639","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency","Barcelona, Spain","2020","9781450369367","","https://doi.org/10.1145/3351095.3372855;http://dx.doi.org/10.1145/3351095.3372855","10.1145/3351095.3372855","Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","explainable artificial intelligence, healthcare data, machine learning","","FAT* '20"
"Conference Paper","Sato Y,Kameyama Y,Watanabe T","Module Generation without Regret","","2020","","","1–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2020 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation","New Orleans, LA, USA","2020","9781450370967","","https://doi.org/10.1145/3372884.3373160;http://dx.doi.org/10.1145/3372884.3373160","10.1145/3372884.3373160","Modules are an indispensable mechanism for providing abstraction to programming languages. To reduce the abstraction overhead in the usage of modules, Watanabe et al. proposed a language for generating and manipulating code of modules, and implemented it via a translation to plain MetaOCaml. Unfortunately, their solution has a serious problem of code explosion if functors are repeatedly applied to modules. Another problem in their solution is that it does not allow nested modules. This paper proposes a refined translation for a two-stage typed language with module generation where nested modules are allowed. Our translation does not suffer from the code-duplication problem. The key idea is to use the genlet operator in latest MetaOCaml, which performs let insertion at the code-generation time to allow sharing of code fragments. To our knowledge, our work is the first to apply genlet to code generation for modules. We conduct an experiment using a microbenchmark, and the result shows that our method is effective to reduce the size of generated code that would have been exponentially large.","Program Transformation, Modules, Type Safety, Program Generation","","PEPM 2020"
"Conference Paper","Vasileiadis L,Ceccato M,Corradini D","Revealing Malicious Remote Engineering Attempts on Android Apps with Magic Numbers","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Workshop on Software Security, Protection, and Reverse Engineering","San Juan, Puerto Rico, USA","2019","9781450377461","","https://doi.org/10.1145/3371307.3371312;http://dx.doi.org/10.1145/3371307.3371312","10.1145/3371307.3371312","Malicious reverse engineering is a prominent activity conducted by attackers to plan their code tampering attacks. Android apps are particularly exposed to malicious reverse engineering, because their code can be easily analyzed and decompiled, or monitored using debugging tools, that were originally meant to be used by developers.In this paper, we propose a solution to identify attempts of malicious reverse engineering on Android apps. Our approach is based on a series of periodic checks on the execution environment (i.e., Android components) and on the app itself. The check outcome is encoded into a Magic Number and send to a sever for validation. The owner of the app is then supposed to take countermeasures and react, by disconnecting or banning the apps under attack.Our empirical validation suggests that the execution overhead caused by our periodic checks is acceptable, because its resource consumption is compatible with the resources commonly available in smartphones.","remote attestation, code tampering, malicious reverse engineering","","SSPREW9 '19"
"Journal Article","Albluwi I","Plagiarism in Programming Assessments: A Systematic Review","ACM Trans. Comput. Educ.","2019","20","1","","Association for Computing Machinery","New York, NY, USA","","","2019-12","","","https://doi.org/10.1145/3371156;http://dx.doi.org/10.1145/3371156","10.1145/3371156","This article is a systematic review of work in the computing education literature on plagiarism. The goal of the review is to summarize the main results found in the literature and highlight areas that need further work. Despite the the large body of work on plagiarism, no systematic reviews have been published so far.The reviewed papers were categorized and analyzed using a theoretical framework from the field of Fraud Deterrence named the Fraud Triangle. According to this framework, fraudulent behavior occurs when the person is under pressure, perceives the availability of an opportunity to commit fraud, and rationalizes the fraudulent behavior in a way that makes it seem not unethical to him or her.The review found the largest amount of the reviewed papers to discuss ways for reducing the opportunity to plagiarize, as well as tools for detecting plagiarism. However, there is a clear lack of empirical work evaluating the deterrent efficacy of these strategies and tools. The reviewed papers also included mentions of a wide range of rationalizations used by computing students when justifying plagiarism, the most important of which are rationalizations that stem from confusion about what constitutes plagiarism. Finally, work on the relationship between pressure in computing courses and plagiarism was found to be very scarce and incommensurate with the significant contribution of this factor to plagiarism.","Introductory programming, academic integrity, cheating, plagiarism","",""
"Conference Paper","Ericson B,McCall A,Cunningham K","Investigating the Affect and Effect of Adaptive Parsons Problems","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th Koli Calling International Conference on Computing Education Research","Koli, Finland","2019","9781450377157","","https://doi.org/10.1145/3364510.3364524;http://dx.doi.org/10.1145/3364510.3364524","10.1145/3364510.3364524","In a Parsons problem the learner places mixed-up code blocks in the correct order to solve a problem. Parsons problems can be used for both practice and assessment in programming courses. While most students correctly solve Parsons problems, some do not. Unsuccessful practice is not conducive to learning, leads to frustration, and lowers self-efficacy. Ericson invented two types of adaptation for Parsons problems, intra-problem and inter-problem, in order to decrease frustration and maximize learning gains. In intra-problem adaptation, if the learner is struggling, the problem can dynamically be made easier. In inter-problem adaptation, the next problem's difficulty is modified based on the learner's performance on the last problem. This paper reports on the first observational studies of five undergraduate students and 11 secondary teachers solving both intra-problem adaptive and non-adaptive Parsons problems. It also reports on a log file analysis with data from over 8,000 users solving non-adaptive and adaptive Parsons problems. The paper reports on teachers' understanding of the intra-problem adaptation process, their preference for adaptive or non-adaptive Parsons problems, their perception of the usefulness of solving Parsons problems in helping them learn to fix and write similar code, and the effect of adaptation (both intra-problem and inter-problem) on problem correctness. Teachers understood most of the intra-problem adaptation process, but not all. Most teachers preferred adaptive Parsons problems and felt that solving Parsons problems helped them learn to fix and write similar code. Analysis of the log file data provided evidence that learners are nearly twice as likely to correctly solve adaptive Parsons problems than non-adaptive ones.","Parsons problems, self-efficacy, Parson's problems, adaptation","","Koli Calling '19"
"Conference Paper","Bauer M,Garland M","Legate NumPy: Accelerated and Distributed Array Computing","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","Denver, Colorado","2019","9781450362290","","https://doi.org/10.1145/3295500.3356175;http://dx.doi.org/10.1145/3295500.3356175","10.1145/3295500.3356175","NumPy is a popular Python library used for performing array-based numerical computations. The canonical implementation of NumPy used by most programmers runs on a single CPU core and is parallelized to use multiple cores for some operations. This restriction to a single-node CPU-only execution limits both the size of data that can be handled and the potential speed of NumPy code. In this work we introduce Legate, a drop-in replacement for NumPy that requires only a single-line code change and can scale up to an arbitrary number of GPU accelerated nodes. Legate works by translating NumPy programs to the Legion programming model and then leverages the scalability of the Legion runtime system to distribute data and computations across an arbitrary sized machine. Compared to similar programs written in the distributed Dask array library in Python, Legate achieves speed-ups of up to 10X on 1280 CPUs and 100X on 256 GPUs.","logical regions, control replication, NumPy, task-based runtimes, legion, GPU, Python, distributed execution, HPC, legate","","SC '19"
"Journal Article","Zheng Lnico,Albano CM,Vora NM,Mai F,Nickerson JV","The Roles Bots Play in Wikipedia","Proc.  ACM Hum. -Comput.  Interact.","2019","3","CSCW","","Association for Computing Machinery","New York, NY, USA","","","2019-11","","","https://doi.org/10.1145/3359317;http://dx.doi.org/10.1145/3359317","10.1145/3359317","Bots are playing an increasingly important role in the creation of knowledge in Wikipedia. In many cases, editors and bots form tightly knit teams. Humans develop bots, argue for their approval, and maintain them, performing tasks such as monitoring activity, merging similar bots, splitting complex bots, and turning off malfunctioning bots. Yet this is not the entire picture. Bots are designed to perform certain functions and can acquire new functionality over time. They play particular roles in the editing process. Understanding these roles is an important step towards understanding the ecosystem, and designing better bots and interfaces between bots and humans. This is important for understanding Wikipedia along with other kinds of work in which autonomous machines affect tasks performed by humans. In this study, we use unsupervised learning to build a nine category taxonomy of bots based on their functions in English Wikipedia. We then build a multi-class classifier to classify 1,601 bots based on labeled data. We discuss different bot activities, including their edit frequency, their working spaces, and their software evolution. We use a model to investigate how bots playing certain roles will have differential effects on human editors. In particular, we build on previous research on newcomers by studying the relationship between the roles bots play, the interactions they have with newcomers, and the ensuing survival rate of the newcomers.","taxonomy, online communities, roles, wikipedia, governance, bots","",""
"Conference Paper","Li Z,Chen QA,Xiong C,Chen Y,Zhu T,Yang H","Effective and Light-Weight Deobfuscation and Semantic-Aware Attack Detection for PowerShell Scripts","","2019","","","1831–1847","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3363187;http://dx.doi.org/10.1145/3319535.3363187","10.1145/3319535.3363187","In recent years, PowerShell is increasingly reported to appear in a variety of cyber attacks ranging from advanced persistent threat, ransomware, phishing emails, cryptojacking, financial threats, to fileless attacks. However, since the PowerShell language is dynamic by design and can construct script pieces at different levels, state-of-the-art static analysis based PowerShell attack detection approaches are inherently vulnerable to obfuscations. To overcome this challenge, in this paper we design the first effective and light-weight deobfuscation approach for PowerShell scripts. To address the challenge in precisely identifying the recoverable script pieces, we design a novel subtree-based deobfuscation method that performs obfuscation detection and emulation-based recovery at the level of subtrees in the abstract syntax tree of PowerShell scripts. Building upon the new deobfuscation method, we are able to further design the first semantic-aware PowerShell attack detection system. To enable semantic-based detection, we leverage the classic objective-oriented association mining algorithm and newly identify 31 semantic signatures for PowerShell attacks. We perform an evaluation on a collection of 2342 benign samples and 4141 malicious samples, and find that our deobfuscation method takes less than 0.5 seconds on average and meanwhile increases the similarity between the obfuscated and original scripts from only 0.5% to around 80%, which is thus both effective and light-weight. In addition, with our deobfuscation applied, the attack detection rates for Windows Defender and VirusTotal increase substantially from 0.3% and 2.65% to 75.0% and 90.0%, respectively. Furthermore, when our deobfuscation is applied, our semantic-aware attack detection system outperforms both Windows Defender and VirusTotal with a 92.3% true positive rate and a 0% false positive rate on average.","abstract syntax tree, powershell, semantic-aware, deobfuscation","","CCS '19"
"Conference Paper","Fass A,Backes M,Stock B","HideNoSeek: Camouflaging Malicious JavaScript in Benign ASTs","","2019","","","1899–1913","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3345656;http://dx.doi.org/10.1145/3319535.3345656","10.1145/3319535.3345656","In the malware field, learning-based systems have become popular to detect new malicious variants. Nevertheless, attackers with specific and internal knowledge of a target system may be able to produce input samples which are misclassified. In practice, the assumption of strong attackers is not realistic as it implies access to insider information. We instead propose HideNoSeek, a novel and generic camouflage attack, which evades the entire class of detectors based on syntactic features, without needing any information about the system it is trying to evade. Our attack consists of changing the constructs of malicious JavaScript samples to reproduce a benign syntax. For this purpose, we automatically rewrite the Abstract Syntax Trees (ASTs) of malicious JavaScript inputs into existing benign ones. In particular, HideNoSeek uses malicious seeds and searches for isomorphic subgraphs between the seeds and traditional benign scripts. Specifically, it replaces benign sub-ASTs by their malicious equivalents (same syntactic structure) and adjusts the benign data dependencies--without changing the AST--so that the malicious semantics is kept. In practice, we leveraged 23 malicious seeds to generate 91,020 malicious scripts, which perfectly reproduce ASTs of Alexa top 10,000 web pages. Also, we can produce on average 14 different malicious samples with the same AST as each Alexa top 10. Overall, a standard trained classifier has 99.98% false negatives with HideNoSeek inputs, while a classifier trained on such samples has over 88.74% false positives, rendering the targeted static detectors unreliable.","web security, AST, malicious JavaScript, adversarial attacks","","CCS '19"
"Conference Paper","He J,Balunović M,Ambroladze N,Tsankov P,Vechev M","Learning to Fuzz from Symbolic Execution with Application to Smart Contracts","","2019","","","531–548","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3363230;http://dx.doi.org/10.1145/3319535.3363230","10.1145/3319535.3363230","Fuzzing and symbolic execution are two complementary techniques for discovering software vulnerabilities. Fuzzing is fast and scalable, but can be ineffective when it fails to randomly select the right inputs. Symbolic execution is thorough but slow and often does not scale to deep program paths with complex path conditions. In this work, we propose to learn an effective and fast fuzzer from symbolic execution, by phrasing the learning task in the framework of imitation learning. During learning, a symbolic execution expert generates a large number of quality inputs improving coverage on thousands of programs. Then, a fuzzing policy, represented with a suitable architecture of neural networks, is trained on the generated dataset. The learned policy can then be used to fuzz new programs. We instantiate our approach to the problem of fuzzing smart contracts, a domain where contracts often implement similar functionality (facilitating learning) and security is of utmost importance. We present an end-to-end system, ILF (for Imitation Learning based Fuzzer), and an extensive evaluation over >18K contracts. Our results show that ILF is effective: (i) it is fast, generating 148 transactions per second, (ii) it outperforms existing fuzzers (e.g., achieving 33% more coverage), and (iii) it detects more vulnerabilities than existing fuzzing and symbolic execution tools for Ethereum.","imitation learning, smart contracts, fuzzing, symbolic execution","","CCS '19"
"Conference Paper","Gruss D,Kraft E,Tiwari T,Schwarz M,Trachtenberg A,Hennessey J,Ionescu A,Fogh A","Page Cache Attacks","","2019","","","167–180","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","London, United Kingdom","2019","9781450367479","","https://doi.org/10.1145/3319535.3339809;http://dx.doi.org/10.1145/3319535.3339809","10.1145/3319535.3339809","We present a new side-channel attack that targets one of the most fundamental software caches in modern computer systems: the operating system page cache. The page cache is a pure software cache that contains all disk-backed pages, including program binaries, shared libraries, and other files. On Windows, dynamic pages are also part of this cache and can be attacked as well, e.g., data, heap, and stacks. Our side channel permits unprivileged monitoring of accesses to these pages of other processes, with a spatial resolution of 4kB and a temporal resolution of 2µs on Linux (≤6.7 measurements per second), and 466ns on Windows 10 (≤223 measurements per second). We systematically analyze the side channel by demonstrating different hardware-agnostic local attacks, including a sandbox-bypassing high-speed covert channel, an ASLR break on Windows 10, and various information leakages that can be used for targeted extortion, spam campaigns, and more directly for UI redressing attacks. We also show that, as with hardware cache attacks, we can attack the generation of temporary passwords on vulnerable cryptographic implementations. Our hardware-agnostic attacks can be mitigated with our proposed security patches, but the basic side channel remains exploitable via timing measurements. We demonstrate this with a remote covert channel exfiltrating information from a colluding process through innocuous server requests.","operating systems, cache attacks, software-based attacks","","CCS '19"
"Conference Paper","Mondal M,Roy B,Roy CK,Schneider KA","Ranking Co-Change Candidates of Micro-Clones","","2019","","","244–253","IBM Corp.","USA","Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering","Toronto, Ontario, Canada","2019","","","","","Identical or nearly similar code fragments in a software system's code-base are known as code clones. Code clones from the same clone class have a tendency of co-changing (changing together) consistently during evolution. Focusing on this co-change tendency, existing studies have investigated prediction and ranking co-change candidates of regular clones. However, a recent study shows that micro-clones which are smaller than the minimum size threshold of regular clones might also need to be co-changed consistently during evolution. Thus, identifying and ranking co-change candidates of micro-clones is also important. In this paper, we investigate factors that influenc the co-change tendency of the co-change candidates of a target micro-clone fragment.We mine fil level evolutionary coupling from thousands of revisions of our subject systems through mining association rules and analyze this coupling for the purpose of ranking. According to our finding on six open-source subject systems written in Java and C, consistent co-change tendency of micro-clones is influenc d by fil proximity of the micro-clone fragments as well as evolutionary coupling of the file containing those micro-clone fragments. On the basis of our finding we propose a composite ranking mechanism by incorporating both fil proximity and file coupling for ranking co-change candidates for micro-clones and fin that our proposed mechanism performs significantl better than File Proximity Ranking mechanism. We believe that our proposed ranking mechanism has the potential to help programmers in updating micro-clones consistently with less effort","","","CASCON '19"
"Conference Paper","Medeiros H,Vilain P,Mylopoulos J,Jacobsen HA","SolUnit: A Framework for Reducing Execution Time of Smart Contract Unit Tests","","2019","","","264–273","IBM Corp.","USA","Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering","Toronto, Ontario, Canada","2019","","","","","Smart contracts are software programs implemented on a blockchain platform that monitor and automate the execution of contracts to ensure compliance with the terms and conditions of a contract. As such, smart contracts represent a new kind of software that poses its own engineering challenges and requires novel software engineering techniques. In particular, smart contracts require thorough testing before they are deployed because they can't be changed after deployment. This paper proposes a novel approach for executing unit tests for smart contracts intended to reduce test execution time. This reduction is achieved through the reuse of the deployment execution of the smart contract in each test and also the reuse of the setup execution of each test. We implemented the framework SolUnit that uses this approach to execute tests written in Java for Ethereum Solidity smart contracts. We also evaluated the framework SolUnit in five projects. The results show that our approach achieves a meaningful reduction of the time to execute the tests, without breaking the principle of independent tests. The experiments were performed in two environments: an in-memory simulated blockchain and a private Ethereum-based blockchain. Overall, our approach was able to reduce the test execution time by up to 70%.","smart contract, Ethereum, software testing, SolUnit, testing framework, blockchain, unit testing, test automation, solidity","","CASCON '19"
"Conference Paper","Marin VJ,Rivero CR","Clustering Recurrent and Semantically Cohesive Program Statements in Introductory Programming Assignments","","2019","","","911–920","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM International Conference on Information and Knowledge Management","Beijing, China","2019","9781450369763","","https://doi.org/10.1145/3357384.3357960;http://dx.doi.org/10.1145/3357384.3357960","10.1145/3357384.3357960","Students taking introductory programming courses are typically required to complete assignments and expect timely feedback to advance their learning. With the current popularity of these courses in both traditional and online versions, graders are seeing themselves overwhelmed by the sheer amount of student programs they have to handle, and the quality of the educational experience provided is often compromised for promptness. Thus, there is a need for automated approaches to effectively increase grading productivity. Existing approaches in this context fail to support flexible grading schemes and customization based on the assignment at hand. This paper presents a data-driven approach for clustering recurrent program statements performing similar but not exact semantics across student programs, which we refer to as core statements. We rely on structural graph clustering over the program dependence graph representations of student programs. Such clustering is performed over the graph resulting from the pairwise approximate graph alignments of programs. Core statements help graders understand solution variations at a glance and, since they group program statements present in individual student programs, can be used to propagate feedback, thus increasing grading productivity. Our experimental results show that, on average, we discover core statements covering more than 50% of individual student programs, and that program statements grouped by core statements are semantically cohesive, which ensures effective grading.","structural graph clustering, program dependence graph, approximate graph alignment","","CIKM '19"
"Conference Paper","Guo R,Gu T,Yao Y,Xu F,Ma X","Speedup Automatic Program Repair Using Dynamic Software Updating: An Empirical Study","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Asia-Pacific Symposium on Internetware","Fukuoka, Japan","2019","9781450377010","","https://doi.org/10.1145/3361242.3361245;http://dx.doi.org/10.1145/3361242.3361245","10.1145/3361242.3361245","A typical generate-and-validate automatic program repair (APR) tool needs to repeatedly run the same test suite to validate each generated patch. This procedure is expensive when the number of patches is huge. Additionally, to scale to large programs, a program repair tool has to consider a small patch space in practice and thus may sacrifice the capability to find potential correct repairs. In this work, we propose to speed up automatic program repair to mitigate the above issues. One the one hand, we found that restarting processes to load patched code consumes the majority of total validation time. This problem is even severe when the program is running in a managed runtime such as Java virtual machine (JVM). On the other hand, dynamic software updating (DSU) can load and execute new code without restarting. To this end, we propose to use DSU techniques to speed up automatic program repair and present an empirical study in this paper. Within our study, DSU can bring up to 66.3 times speedup in comparison with the traditional restart approach. However, DSU may not be able to handle all patches and can also incur unknown side effects that lead to inconsistent validation results. We then further study the feasibility and consistency of applying DSU to speed up APR. Our results show that 1) less than 1% patches cannot be dynamically updated using the builtin DSU ability of JVM, and 2) DSU based validation leads to potentially harmful inconsistency in only 16 of 1,897,518 patches.","patch validation speedup, Automatic program repair, dynamic software update","","Internetware '19"
"Conference Paper","Cao Y,Zou Y,Xie B","Extracting Code-Relevant Description Sentences Based on Structural Similarity","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Asia-Pacific Symposium on Internetware","Fukuoka, Japan","2019","9781450377010","","https://doi.org/10.1145/3361242.3362699;http://dx.doi.org/10.1145/3361242.3362699","10.1145/3361242.3362699","Software developers often need to read code snippets that are dispersed among different documentation, e.g., Q&A posts, to reuse APIs to complete certain tasks. These code snippets are often surrounded by lengthy context text which are used to describe the functions of code snippets. It will be helpful for code comprehension if we can align a code snippet with its description. In this paper, we propose an approach to extracting code-relevant sentences from its context text. To quantify the relevance between code line and natural language sentence, we represent them with structure trees and calculate their structural similarity. We conduct two experiments to evaluate our approach. In Experiment I, the results show that our approach achieves 83.5% precision and 80.1% recall in aligning Lucene code snippets and corresponding comments. Our approach achieves 27.6% 40.2% improvement in precision compared with existing method, and 33.8% 39.7% improvement in recall. In Experiment II, the results show that our approach achieves 66.4% 93.9% precision to extract code-relevant sentences.","code and text alignment, code comprehension, code relevant description, structural similarity","","Internetware '19"
"Conference Paper","Santos IM,Hauswirth M,Nystrom N","Experiences in Bridging from Functional to Object-Oriented Programming","","2019","","","36–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN Symposium on SPLASH-E","Athens, Greece","2019","9781450369893","","https://doi.org/10.1145/3358711.3361628;http://dx.doi.org/10.1145/3358711.3361628","10.1145/3358711.3361628","Understanding how students' prior knowledge affects their learning of new concepts is essential for effective teaching. The same learning activity, or the same explanation, may have very different effects on students with different prior knowledge. In the context of teaching programming, prior knowledge includes the programming languages students studied in prior courses. In this experience report we describe our observations in teaching object-oriented programming in Java to students who previously learned functional programming in Racket's student languages. We highlight four concrete problems we encountered in teaching the second course in this sequence. We detected and addressed these problems primarily thanks to a teaching assistant who assisted in both of the courses. This experience made us realize the importance of explicitly bridging between languages in introductory programming course sequences. It also showed that the sharing of teaching staff across courses can be an effective way to detect aspects that need bridging.","functional programming, object-oriented programming, prior knowledge","","SPLASH-E 2019"
"Conference Paper","Allamanis M","The Adverse Effects of Code Duplication in Machine Learning Models of Code","","2019","","","143–153","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Athens, Greece","2019","9781450369954","","https://doi.org/10.1145/3359591.3359735;http://dx.doi.org/10.1145/3359591.3359735","10.1145/3359591.3359735","The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.","duplication, machine learning, code naturalness, big code, dataset collection","","Onward! 2019"
"Conference Paper","Kesselbacher M,Bollin A","Discriminating Programming Strategies in Scratch: Making the Difference between Novice and Experienced Programmers","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Workshop in Primary and Secondary Computing Education","Glasgow, Scotland, Uk","2019","9781450377041","","https://doi.org/10.1145/3361721.3361727;http://dx.doi.org/10.1145/3361721.3361727","10.1145/3361721.3361727","Nowadays, block-based programming environments are often used to offer a gentle introduction to learning a programming language. However, an assessment of students' programming skills based on the results of a programming task is not sufficient to determine all areas students are struggling with. We therefore introduce a learning analytics approach of measuring and evaluating the programming sequences of students that program with Scratch 3. With our measurement framework, it is possible to record, store and analyze programming sequences done on a publicly-available, instrumented Scratch 3 environment. Changes in the programming sequence are categorized regarding the used block types and types of program change. We conducted an exploratory programming trial with lower and upper secondary school students to investigate small-scale programming strategies in the recorded programming sequences. Our goals are to identify students in need of support and to identify recurring patterns used by students successful in the trial. Clustering with k-means makes it possible to identify struggling students based on both interacted block types and types of program changes. Recurring patterns in the programming sequences of successful students show that small-scale programming strategies are very diverse.","block-based programming, programming patterns, learning analytics","","WiPSCE'19"
"Conference Paper","Baniassad E,Beschastnikh I,Holmes R,Kiczales G,Allen M","Learning to Listen for Design","","2019","","","179–186","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Athens, Greece","2019","9781450369954","","https://doi.org/10.1145/3359591.3359738;http://dx.doi.org/10.1145/3359591.3359738","10.1145/3359591.3359738","In his essay, Designed as Designer, Richard Gabriel suggests that artifacts are agents of their own design. Building on Gabriel’s position, this essay makes three observations (1) Code “speaks” to the programmer through code smells, and it talks about the shape it wants to take by signalling design principle violations. By “listening” to code, even a novice programmer can let the code itself signal its own emergent natural structure. (2) Seasoned programmers listen for code smells, but they hear in the language of design principles (3) Design patterns are emergent structures that naturally arise from designers listening to what the code is signaling and then responding to these signals through refactoring transformations. Rather than seeing design patterns as an educational destination, we see them as a vehicle for teaching the skill of listening. By showing novices the stories of listening to code and unfolding design patterns (starting from code smells, through refactorings, to arrive at principled structure), we can open up the possibility of listening for emergent design.","Education, Design, Patterns","","Onward! 2019"
"Conference Paper","Cabrera Arteaga J,Monperrus M,Baudry B","Scalable Comparison of JavaScript V8 Bytecode Traces","","2019","","","22–31","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages","Athens, Greece","2019","9781450369879","","https://doi.org/10.1145/3358504.3361228;http://dx.doi.org/10.1145/3358504.3361228","10.1145/3358504.3361228","The comparison and alignment of runtime traces are essential, e.g., for semantic analysis or debugging. However, naive sequence alignment algorithms cannot address the needs of the modern web: (i) the bytecode generation process of V8 is not deterministic; (ii) bytecode traces are large. We present STRAC, a scalable and extensible tool tailored to compare bytecode traces generated by the V8 JavaScript engine. Given two V8 bytecode traces and a distance function between trace events, STRAC computes and provides the best alignment. The key insight is to split access between memory and disk. STRAC can identify semantically equivalent web pages and is capable of processing huge V8 bytecode traces whose order of magnitude matches today's web like https://2019.splashcon.org, which generates approx. 150k of V8 bytecode instructions.","Bytecode, JavaScript, V8, Similarity measurement, Sequence alignment","","VMIL 2019"
"Conference Paper","Cronburg K,Guyer SZ","Floorplan: Spatial Layout in Memory Management Systems","","2019","","","81–93","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","Athens, Greece","2019","9781450369800","","https://doi.org/10.1145/3357765.3359519;http://dx.doi.org/10.1145/3357765.3359519","10.1145/3357765.3359519","In modern runtime systems, memory layout calculations are hand-coded in systems languages. Primitives in these languages are not powerful enough to describe a rich set of layouts, leading to reliance on ad-hoc macros, numerous interrelated static constants, and other boilerplate code. Memory management policies must also carefully orchestrate their application of address calculations in order to modify memory cooperatively, a task ill-suited to low-level systems languages at hand which lack proper safety mechanisms. In this paper we introduce Floorplan, a declarative language for specifying high level memory layouts. Constraints formerly implemented by describing how to compute locations are, in Floorplan, defined declaratively using explicit layout constructs. The challenge here was to discover constructs capable of sufficiently enabling the automatic generation of address calculations. Floorplan is implemented as a compiler for generating a Rust library. In a case study of an existing implementation of the immix garbage collection algorithm, Floorplan eliminates 55 out of the 63 unsafe lines of code: 100% of unsafe lines pertaining to memory safety.","Runtime Systems, Memory Management","","GPCE 2019"
"Conference Paper","Mattis T,Rein P,Hirschfeld R","Ambiguous, Informal, and Unsound: Metaprogramming for Naturalness","","2019","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection","Athens, Greece","2019","9781450369855","","https://doi.org/10.1145/3358502.3361270;http://dx.doi.org/10.1145/3358502.3361270","10.1145/3358502.3361270","Program code needs to be understood by both machines and programmers. While the goal of executing programs requires the unambiguity of a formal language, programmers use natural language within these formal constraints to explain implemented concepts to each other. This so called naturalness – the property of programs to resemble human communication – motivated many statistical and machine learning (ML) approaches with the goal to improve software engineering activities. The metaprogramming facilities of most programming environments model the formal elements of a program (meta-objects). If ML is used to support engineering or analysis tasks, complex infrastructure needs to bridge the gap between meta-objects and ML models, changes are not reflected in the ML model, and the mapping from an ML output back into the program’s meta-object domain is laborious. In the scope of this work, we propose to extend metaprogramming facilities to give tool developers access to the representations of program elements within an exchangeable ML model. We demonstrate the usefulness of this abstraction in two case studies on test prioritization and refactoring. We conclude that aligning ML representations with the program’s formal structure lowers the entry barrier to exploit statistical properties in tool development.","metaprogramming, naturalness, machine learning, meta-objects","","META 2019"
"Conference Paper","Koscina M,Lombard-Platet M,Cluchet P","PlasticCoin: An ERC20 Implementation on Hyperledger Fabric for Circular Economy and Plastic Reuse","","2019","","","223–230","Association for Computing Machinery","New York, NY, USA","IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume","Thessaloniki, Greece","2019","9781450369886","","https://doi.org/10.1145/3358695.3361107;http://dx.doi.org/10.1145/3358695.3361107","10.1145/3358695.3361107","Cryptocurrencies have gained popularity in the last few years, thanks to the democratization of Bitcoin. However, most of these currencies have a very general purpose, namely of allowing people to pay their products with an electronic, decentralised currency. Moreover, the lack of trust of these open-network blockchains comes at a significant economic cost. In this paper, we present PlasticCoin, a cryptocurrency empowering plastic reuse in a circular economical model, that anyone can join, and that respects the ERC20 specifications on a consortium blockchain. We also explore the economical ecosystem revolving around PlasticCoin, and also introduce a way to print tickets that can temporarily hold the role of physical banknotes. Finally, we show that our system is flexible, and it can be adapted to many other business purposes.","Blockchain, Hyperledger Fabric, Circular economy, ERC20","","WI '19 Companion"
"Journal Article","Luan S,Yang D,Barnaby C,Sen K,Chandra S","Aroma: Code Recommendation via Structural Code Search","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360578;http://dx.doi.org/10.1145/3360578","10.1145/3360578","Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.","structural code search, code recommendation, clustering, feature-based code representation, clone detection","",""
"Journal Article","Li Y,Wang S,Nguyen TN,Van Nguyen S","Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360588;http://dx.doi.org/10.1145/3360588","10.1145/3360588","Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph (PDG) and Data Flow Graph (DFG) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the AST built from the method’s body. The use of PDG and DFG enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206% in accuracy.","Deep Learning, Network Embedding, Code Representation Learning, Bug Detection, Program Graphs, Attention Neural Networks","",""
"Journal Article","Wu B,Campora III JP,He Y,Schlecht A,Chen S","Generating Precise Error Specifications for C: A Zero Shot Learning Approach","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360586;http://dx.doi.org/10.1145/3360586","10.1145/3360586","In C programs, error specifications, which specify the value range that each function returns to indicate failures, are widely used to check and propagate errors for the sake of reliability and security. Various kinds of C analyzers employ error specifications for different purposes, e.g., to detect error handling bugs, yet a general approach for generating precise specifications is still missing. This limits the applicability of those tools. In this paper, we solve this problem by developing a machine learning-based approach named MLPEx. It generates error specifications by analyzing only the source code, and is thus general. We propose a novel machine learning paradigm based on transfer learning, enabling MLPEx to require only one-time minimal data labeling from us (as the tool developers) and zero manual labeling efforts from users. To improve the accuracy of generated error specifications, MLPEx extracts and exploits project-specific information. We evaluate MLPEx on 10 projects, including 6 libraries and 4 applications. An investigation of 3,443 functions and 17,750 paths reveals that MLPEx generates error specifications with a precision of 91% and a recall of 94%, significantly higher than those of state-of-the-art approaches. To further demonstrate the usefulness of the generated error specifications, we use them to detect 57 bugs in 5 tested projects.","machine learning, Error specification generation, project-specific features","",""
"Journal Article","Miltner A,Gulwani S,Le V,Leung A,Radhakrishna A,Soares G,Tiwari A,Udupa A","On the Fly Synthesis of Edit Suggestions","Proc. ACM Program. Lang.","2019","3","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","","https://doi.org/10.1145/3360569;http://dx.doi.org/10.1145/3360569","10.1145/3360569","When working with a document, users often perform context-specific repetitive edits – changes to the document that are similar but specific to the contexts at their locations. Programming by demonstration/examples (PBD/PBE) systems automate these tasks by learning programs to perform the repetitive edits from demonstration or examples. However, PBD/PBE systems are not widely adopted, mainly because they require modal UIs – users must enter a special mode to give the demonstration/examples. This paper presents Blue-Pencil, a modeless system for synthesizing edit suggestions on the fly. Blue-Pencil observes users as they make changes to the document, silently identifies repetitive changes, and automatically suggests transformations that can apply at other locations. Blue-Pencil is parameterized – it allows the ”plug-and-play” of different PBE engines to support different document types and different kinds of transformations. We demonstrate this parameterization by instantiating Blue-Pencil to several domains – C# and SQL code, markdown documents, and spreadsheets – using various existing PBE engines. Our evaluation on 37 code editing sessions shows that Blue-Pencil synthesized edit suggestions with a precision of 0.89 and a recall of 1.0, and took 199 ms to return suggestions on average. Finally, we report on several improvements based on feedback gleaned from a field study with professional programmers to investigate the use of Blue-Pencil during long code editing sessions. Blue-Pencil has been integrated with Visual Studio IntelliCode to power the IntelliCode refactorings feature.","Programming by example, Program transformation, Refactoring, Program synthesis","",""
"Journal Article","Ziegler A,Geus J,Heinloth B,Hönig T,Lohmann D","Honey, I Shrunk the ELFs: Lightweight Binary Tailoring of Shared Libraries","ACM Trans. Embed. Comput. Syst.","2019","18","5s","","Association for Computing Machinery","New York, NY, USA","","","2019-10","","1539-9087","https://doi.org/10.1145/3358222;http://dx.doi.org/10.1145/3358222","10.1145/3358222","In the embedded domain, industrial sectors (i.e., automotive industry, avionics) are undergoing radical changes. They broadly adopt commodity hardware and move away from special-purpose control units. During this transition, heterogeneous software components are consolidated to run on commodity operating systems.To efficiently consolidate such components, a modular encapsulation of common functionality into reusable binary files (i.e., shared libraries) is essential. However, shared libraries are often unnecessarily large as they entail a lot of generic functionality that is not required in a narrowly defined scenario. As the source code of proprietary components is often unavailable and the industry is heading towards binary-only distribution, we propose an approach towards lightweight binary tailoring.As demonstrated in the evaluation, lightweight binary tailoring effectively reduces the amount of code in all shared libraries on a Linux-based system by 63 percent and shrinks their files by 17 percent. The reduction in size is beneficial to cut down costs (e.g., lower storage and memory footprint) and eases code analyses that are necessary for code audits.","Shared libraries, binary tailoring, Linux","",""
"Conference Paper","Elkhail AA,Svacina J,Cerny T","Intelligent Token-Based Code Clone Detection System for Large Scale Source Code","","2019","","","256–260","Association for Computing Machinery","New York, NY, USA","Proceedings of the Conference on Research in Adaptive and Convergent Systems","Chongqing, China","2019","9781450368438","","https://doi.org/10.1145/3338840.3355654;http://dx.doi.org/10.1145/3338840.3355654","10.1145/3338840.3355654","A code clone refers to code fragments in the source code that are identical or similar to each other. Code clones lead difficulties in software maintenance, bug fixing, present poor design and increase the system size. Code clone detection techniques and tools have been proposed by many researchers, however, there is a lack of clone detection techniques especially for large scale repositories. In this paper, we present a token-based clone detector called Intelligent Clone Detection Tool (ICDT) that can detect both exact and near-miss clones from large repositories using a standard workstation environment. In order to evaluate the scalability and the efficiency of ICDT, we use the most recent benchmark which is a big benchmark of real clones, BigCloneBench. In addition, we compare ICDT to four publicly available and state-of-the-art tools.","case study, BigCloneBench, code clone, clone detection","","RACS '19"
"Conference Paper","Souza IS,Machado I,Seaman C,Gomes G,Chavez C,de Almeida ES,Masiero P","Investigating Variability-Aware Smells in SPLs: An Exploratory Study","","2019","","","367–376","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIII Brazilian Symposium on Software Engineering","Salvador, Brazil","2019","9781450376518","","https://doi.org/10.1145/3350768.3350774;http://dx.doi.org/10.1145/3350768.3350774","10.1145/3350768.3350774","Variability-aware smell is a concept referring to artifact shortcomings in the context of highly-configurable systems that can degrade aspects such as program comprehension, maintainability, and evolvability. To the best of our knowledge, there is very little evidence that variability-aware smells exist in Software Product Lines (SPLs). This work presents an exploratory study that investigated (I) evidence that variability-aware smells exist in SPLs and (II) new types of variability-aware smell not yet documented in the literature based on a quantitative study with open source SPL projects. We collected quantitative data to generate reliable research evidence, by performing feature model and source code inspections on eleven open-source SPL projects. Our findings revealed that (1) instances of variability-aware smells exist in open-source SPL projects and (2) feature information presented significant associations with variability-aware smells. Furthermore, (3) the study presented six new types of variability-aware smells.","Exploratory Study, Variability-Aware Smells, Software Product Lines, Empirical Study","","SBES '19"
"Conference Paper","Assunção E,Souza R","Incidence of Code Smells in the Application of Design Patterns: A Method-Level Analysis","","2019","","","73–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse","Salvador, Brazil","2019","9781450376372","","https://doi.org/10.1145/3357141.3357143;http://dx.doi.org/10.1145/3357141.3357143","10.1145/3357141.3357143","Design patterns are reusable solutions that can be applied to solve specific problems in software design. Such patterns can be misapplied, though, and give rise to code smells, i.e., fragments in the code that indicate possible design flaws. In this study, we aim to understand how often code smells co-occur with design patterns, as well as to determine the most common co-occurrences. To this end, we identified instances of code smells and design patterns in methods of 25 open source Java projects, by using automated detection tools. We also manually inspected fragments of the projects' source code to gather insight on the relationship between specific pairs of smells and patterns. Among other findings, we found that methods that are part of the Adapter pattern are more likely to contain code smells, especially the Feature Envy smell, although it can be argued that the detection of this smell in this context is a false positive.","Design Patterns, Software Design, Code Smells","","SBCARS '19"
"Conference Paper","Lucas W,Bonifácio R,Canedo ED,Marcílio D,Lima F","Does the Introduction of Lambda Expressions Improve the Comprehension of Java Programs?","","2019","","","187–196","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIII Brazilian Symposium on Software Engineering","Salvador, Brazil","2019","9781450376518","","https://doi.org/10.1145/3350768.3350791;http://dx.doi.org/10.1145/3350768.3350791","10.1145/3350768.3350791","Background: The Java programming language version eighth introduced a number of features that encourage the functional style of programming, including the support for lambda expressions and the Stream API. Currently, there is a common wisdom that refactoring a legacy code to introduce lambda expressions, besides other potential benefits, simplifies the code and improves program comprehension. Aims: The purpose of this paper is to investigate this belief, conducting an in depth study to evaluate the effect of introducing lambda expressions on program comprehension. Method: We conduct this research using a mixed-method study. First, we quantitatively analyze 66 pairs of real code snippets, where each pair corresponds to the body of a method before and after the introduction of lambda expressions. We computed two metrics related to source code complexity (number of lines of code and cyclomatic complexity) and two metrics that estimate the readability of the source code. Second, we conduct a survey with practitioners to collect their perceptions about the benefits on program comprehension, with the introduction of lambda expressions. The practitioners evaluate a number between three and six pairs of code snippets, to answer questions about possible improvements. Results: We found contradictory results in our research. Based on the quantitative assessment, we could not find evidences that the introduction of lambda expressions improves software readability--one of the components of program comprehension. Differently, our findings of the qualitative assessment suggest that the introduction of lambda expression improves program comprehension. Implications: We argue in this paper that one can improve program comprehension when she applies particular transformations to introduce lambda expressions (e.g., replacing anonymous inner classes by lambda expressions). In addition, the opinion of the participants shine the opportunities in which a transformation for introducing lambda might be advantageous. This might support the implementation of effective tools for automatic program transformations. Finally, our results suggest that state-of-the-art models for estimating program readability are not helpful to capture the benefits of a program transformation to introduce lambda expressions.","Program Comprehension, Empirical Studies, Java Lambda Expressions","","SBES '19"
"Conference Paper","Mendonça WD,Assunção WK,Vergilio SR","Reusing Test Cases on Graph Product Line Variants: Results from a State-of-the-Practice Test Data Generation Tool","","2019","","","52–61","Association for Computing Machinery","New York, NY, USA","Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing","Salvador, Brazil","2019","9781450376488","","https://doi.org/10.1145/3356317.3356318;http://dx.doi.org/10.1145/3356317.3356318","10.1145/3356317.3356318","Software testing is an essential activity for quality assurance, but, it is an error-prone and effort consuming task when conducted manually. Because of this, the use of automated tools is fundamental, as well as, the evaluation of these tools in practice. However, there is not so much evidence on how such tools perform on highly-configurable systems. Highly-configurable systems are commonly observed in industry as an approach to develop families of products, where products have different configuration options to meet customer needs. To fulfill such a gap, this paper reports results on the use of the tool Randoop, which is widely used in industry, to test variants of the Graph Product Line (GPL) family of products. Our goal is to evaluate reusability of a test data set generated by Randoop for one product when reused for testing other GPL products. Besides, we also investigate the impact of using different values of runtime, the main Randoop parameter, on the number of reused test data. The results show that the used value for runtime in general does not contribute to increase the coverage of test data reused in different products. Furthermore, similarity among products does not ensure a greater reusability.","Test Data Generation, Software Reuse, Highly-configurable systems, Family of Products, Test Coverage","","SAST 2019"
"Conference Paper","Mello R,Uchôa A,Oliveira R,Oliveira D,Oizumi W,Souza J,Fonseca B,Garcia A","Investigating the Social Representations of the Identification of Code Smells by Practitioners and Students from Brazil","","2019","","","457–466","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXIII Brazilian Symposium on Software Engineering","Salvador, Brazil","2019","9781450376518","","https://doi.org/10.1145/3350768.3351794;http://dx.doi.org/10.1145/3350768.3351794","10.1145/3350768.3351794","Context: The identification of code smells is one of the most subjective tasks in software engineering. A key reason is the influence of collective aspects of communities working on this task, such as their beliefs regarding the relevance of certain smells. However, collective aspects are often neglected in the context of smell identification. For this purpose, we can use the social representations theory. Social representations comprise the set of values, behaviors, and practices of communities associated with a social object, such as the task of identifying smells. Aim: To characterize the social representations behind smell identification. Method: We conducted an empirical study on the social representations of smell identification by two communities. One community is composed of postgraduate students from different Brazilian universities. The other community is composed of practitioners located in Brazilian companies, having different levels of experience in code reviews. We analyzed the associations made by the study participants about smell identification, i.e., what immediately comes to their minds when they think about this task. Results: One of the key findings is that the community of students and practitioners have stronger associations with different types of code smells. Students share a strong belief that smell identification is a matter of measurement, while practitioners focus on the structure of the source code and its semantics. Besides, we found that only practitioners frequently associate the task with individual skills. This finding suggests research directions on code smells may be revisited. Conclusion: We found evidence that social representations theory allows identifying research gaps and opportunities by looking beyond the borders of formal knowledge and individual opinions. Therefore, this theory can be considered an important resource for conducting qualitative studies in software engineering.","qualitative research, code smells, Social representations","","SBES '19"
"Conference Paper","Lenarduzzi V,Saarimäki N,Taibi D","The Technical Debt Dataset","","2019","","","2–11","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering","Recife, Brazil","2019","9781450372336","","https://doi.org/10.1145/3345629.3345630;http://dx.doi.org/10.1145/3345629.3345630","10.1145/3345629.3345630","Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are difficult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and -fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.","Dataset, SZZ, Mining Software Repository, SonarQube, Software Quality, Faults, Technical Debt","","PROMISE'19"
"Conference Paper","Olsson T,Ericsson M,Wingkvist A","Semi-Automatic Mapping of Source Code Using Naive Bayes","","2019","","","209–216","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th European Conference on Software Architecture - Volume 2","Paris, France","2019","9781450371421","","https://doi.org/10.1145/3344948.3344984;http://dx.doi.org/10.1145/3344948.3344984","10.1145/3344948.3344984","The software industry has not adopted continuous use of static architecture conformance checking. One hindrance is the needed mapping from source code elements to elements of the architecture. We present a novel approach of generating and combining dependency and semantic information extracted from an initial set of mapped source code files. We use this to train a Naive Bayes classifier that is then used to map the remainder of the source code files. We compare this approach with the HuGMe technique on six open source projects with known mappings. We find that our approach provides an average performance improvement of 0.22 and an average precision and recall F1-score improvement of 0.26 in comparison to HuGMe.","software architecture conformance, HuGMe, text classification, clustering, naive Bayes","","ECSA '19"
"Conference Paper","Rosiak K,Urbaniak O,Schlie A,Seidl C,Schaefer I","Analyzing Variability in 25 Years of Industrial Legacy Software: An Experience Report","","2019","","","65–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B","Paris, France","2019","9781450366687","","https://doi.org/10.1145/3307630.3342410;http://dx.doi.org/10.1145/3307630.3342410","10.1145/3307630.3342410","In certain domains, safety-critical software systems may remain operational for decades. To comply with changing requirements, new system variants are commonly created by copying and modifying existing ones. Typically denoted clone-and-own, software quality and overall maintainability are adversely affected in the long-run. With safety being pivotal, a fault in one variant may require the entire portfolio to be assessed. Thus, engineers need to maintain legacy systems dating back decades, implemented in programming languages such as Pascal. Software product lines (SPLs) can be a remedy but migrating legacy systems requires their prior analysis and comparison. For industrial software systems, this remains a challenge.In this paper, we introduce a comparison procedure and customizable metrics to allow for a fine-grained comparison of Pascal modules to the level of individual expressions. By that, we identify common parts of while also capturing different parts between modules as a basis for a transition towards anSPLs practice. Moreover, we demonstrate the feasibility of our approach using a case study with seven Pascal modules totaling 13,271 lines of code with an evolution-history of 25 years and show our procedure to be fast and precise. Furthermore, we elaborate on the case study and detail peculiarities of the Pascal modules, which are characteristic for an evolution-history of a quarter century.","variability, clone-and-own, software prodct line, legacy software","","SPLC '19"
"Journal Article","Tufano M,Watson C,Bavota G,Penta MD,White M,Poshyvanyk D","An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation","ACM Trans. Softw. Eng. Methodol.","2019","28","4","","Association for Computing Machinery","New York, NY, USA","","","2019-09","","1049-331X","https://doi.org/10.1145/3340544;http://dx.doi.org/10.1145/3340544","10.1145/3340544","Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.","Neural machine translation, bug-fixes","",""
"Conference Paper","Viuginov N,Filchenkov A","A Machine Learning Based Automatic Folding of Dynamically Typed Languages","","2019","","","31–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation","Tallinn, Estonia","2019","9781450368551","","https://doi.org/10.1145/3340482.3342746;http://dx.doi.org/10.1145/3340482.3342746","10.1145/3340482.3342746","The popularity of dynamically typed languages has been growing strongly lately. Elegant syntax of such languages like javascript, python, PHP and ruby pays back when it comes to finding bugs in large codebases. The analysis is hindered by specific capabilities of dynamically typed languages, such as defining methods dynamically and evaluating string expressions. For finding bugs or investigating unfamiliar classes and libraries in modern IDEs and text editors features for folding unimportant code blocks are implemented. In this work, data on user foldings from real projects were collected and two classifiers were trained on their basis. The input to the classifier is a set of parameters describing the structure and syntax of the code block. These classifiers were subsequently used to identify unimportant code fragments. The implemented approach was tested on JavaScript and Python programs and compared with the best existing algorithm for automatic code folding.","Automatic Folding, Dynamically typed languages, JavaScript, Python, Source code analysis, Abstract Syntax tree","","MaLTeSQuE 2019"
"Conference Paper","Feichtner J,Rabensteiner C","Obfuscation-Resilient Code Recognition in Android Apps","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Conference on Availability, Reliability and Security","Canterbury, CA, United Kingdom","2019","9781450371643","","https://doi.org/10.1145/3339252.3339260;http://dx.doi.org/10.1145/3339252.3339260","10.1145/3339252.3339260","Many Android developers take advantage of third-party libraries and code snippets from public sources to add functionality to apps. Besides making development more productive, external code can also be harmful, introduce vulnerabilities, or raise critical privacy issues that threaten the security of sensitive user data and amplify an app's attack surface. Reliably recognizing such code fragments in Android applications is challenging due to the widespread use of obfuscation techniques and a variety of ways, how developers can express semantically similar program statements.We propose a code recognition technique that is resilient against common code transformations and that excels in identifying code fragments and libraries in Android applications. Our method relies on obfuscation-resilient features from the Abstract Syntax Tree of methods and uses them in combination with invariant attributes from method signatures to derive well-characterizing fingerprints. To identify similar code, we elaborate an effective scoring metric that reliably compares fingerprints at method, class, and package level. We investigate how well our solution tackles obfuscated, shrunken, and optimized code by applying our technique to real-world applications. We thoroughly evaluate our solution and demonstrate its practical ability to fingerprint and recognize code with high precision and recall.","Library Detection, Code Recognition, Fingerprinting, Android, Abstract Syntax Tree, Obfuscation, Code Similarity","","ARES '19"
"Conference Paper","Kessel M,Atkinson C","A Platform for Diversity-Driven Test Amplification","","2019","","","35–41","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation","Tallinn, Estonia","2019","9781450368506","","https://doi.org/10.1145/3340433.3342825;http://dx.doi.org/10.1145/3340433.3342825","10.1145/3340433.3342825","Test amplification approaches take a manually written set of tests (input/output mappings) and enhance their effectiveness for some clearly defined engineering goal such as detecting faults. Conceptually, they can either achieve this in a ``black box'' way using only the initial ``seed'' tests or in a ``white box'' way utilizing additional inputs such as the source code or specification of the software under test. However, no fully black box approach to test amplification is currently available even though they can be used to enhance white box approaches. In this paper we introduce a new approach that uses the seed tests to search for existing redundant implementations of the software under test and leverages them as oracles in the generation and evaluation of new tests. The approach can therefore be used as a stand alone black box test amplification method or in tandem with other methods. In this paper we explain the approach, describe its synergies with other approaches and provide some evidence for its practical feasibility.","observations, behavior, mining software repositories, oracle problem, automated testing, test amplification","","A-TEST 2019"
"Conference Paper","Musch M,Wressnegger C,Johns M,Rieck K","Thieves in the Browser: Web-Based Cryptojacking in the Wild","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Conference on Availability, Reliability and Security","Canterbury, CA, United Kingdom","2019","9781450371643","","https://doi.org/10.1145/3339252.3339261;http://dx.doi.org/10.1145/3339252.3339261","10.1145/3339252.3339261","With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. We find that cryptojacking is common, with currently 1 out of 500 sites hosting a mining script. Moreover, we perform several secondary analyses to gain insight into the cryptojacking landscape, including a measurement of code characteristics, an estimate of expected mining revenue, and an evaluation of current blacklist-based countermeasures.","","","ARES '19"
"Conference Paper","Tian H,Weng Q,Wang W","Towards Framework-Independent, Non-Intrusive Performance Characterization for Dataflow Computation","","2019","","","54–60","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGOPS Asia-Pacific Workshop on Systems","Hangzhou, China","2019","9781450368933","","https://doi.org/10.1145/3343737.3343743;http://dx.doi.org/10.1145/3343737.3343743","10.1145/3343737.3343743","Troubleshooting performance bugs for dataflow computation often leads to a ""painful"" process, even for experienced developers. Existing approaches to configuration tuning or performance analysis are either specific to a particular framework or in need of code instrumentation. In this paper, we propose a framework-independent and non-intrusive approach to performance characterization. For each job, we first assemble the information provided by off-the-shelf profilers into a DAG-based execution profile. We then locate, for each DAG node (operation), the source code of its executed functions. Our key insight is that code contains learnable lexical and syntactic patterns that reveal resource information. We hence perform code analysis and infer the operations' resource usage with machine learning classifiers. Based on them, we establish a performance-resource model that correlates the job performance with the resources used. The evaluation with two Spark use cases demonstrates the effectiveness of our approach in detecting program bottlenecks and predicting job completion time under various resource configurations.","Performance Characterization, Dataflow Systems, Multi-Class Classification","","APSys '19"
"Conference Paper","De Coninck Q,Michel F,Piraux M,Rochet F,Given-Wilson T,Legay A,Pereira O,Bonaventure O","Pluginizing QUIC","","2019","","","59–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM Special Interest Group on Data Communication","Beijing, China","2019","9781450359566","","https://doi.org/10.1145/3341302.3342078;http://dx.doi.org/10.1145/3341302.3342078","10.1145/3341302.3342078","Application requirements evolve over time and the underlying protocols need to adapt. Most transport protocols evolve by negotiating protocol extensions during the handshake. Experience with TCP shows that this leads to delays of several years or more to widely deploy standardized extensions. In this paper, we revisit the extensibility paradigm of transport protocols.We base our work on QUIC, a new transport protocol that encrypts most of the header and all the payload of packets, which makes it almost immune to middlebox interference. We propose Pluginized QUIC (PQUIC), a framework that enables QUIC clients and servers to dynamically exchange protocol plugins that extend the protocol on a per-connection basis. These plugins can be transparently reviewed by external verifiers and hosts can refuse non-certified plugins. Furthermore, the protocol plugins run inside an environment that monitors their execution and stops malicious plugins. We demonstrate the modularity of our proposal by implementing and evaluating very different plugins ranging from connection monitoring to multipath or Forward Erasure Correction. Our results show that plugins achieve expected behavior with acceptable overhead. We also show that these plugins can be combined to add their functionalities to a PQUIC connection.","plugin, QUIC, transport protocol, eBPF, PQUIC, protocol operation, network architecture","","SIGCOMM '19"
"Conference Paper","Mostaeen G,Svajlenko J,Roy B,Roy CK,Schneider KA","CloneCognition: Machine Learning Based Code Clone Validation Tool","","2019","","","1105–1109","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3341182;http://dx.doi.org/10.1145/3338906.3341182","10.1145/3338906.3341182","A code clone is a pair of similar code fragments, within or between software systems. To detect each possible clone pair from a software system while handling the complex code structures, the clone detection tools undergo a lot of generalization of the original source codes. The generalization often results in returning code fragments that are only coincidentally similar and not considered clones by users, and hence requires manual validation of the reported possible clones by users which is often both time-consuming and challenging. In this paper, we propose a machine learning based tool 'CloneCognition' (Open Source Codes: https://github.com/pseudoPixels/CloneCognition ; Video Demonstration: https://www.youtube.com/watch?v=KYQjmdr8rsw) to automate the laborious manual validation process. The tool runs on top of any code clone detection tools to facilitate the clone validation process. The tool shows promising clone classification performance with an accuracy of up to 87.4%. The tool also exhibits significant improvement in the results when compared with state-of-the-art techniques for code clone validation.","Machine Learning, Artificial Neural Network, Validation, Clone Management, Code Clones","","ESEC/FSE 2019"
"Conference Paper","Bui ND,Yu Y,Jiang L","SAR: Learning Cross-Language API Mappings with Little Knowledge","","2019","","","796–806","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3338924;http://dx.doi.org/10.1145/3338906.3338924","10.1145/3338906.3338924","To save effort, developers often translate programs from one programming language to another, instead of implementing it from scratch. Translating application program interfaces (APIs) used in one language to functionally equivalent ones available in another language is an important aspect of program translation. Existing approaches facilitate the translation by automatically identifying the API mappings across programming languages. However, these approaches still require large amount of parallel corpora, ranging from pairs of APIs or code fragments that are functionally equivalent, to similar code comments. To minimize the need of parallel corpora, this paper aims at an automated approach that can map APIs across languages with much less a priori knowledge than other approaches. The approach is based on an realization of the notion of domain adaption, combined with code embedding, to better align two vector spaces. Taking as input large sets of programs, our approach first generates numeric vector representations of the programs (including the APIs used in each language), and it adapts generative adversarial networks (GAN) to align the vectors in different spaces of two languages. For a better alignment, we initialize the GAN with parameters derived from API mapping seeds that can be identified accurately with a simple automatic signature-based matching heuristic. Then the cross language API mappings can be identified via nearest-neighbors queries in the aligned vector spaces. We have implemented the approach (SAR, named after three main technical components in the approach) in a prototype for mapping APIs across Java and C# programs. Our evaluation on about 2 million Java files and 1 million C# files shows that the approach can achieve 54% and 82% mapping accuracy in its top-1 and top-10 API mapping results with only 174 automatically identified seeds, more accurate than other approaches using the same or much more mapping seeds.","cross-language API mapping, word embedding, domain adaptation, Generative Adversarial Network, vector space alignment","","ESEC/FSE 2019"
"Conference Paper","Cambronero J,Li H,Kim S,Sen K,Chandra S","When Deep Learning Met Code Search","","2019","","","964–974","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3340458;http://dx.doi.org/10.1145/3338906.3340458","10.1145/3338906.3340458","There have been multiple recent proposals on using deep neural networks for code search using natural language. Common across these proposals is the idea of embedding code and natural language queries into real vectors and then using vector distance to approximate semantic correlation between code and the query. Multiple approaches exist for learning these embeddings, including unsupervised techniques, which rely only on a corpus of code examples, and supervised techniques, which use an aligned corpus of paired code and natural language descriptions. The goal of this supervision is to produce embeddings that are more similar for a query and the corresponding desired code snippet. Clearly, there are choices in whether to use supervised techniques at all, and if one does, what sort of network and training to use for supervision. This paper is the first to evaluate these choices systematically. To this end, we assembled implementations of state-of-the-art techniques to run on a common platform, training and evaluation corpora. To explore the design space in network complexity, we also introduced a new design point that is a minimal supervision extension to an existing unsupervised technique. Our evaluation shows that: 1. adding supervision to an existing unsupervised technique can improve performance, though not necessarily by much; 2. simple networks for supervision can be more effective that more sophisticated sequence-based networks for code search; 3. while it is common to use docstrings to carry out supervision, there is a sizeable gap between the effectiveness of docstrings and a more query-appropriate supervision corpus.","neural networks, code search, joint embedding","","ESEC/FSE 2019"
"Conference Paper","Mordahl A,Oh J,Koc U,Wei S,Gazzillo P","An Empirical Study of Real-World Variability Bugs Detected by Variability-Oblivious Tools","","2019","","","50–61","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3338967;http://dx.doi.org/10.1145/3338906.3338967","10.1145/3338906.3338967","Many critical software systems developed in C utilize compile-time configurability. The many possible configurations of this software make bug detection through static analysis difficult. While variability-aware static analyses have been developed, there remains a gap between those and state-of-the-art static bug detection tools. In order to collect data on how such tools may perform and to develop real-world benchmarks, we present a way to leverage configuration sampling, off-the-shelf “variability-oblivious” bug detectors, and automatic feature identification techniques to simulate a variability-aware analysis. We instantiate our approach using four popular static analysis tools on three highly configurable, real-world C projects, obtaining 36,061 warnings, 80% of which are variability warnings. We analyze the warnings we collect from these experiments, finding that most results are variability warnings of a variety of kinds such as NULL dereference. We then manually investigate these warnings to produce a benchmark of 77 confirmed true bugs (52 of which are variability bugs) useful for future development of variability-aware analyses.","variability bugs, configurable C software, static analysis","","ESEC/FSE 2019"
"Conference Paper","Bagherzadeh M,Khatchadourian R","Going Big: A Large-Scale Study on What Big Data Developers Ask","","2019","","","432–442","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3338939;http://dx.doi.org/10.1145/3338906.3338939","10.1145/3338906.3338939","Software developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work.","Big data topic difficulty, Stackoverflow, Big data topic popularity, Big data topics, Big data topic hierarchy","","ESEC/FSE 2019"
"Conference Paper","Davis JC,Michael IV LG,Coghlan CA,Servant F,Lee D","Why Aren’t Regular Expressions a Lingua Franca? An Empirical Study on the Re-Use and Portability of Regular Expressions","","2019","","","443–454","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3338909;http://dx.doi.org/10.1145/3338906.3338909","10.1145/3338906.3338909","This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics? In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language.We experimentally evaluated the riskiness of this practice using a novel regex corpus — 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences. We report that developers’ belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.","Regular expressions, ReDoS, mining software repositories, developer perceptions, empirical software engineering, re-use, portability","","ESEC/FSE 2019"
"Conference Paper","Stahlbauer A,Kreis M,Fraser G","Testing Scratch Programs Automatically","","2019","","","165–175","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Tallinn, Estonia","2019","9781450355728","","https://doi.org/10.1145/3338906.3338910;http://dx.doi.org/10.1145/3338906.3338910","10.1145/3338906.3338910","Block-based programming environments like Scratch foster engagement with computer programming and are used by millions of young learners. Scratch allows learners to quickly create entertaining programs and games, while eliminating syntactical program errors that could interfere with progress. However, functional programming errors may still lead to incorrect programs, and learners and their teachers need to identify and understand these errors. This is currently an entirely manual process. In this paper, we introduce a formal testing framework that describes the problem of Scratch testing in detail. We instantiate this formal framework with the Whisker tool, which provides automated and property-based testing functionality for Scratch programs. Empirical evaluation on real student and teacher programs demonstrates that Whisker can successfully test Scratch programs, and automatically achieves an average of 95.25 % code coverage. Although well-known testing problems such as test flakiness also exist in the scenario of Scratch testing, we show that automated and property-based testing can accurately reproduce and replace the manually and laboriously produced grading efforts of a teacher, and opens up new possibilities to support learners of programming in their struggles.","Scratch, Education, Property-based Testing, Automated Testing","","ESEC/FSE 2019"
"Conference Paper","Toropov E,Buitrago PA,Moura JM","Shuffler: A Large Scale Data Management Tool for Machine Learning in Computer Vision","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)","Chicago, IL, USA","2019","9781450372275","","https://doi.org/10.1145/3332186.3333046;http://dx.doi.org/10.1145/3332186.3333046","10.1145/3332186.3333046","Datasets in the computer vision academic research community are primarily static. Once a dataset is accepted as a benchmark for a computer vision task, researchers working on this task will not alter it in order to make their results reproducible. At the same time, when exploring new tasks and new applications, datasets tend to be an ever changing entity. A practitioner may combine existing public datasets, filter images or objects in them, change annotations or add new ones to fit a task at hand, visualize sample images, or perhaps output statistics in the form of text or plots. In fact, datasets change as practitioners experiment with data as much as with algorithms, trying to make the most out of machine learning models. Given that ML and deep learning call for large volumes of data to produce satisfactory results, it is no surprise that the resulting data and software management associated to dealing with live datasets can be quite complex. As far as we know, there is no flexible, publicly available instrument to facilitate manipulating image data and their annotations throughout a ML pipeline. In this work, we present Shuffler, an open source tool that makes it easy to manage large computer vision datasets. It stores annotations in a relational, human-readable database. Shuffler defines over 40 data handling operations with annotations that are commonly useful in supervised learning applied to computer vision and supports some of the most well-known computer vision datasets. Finally, it is easily extensible, making the addition of new operations and datasets a task that is fast and easy to accomplish.","computer vision, machine learning, data managing, data reuse, big data","","PEARC '19"
"Journal Article","Hameer A,Pientka B","Teaching the Art of Functional Programming Using Automated Grading (Experience Report)","Proc. ACM Program. Lang.","2019","3","ICFP","","Association for Computing Machinery","New York, NY, USA","","","2019-07","","","https://doi.org/10.1145/3341719;http://dx.doi.org/10.1145/3341719","10.1145/3341719","Online programming platforms have immense potential to improve students' educational experience. They make programming more accessible, as no installation is required; and automatic grading facilities provide students with immediate feedback on their code, allowing them to to fix bugs and address errors in their understanding right away. However, these graders tend to focus heavily on the functional correctness of a solution, neglecting other aspects of students' code and thereby causing students to miss out on a significant amount of valuable feedback. In this paper, we recount our experience in using the Learn-OCaml online programming platform to teach functional programming in a second-year university course on programming languages and paradigms. Moreover, we explore how to leverage Learn-OCaml's automated grading infrastructure to make it easy to write more expressive graders that give students feedback on properties of their code beyond simple input/output correctness, in order to effectively teach elements of functional programming style. In particular, we describe our extensions to the Learn-OCaml platform that evaluate students on test quality and code style. By providing these tools and a suite of our own homework problems and associated graders, we aim to promote functional programming education, enhance students' educational experience, and make teaching and learning typed functional programming more accessible to instructors and students alike, in our community and beyond.","programming style, OCaml, online programming platforms, functional programming, automated grading, test-driven development, programming education","",""
"Conference Paper","Zhang J,Beresford AR,Kollmann SA","LibID: Reliable Identification of Obfuscated Third-Party Android Libraries","","2019","","","55–65","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis","Beijing, China","2019","9781450362245","","https://doi.org/10.1145/3293882.3330563;http://dx.doi.org/10.1145/3293882.3330563","10.1145/3293882.3330563","Third-party libraries are vital components of Android apps, yet they can also introduce serious security threats and impede the accuracy and reliability of app analysis tasks, such as app clone detection. Several library detection approaches have been proposed to address these problems. However, we show these techniques are not robust against popular code obfuscators, such as ProGuard, which is now used in nearly half of all apps. We then present LibID, a library detection tool that is more resilient to code shrinking and package modification than state-of-the-art tools. We show that the library identification problem can be formulated using binary integer programming models. LibID is able to identify specific versions of third-party libraries in candidate apps through static analysis of app binaries coupled with a database of third-party libraries. We propose a novel approach to generate synthetic apps to tune the detection thresholds. Then, we use F-Droid apps as the ground truth to evaluate LibID under different obfuscation settings, which shows that LibID is more robust to code obfuscators than state-of-the-art tools. Finally, we demonstrate the utility of LibID by detecting the use of a vulnerable version of the OkHttp library in nearly 10% of 3,958 most popular apps on the Google Play Store.","Third-party library, Android, Obfuscation, ProGuard","","ISSTA 2019"
"Conference Paper","Lou Y,Chen J,Zhang L,Hao D,Zhang L","History-Driven Build Failure Fixing: How Far Are We?","","2019","","","43–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis","Beijing, China","2019","9781450362245","","https://doi.org/10.1145/3293882.3330578;http://dx.doi.org/10.1145/3293882.3330578","10.1145/3293882.3330578","Build systems are essential for modern software development and maintenance since they are widely used to transform source code artifacts into executable software. Previous work shows that build systems break frequently during software evolution. Therefore, automated build-fixing techniques are in huge demand. In this paper we target a mainstream build system, Gradle, which has become the most widely used build system for Java projects in the open-source community (e.g., GitHub). HireBuild, state-of-the-art build-fixing tool for Gradle, has been recently proposed to fix Gradle build failures via mining the history of prior fixes. Although HireBuild has been shown to be effective for fixing real-world Gradle build failures, it was evaluated on only a limited set of build failures, and largely depends on the quality/availability of historical fix information. To investigate the efficacy and limitations of the history-driven build fix, we first construct a new and large build failure dataset from Top-1000 GitHub projects. Then, we evaluate HireBuild on the extended dataset both quantitatively and qualitatively. Inspired by the findings of the study, we propose a simplistic new technique that generates potential patches via searching from the present project under test and external resources rather than the historical fix information. According to our experimental results, the simplistic approach based on present information successfully fixes 2X more reproducible build failures than the state-of-art HireBuild based on historical fix information. Furthermore, our results also reveal various findings/guidelines for future advanced build failure fixing.","Build System, Build Failure Fixing, Automated Program Repair, location = Beijing, China, series = ISSTA 2019","","ISSTA 2019"
"Conference Paper","Tsimakis A,Zarras AV,Vassiliadis P","The Three-Step Refactoring Detector Pattern","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th European Conference on Pattern Languages of Programs","Irsee, Germany","2019","9781450362061","","https://doi.org/10.1145/3361149.3361168;http://dx.doi.org/10.1145/3361149.3361168","10.1145/3361149.3361168","Developing a tool that provides support for different refactorings, through a set of refactoring detectors which identify opportunities for source code improvements, is not easy. Our experience in developing such a tool for refactoring object-oriented software revealed the Three-Step Refactoring Detector pattern. The main idea behind the pattern is to develop an extensible hierarchy of refactoring detectors, with respect to a general three-step refactoring detection process. The proposed pattern facilitates the expansion of the hierarchy with new refactoring detectors and enables the reuse of existing refactoring detectors, provided by third party developers. Concerning maintainability, the pattern promotes the development of simple, clean and technology independent refactoring detectors. We have used the pattern for the development of 11 different refactoring detectors in the context of our tool. The pattern has not been observed in other contexts. However, the usage of the pattern in our tool brought out specific empirical evidence of its benefits, which we discuss in this paper.","refactoring, patterns, refactoring detection","","EuroPLop '19"
"Conference Paper","Wu D,Gao D,Cheng EK,Cao Y,Jiang J,Deng RH","Towards Understanding Android System Vulnerabilities: Techniques and Insights","","2019","","","295–306","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security","Auckland, New Zealand","2019","9781450367523","","https://doi.org/10.1145/3321705.3329831;http://dx.doi.org/10.1145/3321705.3329831","10.1145/3321705.3329831","As a common platform for pervasive devices, Android has been targeted by numerous attacks that exploit vulnerabilities in its apps and the operating system. Compared to app vulnerabilities, system-level vulnerabilities in Android, however, were much less explored in the literature. In this paper, we perform the first systematic study of Android system vulnerabilities by comprehensively analyzing all 2,179 vulnerabilities on the Android Security Bulletin program over about three years since its initiation in August 2015. To this end, we propose an automatic analysis framework, upon a hierarchical database structure, to crawl, parse, clean, and analyze vulnerability reports and their publicly available patches. This framework includes (i) a lightweight technique to pinpoint the affected modules of given vulnerabilities; (ii) a robust method to study the complexity of patch code; and most importantly, (iii) a similarity-based algorithm to cluster patch code patterns. Our clustering algorithm first extracts patch code's essential changes that not only concisely reflect syntactic changes but also keep important semantics, and then leverages affinity propagation to automatically generate clusters based on their pairwise similarity. It allows us to obtain 16 vulnerability patterns, including six new ones not known in the literature, and we further analyze their characteristics via case studies. Besides identifying these useful patterns, we also find that 92% Android vulnerabilities are located in the low-level modules (mostly in native libraries and the kernel), whereas the framework layer causes only 5% vulnerabilities, and that half of the vulnerabilities can be fixed in fewer than 10 lines of code each, with 110 out of 1,158 cases requiring only one single line of code change. We further discuss the implications of all these results. Overall, we provide a clear overview and new insights about Android system vulnerabilities.","android security, system vulnerability, patch code clustering","","Asia CCS '19"
"Conference Paper","Kasahara R,Sakamoto K,Washizaki H,Fukazawa Y","Applying Gamification to Motivate Students to Write High-Quality Code in Programming Assignments","","2019","","","92–98","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education","Aberdeen, Scotland Uk","2019","9781450368957","","https://doi.org/10.1145/3304221.3319792;http://dx.doi.org/10.1145/3304221.3319792","10.1145/3304221.3319792","Background: Traditional programming education focuses on training students' ability to write correct code that meets the specifications in programming assignments. In addition to correctness, software engineering studies argue that code quality is important. Problem: Nurturing students' ability to write high-quality code in programming assignments is difficult due to two main reasons. (1) Considering code quality while grading is undesirable because there are no objective and fair measurement metrics. (2) Grading assignments from multiple viewpoints (correctness and quality) is difficult and time-consuming. Approach: We propose applying gamification with code metrics to measure code quality in programming assignments. Our approach can motivate students to write code with good metric scores independent of grading. We implemented our approach and conducted a control experiment in a programming course at a university. Result: Our approach did not interfere with students' submissions but improved metric scores significantly. Hence, our approach can engage students to write high-quality code.","gamification, code quality, programming education, code metrics, leaderboard, online judge","","ITiCSE '19"
"Conference Paper","Power JF,Waldron J","Quantifying Activity and Collaboration Levels in Programming Assignments","","2019","","","112–118","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education","Aberdeen, Scotland Uk","2019","9781450368957","","https://doi.org/10.1145/3304221.3319769;http://dx.doi.org/10.1145/3304221.3319769","10.1145/3304221.3319769","This paper presents an experience report from a third-year undergraduate compiler design course that is taught as part of a four year computer science degree. We analyse data from a study of practical assignments, evaluated in the context of take-home formative assignments and a supervised summative examination. We implement metrics to quantify the degrees of similarity between submissions for programming assignments, as well as measuring the level of activity. We present the results of our study, and discuss the utility of these metrics for our teaching practice.","programming assignments, program similarity, jaccard index","","ITiCSE '19"
"Conference Paper","Keuning H,Heeren B,Jeuring J","How Teachers Would Help Students to Improve Their Code","","2019","","","119–125","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education","Aberdeen, Scotland Uk","2019","9781450368957","","https://doi.org/10.1145/3304221.3319780;http://dx.doi.org/10.1145/3304221.3319780","10.1145/3304221.3319780","Code quality has been receiving less attention than program correctness in both the practice of and research into programming education. Writing poor quality code might be a sign of carelessness, or not fully understanding programming concepts and language constructs. Teachers play an important role in addressing quality issues, and encouraging students to write better code as early as possible. In this paper we explore to what extent teachers address code quality in their teaching, which code quality issues they observe and how they would help novices to improve their code. We presented student code of low quality to 30 experienced teachers and asked them which hints they would give and how the student should improve the code step by step. We compare these hints to the output of professional code quality tools. Although most teachers gave similar hints on reducing the algorithmic complexity and removing clutter, they gave varying subsets of hints on other topics. We found a large variety in how they would solve issues in code. We noticed that professional code quality tools do not point out the algorithmic complexity topics that teachers mention. Finally, we give some general guidelines on how to approach code improvement.","programming education, code quality, refactoring","","ITiCSE '19"
"Conference Paper","Fanning C,Garcia S","Below C Level: A Student-Centered X86-64 Simulator","","2019","","","381–387","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education","Aberdeen, Scotland Uk","2019","9781450368957","","https://doi.org/10.1145/3304221.3319762;http://dx.doi.org/10.1145/3304221.3319762","10.1145/3304221.3319762","Learning an assembly language introduces students to important computing concepts such as the program stack and lays the conceptual groundwork for topics such as caching. While many instructors choose a RISC language such as MIPS for teaching assembly languages, the pervasiveness of x86-64 in both the desktop and server environments has compelled many instructors to adopt it in their courses. Unfortunately, x86-64 is a complex assembly language and as a result students often have difficulty understanding and visualizing the execution of an x86-64 program. This is especially the case for introductory level students, who struggle with the concepts of control flow; the difference between registers and memory; and memory organization (e.g. the stack).While students can use debuggers (e.g. GDB) to help them step through the execution of a program and examine the program state, these tools are challenging to learn and are therefore not an ideal fit for introductory level students. This paper introduces Below C Level (BCL), an x86-64 open source simulator aimed at helping novices overcome the barriers to learning this challenging language. BCL offers an intuitive interface and many features that allow students to easily simulate x86-64 code snippets or programs. During simulation BCL helps students decipher individual instructions and visualizes the program stack and register file, allowing students to quickly trace through their program and gain a deeper understanding of x86-64 code.","computer systems education, simulator, gui, assembly languages, x86-64, open source","","ITiCSE '19"
"Conference Paper","Dong Y,Cateté V,Lytle N,Isvik A,Barnes T,Jocius R,Albert J,Joshi D,Robinson R,Andrews A","Infusing Computing: Analyzing Teacher Programming Products in K-12 Computational Thinking Professional Development","","2019","","","278–284","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education","Aberdeen, Scotland Uk","2019","9781450368957","","https://doi.org/10.1145/3304221.3319772;http://dx.doi.org/10.1145/3304221.3319772","10.1145/3304221.3319772","In summer 2018, we conducted two week-long professional development workshops for 116 middle and high school teachers interested in infusing computational thinking (CT) into their classrooms. Teachers learned to program in Snap!, connect CT to their disciplines, and create infused CT learning segments for their classes. This paper investigates the extent to which teachers were able to successfully infuse CT skills of pattern recognition, abstraction, decomposition, and algorithms into their learning products.In this work, we analyzed 58 teacher-designed programming products to look for common characteristics, such as project type, intended coding requirements for their students, and code features/functionality. Teacher-created products were classified into five types: animation, interactive story, quiz, intended game, and simulation/exploration tools. Coding requirements varied from using and/or explaining provided code, modifying existing code, programming with starter code, to building entire programs. Products were classified according to the extent to which they involved sprite manipulation, questions/answers, event handling, drawing, and control blocks. We found that teachers from different disciplines created products that vary in type, coding requirements, and features to suit their specific needs. Moreover, we found relationships between discipline, project type, and the required coding teachers expected students to do.Our results inform future Infusing Computing Professional Development (PD) to provide more targeted training to support different teacher needs.","k-12, computational thinking, programming, professional development","","ITiCSE '19"
"Conference Paper","Karimi R,Koppelman DM,Michael CJ","GPU Road Network Graph Contraction and SSSP Query","","2019","","","250–260","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Supercomputing","Phoenix, Arizona","2019","9781450360791","","https://doi.org/10.1145/3330345.3330368;http://dx.doi.org/10.1145/3330345.3330368","10.1145/3330345.3330368","PHAST is to date one of the fastest algorithms for performing single source shortest path (SSSP) queries on road-network graphs. PHAST operates on graphs produced in part using Geisberger's contraction hierarchy (CH) algorithm. Producing these graphs is time consuming, limiting PHAST's usefulness when graphs are not available in advance. CH iteratively assigns scores to nodes, contracts (removes) the highest-scoring node, and adds shortcut edges to preserve distances. Iteration stops when only one node remains. Scoring and contraction rely on a witness path search (WPS) of nearby nodes. Little work has been reported on parallel and especially GPU CH algorithms. This is perhaps due to issues such as the validity of simultaneous potentially overlapping searches, score staleness, and parallel graph updates.A GPU contraction algorithm, CU-CH, is presented which overcomes these difficulties by partitioning the graph into levels composed of independent sets of nodes (non-adjacent nodes) with similar scores. This allows contracting multiple nodes simultaneously with little coordination between threads. A GPU-efficient WPS is presented in which a small neighborhood is kept in shared memory and a hash table is used to detect path overlap. Low-parallelism regions of contraction and query are avoided by halting contraction early and computing APSP on the remaining graph. A PHAST-like query computes SSSP using this contracted graph. Contraction of some DIMACS road network graphs on an Nvidia P100 GPU achieves a speedup of 20 to 37 over Geisberger's serial code on a Xeon E5-2640 v4. Query times on CU-CH- and CH-contracted graphs were comparable.","","","ICS '19"
"Conference Paper","Dann A,Hermann B,Bodden E","SootDiff: Bytecode Comparison across Different Java Compilers","","2019","","","14–19","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM SIGPLAN International Workshop on State Of the Art in Program Analysis","Phoenix, AZ, USA","2019","9781450367202","","https://doi.org/10.1145/3315568.3329966;http://dx.doi.org/10.1145/3315568.3329966","10.1145/3315568.3329966","Different Java compilers and compiler versions, e.g., javac or ecj, produce different bytecode from the same source code. This makes it hard to trace if the bytecode of an open-source library really matches the provided source code. Moreover, it prevents one from detecting which open-source libraries have been re-compiled and rebundled into a single jar, which is a common way to distribute an application. Such rebundling is problematic because it prevents one to check if the jar file contains open-source libraries with known vulnerabilities. To cope with these problems, we propose the tool SootDiff that uses Soot's intermediate representation Jimple, in combination with code clone detection techniques, to reduce dissimilarities introduced by different compilers, and to identify clones. Our results show that SootDiff successfully identifies clones in 102 of 144 cases, whereas bytecode comparison succeeds in 58 cases only.","Intermediate Representation, Code Clone Detection, Static Analysis","","SOAP 2019"
"Conference Paper","Nichols L,Emre M,Hardekopf B","Fixpoint Reuse for Incremental JavaScript Analysis","","2019","","","2–7","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM SIGPLAN International Workshop on State Of the Art in Program Analysis","Phoenix, AZ, USA","2019","9781450367202","","https://doi.org/10.1145/3315568.3329964;http://dx.doi.org/10.1145/3315568.3329964","10.1145/3315568.3329964","Frequently updated programs cause the cost of static analysis to be multiplied by the number of program versions. When the baseline cost is high (for example, analyzing JavaScript), this multiplicative factor can be prohibitive. As an example, JavaScript-based browser addons are continually updated and there are known instances where malicious code has been injected into such updates; thus the addons must be repeatedly vetted each time an update happens. Incremental analysis reduces this cumulative cost by reusing analysis results of previous versions to reduce the cost of analyzing an updated version. However, existing incremental analyses are not applicable to dynamic programming languages such as JavaScript because they make assumptions that don't hold in this setting. In this paper, we propose the first incremental static analysis for JavaScript. We do not require perfect precision, but we show empirically that there is negligible precision loss in practice. Our technique includes a method for matching code between JavaScript program versions, a non-trivial problem which existing techniques do not solve. For our benchmarks, drawn from real browser addons and node.js programs, our incremental analysis performance is on average within a factor of two of an optimal incremental analysis.","incremental program analysis, javascript analysis","","SOAP 2019"
"Conference Paper","Patriciello N,Lagen S,Giupponi L,Bojovic B","An Improved MAC Layer for the 5G NR Ns-3 Module","","2019","","","41–48","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 Workshop on Ns-3","Florence, Italy","2019","9781450371407","","https://doi.org/10.1145/3321349.3321350;http://dx.doi.org/10.1145/3321349.3321350","10.1145/3321349.3321350","In this paper, we present a novel 5G NR simulator aligned with Release 15 TS 38.300. The work relies on previous implementations of LTE and mmWave modules. The focus of the paper is on the MAC layer, where we present the refactoring and the improvements to support OFDMA as per standard. A novel, user-friendly and modular interface is also proposed for the scheduler part, that allows a symbol-level distribution of resources. We go through the details of the implementation, and then we present scheduler results for a subset of schedulers that we propose. The code is available for interested users.","Network Simulator 3 (ns-3), MAC, Schedulers, New Radio (NR)","","WNS3 '19"
"Conference Paper","Yamato Y,Noguchi H,Kataoka M,Isoda T","Proposal of Environment Adaptive Software","","2019","","","102–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Conference on Control and Computer Vision","Jeju, Republic of Korea","2019","9781450363228","","https://doi.org/10.1145/3341016.3341035;http://dx.doi.org/10.1145/3341016.3341035","10.1145/3341016.3341035","Recently, heterogeneous hardware such as GPU and FPGA is used in many systems and also IoT devices are increased repidly. However, to utilize heterogeneous hardware, the hurdles are currently high because of much technical skills. In order to break down such a situation, we think it is required in the future that application programmers only need to write logics to be processed, then software will adapt to the environments with heterogeneous hardware, to make it easy to utilize heterogeneous hardware and IoT devices. Therefore, in this paper, we propose environment adaptive software to operate an once written application with high performance by automatically converting the code and configuring setting so that we can utilize GPU, FPGA and IoT devices in the location to be deployed. We explain a processing flow and elemental technologies to achieve environment adaptive software. We also describe the details of elemental technologies such as automatic GPU offloading which are already under considered.","GPGPU, FPGA, performance, automatic offloading, Environment adaptive software","","ICCCV '19"
"Journal Article","Mostaeen G,Roy B,Roy C,Schneider K","Designing for Real-Time Groupware Systems to Support Complex Scientific Data Analysis","Proc.  ACM Hum. -Comput.  Interact.","2019","3","EICS","","Association for Computing Machinery","New York, NY, USA","","","2019-06","","","https://doi.org/10.1145/3331151;http://dx.doi.org/10.1145/3331151","10.1145/3331151","Scientific Workflow Management Systems (SWfMSs) have become popular for accelerating the specification, execution, visualization, and monitoring of data-intensive scientific experiments. Unfortunately, to the best of our knowledge no existing SWfMSs directly support collaboration. Data is increasing in complexity, dimensionality, and volume, and the efficient analysis of data often goes beyond the realm of an individual and requires collaboration with multiple researchers from varying domains. In this paper, we propose a groupware system architecture for data analysis that in addition to supporting collaboration, also incorporates features from SWfMSs to support modern data analysis processes. As a proof of concept for the proposed architecture we developed SciWorCS - a groupware system for scientific data analysis. We present two real-world use-cases: collaborative software repository analysis and bioinformatics data analysis. The results of the experiments evaluating the proposed system are promising. Our bioinformatics user study demonstrates that SciWorCS can leverage real-world data analysis tasks by supporting real-time collaboration among users.","scientific, workflow, collaboration, data, analysis","",""
"Journal Article","Raffaillac T,Huot S","Polyphony: Programming Interfaces and Interactions with the Entity-Component-System Model","Proc.  ACM Hum. -Comput.  Interact.","2019","3","EICS","","Association for Computing Machinery","New York, NY, USA","","","2019-06","","","https://doi.org/10.1145/3331150;http://dx.doi.org/10.1145/3331150","10.1145/3331150","This paper introduces a new Graphical User Interface (GUI) and Interaction framework based on the Entity-Component-System model (ECS). In this model, interactive elements (Entities) are characterized only by their data (Components). Behaviors are managed by continuously running processes (Systems) which select entities by the Components they possess. This model facilitates the handling of behaviors and promotes their reuse. It provides developers with a simple yet powerful composition pattern to build new interactive elements with Components. It materializes interaction devices as Entities and interaction techniques as a sequence of Systems operating on them. We present Polyphony, an experimental toolkit implementing this approach, and discuss our interpretation of the ECS model in the context of GUIs programming.","entity-component-system, ECS, interaction programming, GUI, user interface toolkit","",""
"Conference Paper","Troiano GM,Snodgrass S,Argımak E,Robles G,Smith G,Cassidy M,Tucker-Raymond E,Puttick G,Harteveld C","Is My Game OK Dr. Scratch? Exploring Programming and Computational Thinking Development via Metrics in Student-Designed Serious Games for STEM","","2019","","","208–219","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM International Conference on Interaction Design and Children","Boise, ID, USA","2019","9781450366908","","https://doi.org/10.1145/3311927.3323152;http://dx.doi.org/10.1145/3311927.3323152","10.1145/3311927.3323152","Computational thinking (CT) is key to digital literacy and helps develop problem-solving skills, which are fundamental in modern school. As game design shows potential for teaching CT, metrics like Dr. Scratch emerge that help scholars systematically assess the CT of student-designed games, particularly with Scratch. Compared to other CT metrics, Dr. Scratch scores the CT of Scratch projects automatically and can be used to describe CT development. However, previous research using Dr. Scratch summatively assessed CT, but did not look at CT development. We use Dr. Scratch to assess the CT development of Scratch games designed by 8th-grade students in STEM curricula. We show how CT proficiency in student-designed games develops differently in each CT dimension, where parallelism, synchronization, and logic develop proficiently, while developing abstraction seems hard. We discuss insights into game-based CT development for STEM, and suggest improvements for metric-based CT assessment.","game design, Computational thinking, Scratch, constructionist learning, STEM, Dr. Scratch, metrics, assessment","","IDC '19"
"Conference Paper","Lee WC,Liu P,Liu Y,Ma S,Zhang X","Programming Support for Autonomizing Software","","2019","","","702–716","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation","Phoenix, AZ, USA","2019","9781450367127","","https://doi.org/10.1145/3314221.3314593;http://dx.doi.org/10.1145/3314221.3314593","10.1145/3314221.3314593","Most traditional software systems are not built with the artificial intelligence support (AI) in mind. Among them, some may require human interventions to operate, e.g., the manual specification of the parameters in the data processing programs, or otherwise, would behave poorly. We propose a novel framework called Autonomizer to autonomize these systems by installing the AI into the traditional programs. Autonomizeris general so it can be applied to many real-world applications. We provide the primitives and the run-time support, where the primitives abstract common tasks of autonomization and the runtime support realizes them transparently. With the support of Autonomizer, the users can gain the AI support with little engineering efforts. Like many other AI applications, the challenge lies in the feature selection, which we address by proposing multiple automated strategies based on the program analysis. Our experiment results on nine real-world applications show that the autonomization only requires adding a few lines to the source code.Besides, for the data-processing programs, Autonomizer improves the output quality by 161% on average over the default settings. For the interactive programs such as game/driving,Autonomizer achieves higher success rate with lower training time than existing autonomized programs.","AI, Deep Learning, Software Autonomization, Dynamic Program Analysis","","PLDI 2019"
"Conference Paper","Perry DM,Kim D,Samanta R,Zhang X","SemCluster: Clustering of Imperative Programming Assignments Based on Quantitative Semantic Features","","2019","","","860–873","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation","Phoenix, AZ, USA","2019","9781450367127","","https://doi.org/10.1145/3314221.3314629;http://dx.doi.org/10.1145/3314221.3314629","10.1145/3314221.3314629","A fundamental challenge in automated reasoning about programming assignments at scale is clustering student submissions based on their underlying algorithms. State-of-the-art clustering techniques are sensitive to control structure variations, cannot cluster buggy solutions with similar correct solutions, and either require expensive pair-wise program analyses or training efforts. We propose a novel technique that can cluster small imperative programs based on their algorithmic essence: (A) how the input space is partitioned into equivalence classes and (B) how the problem is uniquely addressed within individual equivalence classes. We capture these algorithmic aspects as two quantitative semantic program features that are merged into a program's vector representation. Programs are then clustered using their vector representations. The computation of our first semantic feature leverages model counting to identify the number of inputs belonging to an input equivalence class. The computation of our second semantic feature abstracts the program's data flow by tracking the number of occurrences of a unique pair of consecutive values of a variable during its lifetime. The comprehensive evaluation of our tool SemCluster on benchmarks drawn from solutions to small programming assignments shows that SemCluster (1) generates far fewer clusters than other clustering techniques, (2) precisely identifies distinct solution strategies, and (3) boosts the performance of clustering-based program repair, all within a reasonable amount of time.","Program analysis, Quantitative reasoning, Program clustering","","PLDI 2019"
"Journal Article","Belson B,Holdsworth J,Xiang W,Philippa B","A Survey of Asynchronous Programming Using Coroutines in the Internet of Things and Embedded Systems","ACM Trans. Embed. Comput. Syst.","2019","18","3","","Association for Computing Machinery","New York, NY, USA","","","2019-06","","1539-9087","https://doi.org/10.1145/3319618;http://dx.doi.org/10.1145/3319618","10.1145/3319618","Many Internet of Things and embedded projects are event driven, and therefore require asynchronous and concurrent programming. Current proposals for C++20 suggest that coroutines will have native language support. It is timely to survey the current use of coroutines in embedded systems development. This article investigates existing research which uses or describes coroutines on resource-constrained platforms. The existing research is analysed with regard to: software platform, hardware platform, and capacity; use cases and intended benefits; and the application programming interface design used for coroutines. A systematic mapping study was performed, to select studies published between 2007 and 2018 which contained original research into the application of coroutines on resource-constrained platforms. An initial set of 566 candidate papers, collated from on-line databases, were reduced to only 35 after filters were applied, revealing the following taxonomy. The C 8 C++ programming languages were used by 22 studies out of 35. As regards hardware, 16 studies used 8- or 16-bit processors while 13 used 32-bit processors. The four most common use cases were concurrency (17 papers), network communication (15), sensor readings (9), and data flow (7). The leading intended benefits were code style and simplicity (12 papers), scheduling (9), and efficiency (8). A wide variety of techniques have been used to implement coroutines, including native macros, additional tool chain steps, new language features, and non-portable assembly language. We conclude that there is widespread demand for coroutines on resource-constrained devices. Our findings suggest that there is significant demand for a formalised, stable, well-supported implementation of coroutines in C++, designed with consideration of the special needs of resource-constrained devices, and further that such an implementation would bring benefits specific to such devices.","Embedded, resource-constrained, asynchronous, scheduling, direct style","",""
"Conference Paper","Heaps J,Wang X,Breaux T,Niu J","Toward Detection of Access Control Models from Source Code via Word Embedding","","2019","","","103–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th ACM Symposium on Access Control Models and Technologies","Toronto ON, Canada","2019","9781450367530","","https://doi.org/10.1145/3322431.3326329;http://dx.doi.org/10.1145/3322431.3326329","10.1145/3322431.3326329","Advancement in machine learning techniques in recent years has led to deep learning applications on source code. While there is little research available on the subject, the work that has been done shows great potential. We believe deep learning can be leveraged to obtain new insight into automated access control policy verification. In this paper, we describe our first step in applying learning techniques to access control, which consists of developing word embeddings to bootstrap learning tasks. We also discuss the future work on identifying access control enforcement code and checking access control policy violations, which can be enabled by word embeddings.","deep learning, access control, word embeddings","","SACMAT '19"
"Conference Paper","Yoshida N,Numata S,Choi E,Inoue K","Proactive Clone Recommendation System for Extract Method Refactoring","","2019","","","67–70","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 3rd International Workshop on Refactoring","","2019","","","https://doi.org/10.1109/IWoR.2019.00020;http://dx.doi.org/10.1109/IWoR.2019.00020","10.1109/IWoR.2019.00020","""Extract Method"" refactoring is commonly used for merging code clones into a single new method. In this position paper, we propose a proactive clone recommendation system for ""Extract Method"" refactoring. The proposed system that has been implemented as Eclipse plug-in monitors code modifications on the fly. Once the proposed system detects an ""Extract Method"" refactoring instance based on the analysis of code modifications, it recommends code clones of the refactored code as refactoring candidates. The preliminary user study shows that the users are able to refactor a greater number of clones in less time compared to a code clone analysis environment GemX.","code clone, extract method refactoring, recommendation system","","IWOR '19"
"Conference Paper","Xin Q,Reiss SP","Better Code Search and Reuse for Better Program Repair","","2019","","","10–17","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 6th International Workshop on Genetic Improvement","","2019","","","https://doi.org/10.1109/GI.2019.00012;http://dx.doi.org/10.1109/GI.2019.00012","10.1109/GI.2019.00012","A branch of automated program repair (APR) techniques look at finding and reusing existing code for bug repair. ssFix is one of such techniques that is syntactic search-based: it searches a code database for code fragments that are syntactically similar to the bug context and reuses such code fragments to produce patches. The keys to its success lie in the approaches it uses for code search and code reuse. We investigated the effectiveness of ssFix using the Defects4J bug dataset and found that its code search and code reuse approaches are not truly effective and can be significantly improved. Motivated by the investigation, we developed a new repair technique sharpFix that follows ssFix's basic idea but differs significantly in the approaches used for code search and code reuse. We compared sharpFix and ssFix on the Defects4J dataset and confirmed through experiments that (1) sharpFix's code search and code reuse approaches are better than ssFix's approaches and (2) sharpFix can do better repair. sharpFix successfully repaired a total of 36 Defects4J bugs and outperformed many existing repair techniques in repairing more bugs. We also compared sharpFix, ssFix, and four other techniques on another dataset Bugs.jar-ELIXIR. Our results show that sharpFix did better than others and repaired the largest number of bugs.","code search, automated program repair, code reuse","","GI '19"
"Conference Paper","Ferenc R,Hegedundefineds P,Gyimesi P,Antal G,Bán D,Gyimóthy T","Challenging Machine Learning Algorithms in Predicting Vulnerable JavaScript Functions","","2019","","","8–14","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering","","2019","","","https://doi.org/10.1109/RAISE.2019.00010;http://dx.doi.org/10.1109/RAISE.2019.00010","10.1109/RAISE.2019.00010","The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs.We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset.The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while resampling strategies balanced the IR measures.","dataset, vulnerability, machine learning, JavaScript, code metrics, deep learning","","RAISE '19"
"Conference Paper","AlOmar EA,Mkaouer MW,Ouni A","Can Refactoring Be Self-Affirmed? An Exploratory Study on How Developers Document Their Refactoring Activities in Commit Messages","","2019","","","51–58","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 3rd International Workshop on Refactoring","","2019","","","https://doi.org/10.1109/IWoR.2019.00017;http://dx.doi.org/10.1109/IWoR.2019.00017","10.1109/IWoR.2019.00017","Refactoring is a critical task in software maintenance and is usually performed to enforce best design practices, or to cope with design defects. Previous studies heavily rely on defining a set of keywords to identify refactoring commits from a list of general commits extracted from a small set of software systems. All approaches thus far consider all commits without checking whether refactorings had actually happened or not. In this paper, we aim at exploring how developers document their refactoring activities during the software life cycle. We call such activity Self-Affirmed Refactoring, which is an indication of the developer-related refactoring events in the commit messages. Our approach relies on text mining refactoring-related change messages and identifying refactoring patterns by only considering refactoring commits. We found that (1) developers use a variety of patterns to purposefully target refactoring-related activities; (2) developers tend to explicitly mention the improvement of specific quality attributes and code smells; and (3) commit messages with self-affirmed refactoring patterns tend to have more significant refactoring activity than those without.","software quality, self-affirmed refactoring, mining software repositories","","IWOR '19"
"Conference Paper","Badreddin O,Hamou-Lhadj W,Chauhan S","Susereum: Towards a Reward Structure for Sustainable Scientific Research Software","","2019","","","51–54","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 14th International Workshop on Software Engineering for Science","","2019","","","https://doi.org/10.1109/SE4Science.2019.00015;http://dx.doi.org/10.1109/SE4Science.2019.00015","10.1109/SE4Science.2019.00015","Research software has opened up new pathways of discovery in many and diverse disciplines. This research software is developed under unique budgetary and schedule constraints. Its development is driven by knowledge discovery goals often without documented requirements. As a result, the software code quality is impacted which often hinders its sustainability beyond the immediate research goals. More importantly, the prevalent reward structures favor contributions in terms of research articles and systematically undervalues research codes contributions. As a result, researchers and funding agencies do not allocate appropriate efforts or resources to the development, sustenance, and dissemination of research codebases. This paper presents Susereum, a Blockchain based platform that aims at achieving two goals. First, restructuring prevalent incentives by awarding permanent immutable credit to research code authors similar to the credit awarded to the authors of scientific articles. Second, distributing sovereignty by empowering peers through a consensus process to define code sustainability and impact metrics.","research codes, software sustainability, susereum, consensus algorithm, blockchain, distributed sovereignty","","SE4Science '19"
"Conference Paper","Csuvik V,Kicsi A,Vidács L","Source Code Level Word Embeddings in Aiding Semantic Test-to-Code Traceability","","2019","","","29–36","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 10th International Workshop on Software and Systems Traceability","","2019","","","https://doi.org/10.1109/SST.2019.00016;http://dx.doi.org/10.1109/SST.2019.00016","10.1109/SST.2019.00016","Proper recovery of test-to-code traceability links from source code could considerably aid software maintenance. Scientific research has already shown that this can be achieved to an extent with a range of techniques relying on various information sources. This includes information retrieval which considers the natural language aspects of the source code. Latent Semantic Indexing (LSI) is widely looked upon as the mainstream technique of this approach. Techniques utilizing word embedding information however also use similar data and nowadays enjoy immense popularity in several fields of study. In this work, we present our evaluation of both LSI and word embeddings in aiding class level test-to-code traceability of 4 open source software systems, the assessment relying on naming convention information.","word embeddings, traceability, testing, test-to-code","","SST '19"
"Conference Paper","Wyrich M,Bogner J","Towards an Autonomous Bot for Automatic Source Code Refactoring","","2019","","","24–28","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 1st International Workshop on Bots in Software Engineering","","2019","","","https://doi.org/10.1109/BotSE.2019.00015;http://dx.doi.org/10.1109/BotSE.2019.00015","10.1109/BotSE.2019.00015","Continuous refactoring is necessary to maintain source code quality and to cope with technical debt. Since manual refactoring is inefficient and error-prone, various solutions for automated refactoring have been proposed in the past. However, empirical studies have shown that these solutions are not widely accepted by software developers and most refactorings are still performed manually. For example, developers reported that refactoring tools should support functionality for reviewing changes. They also criticized that introducing such tools would require substantial effort for configuration and integration into the current development environment.In this paper, we present our work towards the Refactoring-Bot, an autonomous bot that integrates into the team like a human developer via the existing version control platform. The bot automatically performs refactorings to resolve code smells and presents the changes to a developer for asynchronous review via pull requests. This way, developers are not interrupted in their workflow and can review the changes at any time with familiar tools. Proposed refactorings can then be integrated into the code base via the push of a button. We elaborate on our vision, discuss design decisions, describe the current state of development, and give an outlook on planned development and research activities.","bot, code smells, maintainability, refactoring, software evolution, software quality improvement","","BotSE '19"
"Conference Paper","Stephan M","Towards a Cognizant Virtual Software Modeling Assistant Using Model Clones","","2019","","","21–24","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results","","2019","","","https://doi.org/10.1109/ICSE-NIER.2019.00014;http://dx.doi.org/10.1109/ICSE-NIER.2019.00014","10.1109/ICSE-NIER.2019.00014","We present our new ideas on taking the first steps towards cultivating synergy between model-driven engineering (MDE), machine learning, and software clones. Specifically, we describe our vision in realizing a cognizant virtual software modeling assistant that uses the latter two to improve software design and MDE. Software engineering has benefited greatly from knowledge-based cognizant source code completion and assistance, but MDE has few and limited analogous capabilities. We outline our research directions by describing our vision for a prototype assistant that provides suggestions to modelers performing model creation or extension in the form of 1) complete models for insertion or guidance, and 2) granular single-step operations. These suggestions are derived by detecting clones of the in-progress model and existing domain, organizational, and exemplar models. We overview our envisioned workflow between modeler and assistant, and, using Simulink as an example, illustrate different manifestations including multiple overlays with percentages and employing variant elements.","model driven engineering, software modeling, model clone detection, machine learning, model clones","","ICSE-NIER '19"
"Conference Paper","Sun W,Wang X,Wu H,Duan D,Sun Z,Chen Z","MAF: Method-Anchored Test Fragmentation for Test Code Plagiarism Detection","","2019","","","110–120","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Software Engineering Education and Training","","2019","","","https://doi.org/10.1109/ICSE-SEET.2019.00020;http://dx.doi.org/10.1109/ICSE-SEET.2019.00020","10.1109/ICSE-SEET.2019.00020","Software engineering education becomes popular due to the rapid development of the software industry. In order to reduce learning costs and improve learning efficiency, some online practice platforms have emerged. This paper proposes a novel test code plagiarism detection technology, namely MAF, by introducing bidirectional static slicing to anchor methods under test and extract fragments of test codes. Combined with similarity measures, MAF can achieve effective plagiarism detection by avoiding massive unrelated noisy test codes. The experiment is conducted on the dataset of Mooctest, which so far has supported hundreds of test activities around the world in the past 3 years. The experimental results show that MAF can effectively improve the performance (precision, recall and F1-measure) of similarity measures for test code plagiarism detection. We believe that MAF can further expand and promote software testing education, and it can also be extended to use in test recommendation, test reuse and other engineering applications.","online training, plagiarism detection, similarity measure, unit testing","","ICSE-SEET '19"
"Conference Paper","Tang C,Chen S,Fan L,Xu L,Liu Y,Tang Z,Dou L","A Large-Scale Empirical Study on Industrial Fake Apps","","2019","","","183–192","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice","","2019","","","https://doi.org/10.1109/ICSE-SEIP.2019.00028;http://dx.doi.org/10.1109/ICSE-SEIP.2019.00028","10.1109/ICSE-SEIP.2019.00028","While there have been various studies towards Android apps and their development, there is limited discussion of the broader class of apps that fall in the fake area. Fake apps and their development are distinct from official apps and belong to the mobile underground industry. Due to the lack of knowledge of the mobile underground industry, fake apps, their ecosystem and nature still remain in mystery.To fill the blank, we conduct the first systematic and comprehensive empirical study on a large-scale set of fake apps. Over 150,000 samples related to the top 50 popular apps are collected for extensive measurement. In this paper, we present discoveries from three different perspectives, namely fake sample characteristics, quantitative study on fake samples and fake authors' developing trend. Moreover, valuable domain knowledge, like fake apps' naming tendency and fake developers' evasive strategies, is then presented and confirmed with case studies, demonstrating a clear vision of fake apps and their ecosystem.","empirical study, Android app, fake app","","ICSE-SEIP '19"
"Conference Paper","Wiese ES,Rafferty AN,Fox A","Linking Code Readability, Structure, and Comprehension among Novices: It's Complicated","","2019","","","84–94","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Software Engineering Education and Training","","2019","","","https://doi.org/10.1109/ICSE-SEET.2019.00017;http://dx.doi.org/10.1109/ICSE-SEET.2019.00017","10.1109/ICSE-SEET.2019.00017","Novices' functionally-correct code is often redundant, verbose, or un-idiomatic. Such code could indicate shallow understanding of the programming language, or unfamiliarity with experts' preferences for code structure. Understanding why novices write poorly is important for designing instruction and tools to help novices write elegantly. 231 novices judged style and readability for sets of code snippets targeting seven topics. Within each set, functionality was the same, but the writing followed either common novice patterns or a more elegant, ""expert"" pattern. Overall, 76% of novices thought the ""expert"" snippets had the best style, but only 64% said those snippets were most readable. However, comprehension was similar for both ""expert"" and novice patterns, regardless of readability preferences. This suggests that students who prefer novice patterns do not necessarily have deep misunderstandings about the programming language. One topic included a code-writing task, and students' readability preferences were predictive of their code-writing patterns, suggesting that readability preferences reflect writing choices rather than comprehension. Thus, novices may benefit from lightweight tools that identify common patterns and suggest an ""expert"" solution, while helping them see that the ""expert"" solution is more readable than they think.","code readability, novice code comprehension, computer science education","","ICSE-SEET '19"
"Conference Paper","Carvalho L,Garcia A,Assunção WK,de Mello R,de Lima MJ","Analysis of the Criteria Adopted in Industry to Extract Microservices","","2019","","","22–29","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Joint 7th International Workshop on Conducting Empirical Studies in Industry and 6th International Workshop on Software Engineering Research and Industrial Practice","","2019","","","https://doi.org/10.1109/CESSER-IP.2019.00012;http://dx.doi.org/10.1109/CESSER-IP.2019.00012","10.1109/CESSER-IP.2019.00012","A microservice architecture is expected to provide a better modularization and management of small and autonomous services. Other expected benefits include increased availability and time to market. There is a growing interest of both industry and academia on streamlining the migration of existing systems to a microservice architecture. However, the success of this migration is largely dependent on the use of appropriate criteria for extracting microservices from a code base. Recent studies indicate the selection and decomposition of microservices represent the main challenge along the migration. Academic techniques tend to support the extraction of microservices with either one or two conventional criteria, namely coupling and cohesion. There is limited knowledge on the criteria actually considered as useful by practitioners. Thus, we have performed an exploratory online survey with 15 specialists experienced on migrating systems to a microservices architecture. In particular, we question the relative usefulness of seven possible criteria for supporting decision-making along microservice extraction. The participants were also questioned about tools they have used, their limitations, and whether the decisions on extracted microservices were considered unsuccessful. Overall, the survey results suggest academic techniques do not totally satisfy the needs of practitioners. Practitioners often need to consider simultaneously at least four dominant criteria as well as their trade-offs to support their decisions. Most practitioners consider existing tooling support insufficient or even irrelevant to support their microservice extraction decisions.","microservices, reengineering, survey, extraction, industry","","CESSER-IP '19"
"Conference Paper","Machalica M,Samylkin A,Porth M,Chandra S","Predictive Test Selection","","2019","","","91–100","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice","","2019","","","https://doi.org/10.1109/ICSE-SEIP.2019.00018;http://dx.doi.org/10.1109/ICSE-SEIP.2019.00018","10.1109/ICSE-SEIP.2019.00018","Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95% of individual test failures and over 99.9% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.","continuous integration, flaky tests, machine learning, test selection","","ICSE-SEIP '19"
"Conference Paper","Perez D,Chiba S","Cross-Language Clone Detection by Learning over Abstract Syntax Trees","","2019","","","518–528","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00078;http://dx.doi.org/10.1109/MSR.2019.00078","10.1109/MSR.2019.00078","Clone detection across programs written in the same programming language has been studied extensively in the literature. On the contrary, the task of detecting clones across multiple programming languages has not been studied as much, and approaches based on comparison cannot be directly applied. In this paper, we present a clone detection method based on semi-supervised machine learning designed to detect clones across programming languages with similar syntax. Our method uses an unsupervised learning approach to learn token-level vector representations and an LSTM-based neural network to predict whether two code fragments are clones. To train our network, we present a cross-language code clone dataset --- which is to the best of our knowledge the first of its kind --- containing around 45,000 code fragments written in Java and Python. We evaluate our approach on the dataset we created and show that our method gives promising results when detecting similarities between code fragments written in Java and Python.","machine learning, source code representation, clone detection","","MSR '19"
"Conference Paper","Nishi MA,Ciborowska A,Damevski K","Characterizing Duplicate Code Snippets between Stack Overflow and Tutorials","","2019","","","240–244","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00048;http://dx.doi.org/10.1109/MSR.2019.00048","10.1109/MSR.2019.00048","Developers are usually unaware of the quality and lineage of information available on popular Web resources, leading to potential maintenance problems and license violations when reusing code snippets from these resources. In this paper, we study the duplication of code snippets between two popular sources of software development information: the Stack Overflow Q&A site and software development tutorials. Our goals are to empirically understand the scale of repeated information between these two sources, to gain insight into why developers copy information from one source to the other, and to understand the evolution of duplicated information over time. To this end, we correlate a set of nearly 600 tutorials on Android available on the Web to the SOTorrent dataset, which isolates code snippets from Stack Overflow posts and tracks their changes over time. Our findings reveal that there are over 1,400 duplicate code snippets related to Android on Stack Overflow. Code that was duplicated on the two sources is effective at answering Stack Overflow questions; a significant number (31%) of answers that contained a duplicate code block were chosen as the accepted answer. Qualitative analysis reveals that developers commonly use Stack Overflow to ask clarifying questions about code they reused from tutorials, and copy code snippets from tutorials to provide answers to questions.","tutorials, duplicate code snippets, stack overflow","","MSR '19"
"Conference Paper","Efstathiou V,Spinellis D","Semantic Source Code Models Using Identifier Embeddings","","2019","","","29–33","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00015;http://dx.doi.org/10.1109/MSR.2019.00015","10.1109/MSR.2019.00015","The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.","vector space models, fastText, semantic similarity, code semantics","","MSR '19"
"Conference Paper","Lerina A,Nardi L","Investigating on the Impact of Software Clones on Technical Debt","","2019","","","108–112","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Second International Conference on Technical Debt","","2019","","","https://doi.org/10.1109/TechDebt.2019.00029;http://dx.doi.org/10.1109/TechDebt.2019.00029","10.1109/TechDebt.2019.00029","Code reuse by copying a code fragment with or without modification generates duplicate copies of exact or similar code fragments in a software system, known as code clones. The debate about the harmfulness of clone in ongoing in the literature, nevertheless, it is widely recognized that clones needs special considerations during software evolution. In this paper, it is proposed a quantitative analysis of technical debt values to understand if it is higher with cloned code than those without cloned code. Moreover, changes performed on these files have been analyzed by analyzing commit logs. According to our inspection on four subject systems, the technical debt of files with cloned code is significantly higher than those without cloned code. Moreover, as expected, files with cloned code are more impacted by changes.","software maintenance, software evolution, technical debt, software clones","","TechDebt '19"
"Conference Paper","Babur Ö,Stephan M","MoCoP: Towards a Model Clone Portal","","2019","","","78–81","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 11th International Workshop on Modelling in Software Engineerings","","2019","","","https://doi.org/10.1109/MiSE.2019.00019;http://dx.doi.org/10.1109/MiSE.2019.00019","10.1109/MiSE.2019.00019","Widespread and mature practice of model-driven engineering is leading to a growing number of modeling artifacts and challenges in their management. Model clone detection (MCD) is an important approach for managing and maintaining modeling artifacts. While its counterpart in traditional source code development, code clone detection, is enjoying popularity and more than two decades of development, MCD is still in its infancy in terms of research and tooling. We aim to develop a portal for model clone detection, MoCoP, as a central hub to mitigate adoption barriers and foster MCD research. In this short paper, we present our vision for MoCoP and its features and goals. We discuss MoCoP's key components that we plan on realizing in the short term including public tooling, curated data sets, and a body of MCD knowledge. Our longer term goals include a dedicated service-oriented infrastructure, contests, and forums. We believe MoCoP will strengthen MCD research, tooling, and the community, which in turn will lead to better quality, maintenance, and scalability for model-driven engineering practices.","model repositories, model management, model-driven engineering, software maintenance, model clone detection, model analytics","","MiSE '19"
"Conference Paper","Kamp M,Kreutzer P,Philippsen M","SeSaMe: A Data Set of Semantically Similar Java Methods","","2019","","","529–533","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00079;http://dx.doi.org/10.1109/MSR.2019.00079","10.1109/MSR.2019.00079","In the past, techniques for detecting similarly behaving code fragments were often only evaluated with small, artificial oracles or with code originating from programming competitions. Such code fragments differ largely from production codes.To enable more realistic evaluations, this paper presents SeSaMe, a data set of method pairs that are classified according to their semantic similarity. We applied text similarity measures on JavaDoc comments mined from 11 open source repositories and manually classified a selection of 857 pairs.","","","MSR '19"
"Conference Paper","Theeten B,Vandeputte F,Van Cutsem T","Import2vec Learning Embeddings for Software Libraries","","2019","","","18–28","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00014;http://dx.doi.org/10.1109/MSR.2019.00014","10.1109/MSR.2019.00014","We consider the problem of developing suitable learning representations (embeddings) for library packages that capture semantic similarity among libraries. Such representations are known to improve the performance of downstream learning tasks (e.g. classification) or applications such as contextual search and analogical reasoning.We apply word embedding techniques from natural language processing (NLP) to train embeddings for library packages (""library vectors""). Library vectors represent libraries by similar context of use as determined by import statements present in source code. Experimental results obtained from training such embeddings on three large open source software corpora reveals that library vectors capture semantically meaningful relationships among software libraries, such as the relationship between frameworks and their plug-ins and libraries commonly used together within ecosystems such as big data infrastructure projects (in Java), front-end and back-end web development frameworks (in JavaScript) and data science toolkits (in Python).","software engineering, machine learning, information retrieval","","MSR '19"
"Conference Paper","Dam HK,Pham T,Ng SW,Tran T,Grundy J,Ghose A,Kim T,Kim CJ","Lessons Learned from Using a Deep Tree-Based Model for Software Defect Prediction in Practice","","2019","","","46–57","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00017;http://dx.doi.org/10.1109/MSR.2019.00017","10.1109/MSR.2019.00017","Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository.","defect prediction, deep learning","","MSR '19"
"Conference Paper","Chatterjee P,Damevski K,Pollock L,Augustine V,Kraft NA","Exploratory Study of Slack Q&A Chats as a Mining Source for Software Engineering Tools","","2019","","","490–501","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00075;http://dx.doi.org/10.1109/MSR.2019.00075","10.1109/MSR.2019.00075","Modern software development communities are increasingly social. Popular chat platforms such as Slack host public chat communities that focus on specific development topics such as Python or Ruby-on-Rails. Conversations in these public chats often follow a Q&A format, with someone seeking information and others providing answers in chat form. In this paper, we describe an exploratory study into the potential usefulness and challenges of mining developer Q&A conversations for supporting software maintenance and evolution tools. We designed the study to investigate the availability of information that has been successfully mined from other developer communications, particularly Stack Overflow. We also analyze characteristics of chat conversations that might inhibit accurate automated analysis. Our results indicate the prevalence of useful information, including API mentions and code snippets with descriptions, and several hurdles that need to be overcome to automate mining that information.","","","MSR '19"
"Conference Paper","di Biase M,Rastogi A,Bruntink M,van Deursen A","The Delta Maintainability Model: Measuring Maintainability of Fine-Grained Code Changes","","2019","","","113–122","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Second International Conference on Technical Debt","","2019","","","https://doi.org/10.1109/TechDebt.2019.00030;http://dx.doi.org/10.1109/TechDebt.2019.00030","10.1109/TechDebt.2019.00030","Existing maintainability models are used to identify technical debt of software systems. Targeting entire codebases, such models lack the ability to determine shortcomings of smaller, fine-grained changes. This paper proposes a new maintainability model - the Delta Maintainability Model (DMM) - to measure fine-grained code changes, such as commits, by adapting and extending the SIG Maintainability Model. DMM categorizes changed lines of code into low and high risk, and then uses the proportion of low risk change to calculate a delta score. The goal of the DMM is twofold: first, producing meaningful and actionable scores; second, compare and rank the maintainability of fine-grained modifications.We report on an initial study of the model, with the goal of understanding if the adapted measurements from the SIG Maintainability Model suit the fine-grained scope of the DMM. In a manual inspection process for 100 commits, 67 cases matched the expert judgment. Furthermore, we report an exploratory empirical study on a data set of DMM scores on 3,017 issue-fixing commits of four open source and four closed source systems. Results show that the scores of DMM can be used to compare and rank commits, providing developers with a means to do root cause analysis on activities that impacted maintainability and, thus, address technical debt at a finer granularity.","","","TechDebt '19"
"Conference Paper","Liu Q,Liu Z,Zhu H,Fan H,Du B,Qian Y","Generating Commit Messages from Diffs Using Pointer-Generator Network","","2019","","","299–309","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00056;http://dx.doi.org/10.1109/MSR.2019.00056","10.1109/MSR.2019.00056","The commit messages in source code repositories are valuable but not easy to be generated manually in time for tracking issues, reporting bugs, and understanding codes. Recently published works indicated that the deep neural machine translation approaches have drawn considerable attentions on automatic generation of commit messages. However, they could not deal with out-of-vocabulary (OOV) words, which are essential context-specific identifiers such as class names and method names in code diffs. In this paper, we propose PtrGNCMsg, a novel approach which is based on an improved sequence-to-sequence model with the pointer-generator network to translate code diffs into commit messages. By searching the smallest identifier set with the highest probability, PtrGNCMsg outperforms recent approaches based on neural machine translation, and first enables the prediction of OOV words. The experimental results based on the corpus of diffs and manual commit messages from the top 2,000 Java projects in GitHub show that PtrGNCMsg outperforms the state-of-the-art approach with improved BLEU by 1.02, ROUGE-1 by 4.00 and ROUGE-L by 3.78, respectively.","code change pattern recognition, sequence-to-sequence model, pointer-generator network, automatic commit message generation","","MSR '19"
"Conference Paper","Haas R,Niedermayr R,Juergens E","Teamscale: Tackle Technical Debt and Control the Quality of Your Software","","2019","","","55–56","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Second International Conference on Technical Debt","","2019","","","https://doi.org/10.1109/TechDebt.2019.00016;http://dx.doi.org/10.1109/TechDebt.2019.00016","10.1109/TechDebt.2019.00016","Teamscale is a software intelligence platform, that is, it creates transparency on code quality and the underlying software development process. This makes it possible for developers, testers and managers to better understand and control technical debt of their systems. In this paper, we give an overview of Teamscale and how this tool can be used in practice to control and lower technical debt in the long run. We explain which code analyses can be used to identify and address technical debt. Teamscale is available for free for research and teaching purposes at www.teamscale.io.","quality control, software quality, technical debt","","TechDebt '19"
"Conference Paper","Sharma T","How Deep is the Mud: Fathoming Architecture Technical Debt Using Designite","","2019","","","59–60","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Second International Conference on Technical Debt","","2019","","","https://doi.org/10.1109/TechDebt.2019.00018;http://dx.doi.org/10.1109/TechDebt.2019.00018","10.1109/TechDebt.2019.00018","The quality of software architecture is an important concern for any software development team. Architecture smells represent quality issues at architecture granularity. Identifying and refactoring them periodically is a necessity to keep architecture quality high. We present Designite, a software design quality assessment tool, that identifies seven well-known architecture smells. Along with the identification, the tool provides supplementary information such as cause and responsible classes for each identified smell instance to help developers understand and refactor the smell. The tool is relevant and useful in both research and practice context. Software developers may use it to identify technical debt instances and to refactor them. On the other hand, software engineering researchers may use the tool to carry out large-scale empirical studies concerning code smells.Demo URL: https://youtu.be/ogrAxQLKwsU","technical debt, maintainability, refactoring, code smells, code quality, architecture smells","","TechDebt '19"
"Conference Paper","Ma Y,Bogart C,Amreen S,Zaretzki R,Mockus A","World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data","","2019","","","143–154","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00031;http://dx.doi.org/10.1109/MSR.2019.00031","10.1109/MSR.2019.00031","Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.","software ecosystem, software mining, software supply chain","","MSR '19"
"Conference Paper","Ahmad M,Cinnéide MÓ","Impact of Stack Overflow Code Snippets on Software Cohesion: A Preliminary Study","","2019","","","250–254","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 16th International Conference on Mining Software Repositories","","2019","","","https://doi.org/10.1109/MSR.2019.00050;http://dx.doi.org/10.1109/MSR.2019.00050","10.1109/MSR.2019.00050","Developers frequently copy code snippets from publicly-available resources such as Stack Overflow (SO). While this may lead to a 'quick fix' for a development problem, little is known about how these copied code snippets affect the code quality of the recipient application, or how the quality of the recipient classes subsequently evolves over the time of the project. This has an impact on whether such code copying should be encouraged, and how classes that receive such code snippets should be monitored during evolution. To investigate this issue, we used instances from the SOTorrent database where Java snippets had been copied from Stack Overflow into GitHub projects. In each case, we measured the quality of the recipient class just prior to the addition of the snippet, immediately after the addition of the snippet, and at a later stage in the project. Our goal was to determine if the addition of the snippet caused quality to improve or deteriorate, and what the long-term implications were for the quality of the recipient class. Code quality was measured using the cohesion metrics Low-level Similarity-based Class Cohesion (LSCC) and Class Cohesion (CC). Over a random sample of 378 classes that received code snippets copied from Stack Overflow to GitHub, we found that in almost 70% of the cases where the copied snippet affected cohesion, the effect was to reduce the cohesion of the recipient class. Furthermore, this deterioration in cohesion tends to persist in the subsequent evolution of recipient class. In over 70% of cases the recipient class never fully regained the cohesion it lost in receiving the snippet. These results suggest that when copying code snippets from external repositories, more attention should be paid to integrating the code with the recipient class.","cohesion, evolution, metrics, quality","","MSR '19"
"Conference Paper","Johnston O,Jarman D,Berry J,Zhou ZQ,Chen TY","Metamorphic Relations for Detection of Performance Anomalies","","2019","","","63–69","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 4th International Workshop on Metamorphic Testing","","2019","","","https://doi.org/10.1109/MET.2019.00017;http://dx.doi.org/10.1109/MET.2019.00017","10.1109/MET.2019.00017","Metamorphic relations can be used to improve performance testing by comparing successive runs of the software under test. We examine one such metamorphic relation for page load times, which we used to discover and repair a race condition in the Adobe Experience Platform Launch Tag Manager. Histograms for page load times had different modalities, which alerted us to the presence of the bug. We discuss the need for performance measures in addition to the popular mean and standard deviation. We describe two approaches to automatically determine modality: Gaussian Mixture Models and the Silverman Test for Multimodality. Metamorphic relations which involve these performance measures can be used to alert engineers to the presence of performance anomalies.","metamorphic testing, race condition, verification, metamorphic relation, Oracle problem, validation, performance testing","","MET '19"
"Conference Paper","von Zitzewitz A","Mitigating Technical and Architectural Debt with Sonargraph: Using Static Analysis to Enforce Architectural Constraints","","2019","","","66–67","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Second International Conference on Technical Debt","","2019","","","https://doi.org/10.1109/TechDebt.2019.00022;http://dx.doi.org/10.1109/TechDebt.2019.00022","10.1109/TechDebt.2019.00022","Sonargraph is a static analyzer with a focus on software architecture and metrics. The motivation to create Sonargraph came from the assumption that architectural debt (aka structural debt) is the most toxic form of technical debt. Repairing a broken architecture requires global and high-risk changes, while fixing other forms of technical debt mostly involves low-risk local changes. Therefore, the tool enables architects and developers to formally describe their architectural blueprint using a custom DSL (domain specific language). Once defined architectural rules can be checked and enforced in an automated way in all stages of the development process. This guarantees that a software system will never end up as the notorious ""big ball of mud"". Sonargraph currently supports Java, C#, C/C++ and Python and is used by hundreds of organizations worldwide.","software artchitecture, software metrics, architectural debt","","TechDebt '19"
"Conference Paper","Zhang J,Wang X,Zhang H,Sun H,Wang K,Liu X","A Novel Neural Source Code Representation Based on Abstract Syntax Tree","","2019","","","783–794","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00086;http://dx.doi.org/10.1109/ICSE.2019.00086","10.1109/ICSE.2019.00086","Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.","abstract syntax tree, code classification, neural network, code clone detection, source code representation","","ICSE '19"
"Conference Paper","Liu H,Yang Z,Jiang Y,Zhao W,Sun J","Enabling Clone Detection for Ethereum via Smart Contract Birthmarks","","2019","","","105–115","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00024;http://dx.doi.org/10.1109/ICPC.2019.00024","10.1109/ICPC.2019.00024","The Ethereum ecosystem has introduced a pervasive blockchain platform with programmable transactions. Everyone is allowed to develop and deploy smart contracts. Such flexibility can lead to a large collection of similar contracts, i.e., clones, especially when Ethereum applications are highly domain-specific and may share similar functionalities within the same domain, e.g., token contracts often provide interfaces for money transfer and balance inquiry. While smart contract clones have a wide range of impact across different applications, e.g., security, they are relatively little studied.Although clone detection has been a long-standing research topic, blockchain smart contracts introduce new challenges, e.g., syntactic diversity due to trade-off between storage and execution, understanding high-level business logic etc.. In this paper, we highlighted the very first attempt to clone detection of Ethereum smart contracts. To overcome the new challenges, we introduce the concept of smart contract birthmark, i.e., a semantic-preserving and computable representation for smart contract bytecode. The birthmark captures high-level semantics by effectively sketching symbolic execution traces (e.g., data access dependencies, path conditions) and maintain syntactic regularities (e.g., type and number of instructions) as well. Then, the clone detection problem is reduced to a computation of statistical similarity between two contract birthmarks. We have implemented a clone detector called EClone and evaluated it on Ethereum. The empirical results demonstrated the potential of EClone in accurately identifying clones. We have also extended EClone for vulnerability search and managed to detect CVE-2018-10376 instances.","clone detection, symbolic execution, ethereum, smart contract birthmark","","ICPC '19"
"Conference Paper","Fakhoury S,Roy D,Hassan SA,Arnaoudova V","Improving Source Code Readability: Theory and Practice","","2019","","","2–12","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00014;http://dx.doi.org/10.1109/ICPC.2019.00014","10.1109/ICPC.2019.00014","There are several widely accepted metrics to measure code quality that are currently being used in both research and practice to detect code smells and to find opportunities for code improvement. Although these metrics have been proposed as a proxy of code quality, recent research suggests that more often than not, state-of-the-art code quality metrics do not successfully capture quality improvements in the source code as perceived by developers. More specifically, results show that there may be inconsistencies between, on the one hand, the results from metrics for cohesion, coupling, complexity, and readability, and, on the other hand, the interpretation of these metrics in practice. As code improvement tools rely on these metrics, there is a clear need to identify and resolve the aforementioned inconsistencies. This will allow for the creation of tools that are more aligned with developers' perception of quality, and can more effectively help source code improvement efforts.In this study, we investigate 548 instances of source code readability improvements, as explicitly stated by internal developers in practice, from 63 engineered software projects. We show that current readability models fail to capture readability improvements. We also show that tools to calculate additional metrics, to detect refactorings, and to detect style problems are able to capture characteristics that are specific to readability changes and thus should be considered by future readability models.","readability, developers'perception, code quality metrics","","ICPC '19"
"Conference Paper","Yu H,Lam W,Chen L,Li G,Xie T,Wang Q","Neural Detection of Semantic Code Clones via Tree-Based Convolution","","2019","","","70–80","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00021;http://dx.doi.org/10.1109/ICPC.2019.00021","10.1109/ICPC.2019.00021","Code clones are similar code fragments that share the same semantics but may differ syntactically to various degrees. Detecting code clones helps reduce the cost of software maintenance and prevent faults. Various approaches of detecting code clones have been proposed over the last two decades, but few of them can detect semantic clones, i.e., code clones with dissimilar syntax. Recent research has attempted to adopt deep learning for detecting code clones, such as using tree-based LSTM over Abstract Syntax Tree (AST). However, it does not fully leverage the structural information of code fragments, thereby limiting its clone-detection capability.To fully unleash the power of deep learning for detecting code clones, we propose a new approach that uses tree-based convolution to detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical information from code tokens. Additionally, our approach addresses the limitation that source code has an unlimited vocabulary of tokens and models, and thus exploiting lexical information from code tokens is often ineffective when dealing with unseen tokens. Particularly, we propose a new embedding technique called position-aware character embedding (PACE), which essentially treats any token as a position-weighted combination of character one-hot embeddings. Our experimental results show that our approach substantially outperforms an existing state-of-the-art approach with an increase of 0.42 and 0.15 in F1-score on two popular code-clone benchmarks (OJClone and BigCloneBench), respectively, while being more computationally efficient. Our experimental results also show that PACE enables our approach to be substantially more effective when code clones contain unseen tokens.","AST, source code, semantic clone, tree-based convolution, token, embedding, clone detection, structural information, lexical information, generalization","","ICPC '19"
"Conference Paper","Honda H,Tokui S,Yokoi K,Choi E,Yoshida N,Inoue K","CCEvovis: A Clone Evolution Visualization System for Software Maintenance","","2019","","","122–125","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00026;http://dx.doi.org/10.1109/ICPC.2019.00026","10.1109/ICPC.2019.00026","Understanding the evolution of code clones is important in software maintenance. With the information about how code clones evolve, both developers and researchers can understand the impacts of code clones and build a more robust code clone management system. So far, many studies have investigated the evolution of code clones to better understand the effects of code clones. However, only a few systems have been presented to support managing code clones based on the information about how code clone evolves. To mitigate this problem, in this paper, we present CCEvovis, a system that visualizes the evolved code clones across multiple versions of a program. CCEvovis highlights and visualizes the clone change to support software maintenance. CCEvovis is available at: https://github.com/hirotaka0616/CCEvovis.","code clone, software maintenance, visualization","","ICPC '19"
"Conference Paper","Saini V,Farmahinifarahani F,Lu Y,Yang D,Martins P,Sajnani H,Baldi P,Lopes CV","Towards Automating Precision Studies of Clone Detectors","","2019","","","49–59","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00023;http://dx.doi.org/10.1109/ICSE.2019.00023","10.1109/ICSE.2019.00023","Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting inter-tool comparison. Precision-assessment tools are simply lacking.We present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research.","clone detection, machine learning, open source labeled datasets, precision evaluation","","ICSE '19"
"Conference Paper","Saha S,Saha RK,Prasad MR","Harnessing Evolution for Multi-Hunk Program Repair","","2019","","","13–24","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00020;http://dx.doi.org/10.1109/ICSE.2019.00020","10.1109/ICSE.2019.00020","Despite significant advances in automatic program repair (APR) techniques over the past decade, practical deployment remains an elusive goal. One of the important challenges in this regard is the general inability of current APR techniques to produce patches that require edits in multiple locations, i.e., multi-hunk patches. In this work, we present a novel APR technique that generalizes single-hunk repair techniques to include an important class of multi-hunk bugs, namely bugs that may require applying a substantially similar patch at a number of locations. We term such sets of repair locations as evolutionary siblings - similar looking code, instantiated in similar contexts, that are expected to undergo similar changes. At the heart of our proposed method is an analysis to accurately identify a set of evolutionary siblings, for a given bug. This analysis leverages three distinct sources of information, namely the test-suite spectrum, a novel code similarity analysis, and the revision history of the project. The discovered siblings are then simultaneously repaired in a similar fashion. We instantiate this technique in a tool called Hercules and demonstrate that it is able to correctly fix 46 bugs in the Defects4J dataset, the highest of any individual APR technique to date. This includes 15 multi-hunk bugs and overall 11 bugs which have not been fixed by any other technique so far.","multi-hunk patches, code similarity, automatic program repair","","ICSE '19"
"Conference Paper","Islam JF,Mondal M,Roy CK,Schneider KA","Comparing Bug Replication in Regular and Micro Code Clones","","2019","","","81–92","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00022;http://dx.doi.org/10.1109/ICPC.2019.00022","10.1109/ICPC.2019.00022","Copying and pasting source code during software development is known as code cloning. Clone fragments with a minimum size of 5 LOC were usually considered in previous studies. In recent studies, clone fragments which are less than 5 LOC are referred as micro-clones. It has been established by the literature that code clones are closely related with software bugs as well as bug replication. None of the previous studies have been conducted on bug-replication of micro-clones. In this paper we investigate and compare bug-replication in between regular and micro-clones. For the purpose of our investigation, we analyze the evolutionary history of our subject systems and identify occurrences of similarity preserving co-changes (SPCOs) in both regular and micro-clones where they experienced bug-fixes. From our experiment on thousands of revisions of six diverse subject systems written in three different programming languages, C, C# and Java we find that the percentage of clone fragments that take part in bug-replication is often higher in micro-clones than in regular code clones. The percentage of bugs that get replicated in micro-clones is almost the same as the percentage in regular clones. Finally, both regular and micro-clones have similar tendencies of replicating severe bugs according to our experiment. Thus, micro-clones in a code-base should not be ignored. We should rather consider these equally important as of the regular clones when making clone management decisions.","severe bugs, code clones, replicated bugs, micro-clones","","ICPC '19"
"Conference Paper","Tran N,Tran H,Nguyen S,Nguyen H,Nguyen TN","Does BLEU Score Work for Code Migration?","","2019","","","165–176","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00034;http://dx.doi.org/10.1109/ICPC.2019.00034","10.1109/ICPC.2019.00034","Statistical machine translation (SMT) is a fast-growing sub-field of computational linguistics. Until now, the most popular automatic metric to measure the quality of SMT is BiLingual Evaluation Understudy (BLEU) score. Lately, SMT along with the BLEU metric has been applied to a Software Engineering task named code migration. (In)Validating the use of BLEU score could advance the research and development of SMT-based code migration tools. Unfortunately, there is no study to approve or disapprove the use of BLEU score for source code. In this paper, we conducted an empirical study on BLEU score to (in)validate its suitability for the code migration task due to its inability to reflect the semantics of source code. In our work, we use human judgment as the ground truth to measure the semantic correctness of the migrated code. Our empirical study demonstrates that BLEU does not reflect translation quality due to its weak correlation with the semantic correctness of translated code. We provided counter-examples to show that BLEU is ineffective in comparing the translation quality between SMT-based models. Due to BLEU's ineffectiveness for code migration task, we propose an alternative metric RUBY, which considers lexical, syntactical, and semantic representations of source code. We verified that RUBY achieves a higher correlation coefficient with the semantic correctness of migrated code, 0.775 in comparison with 0.583 of BLEU score. We also confirmed the effectiveness of RUBY in reflecting the changes in translation quality of SMT-based translation models. With its advantages, RUBY can be used to evaluate SMT-based code migration models.","code migration, statistical machine translation","","ICPC '19"
"Conference Paper","Huang Y,Kong Q,Jia N,Chen X,Zheng Z","Recommending Differentiated Code to Support Smart Contract Update","","2019","","","260–270","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00045;http://dx.doi.org/10.1109/ICPC.2019.00045","10.1109/ICPC.2019.00045","Blockchain has attracted wide attention. A smart contract is a program that runs on the blockchain, and there is evidence that most of the smart contracts on the Ethereum are highly similar, as they share lots of repetitive code. In this study, we empirically study the repetitiveness of the smart contracts via cluster analysis and try to extract the differentiated code from the similar contracts. Differentiated code is defined as the source code except the repeated ones in two similar smart contracts, which usually illustrates how a software feature is implemented or a programming issue is solved. Then, differentiated code might be used to guide the update of a smart contract in its next version. In this paper, to support the update of a target smart contract, we apply syntax and semantic similarities to discover its similar smart contracts from more than 120,000 smart contracts, and recommend the differentiated code to the target smart contract. The promising experimental results demonstrated the differentiated code can effectively support smart contract update.","code retrieve, evolution analysis, smart contract, contract update, differentiated code","","ICPC '19"
"Conference Paper","Zhang T,Yang D,Lopes C,Kim M","Analyzing and Supporting Adaptation of Online Code Examples","","2019","","","316–327","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00046;http://dx.doi.org/10.1109/ICSE.2019.00046","10.1109/ICSE.2019.00046","Developers often resort to online Q&A forums such as Stack Overflow (SO) for filling their programming needs. Although code examples on those forums are good starting points, they are often incomplete and inadequate for developers' local program contexts; adaptation of those examples is necessary to integrate them to production code. As a consequence, the process of adapting online code examples is done over and over again, by multiple developers independently. Our work extensively studies these adaptations and variations, serving as the basis for a tool that helps integrate these online code examples in a target context in an interactive manner.We perform a large-scale empirical study about the nature and extent of adaptations and variations of SO snippets. We construct a comprehensive dataset linking SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. We then qualitatively inspect 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, we build an automated adaptation analysis technique on top of GumTree to classify the entire dataset into these types. We build a Chrome extension called ExampleStack that automatically lifts an adaptation-aware template from each SO example and its GitHub counterparts to identify hot spots where most changes happen. A user study with sixteen programmers shows that seeing the commonalities and variations in similar GitHub counterparts increases their confidence about the given SO example, and helps them grasp a more comprehensive view about how to reuse the example differently and avoid common pitfalls.","online code examples, code adaptation","","ICSE '19"
"Conference Paper","Hoang T,Lawall J,Oentaryo RJ,Tian Y,Lo D","PatchNet: A Tool for Deep Patch Classification","","2019","","","83–86","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings","","2019","","","https://doi.org/10.1109/ICSE-Companion.2019.00044;http://dx.doi.org/10.1109/ICSE-Companion.2019.00044","10.1109/ICSE-Companion.2019.00044","This work proposes PatchNet, an automated tool based on hierarchical deep learning for classifying patches by extracting features from commit messages and code changes. PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of a code change, differentiating it from the existing deep learning models on source code. PatchNet provides several options allowing users to select parameters for the training process. The tool has been validated in the context of automatic identification of stable-relevant patches in the Linux kernel and is potentially applicable to automate other software engineering tasks that can be formulated as patch classification problems. A video demonstrating PatchNet is available at https://goo.gl/CZjG6X. The PatchNet implementation is available at https://github.com/hvdthong/PatchNetTool.","","","ICSE '19"
"Conference Paper","Chen M,Fischer F,Meng N,Wang X,Grossklags J","How Reliable is the Crowdsourced Knowledge of Security Implementation?","","2019","","","536–547","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00065;http://dx.doi.org/10.1109/ICSE.2019.00065","10.1109/ICSE.2019.00065","Stack Overflow (SO) is the most popular online Q&A site for developers to share their expertise in solving programming issues. Given multiple answers to a certain question, developers may take the accepted answer, the answer from a person with high reputation, or the one frequently suggested. However, researchers recently observed that SO contains exploitable security vulnerabilities in the suggested code of popular answers, which found their way into security-sensitive high-profile applications that millions of users install every day. This observation inspires us to explore the following questions: How much can we trust the security implementation suggestions on SO? If suggested answers are vulnerable, can developers rely on the community's dynamics to infer the vulnerability and identify a secure counterpart?To answer these highly important questions, we conducted a comprehensive study on security-related SO posts by contrasting secure and insecure advice with the community-given content evaluation. Thereby, we investigated whether SO's gamification approach on incentivizing users is effective in improving security properties of distributed code examples. Moreover, we traced the distribution of duplicated samples over given answers to test whether the community behavior facilitates or prevents propagation of secure and insecure code suggestions within SO.We compiled 953 different groups of similar security-related code examples and labeled their security, identifying 785 secure answer posts and 644 insecure answer posts. Compared with secure suggestions, insecure ones had higher view counts (36,508 vs. 18,713), received a higher score (14 vs. 5), and had significantly more duplicates (3.8 vs. 3.0) on average. 34% of the posts provided by highly reputable so-called trusted users were insecure.Our findings show that based on the distribution of secure and insecure code on SO, users being laymen in security rely on additional advice and guidance. However, the community-given feedback does not allow differentiating secure from insecure choices. The reputation mechanism fails in indicating trustworthy users with respect to security questions, ultimately leaving other users wandering around alone in a software security minefield.","social dynamics, crowdsourced knowledge, stack overflow, security implementation","","ICSE '19"
"Conference Paper","Steinbeck M,Koschke R,Rüdel MO","Comparing the EvoStreets Visualization Technique in Two D and Three-Dimensional Environments: A Controlled Experiment","","2019","","","231–242","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/icpc.2019.00042;http://dx.doi.org/10.1109/icpc.2019.00042","10.1109/icpc.2019.00042","Analyzing and maintaining large software systems is a challenging task due to the sheer amount of information contained therein. To overcome this problem, Steinbrückner developed a visualization technique named EvoStreets. Utilizing the city metaphor, EvoStreets are well suited to visualize the hierarchical structure of a software as well as hotspots regarding certain aspects. Early implementations of this approach use three-dimensional rendering on regular two-dimensional displays. Recently, though, researchers have begun to visualize EvoStreets in virtual reality using head-mounted displays, claiming that this environment enhances user experience. Yet, there is little research on comparing the differences of EvoStreets visualized in virtual reality with EvoStreets visualized in conventional environments.This paper presents a controlled experiment, involving 34 participants, in which we compared the EvoStreet visualization technique in different environments, namely, orthographic projection with keyboard and mouse, 2.5D projection with keyboard and mouse, and virtual reality with head-mounted displays and hand-held controllers. Using these environments, the participants had to analyze existing Java systems regarding software clones. According to our results, it cannot be assumed that: 1) the orthographic environment takes less time to find an answer, 2) the 2.5D and virtual reality environments provide better results regarding the correctness of edge-related tasks compared to the orthographic environment, and 3) the performance regarding time and correctness differs between the 2.5D and virtual reality environments.","","","ICPC '19"
"Conference Paper","Schnappinger M,Osman MH,Pretschner A,Fietzke A","Learning a Classifier for Prediction of Maintainability Based on Static Analysis Tools","","2019","","","243–248","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00043;http://dx.doi.org/10.1109/ICPC.2019.00043","10.1109/ICPC.2019.00043","Static Code Analysis Tools are a popular aid to monitor and control the quality of software systems. Still, these tools only provide a large number of measurements that have to be interpreted by the developers in order to obtain insights about the actual quality of the software. In cooperation with professional quality analysts, we manually inspected source code from three different projects and evaluated its maintainability. We then trained machine learning algorithms to predict the human maintainability evaluation of program classes based on code metrics. The code metrics include structural metrics such as nesting depth, cloning information and abstractions like the number of code smells. We evaluated this approach on a dataset of more than 115,000 Lines of Code. Our model is able to predict up to 81% of the threefold labels correctly and achieves a precision of 80%. Thus, we believe this is a promising contribution towards automated maintainability prediction. In addition, we analyzed the attributes in our created dataset and identified the features with the highest predictive power, i.e. code clones, method length, and the number of alerts raised by the tool Teamscale. This insight provides valuable help for users needing to prioritize tool measurements.","software quality, software maintenance, maintenance tools, code comprehension, static code analysis","","ICPC '19"
"Conference Paper","Ramsauer R,Lohmann D,Mauerer W","The List is the Process: Reliable Pre-Integration Tracking of Commits on Mailing Lists","","2019","","","807–818","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00088;http://dx.doi.org/10.1109/ICSE.2019.00088","10.1109/ICSE.2019.00088","A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history.We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth.Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.","","","ICSE '19"
"Conference Paper","Aghajani E,Nagy C,Vega-Márquez OL,Linares-Vásquez M,Moreno L,Bavota G,Lanza M","Software Documentation Issues Unveiled","","2019","","","1199–1210","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00122;http://dx.doi.org/10.1109/ICSE.2019.00122","10.1109/ICSE.2019.00122","(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavorable take on documentation.Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.","empirical study, documentation","","ICSE '19"
"Conference Paper","Tufano M,Pantiuchina J,Watson C,Bavota G,Poshyvanyk D","On Learning Meaningful Code Changes via Neural Machine Translation","","2019","","","25–36","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00021;http://dx.doi.org/10.1109/ICSE.2019.00021","10.1109/ICSE.2019.00021","Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL.Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.","neural-machine translation, empirical study","","ICSE '19"
"Conference Paper","Sivaraman A,Zhang T,Van den Broeck G,Kim M","Active Inductive Logic Programming for Code Search","","2019","","","292–303","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00044;http://dx.doi.org/10.1109/ICSE.2019.00044","10.1109/ICSE.2019.00044","Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique Alice is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, Alice automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). Alice then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that Alice removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, Alice does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that Alice is easy to use and helps express complex code patterns.","code search, active learning, inductive logic programming","","ICSE '19"
"Conference Paper","Yu Y","FAST: Flattening Abstract Syntax Trees for Efficiency","","2019","","","278–279","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings","","2019","","","https://doi.org/10.1109/ICSE-Companion.2019.00113;http://dx.doi.org/10.1109/ICSE-Companion.2019.00113","10.1109/ICSE-Companion.2019.00113","Frequently source code analysis tools need to exchange internal representations of abstract syntax trees (AST) with each other. Conveniently, and intuitively, the externalised representations are in the form of hierarchical trees. We argue, counter-intuitively, that hierarchical representation is not the most efficient way for source analysis tools to exchange parsed AST. In this work, we propose to speed up AST parsing whilst preserving the equivalence of hierarchies in binary forms: (1) AST could be saved as a flat one-dimensional array where pointers to tree nodes are converted into integer offsets, and (2) such flattened AST are more efficient to access by programming tools through the generated application programming interfaces (API). In programming language-agnostic evaluations, we show that parsing flattened AST becomes 100x faster than in textual form AST on a benchmark of open-source projects of 6 different programming languages.","","","ICSE '19"
"Conference Paper","Liu K,Kim D,Bissyandé TF,Kim T,Kim K,Koyuncu A,Kim S,Traon YL","Learning to Spot and Refactor Inconsistent Method Names","","2019","","","1–12","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00019;http://dx.doi.org/10.1109/ICSE.2019.00019","10.1109/ICSE.2019.00019","To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1-measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.","","","ICSE '19"
"Conference Paper","Wei L,Liu Y,Cheung SC","Pivot: Learning API-Device Correlations to Facilitate Android Compatibility Issue Detection","","2019","","","878–888","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00094;http://dx.doi.org/10.1109/ICSE.2019.00094","10.1109/ICSE.2019.00094","The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps.","Android fragmentation, learning, compatibility, static analysis","","ICSE '19"
"Conference Paper","Li Z,Chen TH,Yang J,Shang W","Dlfinder: Characterizing and Detecting Duplicate Logging Code Smells","","2019","","","152–163","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00032;http://dx.doi.org/10.1109/ICSE.2019.00032","10.1109/ICSE.2019.00032","Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.","log, duplicate log, static analysis, empirical study, code smell","","ICSE '19"
"Conference Paper","Mondal D,Mondal M,Roy CK,Schneider K,Wang S,Li Y","Towards Visualizing Large Scale Evolving Clones","","2019","","","302–303","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings","","2019","","","https://doi.org/10.1109/ICSE-Companion.2019.00125;http://dx.doi.org/10.1109/ICSE-Companion.2019.00125","10.1109/ICSE-Companion.2019.00125","Software systems in this big data era are growing larger and becoming more intricate. Tracking and managing code clones in such evolving software systems are challenging tasks. To understand how clone fragments are evolving, the programmers often analyze the co-evolution of clone fragments manually to decide about refactoring, tracking, and bug removal. Such manual analysis is infeasible for a large number of clones with clones evolving over hundreds of software revisions. We propose a visual analytics framework, that leverages big data visualization techniques to manage code clones in large software systems. Our framework combines multiple information-linked zoomable views, where users can explore and analyze clones through interactive exploration in real time. We discuss several scenarios where our framework may assist developers in real-life software development and clone maintenance. Experts' reviews reveal many future potentials of our framework.","","","ICSE '19"
"Conference Paper","Xu S,Dong Z,Meng N","Meditor: Inference and Application of API Migration Edits","","2019","","","335–346","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00052;http://dx.doi.org/10.1109/ICPC.2019.00052","10.1109/ICPC.2019.00052","Developers build programs based on software libraries. When a library evolves, programmers need to migrate their client code from the library's old release(s) to new release(s). Due to the API backwards incompatibility issues, such code migration may require developers to replace API usage and apply extra edits (e.g., statement insertions or deletions) to ensure the syntactic or semantic correctness of migrated code. Existing tools extract API replacement rules without handling the additional edits necessary to fulfill a migration task. This paper presents our novel approach, Meditor, which extracts and applies the necessary edits together with API replacement changes.Meditor has two phases: inference and application of migration edits. For edit inference, Meditor mines open source repositories for migration-related (MR) commits, and conducts program dependency analysis on changed Java files to locate and cluster MR code changes. From these changes, Meditor further generalizes API migration edits by abstracting away unimportant details (e.g., concrete variable identifiers). For edit application, Meditor matches a given program with inferred edits to decide which edit is applicable, customizes each applicable edit, and produces a migrated version for developers to review.We applied Meditor to four popular libraries: Lucene, Craft-Bukkit, Android SDK, and Commons IO. By searching among 602,249 open source projects on GitHub, Meditor identified 1,368 unique migration edits. Among these edits, 885 edits were extracted from single updated statements, while the other 483 more complex edits were from multiple co-changed statements. We sampled 937 inferred edits for manual inspection and found all of them to be correct. Our evaluation shows that Meditor correctly applied code migrations in 218 out of 225 cases. This research will help developers automatically adapt client code to different library versions.","program dependency analysis, API migration edits, automatic program transformation","","ICPC '19"
"Conference Paper","Silva RF,Roy CK,Rahman MM,Schneider KA,Paixao K,de Almeida Maia M","Recommending Comprehensive Solutions for Programming Tasks by Mining Crowd Knowledge","","2019","","","358–368","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 27th International Conference on Program Comprehension","","2019","","","https://doi.org/10.1109/ICPC.2019.00054;http://dx.doi.org/10.1109/ICPC.2019.00054","10.1109/ICPC.2019.00054","Developers often search for relevant code examples on the web for their programming tasks. Unfortunately, they face two major problems. First, the search is impaired due to a lexical gap between their query (task description) and the information associated with the solution. Second, the retrieved solution may not be comprehensive, i.e., the code segment might miss a succinct explanation. These problems make the developers browse dozens of documents in order to synthesize an appropriate solution. To address these two problems, we propose CROKAGE (Crowd Knowledge Answer Generator), a tool that takes the description of a programming task (the query) and provides a comprehensive solution for the task. Our solutions contain not only relevant code examples but also their succinct explanations. Our proposed approach expands the task description with relevant API classes from Stack Overflow Q & A threads and then mitigates the lexical gap problems. Furthermore, we perform natural language processing on the top quality answers and then return such programming solutions containing code examples and code explanations unlike earlier studies. We evaluate our approach using 97 programming queries, of which 50% was used for training and 50% was used for testing, and show that it outperforms six baselines including the state-of-art by a statistically significant margin. Furthermore, our evaluation with 29 developers using 24 tasks (queries) confirms the superiority of CROKAGE over the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality (code + explanation).","word embedding, mining crowd knowledge, stack overflow","","ICPC '19"
"Conference Paper","Li Z","Characterizing and Detecting Duplicate Logging Code Smells","","2019","","","147–149","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings","","2019","","","https://doi.org/10.1109/ICSE-Companion.2019.00062;http://dx.doi.org/10.1109/ICSE-Companion.2019.00062","10.1109/ICSE-Companion.2019.00062","Software logs are widely used by developers to assist in various tasks. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems and uncovered five patterns of duplicate logging code smells. For each instance of the problematic code smell, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the manually studied systems and two additional systems. In total, combining the results of DLFinder and our manual analysis, DLFinder is able to detect over 85% of the instances which were reported to developers and then fixed.","log, static analysis, code smell, duplicate log","","ICSE '19"
"Conference Paper","Fernandes E","Stuck in the Middle: Removing Obstacles to New Program Features through Batch Refactoring","","2019","","","206–209","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings","","2019","","","https://doi.org/10.1109/ICSE-Companion.2019.00083;http://dx.doi.org/10.1109/ICSE-Companion.2019.00083","10.1109/ICSE-Companion.2019.00083","Developers may introduce poor code structures spotted by code smells along the software maintenance. However, only some of these structures eventually become obstacles to the addition of new features. Developers are forced to remove these obstacles before adding features. Identifying which poor structures are actual obstacles is hard, due to their subtlety and scattering across code elements. Such identification has been largely debated by developers in public platforms such as Gerrit Code Review. Fully removing obstacles is also hard, as developers often have to perform non-trivial sets of interrelated code transformations. Despite enabling the feature addition, certain sets of transformations tend to introduce rather than remove code smells. The scarce knowledge on recurring obstacles, and how refactoring can cope with them, helps little in guiding the feature addition. This doctoral research aims to address the current scarceness via empirical studies with projects and their developers. Our major goal is three-fold: (1) to assess past feature additions in order to elicit recurring obstacles; (2) to understand when interrelated transformations unexpectedly introduce poor code structures; and (3) to propose a refactoring recommender system. Contrarily to the few existing ones, our system aims to guide developers along the feature addition while removing poor code structures.","recommender system, poor code structure, feature addition, code refactoring, software maintenance","","ICSE '19"
"Conference Paper","Miller K,Kwon Y,Sun Y,Zhang Z,Zhang X,Lin Z","Probabilistic Disassembly","","2019","","","1187–1198","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00121;http://dx.doi.org/10.1109/ICSE.2019.00121","10.1109/ICSE.2019.00121","Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7% FP. In comparison, a state-of-the-art superset disassembly technique has 85% FP. A rewriter built on our disassembly can generate binaries that are only half of the size of those by superset disassembly and run 3% faster. While many widely-used disassemblers such as IDA and BAP suffer from missing function entries, our experiment also shows that even without any function entry information, our disassembler can still achieve 0 FN and 6.8% FP.","","","ICSE '19"
"Conference Paper","Fan M,Luo X,Liu J,Wang M,Nong C,Zheng Q,Liu T","Graph Embedding Based Familial Analysis of Android Malware Using Unsupervised Learning","","2019","","","771–782","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 41st International Conference on Software Engineering","","2019","","","https://doi.org/10.1109/ICSE.2019.00085;http://dx.doi.org/10.1109/ICSE.2019.00085","10.1109/ICSE.2019.00085","The rapid growth of Android malware has posed severe security threats to smartphone users. On the basis of the familial trait of Android malware observed by previous work, the familial analysis is a promising way to help analysts better focus on the commonalities of malware samples within the same families, thus reducing the analytical workload and accelerating malware analysis. The majority of existing approaches rely on supervised learning and face three main challenges, i.e., low accuracy, low efficiency, and the lack of labeled dataset. To address these challenges, we first construct a fine-grained behavior model by abstracting the program semantics into a set of subgraphs. Then, we propose SRA, a novel feature that depicts the similarity relationships between the Structural Roles of sensitive API call nodes in subgraphs. An SRA is obtained based on graph embedding techniques and represented as a vector, thus we can effectively reduce the high complexity of graph matching. After that, instead of training a classifier with labeled samples, we construct malware link network based on SRAs and apply community detection algorithms on it to group the unlabeled samples into groups. We implement these ideas in a system called GefDroid that performs Graph embedding based familial analysis of AnDroid malware using unsupervised learning. Moreover, we conduct extensive experiments to evaluate GefDroid on three datasets with ground truth. The results show that GefDroid can achieve high agreements (0.707--0.883 in term of NMI) between the clustering results and the ground truth. Furthermore, GefDroid requires only linear run-time overhead and takes around 8.6s to analyze a sample on average, which is considerably faster than the previous work.","Android malware, familial analysis, unsupervised learning, graph embedding","","ICSE '19"
"Conference Paper","Wang H,Wang X,Guo Y","Characterizing the Global Mobile App Developers: A Large-Scale Empirical Study","","2019","","","150–161","IEEE Press","Montreal, Quebec, Canada","Proceedings of the 6th International Conference on Mobile Software Engineering and Systems","","2019","","","","","The rapid growth of the mobile app ecosystem has attracted a great number of mobile app developers. However, few previous studies have analyzed app developers comprehensively at a large scale, and little is known of the characteristics of mobile app developers. In this work, we take the first step to understand global app developers from the view of mobile app life-cycle. We have created a repository of over 1.03 million Android app developers distributed at Google Play and 16 popular alternative markets, along with over 6.2 million Android apps (including apks and metadata) they created. We then explored various characteristics of these app developers from different angles including developing behaviors, releasing behaviors, app maintenance behaviors and malicious behaviors. The experimental results have revealed various interesting findings, as well as insights for future research directions. Our efforts can contribute to different stakeholders of the mobile app ecosystem.","","","MOBILESoft '19"
"Conference Paper","Zhu C,Tang Y,Wang Q,Li M","Enhancing Code Similarity Analysis for Effective Vulnerability Detection","","2019","","","153–158","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Conference on Computer Science and Software Engineering","Xi'an, China","2019","9781450371728","","https://doi.org/10.1145/3339363.3339383;http://dx.doi.org/10.1145/3339363.3339383","10.1145/3339363.3339383","Vulnerabilities may spread through different software projects via code reusing. Therefore, it is important to identify the components of a project, to reveal vulnerabilities inside a component that may affect the project. Code similarity analysis is crucial for improving the recall of component identification.The local sensitive hashing (LSH) solves the problem that the traditional hash cannot record the difference information between similar files. Two files with nuances cannot be tested by the MD5 matching algorithm, but they can generate similar fingerprints with LSH. The similarity degree between files can be measured by calculating the Hamming distance between fingerprints.The Simhash algorithm can complement the file-level homology analysis algorithm based on MD5 matching. We can combine the Simhash algorithm and the MD5 matching algorithm, i.e., quickly find all the identical clone files through MD5 matching, and then find similar files with slight modifications through the similar hash algorithm, which makes the final analysis result more accurate.In the massive data set, the process of finding the fingerprint value that is similar (in this experiment, the Hamming distance <=3) to the Simhash fingerprint generated by the code to be tested has efficiency problems. In the process of optimizing query efficiency, we generally carry out time and space trade-offs. Therefore, we leverage the optimization method used by Gurmeet to make a compromise between time efficiency and space efficiency.","Simhash, File Similarity, Vulnerability Analysis","","CSSE '19"
"Journal Article","Novak M,Joy M,Kermek D","Source-Code Similarity Detection and Detection Tools Used in Academia: A Systematic Review","ACM Trans. Comput. Educ.","2019","19","3","","Association for Computing Machinery","New York, NY, USA","","","2019-05","","","https://doi.org/10.1145/3313290;http://dx.doi.org/10.1145/3313290","10.1145/3313290","Teachers deal with plagiarism on a regular basis, so they try to prevent and detect plagiarism, a task that is complicated by the large size of some classes. Students who cheat often try to hide their plagiarism (obfuscate), and many different similarity detection engines (often called plagiarism detection tools) have been built to help teachers. This article focuses only on plagiarism detection and presents a detailed systematic review of the field of source-code plagiarism detection in academia. This review gives an overview of definitions of plagiarism, plagiarism detection tools, comparison metrics, obfuscation methods, datasets used for comparison, and algorithm types. Perspectives on the meaning of source-code plagiarism detection in academia are presented, together with categorisations of the available detection tools and analyses of their effectiveness. While writing the review, some interesting insights have been found about metrics and datasets for quantitative tool comparison and categorisation of detection algorithms. Also, existing obfuscation methods classifications have been expanded together with a new definition of “source-code plagiarism detection in academia.”","programming, systematic review, Source-code, detection, academia, similarity, plagiarism, education","",""
"Conference Paper","Batista ME,Parreira PA,Costa H","An Exploratory Study on Detection of Cloned Code in Information Systems","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XV Brazilian Symposium on Information Systems","Aracaju, Brazil","2019","9781450372374","","https://doi.org/10.1145/3330204.3330277;http://dx.doi.org/10.1145/3330204.3330277","10.1145/3330204.3330277","Code clones are source code parts that are identical or have some degree of similarity to another part of the code. Cloning arises for a variety of reasons, including copy and paste and the reuse of ad-hoc code by programmers. Detection of information system clones is aimed at propagating changes by all clones at the development, maintenance and evolution stages, preserving data consistency, correcting errors, and so on. Clones can be classified as 1, 2, 3 and 4, depending on their similarity and characteristics that classify them as such. Several techniques and tools have been created with the objective of detecting code clones, and for this, they use techniques of representation of the source code in text, token, tree, graphic, hybrid and metrics. This systematic mapping work presents answers to the four research questions, which aim to identify, count and catalog, data from a set of 875 articles, of which 128 were selected, for the selection of relevant information seeking to provide content for the collection of data objectified. In all, 52 clone detection tools were identified, which reinforce the current theme; 26 ways of presenting source code to detect clones, where the commonly used ones stand out for ease of understanding and handling; 13 programming languages in 6 paradigms and the identification, highlighting the great presence of clones detection in object oriented information systems, of all 4 types of clones, as well as semantic and syntactic clones, which reinforces the current questioning of authors of this division search line into four types.","Clone Detection, Programming Paradigm, Clone Detection Tools, Clone Detection Approaches, Clone Code, Programming Language","","SBSI'19"
"Conference Paper","Martins J,Bezerra CI,Uchôa A","Analyzing the Impact of Inter-Smell Relations on Software Maintainability: An Empirical Study with Software Product Lines","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XV Brazilian Symposium on Information Systems","Aracaju, Brazil","2019","9781450372374","","https://doi.org/10.1145/3330204.3330254;http://dx.doi.org/10.1145/3330204.3330254","10.1145/3330204.3330254","A Software Product Line (SPL) consists of a systematic reuse strategy to construct systems with less effort as long as they belong to the same family that share the same components and belong to the same domain of Marketplace. In this context, to support large-scale reuse, components of a Software Product Line should be easy to maintain. Thus, developers should be more concerned with anomalies known as code smells and more than that, co-occurrences known as Inter-smell deserve to be further studied to verify their real impact on maintainability in SPL. Thus, this paper conducts a study to investigate the impact of Inter-smell occurrences on maintainability in MobileMedia and Health Watcher SPLs. The results show that the presence of co-occurrences of Inter-smell did not negatively impact the maintenance of MobileMedia and Health Watcher SPLs, unlike results found in other studies in the literature, and even more, our results indicate that the metric Lack of Cohesion of Methods is one of the most important for the maintainability of object-oriented SPLs.","Software - QualityControl, Software product line engineering, Code Smell, Maintainability","","SBSI'19"
"Conference Paper","de F. Farias MA,Xisto R,Santos MS,Fontes RS,Colaço M,Spínola R,Mendonça M","Identifying Technical Debt through a Code Comment Mining Tool","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XV Brazilian Symposium on Information Systems","Aracaju, Brazil","2019","9781450372374","","https://doi.org/10.1145/3330204.3330227;http://dx.doi.org/10.1145/3330204.3330227","10.1145/3330204.3330227","Context: The software industry often has to deal with several challenges to deliver and maintain products, such as providing useful software with high quality, on time, and on the budget. This challenge is difficult, if not impossible, to overcome, and software engineers end up developing immature artifacts that cause unexpected delays and make the whole system difficult to maintain and evolve in the future. That is what the Software Engineering (SE) community now calls Technical Debts. Objective: The main goal of this paper is to propose an approach to support and automate the identification of different types of TD through code comment analysis, as well as to propose and evaluate the eXcomment. Method: We carry out a proof-of-concept study in two Open Source Projects: ArgoUML and JFreeChart. Results: Our findings indicate that the eXcomment make it possible to select a list of suitable comments to support TD identification automatically. The study provided new evidence on how software engineers can use code comments to detect and classify TD items automatically. Conclusion: This work contributes to bridge the gap between the TD identification area and code comment analysis, successfully using code comments to detect several types of TD.","Technical Debt Identification, Code Comment Analysis, Software Engineering","","SBSI'19"
"Conference Paper","Gonçales LJ,Farias K,Bischoff V","On the Effects of Developers' Intuition on Measuring Similarity Between UML Models","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XV Brazilian Symposium on Information Systems","Aracaju, Brazil","2019","9781450372374","","https://doi.org/10.1145/3330204.3330238;http://dx.doi.org/10.1145/3330204.3330238","10.1145/3330204.3330238","Software design models play a key role in many activities of information systems engineering, such as documenting software artefacts, communicating project decisions, and code generation. In this scenario, the techniques for comparison of software design models are used for several purposes, such as, for detecting clones, and model evolution. In the last decades, academia proposed different techniques for comparing software models. Even using these different techniques for model comparison, this process is still an activity of a subjective nature, because during this process, different developers can interpret the similarity differently. Thus, the problem is that it is still unknown if developers has the same intuition in order to resolve comparison of software design models. For this, the main objective of this work is to explore the effects of their experience level, i.e., experienced and inexperienced developers, relative to their effort and correctness for resolving activities of comparing software design models. Therefore, a controlled experiment was conducted to evaluate the developer's experience level regarding on similarities of UML Models. The results show that the developer's experience does not affect the understanding of similarities activities.","Information Systems, Controlled Experiment, UML, Software Engineering, Unified Modelling Language","","SBSI'19"
"Conference Paper","Classen J,Hollick M","Inside Job: Diagnosing Bluetooth Lower Layers Using off-the-Shelf Devices","","2019","","","186–191","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks","Miami, Florida","2019","9781450367264","","https://doi.org/10.1145/3317549.3319727;http://dx.doi.org/10.1145/3317549.3319727","10.1145/3317549.3319727","Bluetooth is among the dominant standards for wireless short-range communication with multi-billion Bluetooth devices shipped each year. Basic Bluetooth analysis inside consumer hardware such as smartphones can be accomplished observing the Host Controller Interface (HCI) between the operating system's driver and the Bluetooth chip. However, the HCI does not provide insights to tasks running inside a Bluetooth chip or Link Layer (LL) packets exchanged over the air. As of today, consumer hardware internal behavior can only be observed with external, and often expensive tools, that need to be present during initial device pairing. In this paper, we leverage standard smartphones for on-device Bluetooth analysis and reverse engineer a diagnostic protocol that resides inside Broadcom chips. Diagnostic features include sniffing lower layers such as LL for Classic Bluetooth and Bluetooth Low Energy (BLE), transmission and reception statistics, test mode, and memory peek and poke.","","","WiSec '19"
"Conference Paper","Lou C,Huang P,Smith S","Comprehensive and Efficient Runtime Checking in System Software through Watchdogs","","2019","","","51–57","Association for Computing Machinery","New York, NY, USA","Proceedings of the Workshop on Hot Topics in Operating Systems","Bertinoro, Italy","2019","9781450367271","","https://doi.org/10.1145/3317550.3321440;http://dx.doi.org/10.1145/3317550.3321440","10.1145/3317550.3321440","Systems software today is composed of numerous modules and exhibits complex failure modes. Existing failure detectors focus on catching simple, complete failures and treat programs uniformly at the process level. In this paper, we argue that modern software needs intrinsic failure detectors that are tailored to individual systems and can detect anomalies within a process at finer granularity. We particularly advocate a notion of intrinsic software watchdogs and propose an abstraction for it. Among the different styles of watchdogs, we believe watchdogs that imitate the main program can provide the best combination of completeness, accuracy and localization for detecting gray failures. But, manually constructing such mimic-type watchdogs is challenging and time-consuming. To close this gap, we present an early exploration for automatically generating mimic-type watchdogs.","","","HotOS '19"
"Conference Paper","Wang Y,Shen H,Gao J,Cheng X","Learning Binary Hash Codes for Fast Anchor Link Retrieval across Networks","","2019","","","3335–3341","Association for Computing Machinery","New York, NY, USA","The World Wide Web Conference","San Francisco, CA, USA","2019","9781450366748","","https://doi.org/10.1145/3308558.3313430;http://dx.doi.org/10.1145/3308558.3313430","10.1145/3308558.3313430","Users are usually involved in multiple social networks, without explicit anchor links that reveal the correspondence among different accounts of the same user across networks. Anchor link prediction aims to identify the hidden anchor links, which is a fundamental problem for user profiling, information cascading, and cross-domain recommendation. Although existing methods perform well in the accuracy of anchor link prediction, the pairwise search manners on inferring anchor links suffer from big challenge when being deployed in practical systems. To combat the challenges, in this paper we propose a novel embedding and matching architecture to directly learn binary hash code for each node. Hash codes offer us an efficient index to filter out the candidate node pairs for anchor link prediction. Extensive experiments on synthetic and real world large-scale datasets demonstrate that our proposed method has high time efficiency without loss of competitive prediction accuracy in anchor link prediction.","scalability, anchor link prediction, hash code","","WWW '19"
"Conference Paper","Brown G,Reyes R,Wong M","Towards Heterogeneous and Distributed Computing in C++","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on OpenCL","Boston, MA, USA","2019","9781450362306","","https://doi.org/10.1145/3318170.3318196;http://dx.doi.org/10.1145/3318170.3318196","10.1145/3318170.3318196","Current semiconductor trends have shown a significant s h ift in computer system architectures towards distributed and heterogeneous systems that combine multiple different processors such as CPUs, GPUs, and FPGAs that all work together, performing many different kinds of tasks in parallel. Recent trends show C++ to be a particularly popular programming language for heterogeneous and distributed systems due to its ability to both provide user-friendly high-level abstractions but also the ability to generate highly optimised code. However heterogeneous and distributed computing is not yet a first-class citizen of the C++ language or the STL.In the recent C++ standards; C++11/14/17, many new features have been introduced which provide much more control over parallelism and concurrency including parallel implementations of many of the standard algorithms, which now have several implementations including one which targets a range of heterogeneous platforms via SYCL. However, as this is all still based on the standard C++ machine model, which is strictly CPU focused, C++ continues to hold many limitations over the ability to execute work on accelerators due to the difference in memory and execution models.This paper will provide an overview of the recent efforts to introduce heterogeneous and distributed computing to the ISO C++ standard. It is an up-to-date status for 2019 and supersedes our previous DHPCC++ paper.","heterogeneous programming, distributed programming models, C++, executors, parallelism, concurrency","","IWOCL'19"
"Conference Paper","Gao L,Wan B,Fang C,Li Y,Chen C","Automatic Clustering of Different Solutions to Programming Assignments in Computing Education","","2019","","","164–170","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM Conference on Global Computing Education","Chengdu,Sichuan, China","2019","9781450362597","","https://doi.org/10.1145/3300115.3309515;http://dx.doi.org/10.1145/3300115.3309515","10.1145/3300115.3309515","A computer programming assignment may have various solutions, and extracting them is of great significance for both teaching and learning. However, it could be challenging for instructors and students to identify the differences between those solutions if they are on a large scale. Since code similarity is of vital importance in identifying the differences between solutions, we review previous researches on code similarity and design a neural network-based algorithm for detecting the similarity between codes in a pair as well as identifying the features that have a big impact on code similarity. Then we develop a clustering algorithm based on code similarity that can automatically generate clusters for all correct solutions to a given programming assignment. Our experiment demonstrates that the clustering algorithm can successfully obtain distinctive clusters in our dataset. Our analysis of typical solutions can provide inspirations for instructors and students.","code similarity, programming education, neural network, code clustering","","CompEd '19"
"Conference Paper","Head A,Hohman F,Barik T,Drucker SM,DeLine R","Managing Messes in Computational Notebooks","","2019","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","Glasgow, Scotland Uk","2019","9781450359702","","https://doi.org/10.1145/3290605.3300500;http://dx.doi.org/10.1145/3290605.3300500","10.1145/3290605.3300500","Data analysts use computational notebooks to write code for analyzing and visualizing data. Notebooks help analysts iteratively write analysis code by letting them interleave code with output, and selectively execute cells. However, as analysis progresses, analysts leave behind old code and outputs, and overwrite important code, producing cluttered and inconsistent notebooks. This paper introduces code gathering tools, extensions to computational notebooks that help analysts find, clean, recover, and compare versions of code in cluttered, inconsistent notebooks. The tools archive all versions of code outputs, allowing analysts to review these versions and recover the subsets of code that produced them. These subsets can serve as succinct summaries of analysis activity or starting points for new analyses. In a qualitative usability study, 12 professional analysts found the tools useful for cleaning notebooks and writing analysis code, and discovered new ways to use them, like generating personal documentation and lightweight versioning.","messes, inconsistency, program slicing, code history, computational notebooks, clutter, exploratory programming","","CHI '19"
"Conference Paper","Kelleher C,Hnin W","Predicting Cognitive Load in Future Code Puzzles","","2019","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","Glasgow, Scotland Uk","2019","9781450359702","","https://doi.org/10.1145/3290605.3300487;http://dx.doi.org/10.1145/3290605.3300487","10.1145/3290605.3300487","Code puzzles are an increasingly popular way to introduce youth to programming. Yet our knowledge about how to maximize learning from puzzles is incomplete. We conducted a data collection study and trained a model that predicts cognitive load, the mental effort necessary to complete a task, on a future puzzle. Controlling cognitive load can lead to more effective learning. Our model suggests that it is possible to predict Cognitive Load on future problems; the model could correctly distinguish the more difficult puzzle within a pair 71%-79% of the time. Further, studying the model itself provides new insights into the sources of puzzle difficulty, the factors that contribute to Cognitive Load, and their inter-relationships. Finally, the ability to predict Cognitive Load on a future puzzle is an important step towards the creation of adaptive code puzzle systems.","code puzzles, cognitive load, adaptive learning system","","CHI '19"
"Conference Paper","Chen L,Ye W,Zhang S","Capturing Source Code Semantics via Tree-Based Convolution over API-Enhanced AST","","2019","","","174–182","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM International Conference on Computing Frontiers","Alghero, Italy","2019","9781450366854","","https://doi.org/10.1145/3310273.3321560;http://dx.doi.org/10.1145/3310273.3321560","10.1145/3310273.3321560","When deep learning meets big code, a key question is how to efficiently learn a distributed representation for source code that can capture its semantics effectively. We propose to use tree-based convolution over API-enhanced AST. To demonstrate the effectiveness of our approach, we apply it to detect semantic clones---code fragments with similar semantics but dissimilar syntax. Experiment results show that our approach outperforms an existing state-of-the-art approach that uses tree-based LSTM, with an increase of 0.39 and 0.12 in F1-score on OJClone and BigCloneBench respectively. We further propose architectures that incorporate our approach for code search and code summarization.","big code, clone detection, code search, code semantics, representation learning, code summarization, semantic clone, tree-based LSTM, tree-based convolution, API, AST","","CF '19"
"Conference Paper","Gu R,Becchi M","A Comparative Study of Parallel Programming Frameworks for Distributed GPU Applications","","2019","","","268–273","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM International Conference on Computing Frontiers","Alghero, Italy","2019","9781450366854","","https://doi.org/10.1145/3310273.3323071;http://dx.doi.org/10.1145/3310273.3323071","10.1145/3310273.3323071","Parallel programming frameworks such as MPI, OpenSHMEM, Charm++ and Legion have been widely used in many scientific domains (from bioinformatics, to computational physics, chemistry, among others) to implement distributed applications. While they have the same purpose, these frameworks differ in terms of programmability, performance, and scalability under different applications and cluster types. Hence, it is important for programmers to select the programming framework that is best suited to the characteristics of their application types (i.e. its computation and communication patterns) and the hardware setup of the target high-performance computing cluster.In this work, we consider several popular parallel programming frameworks for distributed applications. We first analyze their memory model, execution model, synchronization model and GPU support. We then compare their programmability, performance, scalability, and load-balancing capability on homogeneous computing cluster equipped with GPUs.","homogeneous cluster, parallel computing, distributed applications","","CF '19"
"Conference Paper","Qiubo H,Jingdong T,Guozheng F","Research on Code Plagiarism Detection Model Based on Random Forest and Gradient Boosting Decision Tree","","2019","","","97–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 International Conference on Data Mining and Machine Learning","Hong Kong, Hong Kong","2019","9781450360906","","https://doi.org/10.1145/3335656.3335692;http://dx.doi.org/10.1145/3335656.3335692","10.1145/3335656.3335692","This paper studies the Online Judge System for assignments such as programming. Sometimes there are plagiarismsin codes submitted by students[1]. In addition to calculating the similarity degree between the codes, we also extract other features to determine whether there isplagiarismsuspicion of a submitted code or not. By using combination of Random Forest and Gradient Boosting Decision Tree, we also can getitssuspicion level. The model first calculates the similarity degree between the newly submitted code and all submitted codes, and determines plagiarism suspect. For some codes that are difficult to confirm whetherisplagiarismor not, we extract the programming style similarity degree, and the student's submission behavior pattern (such as similar target concentration degree) and other features, to create decision trees such as Random Forestand Gradient Boosting Decision Trees, which can help determine the level of plagiarism suspect. If the level is medium, the teacher will mark the code as plagiarized or not. Finally, the learning model is incrementally trained to improve the accuracy of the model and the classification results. Experiment results show that the accuracy rate can reach 95.9%. As a result, the model can prevent students from plagiarizing while minimizing the workload of the teacher.","Random Forest, Code Similarity Degree, Gradient Boosting Decision Tree, Code Plagiarism Detection","","ICDMML 2019"
"Journal Article","Islam MN,Kundu S","Enabling IC Traceability via Blockchain Pegged to Embedded PUF","ACM Trans. Des. Autom. Electron. Syst.","2019","24","3","","Association for Computing Machinery","New York, NY, USA","","","2019-04","","1084-4309","https://doi.org/10.1145/3315669;http://dx.doi.org/10.1145/3315669","10.1145/3315669","Globalization of IC supply chain has increased the risk of counterfeit, tampered, and re-packaged chips in the market. Counterfeit electronics poses a security risk in safety critical applications like avionics, SCADA systems, and defense. It also affects the reputation of legitimate suppliers and causes financial losses. Hence, it becomes necessary to develop traceability solutions to ensure the integrity of supply chain, from the time of fabrication to the end of product-life, which allows a customer to verify the provenance of a device or a system. In this article, we present an IC traceability solution based on blockchain. A blockchain is a public immutable database that maintains a continuously growing list of data records secured from tampering and revision. Over the lifetime of an IC, all ownership transfer information is recorded and archived in a blockchain. This safe, verifiable method prevents any party from altering or challenging the legitimacy of the information being exchanged. However, a chain of sales record is not enough to ensure provenance of an IC. There is a need for clone-proof method for securely binding the identity of an IC to the blockchain information. In this article, we propose a method of IC supply chain traceability via blockchain pegged to embedded physically unclonable function (PUF). The blockchain provides ownership transfer record, while the PUF provides unique identification for an IC allowing it to be linked uniquely to a blockchain. Our proposed solution automates hardware and software protocols using blockchain-powered Smart Contract that allows supply chain participants to authenticate, track, trace, analyze, and provision chips throughout their entire life cycle.","smart contract, Blockchain, supply chain, ownership transfer, traceability, physically unclonable function","",""
"Conference Paper","Ishizaki K","Analyzing and Optimizing Java Code Generation for Apache Spark Query Plan","","2019","","","91–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering","Mumbai, India","2019","9781450362399","","https://doi.org/10.1145/3297663.3310300;http://dx.doi.org/10.1145/3297663.3310300","10.1145/3297663.3310300","Big data processing frameworks have received attention because of the importance of high performance computation. They are expected to quickly process a huge amount of data in memory with a simple programming model in a cluster. Apache Spark is becoming one of the most popular frameworks. Several studies have analyzed Spark programs and optimized their performance. Recent versions of Spark generate optimized Java code from a Spark program, but few research works have analyzed and improved such generated code to achieve better performance. Here, two types of problems were analyzed by inspecting generated code, namely, access to column-oriented storage and to a primitive-type array. The resulting performance issues in the generated code and were analyzed, and optimizations that can eliminate inefficient code were devised to solve the issues. The proposed optimizations were then implemented for Spark. Experimental results with the optimizations on a cluster of five Intel machines indicated performance improvement by up to 1.4x for TPC-H queries and by up to 1.4x for machine-learning programs. These optimizations have since been integrated into the release version of Apache Spark 2.3.","keywordsapache spark, code generation, optimization","","ICPE '19"
"Journal Article","Zhang L,Yang Z,He Y,Li M,Yang S,Yang M,Zhang Y,Qian Z","App in the Middle: Demystify Application Virtualization in Android and Its Security Threats","Proc. ACM Meas. Anal. Comput. Syst.","2019","3","1","","Association for Computing Machinery","New York, NY, USA","","","2019-03","","","https://doi.org/10.1145/3322205.3311088;http://dx.doi.org/10.1145/3322205.3311088","10.1145/3322205.3311088","Customizability is a key feature of the Android operating system that differentiates it from Apple's iOS. One concrete feature that gaining popularity is called ""app virtualization''. This feature allows multiple copies of the same app to be installed and opened simultaneously (e.g., with multiple accounts logged in). Virtualization frameworks are used by more than 100 million users worldwide. As with any new system features, we are interested in two aspects: (1) whether the feature itself introduces security risks and (2) whether the feature is abused for unintended purposes. This paper conducts a systematic study on the two aspects of the app virtualization techniques.With a thorough study of 32 popular virtualization frameworks from Google Play, we identify seven areas of potential attack vectors and find that most of the frameworks are susceptible to them. By deeply investigating their ecosystem, we show, with demonstrations, that attackers can easily distribute malware that takes advantage of these attack vectors. In addition, we show that the same virtualization techniques are also abused by malware as an alternative and easy-to-use repackaging mechanism. To this end, we design and implement a new app repackage detector. After scanning 250,145 apps from app markets, it finds 164 repackaged apps that attempt to steal user credentials and private data.","android security, application virtualization, access control","",""
"Conference Paper","Ul Ain Q,Azam F,Anwar MW,Kiran A","A Model-Driven Approach for Token Based Code Clone Detection Techniques - An Introduction to UMLCCD","","2019","","","312–317","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 8th International Conference on Educational and Information Technology","Cambridge, United Kingdom","2019","9781450362672","","https://doi.org/10.1145/3318396.3318440;http://dx.doi.org/10.1145/3318396.3318440","10.1145/3318396.3318440","Reusing code through copy and paste is a well-known process in software development. The replicated code is known as code clone and the process is known as code cloning. Software clones can cause bug propagation and severe issues related to maintenance. Code clone detection process consist of many steps, which is complex to some extent for user to perform. Therefore, we are trying to propose a generic approach for code clone detection approaches at model level. This study particularly presents Unified Modeling Language profile for Code Clone Detection (UMLCCD) to model the process of token-based code clone detection approaches as token-based approaches has good execution time while maintaining high precision and recall. It automate the clone detection process of token-based approaches which is helpful for user because user simply gives input and whole process is done automatically. It can be used for source code of various programming languages. The proposed UML profile provides the bases to transform the UMLCCD source model into required text. The applicability of UMLCCD is validated by taking 484 lines of source code of Library Management System case study.","MDA, Token based approaches, Code clone detection, UMLCCD","","ICEIT 2019"
"Conference Paper","Yan L,Hu A,Piech C","Pensieve: Feedback on Coding Process for Novices","","2019","","","253–259","Association for Computing Machinery","New York, NY, USA","Proceedings of the 50th ACM Technical Symposium on Computer Science Education","Minneapolis, MN, USA","2019","9781450358903","","https://doi.org/10.1145/3287324.3287483;http://dx.doi.org/10.1145/3287324.3287483","10.1145/3287324.3287483","In large undergraduate computer science classrooms, student learning on assignments is often gauged only by the work on their final solution, not by their programming process. As a consequence, teachers are unable to give detailed feedback on how students implement programming methodology, and novice students often lack a metacognitive understanding of how they learn. We introduce Pensieve as a drag-and-drop, open-source tool that organizes snapshots of student code as they progress through an assignment. The tool is designed to encourage sit-down conversations between student and teacher about the programming process. The easy visualization of code evolution over time facilitates the discussion of intermediate work and progress towards learning goals, both of which would otherwise be unapparent from a single final submission. This paper discusses the pedagogical foundations and technical details of Pensieve and describes results from a particular 207-student classroom deployment, suggesting that the tool has meaningful impacts on education for both the student and the teacher.","programming courses, pedagogy, assessment, formative feedback, metacognition","","SIGCSE '19"
"Conference Paper","Olsen JK,Fox A","Usage of Hints on Coding-Based Summative Assessments","","2019","","","839–844","Association for Computing Machinery","New York, NY, USA","Proceedings of the 50th ACM Technical Symposium on Computer Science Education","Minneapolis, MN, USA","2019","9781450358903","","https://doi.org/10.1145/3287324.3287472;http://dx.doi.org/10.1145/3287324.3287472","10.1145/3287324.3287472","A recent in-class exam in a software engineering course included a section in which students had to write code and/or tests under conditions they might face on real software engineering projects, including complex, multi-part coding questions in which later parts of the question build on earlier parts. To avoid ""cascading penalties"" on later questions that build on earlier ones, students could reveal the answer for any question during the exam for a ""reveal penalty"": If the student identified the correct place in the code to copy and paste the revealed answer, they would get 20% of the question points. That is, by simply revealing every answer and copy-pasting it correctly, even the weakest student should be able to achieve 20% of the points for the coding portion of the exam. In this paper, we study how students used this mechanism. Surprisingly, some students chose to take zero points for certain questions rather than revealing the answer, and some who asked for an answer to be revealed were unable to correctly incorporate it to get the question correct. We also find a correlation between student scores on the non-coding multiple-choice section of the exam, which had no hints/reveals available, and the coding section, even though students who performed poorly on the non-coding first half of the exam should have been able to score well on the coding part by using the reveal mechanism. That is, weak students did not benefit as much as would be expected from revealable answers. Pedagogical interpretation of these results informs our future use of the ""buy a hint"" format for coding-based exams.","hints, summative assessments","","SIGCSE '19"
"Conference Paper","Zhao Q,Qiu Z,Jin G","Semantics-Aware Scheduling Policies for Synchronization Determinism","","2019","","","242–256","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming","Washington, District of Columbia","2019","9781450362252","","https://doi.org/10.1145/3293883.3295731;http://dx.doi.org/10.1145/3293883.3295731","10.1145/3293883.3295731","A common task for all deterministic multithreading (DMT) systems is to enforce synchronization determinism. However, synchronization determinism has not been the focus of existing DMT research. Instead, most DMT systems focused on how to order data races remained after synchronization determinism is enforced. Consequently, existing scheduling policies for synchronization determinism all have limitations. They may either require performance annotations to achieve good performance or fail to provide schedule stability.In this paper, we argue that synchronization determinism is more fundamental to DMT systems than existing research suggests and propose efficient and effective scheduling policies. Our key insight is that synchronization operations actually encode programmers' intention on how inter-thread communication should be done and can be used as hints while scheduling synchronization operations. Based on this insight, we have built QiThread, a synchronization-determinism system with semantics-aware scheduling policies. Results of a diverse set of 108 programs show that QiThread is able to achieve comparable low overhead as state-of-the-art synchronization-determinism systems without the limitations associated with them.","synchronization determinism, synchronization scheduling, deterministic multithreading, semantics-aware policies, stable multithreading","","PPoPP '19"
"Conference Paper","García S,Strüber D,Brugali D,Di Fava A,Schillinger P,Pelliccione P,Berger T","Variability Modeling of Service Robots: Experiences and Challenges","","2019","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems","Leuven, Belgium","2019","9781450366489","","https://doi.org/10.1145/3302333.3302350;http://dx.doi.org/10.1145/3302333.3302350","10.1145/3302333.3302350","Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.","","","VaMoS '19"
"Conference Paper","Ahadi A,Mathieson L","A Comparison of Three Popular Source Code Similarity Tools for Detecting Student Plagiarism","","2019","","","112–117","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-First Australasian Computing Education Conference","Sydney, NSW, Australia","2019","9781450366229","","https://doi.org/10.1145/3286960.3286974;http://dx.doi.org/10.1145/3286960.3286974","10.1145/3286960.3286974","This paper investigates automated code plagiarism detection in the context of an undergraduate level data structures and algorithms module. We compare three software tools which aim to detect plagiarism in the students' programming source code. We evaluate the performance of these tools on an individual basis and the degree of agreement between them. Based on this evaluation we show that the degree of agreement between these tools is relatively low. We also report the challenges faced during utilization of these methods and suggest possible future improvements for tools of this kind. The discrepancies in the results obtained by these detection techniques were used to devise guidelines for effectively detecting code plagiarism.","Software Similarity Detection, Computer Science Education, Plagiarism, Programming","","ACE '19"
"Journal Article","Domínguez C,Jaime A,Heras J,García-Izquierdo FJ","The Effects of Adding Non-Compulsory Exercises to an Online Learning Tool on Student Performance and Code Copying","ACM Trans. Comput. Educ.","2019","19","3","","Association for Computing Machinery","New York, NY, USA","","","2019-01","","","https://doi.org/10.1145/3264507;http://dx.doi.org/10.1145/3264507","10.1145/3264507","This study analyzes the impact of adding a review exercises module to an online tool used in a software engineering degree program. The objective of the module is to promote students’ self-learning effort to improve their performance. We also intend to determine if this new feature has any effect on the amount of code copies detected in lab sessions when using the same online tool. Two groups of students were compared quantitatively: the first group used the tool exclusively during lab sessions, whereas the second group had the option of employing the tool's new module to enhance their study. The tool allows us to collect interesting data related to the focus of this research: supplementary work completed voluntarily by students and the percentage of students copying others’ code during compulsory lab sessions. The results show that the students in the second group achieved better academic results and copied less in lab sessions. In the second group, the students who invested more effort in doing revision exercises and copied less in lab sessions obtained better results; and, interestingly, the effort invested in completing review exercises did not seem to compensate for the learning effort avoided by copying others’ exercises during lab sessions. The results show the advantages of a tool used with a dual orientation: compulsory and voluntary. Mandatory usage in lab sessions establishes some milestones that, eventually, act as an incentive fostering learning, while voluntary use reinforces students’ perception of the tool's usefulness in terms of learning.","academic performance, non-compulsory exercises, Online learning tool, code copying","",""
"Conference Paper","Min H,Li Ping Z","Survey on Software Clone Detection Research","","2019","","","9–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2019","9781450361897","","https://doi.org/10.1145/3312662.3312707;http://dx.doi.org/10.1145/3312662.3312707","10.1145/3312662.3312707","In order to improve the efficiency of software development, developers often copy-paste code. It is found that the clone code may affect the quality of the software system, especially the maintenance and comprehension of the software, so it is necessary to find and locate it. Many clone detection techniques and tools have been proposed in the search for clone code. How to make better use of these detection techniques and tools will be very important. This paper describes the clone code and general process of clone code detection; introduces different clone code detection methods and related technologies; then conducts a summary analysis, the challenges and development direction faced by clone detection technology.","Clone Management, Clone Type, Clone Code, Clone Detection, Software maintenance","","ICMSS 2019"
"Conference Paper","Fengrong Z,Liping Z,Junqi Z","Research on the Tools of Clone Code Refactoring","","2019","","","27–31","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2019","9781450361897","","https://doi.org/10.1145/3312662.3312693;http://dx.doi.org/10.1145/3312662.3312693","10.1145/3312662.3312693","Clone code is the code fragment that is identical or similar in syntax or semantics, which has great impact on software development and maintenance. According to the large amount of clone code in software and its complex changes, researchers have proposed many methods to eliminate the harmful clone code, in which refactoring is an effective measure. In this paper, concepts of clone code and refactoring are introduced firstly. And then the main methods of current clone code refactoring are compared and analyzed, so the related tools of clone code refactoring can be elaborated and their advantages and disadvantages are summarized. At last, the shortcomings and limitations of the clone code refactoring are discussed.","Clone Code, Refactoring, Refactorability Analysis","","ICMSS 2019"
"Conference Paper","Rongrong S,Liping Z,Fengrong Z","A Method for Identifying and Recommending Reconstructed Clones","","2019","","","39–44","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2019","9781450361897","","https://doi.org/10.1145/3312662.3312709;http://dx.doi.org/10.1145/3312662.3312709","10.1145/3312662.3312709","Reconstruction of existing clone code is limited to a single version of static analysis and ignores the evolution of cloned code, resulting in a lack of effective methods for cloning code refactoring decisions. Therefore, this paper proposes a clone code that needs to be reconstructed and tracked from the perspective of software evolution, and recommends cloned code that needs to be reconstructed. Firstly, the evolution history information closely related to the cloned code is extracted from the clone detection, clone mapping, clone family and software maintenance log management system. Secondly, the clone code that needs to be reconstructed is identified, and the cloned code of the trace is identified, and then extracted and weighted. Construct related static and evolution features and build a feature sample database. Finally, a variety of machine learning methods are used to compare and select the best classifiers to recommend refactoring clones. This article conducts experiments on nearly 170 versions of 7 software, and recommends that the accuracy of refactoring cloned code reaches over 90%. Provide more accurate and reasonable code refactoring advice for software development and maintenance personnel.","clone family, clone tracking, Clone code, clone reconstruction, feature extraction","","ICMSS 2019"
"Conference Paper","Yang B,Liping Z,Fengrong Z","A Survey on Research of Code Comment","","2019","","","45–51","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2019 3rd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2019","9781450361897","","https://doi.org/10.1145/3312662.3312710;http://dx.doi.org/10.1145/3312662.3312710","10.1145/3312662.3312710","Code comments are one of the effective means for assisting programmers to understand the source code. High-quality code comments play an important role in areas such as software maintenance and software reuse. Good code comments can help programmers understand the role of source code and facilitate comprehension of programs and software maintenance tasks quickly. But in reality, most programmers only pay attention to the code and ignore comments and documents, which greatly reduce the program's readability and maintainability. This paper has compiled the relevant research on code comments so far, mainly including four aspects: automatic generation of code comments, consistency of code comments, classification of code comments, and quality evaluation of code comments. By analyzing relevant methods in this research field, it provides more complete information for future research.","Quality evaluation, Automatic generation of code comments, Code Comment, Consistency changes, Code comment classification, Software maintenance","","ICMSS 2019"
"Conference Paper","Wang Y,Jin D,Gong Y","A Diversified Feature Extraction Approach for Program Similarity Analysis","","2019","","","96–101","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Conference on Software Engineering and Information Management","Bali, Indonesia","2019","9781450366427","","https://doi.org/10.1145/3305160.3305189;http://dx.doi.org/10.1145/3305160.3305189","10.1145/3305160.3305189","As code plagiarism becomes more and more prevalent, the need for code similarity detection technology is growing greatly. The feature of program is the basic unit that can represent the procedure and structure. Therefore, the quality of the feature will directly impact the accuracy of the similarity detection results. In this paper, we propose a diversified feature extraction approach, which extracts feature information from attribute counting, statement structure, program structure and program function. In the process of feature extraction, we comprehensively consider multiple factors of program, such as program structure, semantics and data flow. Evaluation results shows that this approach can eliminate the interference caused by multiple plagiarism methods, and it also has certain improvement in accuracy and detection efficiency.","Similarity detection, feature extraction, code plagiarism","","ICSIM 2019"
"Conference Paper","Hanakawa N,Obana M","A Computer System Quality Metric for Infrastructure with Configuration Files' Changes","","2019","","","39–43","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Conference on Software Engineering and Information Management","Bali, Indonesia","2019","9781450366427","","https://doi.org/10.1145/3305160.3305168;http://dx.doi.org/10.1145/3305160.3305168","10.1145/3305160.3305168","We propose a system quality metric for computer infrastructure development. The feature of the metric is the usage of configuration files. The growths and changes of configuration files present characteristics of infrastructure development. The proposed metric consists of 4 element metrics and a total metric. The proposed metric was applied to a real large-scale project. The metric helps us understand how the configuration files were developed. There are two patterns, one is growing little by little, and another is suddenly large growth and suddenly decreasing. For evaluation of the metric, we compared the metric values with system faults that occurred in the real running. As a result, the two patterns of configuration files' development are not related to the system faults. However, the one element metric Scale is related to the system faults. Therefore, the Scale element metric may present system quality in the target project.","firewall, network equipment, infrastructure, detecting faults, software, Configuration files","","ICSIM 2019"
"Journal Article","Pomonis M,Petsios T,Keromytis AD,Polychronakis M,Kemerlis VP","Kernel Protection Against Just-In-Time Code Reuse","ACM Trans. Priv. Secur.","2019","22","1","","Association for Computing Machinery","New York, NY, USA","","","2019-01","","2471-2566","https://doi.org/10.1145/3277592;http://dx.doi.org/10.1145/3277592","10.1145/3277592","The abundance of memory corruption and disclosure vulnerabilities in kernel code necessitates the deployment of hardening techniques to prevent privilege escalation attacks. As stricter memory isolation mechanisms between the kernel and user space become commonplace, attackers increasingly rely on code reuse techniques to exploit kernel vulnerabilities. Contrary to similar attacks in more restrictive settings, as in web browsers, in kernel exploitation, non-privileged local adversaries have great flexibility in abusing memory disclosure vulnerabilities to dynamically discover, or infer, the location of code snippets in order to construct code-reuse payloads. Recent studies have shown that the coupling of code diversification with the enforcement of a “read XOR execute” (R∧X) memory safety policy is an effective defense against the exploitation of userland software, but so far this approach has not been applied for the protection of the kernel itself.In this article, we fill this gap by presenting kR∧X: a kernel-hardening scheme based on execute-only memory and code diversification. We study a previously unexplored point in the design space, where a hypervisor or a super-privileged component is not required. Implemented mostly as a set of GCC plugins, kR∧X is readily applicable to x86 Linux kernels (both 32b and 64b) and can benefit from hardware support (segmentation on x86, MPX on x86-64) to optimize performance. In full protection mode, kR∧X incurs a low runtime overhead of 4.04%, which drops to 2.32% when MPX is available, and 1.32% when memory segmentation is in use.","Execute-only memory, code diversification","",""
"Journal Article","Alon U,Zilberstein M,Levy O,Yahav E","Code2vec: Learning Distributed Representations of Code","Proc. ACM Program. Lang.","2019","3","POPL","","Association for Computing Machinery","New York, NY, USA","","","2019-01","","","https://doi.org/10.1145/3290353;http://dx.doi.org/10.1145/3290353","10.1145/3290353","We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.","Machine Learning, Distributed Representations, Big Code","",""
"Journal Article","Badenhop CW,Graham SR,Mullins BE,Mailloux LO","Looking Under the Hood of Z-Wave: Volatile Memory Introspection for the ZW0301 Transceiver","ACM Trans. Cyber-Phys. Syst.","2018","3","2","","Association for Computing Machinery","New York, NY, USA","","","2018-12","","2378-962X","https://doi.org/10.1145/3285030;http://dx.doi.org/10.1145/3285030","10.1145/3285030","Z-Wave is a proprietary Internet of Things substrate providing distributed home and office automation services. The proprietary nature of Z-Wave devices makes it difficult to determine their security aptitude. While there are a variety of open source tools for analyzing Z-Wave frames, inspecting non-volatile memory, and disassembling firmware, there are no dynamic analysis tools allowing one to inspect the internal state of a Z-Wave transceiver while it is running. In this work, a memory introspection capability is developed for three Z-Wave devices containing a ZW0301, a Z-Wave transceiver system-on-chip. In all three devices, the firmware image is modified to include the memory introspection capability by hooking an existing data exfiltration mechanism used by the device. The memory introspection capability is applied to determine how nonces are generated by Z-Wave devices to prevent replay attacks. Through a combination of static and dynamic analysis, the nonce generating algorithm is found to be based on a nonce round key that updates every secure frame transaction.","firmware modification, Z-Wave, Internet of Things, volatile memory introspection","",""
"Conference Paper","Pooput P,Muenchaisri P","Finding Impact Factors for Rejection of Pull Requests on GitHub","","2018","","","70–76","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 VII International Conference on Network, Communication and Computing","Taipei City, Taiwan","2018","9781450365536","","https://doi.org/10.1145/3301326.3301380;http://dx.doi.org/10.1145/3301326.3301380","10.1145/3301326.3301380","A pull request is an important method for code contributions in GitHub that will be submitted when the developers would like to merge their code changes from their local machine to the main repository on which all source code in the project are stored. Before merging the code changes into the main repository, the developers have to request for a permission. If their source code is allowed to merge, the pull request status is accepted. On the other hand, if their source code is not allowed to merge, the pull request status is rejected. The pull request status may be rejected due to several factors, such as code complexity, code quality, the number of changed files, etc. Fixing the rejected pull requests will take some extra effort and time which may affect the project cost and timeline. This paper aims at finding the impact factors that are associated with the rejection of pull requests on GitHub and also discovering the relationships among impact factors by using the association rules in data mining.","Pull Request, GitHub, Association rules, Data mining, Ansible","","ICNCC 2018"
"Conference Paper","Zhang S,Ning Z,Cao Y","Effective Clone Detection Model Based on Compressed Sensing in Clustered WSNs","","2018","","","468–473","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence","Shenzhen, China","2018","9781450366069","","https://doi.org/10.1145/3297156.3297226;http://dx.doi.org/10.1145/3297156.3297226","10.1145/3297156.3297226","Wireless sensor networks deployed in open environments are vulnerable to clone attacks. For this clone attack, a clone detection method based on compressed sensing in a cluster network is proposed. It combines the local and global detection of nodes. The cluster head node detects the nodes in the cluster and sets the monitoring nodes to the cluster head nodes. The convergence node uses the sparse characteristics of the cloned nodes to collect the identity information of the nodes in the network. Determine the identity of the cloned node. The experimental results show that the detection rate and communication cost are superior to other methods, and can further enhance the security and life of the network.","Compressing Sensing, Wireless Sensor Networks, Clone Detection, Clustered Networks","","CSAI '18"
"Conference Paper","Yang C,Liu Y,Yu J","Exploring Violations of Programming Styles: Insights from Open Source Projects","","2018","","","185–189","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence","Shenzhen, China","2018","9781450366069","","https://doi.org/10.1145/3297156.3297227;http://dx.doi.org/10.1145/3297156.3297227","10.1145/3297156.3297227","Software project is usually a huge cooperative teamwork, programmers in the project usually have to read the code written by others and understand its implementation. A uniform and clean programming style could ensure the readability and maintainability of the project source code, especially when it becomes a legacy project. However, each programmer has his own programming habit and because of the heavy developing tasks, the programming style of the software project is far from satisfactory. Programming style does not resemble software defects which has a serious effect on program executing. Therefore, many programmers ignore the programming style directly instead of improving it. Programming style should be checked before new features are merged into software projects, just like software testing. Developing with the size of software project, some special programming style rules are violated more seriously, which need be highly focused. Furthermore, one of ultimate targets in software quality engineering is to check the programming style automatically with analysis tools because the software projects usually have an enormous quantity of source code. In this paper, static source code analysis is used for detecting the programming style problems. The source file directly or the class files generated by the compiler are scanned then the abstract syntax tree for the source code is generated. With the help of abstract syntax tree, it is possible to detect code snippets that violate the programming style rules by traverse the tree. Our method employs the static code analysis tools to analyze several Java open source projects, and find that the programming style problems which are violated most. According to our method, each problem is also explained from personal habits, JDK version, and other aspects later. Considering all of the analysis results, a special ruleset that is recommended to pay more attention to in the future software developing is proposed. At last, programming style should be highly valued in software development processes in further project management.","Code review, Programming style, Static source code analysis","","CSAI '18"
"Conference Paper","Mishra S,Polychronakis M","Shredder: Breaking Exploits through API Specialization","","2018","","","1–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 34th Annual Computer Security Applications Conference","San Juan, PR, USA","2018","9781450365697","","https://doi.org/10.1145/3274694.3274703;http://dx.doi.org/10.1145/3274694.3274703","10.1145/3274694.3274703","Code reuse attacks have been a threat to software security since the introduction of non-executable memory protections. Despite significant advances in various types of additional defenses, such as control flow integrity (CFI) and leakage-resilient code randomization, recent code reuse attacks have demonstrated that these defenses are often not enough to prevent successful exploitation. Sophisticated exploits can reuse code comprising larger code fragments that conform to the enforced CFI policy and which are not affected by randomization.As a step towards improving our defenses against code reuse attacks, in this paper we present Shredder, a defense-in-depth exploit mitigation tool for the protection of closed-source applications. In a preprocessing phase, Shredder statically analyzes a given application to pinpoint the call sites of potentially useful (to attackers) system API functions, and uses backwards data flow analysis to derive their expected argument values and generate whitelisting policies in a best-effort way. At runtime, using library interposition, Shredder exposes to the protected application only specialized versions of these critical API functions, and blocks any invocation that violates the enforced policy. We have experimentally evaluated our prototype implementation for Windows programs using a large set of 251 shellcode and 30 code reuse samples, and show that it improves significantly upon code stripping, a state-of-the-art code surface reduction technique, by blocking a larger number of malicious payloads with negligible runtime overhead.","","","ACSAC '18"
"Conference Paper","Tofighi-Shirazi R,Christofi M,Elbaz-Vincent P,Le TH","DoSE: Deobfuscation Based on Semantic Equivalence","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Software Security, Protection, and Reverse Engineering Workshop","San Juan, PR, USA","2018","9781450360968","","https://doi.org/10.1145/3289239.3289243;http://dx.doi.org/10.1145/3289239.3289243","10.1145/3289239.3289243","Software deobfuscation is a key challenge in malware analysis to understand the internal logic of the code and establish adequate countermeasures. In order to defeat recent obfuscation techniques, state-of-the-art generic deobfuscation methodologies are based on dynamic symbolic execution (DSE). However, DSE suffers from limitations such as code coverage and scalability. In the race to counter and remove the most advanced obfuscation techniques, there is a need to reduce the amount of code to cover. To that extend, we propose a novel deobfuscation approach based on semantic equivalence, called DoSE. With DoSE, we aim to improve and complement DSE-based deobfuscation techniques by statically eliminating obfuscation transformations (built on code-reuse). This improves the code coverage. Our method's novelty comes from the transposition of existing binary diffing techniques, namely semantic equivalence checking, to the purpose of the deobfuscation of untreated techniques, such as two-way opaque constructs, that we encounter in surreptitious software. In order to challenge DoSE, we used both known malwares such as Cryptowall, WannaCry, Flame and BitCoinMiner and obfuscated code samples. Our experimental results show that DoSE is an efficient strategy of detecting obfuscation transformations based on code-reuse with low rates of false positive and/or false negative results in practice, and up to 63% of code reduction on certain types of malwares.","malware analysis, reverse engineering, code cloning, opaque predicate, Obfuscation, deobfuscation, symbolic execution, control-flow graph","","SSPREW-8"
"Conference Paper","Tu H,Nair V","Is One Hyperparameter Optimizer Enough?","","2018","","","19–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics","Lake Buena Vista, FL, USA","2018","9781450360562","","https://doi.org/10.1145/3278142.3278145;http://dx.doi.org/10.1145/3278142.3278145","10.1145/3278142.3278145","Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics.To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be “best” and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations. We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be applied to a new dataset.","SBSE, Defect Prediction, Hyperparameter Tuning","","SWAN 2018"
"Conference Paper","Jiang Z,Dai S,Suh GE,Zhang Z","High-Level Synthesis with Timing-Sensitive Information Flow Enforcement","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Computer-Aided Design","San Diego, California","2018","9781450359504","","https://doi.org/10.1145/3240765.3243415;http://dx.doi.org/10.1145/3240765.3243415","10.1145/3240765.3243415","Specialized hardware accelerators are being increasingly integrated into today's computer systems to achieve improved performance and energy efficiency. However, the resulting variety and complexity make it challenging to ensure the security of these accelerators. To mitigate complexity while guaranteeing security, we propose a high-level synthesis (HLS) infrastructure that incorporates static information flow analysis to enforce security policies on HLS-generated hardware accelerators. Our security-constrained HLS infrastructure is able to effectively identify both explicit and implicit information leakage. By detecting the security vulnerabilities at the behavioral level, our tool allows designers to address these vulnerabilities at an early stage of the design flow. We further propose a novel synthesis technique in HLS to eliminate timing channels in the generated accelerator. Our approach is able to remove timing channels in a verifiable manner while incurring lower performance overhead for high-security tasks on the accelerator.","","","ICCAD '18"
"Journal Article","Pascarella L,Spadini D,Palomba F,Bruntink M,Bacchelli A","Information Needs in Contemporary Code Review","Proc.  ACM Hum. -Comput.  Interact.","2018","2","CSCW","","Association for Computing Machinery","New York, NY, USA","","","2018-11","","","https://doi.org/10.1145/3274404;http://dx.doi.org/10.1145/3274404","10.1145/3274404","Contemporary code review is a widespread practice used by software engineers to maintain high software quality and share project knowledge. However, conducting proper code review takes time and developers often have limited time for review. In this paper, we aim at investigating the information that reviewers need to conduct a proper code review, to better understand this process and how research and tool support can make developers become more effective and efficient reviewers. Previous work has provided evidence that a successful code review process is one in which reviewers and authors actively participate and collaborate. In these cases, the threads of discussions that are saved by code review tools are a precious source of information that can be later exploited for research and practice. In this paper, we focus on this source of information as a way to gather reliable data on the aforementioned reviewers' needs. We manually analyze 900 code review comments from three large open-source projects and organize them in categories by means of a card sort. Our results highlight the presence of seven high-level information needs, such as knowing the uses of methods and variables declared/modified in the code under review. Based on these results we suggest ways in which future code review tools can better support collaboration and the reviewing task.","code review, information needs, mining software repositories","",""
"Journal Article","Guzdial M","What We Care about Now, What We'll Care about in the Future","ACM Inroads","2018","9","4","63–64","Association for Computing Machinery","New York, NY, USA","","","2018-11","","2153-2184","https://doi.org/10.1145/3276304;http://dx.doi.org/10.1145/3276304","10.1145/3276304","","","",""
"Conference Paper","Wang H,Liu Z,Liang J,Vallina-Rodriguez N,Guo Y,Li L,Tapiador J,Cao J,Xu G","Beyond Google Play: A Large-Scale Comparative Study of Chinese Android App Markets","","2018","","","293–307","Association for Computing Machinery","New York, NY, USA","Proceedings of the Internet Measurement Conference 2018","Boston, MA, USA","2018","9781450356190","","https://doi.org/10.1145/3278532.3278558;http://dx.doi.org/10.1145/3278532.3278558","10.1145/3278532.3278558","China is one of the largest Android markets in the world. As Chinese users cannot access Google Play to buy and install Android apps, a number of independent app stores have emerged and compete in the Chinese app market. Some of the Chinese app stores are pre-installed vendor-specific app markets (e.g., Huawei, Xiaomi and OPPO), whereas others are maintained by large tech companies (e.g., Baidu, Qihoo 360 and Tencent). The nature of these app stores and the content available through them vary greatly, including their trustworthiness and security guarantees.As of today, the research community has not studied the Chinese Android ecosystem in depth. To fill this gap, we present the first large-scale comparative study that covers more than 6 million Android apps downloaded from 16 Chinese app markets and Google Play. We focus our study on catalog similarity across app stores, their features, publishing dynamics, and the prevalence of various forms of misbehavior (including the presence of fake, cloned and malicious apps). Our findings also suggest heterogeneous developer behavior across app stores, in terms of code maintenance, use of third-party services, and so forth. Overall, Chinese app markets perform substantially worse when taking active measures to protect mobile users and legit developers from deceptive and abusive actors, showing a significantly higher prevalence of malware, fake, and cloned apps than Google Play.","malware, cloned app, App ecosystem, Google Play, Android market, third-party library, permission","","IMC '18"
"Conference Paper","Baddreddin O,Rahad K","The Impact of Design and UML Modeling on Codebase Quality and Sustainability","","2018","","","236–244","IBM Corp.","USA","Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering","Markham, Ontario, Canada","2018","","","","","The general consensus of researchers and practitioners is that up-front and continuous software design using modeling languages such as UML improve code quality and reliability particularly as the software evolves over time. Software designs and models help in managing the underlying code complexities which are crucial for sustainability. Recently, there has been increasing evidence suggesting broader adoption of modeling languages such as UML. However, our understanding of the impact of using such modeling and design languages remains limited. This paper reports on a study that aims to characterize this impact on code quality and sustainability. We identify a sample of open source software repositories with extensive use of designs and modeling and compare their code qualities with similar code-centric repositories. Our evaluation focuses on various code quality attributes such as code smells and technical debt. We also conduct code evolution analysis over five-year period and collect additional data from questionnaires and interviews with active repository contributors. This study finds that repositories with significant use of models and design activities are associated with reduced critical code smells but are also associated with increase in non-critical code smells. The study also finds that modeling and design activities are associated with significant reduction in measures of technical debt. Analyzing code evolution over five year period reveals that UML repositories start with significantly lower technical debt density measures but tend to decline over time.","UML, code smells, code-centric development, open source repositories, model driven software development, software design, software maintenance, sustainability, technical debt, empirical investigation","","CASCON '18"
"Conference Paper","Saini V,Farmahinifarahani F,Lu Y,Baldi P,Lopes CV","Oreo: Detection of Clones in the Twilight Zone","","2018","","","354–365","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Lake Buena Vista, FL, USA","2018","9781450355735","","https://doi.org/10.1145/3236024.3236026;http://dx.doi.org/10.1145/3236024.3236026","10.1145/3236024.3236026","Source code clones are categorized into four types of increasing difficulty of detection, ranging from purely textual (Type-1) to purely semantic (Type-4). Most clone detectors reported in the literature work well up to Type-3, which accounts for syntactic differences. In between Type-3 and Type-4, however, there lies a spectrum of clones that, although still exhibiting some syntactic similarities, are extremely hard to detect – the Twilight Zone. Most clone detectors reported in the literature fail to operate in this zone. We present Oreo, a novel approach to source code clone detection that not only detects Type-1 to Type-3 clones accurately, but is also capable of detecting harder-to-detect clones in the Twilight Zone. Oreo is built using a combination of machine learning, information retrieval, and software metrics. We evaluate the recall of Oreo on BigCloneBench, and perform manual evaluation for precision. Oreo has both high recall and precision. More importantly, it pushes the boundary in detection of clones with moderate to weak syntactic similarity in a scalable manner","Clone detection, Software Metrics, Machine Learning","","ESEC/FSE 2018"
"Conference Paper","Zhao G,Huang J","DeepSim: Deep Learning Code Functional Similarity","","2018","","","141–151","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Lake Buena Vista, FL, USA","2018","9781450355735","","https://doi.org/10.1145/3236024.3236068;http://dx.doi.org/10.1145/3236024.3236068","10.1145/3236024.3236068","Measuring code similarity is fundamental for many software engineering tasks, e.g., code search, refactoring and reuse. However, most existing techniques focus on code syntactical similarity only, while measuring code functional similarity remains a challenging problem. In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector, and we design a new deep learning model that measures code functional similarity based on this representation. By concatenating hidden representations learned from a code pair, this new model transforms the problem of detecting functionally similar code to binary classification, which can effectively learn patterns between functionally similar code with very different syntactics. We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models.","Classification, Control/Data flow, Code functional similarity, Deep Learning","","ESEC/FSE 2018"
"Conference Paper","Gao J,Yang X,Fu Y,Jiang Y,Shi H,Sun J","VulSeeker-pro: Enhanced Semantic Learning Based Binary Vulnerability Seeker with Emulation","","2018","","","803–808","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Lake Buena Vista, FL, USA","2018","9781450355735","","https://doi.org/10.1145/3236024.3275524;http://dx.doi.org/10.1145/3236024.3275524","10.1145/3236024.3275524","Learning-based clone detection is widely exploited for binary vulnerability search. Although they solve the problem of high time overhead of traditional dynamic and static search approaches to some extent, their accuracy is limited, and need to manually identify the true positive cases among the top-M search results during the industrial practice. This paper presents VulSeeker-Pro, an enhanced binary vulnerability seeker that integrates function semantic emulation at the back end of semantic learning, to release the engineers from the manual identification work. It first uses the semantic learning based predictor to quickly predict the top-M candidate functions which are the most similar to the vulnerability from the target binary. Then the top-M candidates are fed to the emulation engine to resort, and more accurate top-N candidate functions are obtained. With fast filtering of semantic learning and dynamic trace generation of function semantic emulation, VulSeeker-Pro can achieve higher search accuracy with little time overhead. The experimental results on 15 known CVE vulnerabilities involving 6 industry widely used programs show that VulSeeker-Pro significantly outperforms the state-of-the-art approaches in terms of accuracy. In a total of 45 searches, VulSeeker-Pro finds 40 and 43 real vulnerabilities in the top-1 and top-5 candidate functions, which are 12.33× and 2.58× more than the most recent and related work Gemini. In terms of efficiency, it takes 0.22 seconds on average to determine whether the target binary function contains a known vulnerability or not.","semantic learning, vulnerability search, function emulation","","ESEC/FSE 2018"
"Conference Paper","Sherman E,Dyer R","Software Engineering Collaboratories (SEClabs) and Collaboratories as a Service (CaaS)","","2018","","","760–764","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Lake Buena Vista, FL, USA","2018","9781450355735","","https://doi.org/10.1145/3236024.3264839;http://dx.doi.org/10.1145/3236024.3264839","10.1145/3236024.3264839","Novel research ideas require strong evaluations. Modern software engineering research evaluation typically requires a set of benchmark programs. Open source software repositories have provided a great opportunity for researchers to find such programs for use in their evaluations. Many tools/techniques have been developed to help automate the curation of open source software. There has also been encouragement for researchers to provide their research artifacts so that other researchers can easily reproduce the results. We argue that these two trends (i.e., curating open source software for research evaluation and the providing of research artifacts) drive the need for Software Engineer Collaboratories (SEClabs). We envision research communities coming together to create SEClab instances, where research artifacts can be made publicly available to other researchers. The community can then vet such artifacts and make them available as a service, thus turning the collaboratory into a Collaboratory as a Service (CaaS). If our vision is realized, the speed and transparency of research will drastically increase.","research as a service, collaboratory, software as a service","","ESEC/FSE 2018"
"Conference Paper","Hu G,Zhu L,Yang J","AppFlow: Using Machine Learning to Synthesize Robust, Reusable UI Tests","","2018","","","269–282","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Lake Buena Vista, FL, USA","2018","9781450355735","","https://doi.org/10.1145/3236024.3236055;http://dx.doi.org/10.1145/3236024.3236055","10.1145/3236024.3236055","UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.","UI recognition, test reuse, mobile testing, machine learning, UI testing, test synthesis","","ESEC/FSE 2018"
"Journal Article","Dietz W,Adve V","Software Multiplexing: Share Your Libraries and Statically Link Them Too","Proc. ACM Program. Lang.","2018","2","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2018-10","","","https://doi.org/10.1145/3276524;http://dx.doi.org/10.1145/3276524","10.1145/3276524","We describe a compiler strategy we call “Software Multiplexing” that achieves many benefits of both statically linked and dynamically linked libraries, and adds some additional advantages. Specifically, it achieves the code size benefits of dynamically linked libraries while eliminating the major disadvantages: unexpected failures due to missing dependences, slow startup times, reduced execution performance due to indirect references to globals, and the potential for security vulnerabilities. We design Software Multiplexing so that it works even in the common case where application build systems support only dynamic and not static linking; we have automatically built thousands of Linux software packages in this way. Software Multiplexing combines two ideas: Automatic Multicall, i.e., where multiple independent programs are automatically merged into a single executable, and Static Linking of Shared Libraries, which works by linking an IR-level version of application code and all its libraries, even if the libraries are normally compiled as shared, before native code generation. The benefits are achieved primarily through deduplication of libraries across the multiplexed programs, while using static linking, and secondly through more effective unused code elimination for statically linked shared libraries. Compared with equivalent dynamically linked programs, allmux-optimized programs start more quickly and even have slightly lower memory usage and total disk size. Compared with equivalent statically linked programs, allmux-optimized programs are much smaller in both aggregate size and memory usage, and have similar startup times and execution performance. We have implemented Software Multiplexing in a tool called allmux, part of the open-source ALLVM project. Example results show that when the LLVM Compiler Infrastructure is optimized using allmux, the resulting binaries and libraries are 18.3% smaller and 30% faster than the default production version. For 74 other packages containing 2–166 programs each, multiplexing each package into one static binary reduces the aggregate package size by 39% (geometric mean) compared with dynamic linking.","IR, LLVM, Link-Time Optimization, LTO, Code deduplication","",""
"Journal Article","Vedurada J,Nandivada VK","Identifying Refactoring Opportunities for Replacing Type Code with Subclass and State","Proc. ACM Program. Lang.","2018","2","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2018-10","","","https://doi.org/10.1145/3276508;http://dx.doi.org/10.1145/3276508","10.1145/3276508","Refactoring is a program transformation that restructures existing code without altering its behaviour and is a key practice in popular software design movements, such as Agile. Identification of potential refactoring opportunities is an important step in the refactoring process. In large systems, manual identification of useful refactoring opportunities requires a lot of effort and time. Hence, there is a need for automatic identification of refactoring opportunities. However, this problem has not been addressed well for many non-trivial refactorings. Two such non-trivial, yet popular refactorings are “Replace Type Code with Subclass” (SC) and “Replace Type Code with State” (ST) refactorings. In this paper, we present new approaches to identify SC and ST refactoring opportunities. Our proposed approach is based around the notion of control-fields. A control-field is a field of a class that exposes the different underlying behaviors of the class. Each control-field can lead to a possible SC/ST refactoring of the associated/interacting classes. We first present a formal definition of control-fields and then present algorithms to identify and prune them; each of these pruned control-fields represents a refactoring opportunity. Further, we present a novel flow- and context-sensitive analysis to classify each of these refactoring opportunities into one of the SC and ST opportunities. We have implemented our proposed approach in a tool called Auto-SCST, and demonstrated its effectiveness by evaluating it against eight open-source Java applications.","Replace Type Code with State, Static Program Analysis, Replace Type Code with Subclass, Refactoring, Replace Conditionals with Polymorphism, Points-to Analysis","",""
"Conference Paper","Sullivan MB,Hari SK,Zimmer B,Tsai T,Keckler SW","SwapCodes: Error Codes for Hardware-Software Cooperative GPU Pipeline Error Detection","","2018","","","762–774","IEEE Press","Fukuoka, Japan","Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture","","2018","9781538662403","","https://doi.org/10.1109/MICRO.2018.00067;http://dx.doi.org/10.1109/MICRO.2018.00067","10.1109/MICRO.2018.00067","Intra-thread instruction duplication offers straightforward and effective pipeline error detection for data-intensive processors. However, software-enforced instruction duplication uses explicit checking instructions, roughly doubles program register usage, and doubles the arithmetic operation count per thread, potentially leading to severe slowdowns. This paper investigates SwapCodes, a family of software-hardware cooperative mechanisms to accelerate intra-thread duplication in GPUs. SwapCodes leverages the register file ECC hardware to detect pipeline errors without sacrificing the ability of ECC to detect and correct storage errors. By implicitly checking for pipeline errors on each register read, SwapCodes avoids the overheads of instruction checking without adding new hardware error checkers or buffers. We describe a family of SwapCodes implementations that successively eliminate the sources of inefficiency in intra-thread duplication with different complexities and error detection and correction trade-offs. We apply SwapCodes to protect a GPU-based processor against pipeline errors, and demonstrate that it is able to detect more than 99.3% of pipeline errors while improving performance and system efficiency relative to software-enforced duplication---the most performant SwapCodes organization incurs just 15% average slowdown over the un-duplicated program.","","","MICRO-51"
"Conference Paper","Santos EJ,Maciel RS,Sant'Anna C","A Catalogue of Bad Smells for Software Process","","2018","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the XVII Brazilian Symposium on Software Quality","Curitiba, Brazil","2018","9781450365659","","https://doi.org/10.1145/3275245.3275264;http://dx.doi.org/10.1145/3275245.3275264","10.1145/3275245.3275264","Software processes play an important role in the software industry, as they influence the quality of the product and determine the efficiency of the company that develops these software products. To be used systematically in different projects, software processes need to be disseminated in the organization and continuously evaluated when one wants to understand their quality. The evaluation of a software process maintains and promotes its quality and evolution. However, if these evaluations happen from data directly collected from a process that has been applied to a given development project, process quality problems have already influenced the outcome of the process and possibly the software product. Software process models, commonly specified in a process modeling language (PML), specify in a standardized way the elements of a process and the appropriate interactions between them. In addition to assigning to the understanding, communication and execution of a software process in a company, process models offer an opportunity for them to be evaluated before their first execution or even to help identify problems in the process of ongoing projects. This paper presents a proposal to use the concept of bad smells in software process models with the objective of identifying possible disharmonies in the models. Initially bad smells of object-oriented code were analyzed and adapted to SPEM (Software & Systems Process Engineering Meta-Model) to generate a catalog. Subsequently a survey was carried out to validate the definitions, representations and possible impacts of the proposed bad smells, resulting in a validation that presented an overall rate of 86% agreement. It is expected that being possible to characterize bad smells for software processes, to enable their applicability in real software development process.","Processo de software, bad smells, process smells, atributos de qualidade, avaliação de processo","","SBQS '18"
"Conference Paper","Das A,Acar G,Borisov N,Pradeep A","The Web's Sixth Sense: A Study of Scripts Accessing Smartphone Sensors","","2018","","","1515–1532","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security","Toronto, Canada","2018","9781450356930","","https://doi.org/10.1145/3243734.3243860;http://dx.doi.org/10.1145/3243734.3243860","10.1145/3243734.3243860","We present the first large-scale measurement of smartphone sensor API usage and stateless tracking on the mobile web. We extend the OpenWPM web privacy measurement tool to develop OpenWPM-Mobile, adding the ability to emulate plausible sensor values for different smartphone sensors such as motion, orientation, proximity and light. Using OpenWPM-Mobile we find that one or more sensor APIs are accessed on 3695 of the top 100K websites by scripts originating from 603 distinct domains. We also detect fingerprinting attempts on mobile platforms, using techniques previously applied in the desktop setting. We find significant overlap between fingerprinting scripts and scripts accessing sensor data. For example, 63% of the scripts that access motion sensors also engage in browser fingerprinting. To better understand the real-world uses of sensor APIs, we cluster JavaScript programs that access device sensors and then perform automated code comparison and manual analysis. We find a significant disparity between the actual and intended use cases of device sensor as drafted by W3C. While some scripts access sensor data to enhance user experience, such as orientation detection and gesture recognition, tracking and analytics are the most common use cases among the scripts we analyzed. We automated the detection of sensor data exfiltration and observed that the raw readings are frequently sent to remote servers for further analysis. Finally, we evaluate available countermeasures against the misuse of sensor APIs. We find that popular tracking protection lists such as EasyList and Disconnect commonly fail to block most tracking scripts that misuse sensors. Studying nine popular mobile browsers we find that even privacy-focused browsers, such as Brave and Firefox Focus, fail to implement mitigations suggested by W3C, which includes limiting sensor access from insecure contexts and cross-origin iframes. We have reported these issues to the browser vendors.","mobile browser, on-line tracking, sensors, fingerprinting","","CCS '18"
"Conference Paper","He J,Ivanov P,Tsankov P,Raychev V,Vechev M","Debin: Predicting Debug Information in Stripped Binaries","","2018","","","1667–1680","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security","Toronto, Canada","2018","9781450356930","","https://doi.org/10.1145/3243734.3243866;http://dx.doi.org/10.1145/3243734.3243866","10.1145/3243734.3243866","We present a novel approach for predicting debug information in stripped binaries. Using machine learning, we first train probabilistic models on thousands of non-stripped binaries and then use these models to predict properties of meaningful elements in unseen stripped binaries. Our focus is on recovering symbol names, types and locations, which are critical source-level information wiped off during compilation and stripping. Our learning approach is able to distinguish and extract key elements such as register-allocated and memory-allocated variables usually not evident in the stripped binary. To predict names and types of extracted elements, we use scalable structured prediction algorithms in probabilistic graphical models with an extensive set of features which capture key characteristics of binary code. Based on this approach, we implemented an automated tool, called Debin, which handles ELF binaries on three of the most popular architectures: x86, x64 and ARM. Given a stripped binary, Debin outputs a binary augmented with the predicted debug information. Our experimental results indicate that Debin is practically useful: for x64, it predicts symbol names and types with 68.8% precision and 68.3% recall. We also show that Debin is helpful for the task of inspecting real-world malware -- it revealed suspicious library usage and behaviors such as DNS resolver reader.","binary code, debug information, machine learning, security","","CCS '18"
"Conference Paper","Liu TF,Craft M,Situ J,Yumer E,Mech R,Kumar R","Learning Design Semantics for Mobile Apps","","2018","","","569–579","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology","Berlin, Germany","2018","9781450359481","","https://doi.org/10.1145/3242587.3242650;http://dx.doi.org/10.1145/3242587.3242650","10.1145/3242587.3242650","Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78% of the total visible, non-redundant elements.","design semantics, mobile app design, machine learning","","UIST '18"
"Conference Paper","Zhang X,Guo PJ","Fusion: Opportunistic Web Prototyping with UI Mashups","","2018","","","951–962","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology","Berlin, Germany","2018","9781450359481","","https://doi.org/10.1145/3242587.3242632;http://dx.doi.org/10.1145/3242587.3242632","10.1145/3242587.3242632","Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.","opportunistic programming, ui mashups, web prototyping","","UIST '18"
"Conference Paper","Ferenc R,Tóth Z,Ladányi G,Siket I,Gyimóthy T","A Public Unified Bug Dataset for Java","","2018","","","12–21","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering","Oulu, Finland","2018","9781450365932","","https://doi.org/10.1145/3273934.3273936;http://dx.doi.org/10.1145/3273934.3273936","10.1145/3273934.3273936","Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.","code metrics, static code analysis, Bug dataset","","PROMISE'18"
"Conference Paper","Jung B,Kim T,Im EG","Malware Classification Using Byte Sequence Information","","2018","","","143–148","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 Conference on Research in Adaptive and Convergent Systems","Honolulu, Hawaii","2018","9781450358859","","https://doi.org/10.1145/3264746.3264775;http://dx.doi.org/10.1145/3264746.3264775","10.1145/3264746.3264775","The number of new malware and new malware variants have been increasing continuously. Security experts analyze malware to capture the malicious properties of malware and to generate signatures or detection rules, but the analysis overheads keep increasing with the increasing number of malware. To analyze a large amount of malware, various kinds of automatic analysis methods are in need. Recently, deep learning techniques such as convolutional neural network (CNN) and recurrent neural network (RNN) have been applied for malware classifications. The features used in the previous approches are mostly based on API (Application Programming Interface) information, and the API invocation information can be obtained through dynamic analysis. However, the invocation information may not reflect malicious behaviors of malware because malware developers use various analysis avoidance techniques. Therefore, deep learning-based malware analysis using other features still need to be developed to improve malware analysis performance. In this paper, we propose a malware classification method using the deep learning algorithm based on byte information. Our proposed method uses images generated from malware byte information that can reflect malware behavioral context, and the convolutional neural network-based sentence analysis is used to process the generated images. We performed several experiments to show the effecitveness of our proposed method, and the experimental results show that our method showed higher accuracy than the naive CNN model, and the detection accuracy was about 99%.","CNN, deep learning, malware classification, static analysis","","RACS '18"
"Conference Paper","Olsson T,Ericsson M,Wingkvist A","Towards Improved Initial Mapping in Semi Automatic Clustering","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings","Madrid, Spain","2018","9781450364836","","https://doi.org/10.1145/3241403.3241456;http://dx.doi.org/10.1145/3241403.3241456","10.1145/3241403.3241456","An important step in Static Architecture Conformance Checking (SACC) is the mapping of source code entities to entities in the intended architecture. This step is currently relying on manual work, which is one hindrance for more widespread adoption of SACC in industry. Semi-automatic clustering is a promising approach to improve this, and the HuGMe clustering algorithm is an example of such a technique for use in SACC. But HuGMe relies on an initial set of clustered source code elements and algorithm parameters. We investigate the automatic mapping performance of HuGMe in two experiments to gain insight into what influence the starting set has in a medium-sized open source system, JabRef, which contain a relatively large number of architectural violations. Our results show that the highest automatic mapping performance can be achieved with a low number of elements within the initial set. However, the variability of the performance is high. We find a benefit in favoring source code elements with a high fan-out in the initial set.","HuGMe, clustering, software architecture conformance","","ECSA '18"
"Conference Paper","Sinkala ZT,Blom M,Herold S","A Mapping Study of Software Architecture Recovery for Software Product Lines","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings","Madrid, Spain","2018","9781450364836","","https://doi.org/10.1145/3241403.3241454;http://dx.doi.org/10.1145/3241403.3241454","10.1145/3241403.3241454","Migrating a family of software systems from ad-hoc development approaches such as `clone-and-own' towards software product lines (SPL) is a challenging task. Software architecture recovery techniques can play a crucial role in such a migration. However, it is to date still unclear how these techniques, which have been mostly developed for single system architecture recovery in mind, can be utilized in an SPL context most effectively. In this paper, we present a mapping study examining 35 research articles with the purpose of discussing the current state of the art in applying software architecture recovery techniques for SPL and identifying potential research gaps in this area. The results provide evidence that currently used approaches do not seem to consider the potential architectural degradation that might exist in the family of systems to be migrated. Moreover, it is hard to generalize across empirical studies as currently it seems difficult to compare and benchmark the approaches applied for software product line architecture (SPLA) extraction/reconstruction.","mapping study, software product lines, software architecture recovery","","ECSA '18"
"Conference Paper","Gropp WD","Using Node Information to Implement MPI Cartesian Topologies","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th European MPI Users' Group Meeting","Barcelona, Spain","2018","9781450364928","","https://doi.org/10.1145/3236367.3236377;http://dx.doi.org/10.1145/3236367.3236377","10.1145/3236367.3236377","The MPI API provides support for Cartesian process topologies, including the option to reorder the processes to achieve better communication performance. But MPI implementations rarely provide anything useful for the reorder option, typically ignoring it. One argument made is that modern interconnects are fast enough that applications are less sensitive to the exact layout of processes onto the system. However, intranode communication performance is much greater than internode communication performance. In this paper, we show a simple approach that takes into account only information about which MPI processes are on the same node to provide a fast and effective implementation of the MPI Cartesian topology. While not optimal, this approach provides a significant improvement over all tested MPI implementations and provides an implementation that may be used as the default in any MPI implementation of MPI_Cart_create.","MPI, Message passing, Cartesian process topology, Process topology","","EuroMPI'18"
"Conference Paper","Rodrigues E,Durelli RS,de Bettio RW,Montecchi L,Terra R","Refactorings for Replacing Dynamic Instructions with Static Ones: The Case of Ruby","","2018","","","59–66","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXII Brazilian Symposium on Programming Languages","Sao Carlos, Brazil","2018","9781450364805","","https://doi.org/10.1145/3264637.3264645;http://dx.doi.org/10.1145/3264637.3264645","10.1145/3264637.3264645","Dynamic features offered by programming languages provide greater flexibility to the programmer (e.g., dynamic constructions of classes and methods) and reduction of duplicate code snippets. However, the unnecessary use of dynamic features may detract from the code in many ways, such as readability, comprehension, and maintainability of software. Therefore, this paper proposes 20 refactorings that replace dynamic instructions with static ones. In an evaluation on 28 open-source Ruby systems, we could refactor 743 of 1,651 dynamic statements (45%).","refactoring, dynamic languages, dynamic statements, frameworks","","SBLP '18"
"Conference Paper","Barbosa J,Andrade RM,Filho JB,Bezerra CI,Barreto I,Capilla R","Cloning in Customization Classes: A Case of a Worldwide Software Product Line","","2018","","","43–52","Association for Computing Machinery","New York, NY, USA","Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse","Sao Carlos, Brazil","2018","9781450365543","","https://doi.org/10.1145/3267183.3267188;http://dx.doi.org/10.1145/3267183.3267188","10.1145/3267183.3267188","Cloning-and-owning, in the long run, can severely affect evolution, as changes in cloned fragments may require modifications in various parts of the system. This problem scales if cloning is used in classes that derive products in a Software Product Line, because these classes can impact in several features and products. However, it is hard to know to which extent cloning in customization classes can impact in a project. We conduct a study, within an SPL that generates mobile software for over 150 countries, to analyze cloning practices and how cloned parts relate to the maintainability of customization classes. We collect and identify clones inside customization classes during a period of 13 months, involving 70 customization classes and 5 branches. In parallel, we collect the respective issues from the issue tracking tool of the SPL project, obtaining over 140 issues related to customization classes. We then confront the time spent to solve each issue with its nature (i.e., if it relates to cloned code or not). As first result, we verify that issues related to cloning take in average 136% more time to be solved. Our study helps to understand how cloning relates to maintainability in the context of mass customization, giving insights about cloned code evolution and its impacts in a software product line project.","Software Product Line, Customization, Clone","","SBCARS '18"
"Conference Paper","Araújo CW,Zapalowski V,Nunes I","Using Code Quality Features to Predict Bugs in Procedural Software Systems","","2018","","","122–131","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXII Brazilian Symposium on Software Engineering","Sao Carlos, Brazil","2018","9781450365031","","https://doi.org/10.1145/3266237.3266271;http://dx.doi.org/10.1145/3266237.3266271","10.1145/3266237.3266271","A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.","procedural languanges, bug prediction, code metrics","","SBES '18"
"Conference Paper","Zheng Z,Wang L,Xu J,Wu T,Wu S,Tao X","Measuring and Predicting the Relevance Ratings between FLOSS Projects Using Topic Features","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Asia-Pacific Symposium on Internetware","Beijing, China","2018","9781450365901","","https://doi.org/10.1145/3275219.3275222;http://dx.doi.org/10.1145/3275219.3275222","10.1145/3275219.3275222","Understanding the relevance between the Free/Libra Open Source Software projects is important for developers to perform code and design reuse, discover and develop new features, keep their projects up-to-date, and etc. However, it is challenging to perform relevance ratings between the FLOSS projects mainly because: 1) beyond simple code similarity, there are complex aspects considered when measuring the relevance; and 2) the prohibitive large amount of FLOSS projects available. To address the problem, in this paper, we propose a method to measure and further predict the relevance ratings between FLOSS projects. Our method uses topic features extracted by the LDA topic model to describe the characteristics of a project. By using the topic features, multiple aspects of FLOSS projects such as the application domain, technology used, and programming language are extracted and further used to measure and predict their relevance ratings. Based on the topic features, our method uses matrix factorization to leverage the partially known relevance ratings between the projects to learn the mapping between different topic features to the relevance ratings. Finally, our method combines the topic modeling and matrix factorization technologies to predict the relevance ratings between software projects without human intervention, which is scalable to a large amount of projects. We evaluate the performance of the proposed method by applying our topic extraction and relevance modeling methods using 300 projects from GitHub. The result of topic extraction experiment shows that, for topic modeling, our LDA-based approach achieves the highest hit rate of 98.3% and the highest average accuracy of 29.8%. And the relevance modeling experiment shows that our relevance modeling approach achieves the minimum average predict error of 0.093, suggesting the effectiveness of applying the proposed method on real-world data sets.","Relevance Rating, Topic Modeling, FLOSS Projects, Matrix Factorization","","Internetware '18"
"Conference Paper","Leopoldseder D,Schatz R,Stadler L,Rigger M,Würthinger T,Mössenböck H","Fast-Path Loop Unrolling of Non-Counted Loops to Enable Subsequent Compiler Optimizations","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Managed Languages & Runtimes","Linz, Austria","2018","9781450364249","","https://doi.org/10.1145/3237009.3237013;http://dx.doi.org/10.1145/3237009.3237013","10.1145/3237009.3237013","Java programs can contain non-counted loops, that is, loops for which the iteration count can neither be determined at compile time nor at run time. State-of-the-art compilers do not aggressively optimize them, since unrolling non-counted loops often involves duplicating also a loop's exit condition, which thus only improves run-time performance if subsequent compiler optimizations can optimize the unrolled code.This paper presents an unrolling approach for non-counted loops that uses simulation at run time to determine whether unrolling such loops enables subsequent compiler optimizations. Simulating loop unrolling allows the compiler to determine performance and code size effects for each potential transformation prior to performing it.We implemented our approach on top of the GraalVM, a high-performance virtual machine for Java, and evaluated it with a set of Java and JavaScript benchmarks in terms of peak performance, compilation time and code size increase. We show that our approach can improve performance by up to 150% while generating a median code size and compile-time increase of not more than 25%. Our results indicate that fast-path unrolling of non-counted loops can be used in practice to increase the performance of Java applications.","virtual machines, loop unrolling, non-counted loops, just-in-time compilation, loop-carried dependencies, compiler optimizations, code duplication","","ManLang '18"
"Conference Paper","Hamza M,Walker RJ,Elaasar M","CIAhelper: Towards Change Impact Analysis in Delta-Oriented Software Product Lines","","2018","","","31–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233036;http://dx.doi.org/10.1145/3233027.3233036","10.1145/3233027.3233036","Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.","feature model, variability model, change impact analysis, code assets, delta-oriented programming","","SPLC '18"
"Conference Paper","Krüger J,Al-Hajjaji M,Schulze S,Saake G,Leich T","Towards Automated Test Refactoring for Software Product Lines","","2018","","","143–148","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233040;http://dx.doi.org/10.1145/3233027.3233040","10.1145/3233027.3233040","In practice, organizations often rely on the clone-and-own approach to reuse and customize existing systems. While increasing maintenance costs encourage some organizations to adopt their development processes towards more systematic reuse, others still avoid migrating to a reusable platform. Based on our experiences, a barrier preventing the adoption of software product lines is the fear of introducing new and more problematic bugs---during the migration or later on. We are aware of several works that automate software-product-line adoption, but they neglect the migration and maintenance of test cases. Automating the refactoring of tests can help to facilitate the adoption barrier, compare the quality after migrations, and support maintenance. In this vision paper, we i) discuss open research challenges that are based on our experiences and ii) sketch a first framework to develop automated solutions. Overall, we aim to illustrate our idea and initiate further research to facilitate the adoption and maintenance of software product lines.","extractive approach, software product line, migration, legacy system, testing, maintenance","","SPLC '18"
"Conference Paper","Martinez J,Tërnava X,Ziadi T","Software Product Line Extraction from Variability-Rich Systems: The Robocode Case Study","","2018","","","132–142","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233038;http://dx.doi.org/10.1145/3233027.3233038","10.1145/3233027.3233038","The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.","extractive software product line adoption, education, reverse-engineering, robocode, software product lines","","SPLC '18"
"Conference Paper","Krüger J,Fenske W,Thüm T,Aporius D,Saake G,Leich T","Apo-Games: A Case Study for Reverse Engineering Variability from Cloned Java Variants","","2018","","","251–256","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3236403;http://dx.doi.org/10.1145/3233027.3236403","10.1145/3233027.3236403","Software-product-line engineering is an approach to systematically manage reusable software features and has been widely adopted in practice. Still, in most cases, organizations start with a single product that they clone and modify when new customer requirements arise (a.k.a. clone-and-own). With an increasing number of variants, maintenance can become challenging and organizations may consider migrating towards a software product line, which is referred to as extractive approach. While this is the most common approach in practice, techniques to extract variability from cloned variants still fall short in several regards. In particular, this accounts for the low accuracy of automated analyses and refactoring, our limited understanding of the costs involved, and the high manual effort. A main reason for these limitations is the lack of realistic case studies. To tackle this problem, we provide a set of cloned variants. In this paper, we characterize these variants and challenge the research community to apply techniques for reverse engineering feature models, feature location, code smell analysis, architecture recovery, and the migration towards a software product line. By evaluating solutions with the developer of these variants, we aim to contribute to a larger body of knowledge on this real-world case study.","extractive approach, case study, feature location, reverse engineering, software-product-line engineering, data set","","SPLC '18"
"Conference Paper","Becker M,Zhang B","How Do Our Neighbours Do Product Line Engineering? A Comparison of Hardware and Software Product Line Engineering Approaches from an Industrial Perspective","","2018","","","190–195","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233045;http://dx.doi.org/10.1145/3233027.3233045","10.1145/3233027.3233045","Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.","software product lines, industry, academia, SPLC","","SPLC '18"
"Conference Paper","Kuiter E,Krüger J,Krieter S,Leich T,Saake G","Getting Rid of Clone-and-Own: Moving to a Software Product Line for Temperature Monitoring","","2018","","","179–189","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233050;http://dx.doi.org/10.1145/3233027.3233050","10.1145/3233027.3233050","Due to its fast and simple applicability, clone-and-own is widely used in industry to develop software variants. In cooperation with different companies for thermoelectric products, we implemented multiple variants of a heat monitoring tool based on clone-and-own. After encountering redundancy-related problems during development and maintenance, we decided to migrate towards a software product line. Within this paper, we describe this case study of migrating cloned variants to a software product line based on the extractive approach. The resulting software product line encapsulates variability on several levels, including the underlying hardware systems, interfaces, and use cases. Currently, we support monitoring hardware from three different companies that use the same core system and provide a configurable front-end. We share our experiences and encountered problems with cloning and migration towards a software product line---focusing on feature extraction and modeling in particular. Furthermore, we provide a lightweight, web-based tool for modeling, configuring, and implementing software product lines, which we use to migrate and manage features. Besides this experience report, we contribute most of the created artifacts as open-source and freely available for the research community.","software product line, feature modeling, case study, extraction","","SPLC '18"
"Conference Paper","Al-Hajjaji M,Schulze M,Ryssel U","Similarity Analysis of Product-Line Variants","","2018","","","226–235","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233044;http://dx.doi.org/10.1145/3233027.3233044","10.1145/3233027.3233044","Many existing approaches have exploited the similarity notion to analyze software systems. In product-line engineering, similarity notion has been considered to fulfill analysis objectives, such as improving the testing effectiveness and reducing the testing efforts. However, most of the existing approaches consider in the similarity measurement only information of high level of abstraction, such as the feature selections of variants. In this paper, we present the notion of similarity in product-line engineering using different types of problem-space as well as solution-space information. In particular, we discuss different scenarios of measuring the similarity between variants and the possibility of combining different types of information to output the similarity between the compared variants. Moreover, we realized these scenarios in the industrial variant management tool pure::variants to fulfill analysis functionalities.","software product lines, similarity, highly configurable systems, variants analysis","","SPLC '18"
"Conference Paper","Montalvillo L,Díaz O,Fogdal T","Reducing Coordination Overhead in SPLs: Peering in on Peers","","2018","","","110–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1","Gothenburg, Sweden","2018","9781450364645","","https://doi.org/10.1145/3233027.3233041;http://dx.doi.org/10.1145/3233027.3233041","10.1145/3233027.3233041","SPL product customers might not always wait for the next core asset release. When an organization aims to react to market events, quick bug fixes or urgent customer requests, strategies are needed to support fast adaptation, e.g. with product-specific extensions, which are later propagated into the SPL. This leads to the grow-and-prune model where quick reaction to changes often requires copying and specialization (grow) to be later cleaned up by merging and refactoring (prune). This paper focuses on the grow stage. Here, application engineers branch off the core-asset Master branch to account for their products' specifics within the times and priorities of their customers without having to wait for the next release of the core assets. However, this practice might end up in the so-called ""integration hell"". When long-living branches are merged back into the Master, the amount of code to be integrated might cause build failures or requires complex troubleshooting. On these premises, we advocate for making application engineers aware of potential coordination problems right during coding rather than deferring it until merge time. To this end, we introduce the notion of ""peering bar"" for Version Control Systems, i.e. visual bars that reflect whether your product's features are being upgraded in other product branches. In this way, engineers are aware of what their peers are doing on the other SPL's products. Being products from the same SPL, they are based on the very same core assets, and hence, bug ixes or functional enhancements undertaken for a product might well serve other products. This work introduces design principles for peering bars. These principles are fleshed out for GitHub as the Version Control System, and pure::variants as the SPL framework.","","","SPLC '18"
"Conference Paper","Jones MP,Bailey J,Cooper TR","MIL, a Monadic Intermediate Language for Implementing Functional Languages","","2018","","","71–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Symposium on Implementation and Application of Functional Languages","Lowell, MA, USA","2018","9781450371438","","https://doi.org/10.1145/3310232.3310238;http://dx.doi.org/10.1145/3310232.3310238","10.1145/3310232.3310238","This paper describes MIL, a ""monadic intermediate language"" that is designed for use in optimizing compilers for strict, strongly typed functional languages. By using a notation that exposes the construction and use of closures and algebraic datatype values, for example, the MIL optimizer is able to detect and eliminate many unnecessary uses of these structures prior to code generation. One feature that distinguishes MIL from other intermediate languages in this area is the use of a typed, parameterized notion for basic blocks. This both enables new optimization techniques, such as the ability to create specialized versions of basic blocks, and leads to a new approach for implementing changes in data representation.","","","IFL 2018"
"Conference Paper","Liu B,Huo W,Zhang C,Li W,Li F,Piao A,Zou W","αDiff: Cross-Version Binary Code Similarity Detection with DNN","","2018","","","667–678","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238199;http://dx.doi.org/10.1145/3238147.3238199","10.1145/3238147.3238199","Binary code similarity detection (BCSD) has many applications, including patch analysis, plagiarism detection, malware detection, and vulnerability search etc. Existing solutions usually perform comparisons over specific syntactic features extracted from binary code, based on expert knowledge. They have either high performance overheads or low detection accuracy. Moreover, few solutions are suitable for detecting similarities between cross-version binaries, which may not only diverge in syntactic structures but also diverge slightly in semantics. In this paper, we propose a solution αDiff, employing three semantic features, to address the cross-version BCSD challenge. It first extracts the intra-function feature of each binary function using a deep neural network (DNN). The DNN works directly on raw bytes of each function, rather than features (e.g., syntactic structures) provided by experts. αDiff further analyzes the function call graph of each binary, which are relatively stable in cross-version binaries, and extracts the inter-function and inter-module features. Then, a distance is computed based on these three features and used for BCSD. We have implemented a prototype of αDiff, and evaluated it on a dataset with about 2.5 million samples. The result shows that αDiff outperforms state-of-the-art static solutions by over 10 percentages on average in different BCSD settings.","DNN, Code Similarity Detection","","ASE '18"
"Conference Paper","Wan Y,Zhao Z,Yang M,Xu G,Ying H,Wu J,Yu PS","Improving Automatic Source Code Summarization via Deep Reinforcement Learning","","2018","","","397–407","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238206;http://dx.doi.org/10.1145/3238147.3238206","10.1145/3238147.3238206","Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.","deep learning, comment generation, Code summarization, reinforcement learning","","ASE '18"
"Conference Paper","Marastoni N,Giacobazzi R,Dalla Preda M","A Deep Learning Approach to Program Similarity","","2018","","","26–35","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis","Montpellier, France","2018","9781450359726","","https://doi.org/10.1145/3243127.3243131;http://dx.doi.org/10.1145/3243127.3243131","10.1145/3243127.3243131","In this work we tackle the problem of binary code similarity by using deep learning applied to binary code visualization techniques. Our idea is to represent binaries as images and then to investigate whether it is possible to recognize similar binaries by applying deep learning algorithms for image classification. In particular, we apply the proposed deep learning framework to a dataset of binary code variants obtained through code obfuscation. These binary variants exhibit similar behaviours while being syntactically different. Our results show that the problem of binary code recognition is strictly separated from simple image recognition problems. Moreover, the analysis of the results of the experiments conducted in this work lead us to the identification of interesting research challenges. For example, in order to use image recognition approaches to recognize similar binary code samples it is important to further investigate how to build a suitable mapping from executables to images.","Code similarity, obfuscation, deep-learning, code visualization","","MASES 2018"
"Conference Paper","He P,Chen Z,He S,Lyu MR","Characterizing the Natural Language Descriptions in Software Logging Statements","","2018","","","178–189","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238193;http://dx.doi.org/10.1145/3238147.3238193","10.1145/3238147.3238193","Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released.","empirical study, natural language processing, Logging","","ASE '18"
"Conference Paper","Huang K,Chen B,Peng X,Zhou D,Wang Y,Liu Y,Zhao W","ClDiff: Generating Concise Linked Code Differences","","2018","","","679–690","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238219;http://dx.doi.org/10.1145/3238147.3238219","10.1145/3238147.3238219","Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named CLDIFF, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of CLDIFF is to generate more easily understandable code differences. CLDIFF takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declara- tions from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of CLDIFF.","AST, Code Differencing, Program Comprehension","","ASE '18"
"Conference Paper","Bajammal M,Mazinanian D,Mesbah A","Generating Reusable Web Components from Mockups","","2018","","","601–611","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238194;http://dx.doi.org/10.1145/3238147.3238194","10.1145/3238147.3238194","The transformation of a user interface mockup designed by a graphic designer to web components in the final app built by a web developer is often laborious, involving manual and time consuming steps. We propose an approach to automate this aspect of web development by generating reusable web components from a mockup. Our approach employs visual analysis of the mockup, and unsupervised learning of visual cues to create reusable web components (e.g., React components). We evaluated our approach, implemented in a tool called VizMod, on five real-world web mockups, and assessed the transformations and generated components through comparison with web development experts. The results show that VizMod achieves on average 94% precision and 75% recall in terms of agreement with the developers' assessment. Furthermore, the refactorings yielded 22% code reusability, on average.","computer vision, web UI, machine learning, web refactoring, web components","","ASE '18"
"Conference Paper","Huang Q,Xia X,Xing Z,Lo D,Wang X","API Method Recommendation without Worrying about the Task-API Knowledge Gap","","2018","","","293–304","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238191;http://dx.doi.org/10.1145/3238147.3238191","10.1145/3238147.3238191","Developers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query’s similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search.","API Documentation, Word Embedding, API Recommendation, Stack Overflow","","ASE '18"
"Conference Paper","Liu X,Huang L,Ng V","Effective API Recommendation without Historical Software Repositories","","2018","","","282–292","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238216;http://dx.doi.org/10.1145/3238147.3238216","10.1145/3238147.3238216","It is time-consuming and labor-intensive to learn and locate the correct API for programming tasks. Thus, it is beneficial to perform API recommendation automatically. The graph-based statistical model has been shown to recommend top-10 API candidates effectively. It falls short, however, in accurately recommending an actual top-1 API. To address this weakness, we propose RecRank, an approach and tool that applies a novel ranking-based discriminative approach leveraging API usage path features to improve top-1 API recommendation. Empirical evaluation on a large corpus of (1385+8) open source projects shows that RecRank significantly improves top-1 API recommendation accuracy and mean reciprocal rank when compared to state-of-the-art API recommendation approaches.","Machine Learning, API Recommendation","","ASE '18"
"Conference Paper","Hassan F,Rodriguez R,Wang X","RUDSEA: Recommending Updates of Dockerfiles via Software Environment Analysis","","2018","","","796–801","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3240470;http://dx.doi.org/10.1145/3238147.3240470","10.1145/3238147.3240470","Dockerfiles are configuration files of docker images which package all dependencies of a software to enable convenient software deployment and porting. In other words, dockerfiles list all environment assumptions of a software application's build and / or execution, so they need to be frequently updated when the environment assumptions change during fast software evolution. In this paper, we propose RUDSEA, a novel approach to recommend updates of dockerfiles to developers based on analyzing changes on software environment assumptions and their impacts. Our evaluation on 1,199 real-world instruction updates shows that RUDSEA can recommend correct update locations for 78.5% of the updates, and correct code changes for 44.1% of the updates.","Software Environment, String Analysis, Dockerfile","","ASE '18"
"Conference Paper","Liu Z,Xia X,Hassan AE,Lo D,Xing Z,Wang X","Neural-Machine-Translation-Based Commit Message Generation: How Far Are We?","","2018","","","373–384","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3238190;http://dx.doi.org/10.1145/3238147.3238190","10.1145/3238147.3238190","Commit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test diffs from which NMT can generate high-quality messages are similar to one or more training diffs at the token level. (2) About 16% of the commit messages in Jiang et al.’s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers.","Nearest neighbor algorithm, Commit message generation, Neural machine translation","","ASE '18"
"Conference Paper","Clarisó R,Cabot J","Applying Graph Kernels to Model-Driven Engineering Problems","","2018","","","1–5","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis","Montpellier, France","2018","9781450359726","","https://doi.org/10.1145/3243127.3243128;http://dx.doi.org/10.1145/3243127.3243128","10.1145/3243127.3243128","Machine Learning (ML) can be used to analyze and classify large collections of graph-based information, e.g. images, location information, the structure of molecules and proteins, ... Graph kernels is one of the ML techniques typically used for such tasks. In a software engineering context, models of a system such as structural or architectural diagrams can be viewed as labeled graphs. Thus, in this paper we propose to employ graph kernels for clustering software modeling artifacts. Among other benefits, this would improve the efficiency and usability of a variety of software modeling activities, e.g., design space exploration, testing or verification and validation.","model diversity, Model-Driven Engineering, clustering, Machine Learning, graph kernel","","MASES 2018"
"Conference Paper","Tufano M,Watson C,Bavota G,Di Penta M,White M,Poshyvanyk D","An Empirical Investigation into Learning Bug-Fixing Patches in the Wild via Neural Machine Translation","","2018","","","832–837","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","Montpellier, France","2018","9781450359375","","https://doi.org/10.1145/3238147.3240732;http://dx.doi.org/10.1145/3238147.3240732","10.1145/3238147.3240732","Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9% of the cases.","bug-fixes, neural machine translation","","ASE '18"
"Journal Article","Polikarpova N,Tschannen J,Furia CA","A Fully Verified Container Library","Form. Asp. Comput.","2018","30","5","495–523","Springer-Verlag","Berlin, Heidelberg","","","2018-09","","0934-5043","https://doi.org/10.1007/s00165-017-0435-1;http://dx.doi.org/10.1007/s00165-017-0435-1","10.1007/s00165-017-0435-1","The comprehensive functionality and nontrivial design of realistic general-purpose container libraries pose challenges to formal verification that go beyond those of individual benchmark problems mainly targeted by the state of the art. We present our experience verifying the full functional correctness of EiffelBase2: a container library offering all the features customary in modern language frameworks, such as external iterators, and hash tables with generic mutable keys and load balancing. Verification uses the automated deductive verifier AutoProof, which we extended as part of the present work. Our results indicate that verification of a realistic container library (135 public methods, 8400 LOC) is possible with moderate annotation overhead (1.4 lines of specification per LOC) and good performance (0.2 s per method on average).","SMT, AutoProof, Containers, Deductive verification, Object-oriented software","",""
"Conference Paper","Kalysch A,Milisterfer O,Protsenko M,Müller T","Tackling Androids Native Library Malware with Robust, Efficient and Accurate Similarity Measures","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Availability, Reliability and Security","Hamburg, Germany","2018","9781450364485","","https://doi.org/10.1145/3230833.3232828;http://dx.doi.org/10.1145/3230833.3232828","10.1145/3230833.3232828","Code similarity measures create a comparison metric showing to what degree two code samples have the same functionality, e.g., to statically detect the use of known libraries in binary code. They are both an indispensable part of automated malware analysis, as well as a helper for the detection of plagiarism (IP protection) and the illegal use of open-source libraries in commercial apps. The centroid similarity metric extracts control-flow features from binary code and encodes them as geometric structures before comparing them. In our paper, we propose novel improvements to the centroid approach and apply it to the ARM architecture for the first time. We implement our approach as a plug-in for the IDA Pro disassembler and evaluate it regarding efficiency, accuracy and robustness on Android. Based on a dataset of 508,745 APKs, collected from 18 third-party app markets, we achieve a detection rate of 89% for the use of native code libraries, with an FPR of 10.8%. To test the robustness of our approach against the compiler version, optimization level, and other code transformations, we obfuscate and recompile known open-source libraries to evaluate which code transformations are resisted. Based on our results, we discuss how code re-use can be hidden by obfuscation and conclude with possible improvements.","Reverse Engineering, Android Static Analysis, Code Similarity","","ARES 2018"
"Conference Paper","Kalgutkar V,Stakhanova N,Cook P,Matyukhina A","Android Authorship Attribution through String Analysis","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Availability, Reliability and Security","Hamburg, Germany","2018","9781450364485","","https://doi.org/10.1145/3230833.3230849;http://dx.doi.org/10.1145/3230833.3230849","10.1145/3230833.3230849","With the rising popularity of Android mobile devices, the amount of malicious applications targeting the Android platform has been increasing tremendously. To mitigate the risk of malicious apps, there is a need for an automated system to detect these applications. Current detection techniques rely on the signatures of well-documented malware, and hence may not be able to detect new malware samples. Instead of generating signatures for malware samples themselves, in this work, we propose to develop a lightweight system that can generate signatures of malware writers by leveraging the string components present in their Android binaries. Using these author signatures, we can effectively detect a wide range of existing, as well as any new, malware samples generated by particular authors. The proposed system achieved 98%, 96%, and 71% accuracy over datasets of 1559 benign, 262 malicious, and 96 obfuscated Android applications, respectively. The string-based approach achieved 71% of accuracy compared to only 50% obtained with the existing Ding and Samadzadeh's system.","Android, String analysis, Authorship attribution, Mobile malware, Obfuscation","","ARES 2018"
"Conference Paper","Tahaei N,Noelle DC","Automated Plagiarism Detection for Computer Programming Exercises Based on Patterns of Resubmission","","2018","","","178–186","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 ACM Conference on International Computing Education Research","Espoo, Finland","2018","9781450356282","","https://doi.org/10.1145/3230977.3231006;http://dx.doi.org/10.1145/3230977.3231006","10.1145/3230977.3231006","Plagiarism detection for computer programming exercises is a difficult problem. A traditional strategy has been to compare the submissions from all of the students in a class, searching for similarities between submissions suggestive of copying. Automated tools exist that compare submissions in order to help with this search. Increasingly, however, instructors have allowed students to submit multiple solutions, receiving formative feedback between submissions, with feedback often generated by automated assessment systems. Allowing multiple submissions allows for a fundamentally new way to detect plagiarism. Specifically, students may struggle with an exercise until frustration leads them to submit work that is not their own. We present a method for detecting plagiarism from the sequence of submissions made by an individual student. We have explored a variety of measures of program change over submissions, and we have found a set of features that can be transformed, using logistic regression, into a score capturing the likelihood of plagiarism. We have applied this method to data from four exercises from an undergraduate programming class. We show that our automatically generated scores are strongly correlated with the assessments of plagiarism made by an expert instructor. Thus, the scores can act as a powerful tool for searching for cases of academic dishonesty.","computer programming instruction, plagiarism detection, submission pattern, automated assessment system","","ICER '18"
"Conference Paper","Burnette M,Kooper R,Maloney JD,Rohde GS,Terstriep JA,Willis C,Fahlgren N,Mockler T,Newcomb M,Sagan V,Andrade-Sanchez P,Shakoor N,Sidike P,Ward R,LeBauer D","TERRA-REF Data Processing Infrastructure","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Practice and Experience on Advanced Research Computing","Pittsburgh, PA, USA","2018","9781450364461","","https://doi.org/10.1145/3219104.3219152;http://dx.doi.org/10.1145/3219104.3219152","10.1145/3219104.3219152","The Transportation Energy Resources from Renewable Agriculture Phenotyping Reference Platform (TERRA-REF) provides a data and computation pipeline responsible for collecting, transferring, processing and distributing large volumes of crop sensing and genomic data from genetically informative germplasm sets. The primary source of these data is a field scanner system built over an experimental field at the University of Arizona Maricopa Agricultural Center. The scanner uses several different sensors to observe the field at a dense collection frequency with high resolution. These sensors include RGB stereo, thermal, pulse-amplitude modulated chlorophyll fluorescence, imaging spectrometer cameras, a 3D laser scanner, and environmental monitors. In addition, data from sensors mounted on tractors, UAVs, an indoor controlled-environment facility, and manually collected measurements are integrated into the pipeline. Up to two TB of data per day are collected and transferred to the National Center for Supercomputing Applications at the University of Illinois (NCSA) where they are processed.In this paper we describe the technical architecture for the TERRA-REF data and computing pipeline. This modular and scalable pipeline provides a suite of components to convert raw imagery to standard formats, geospatially subset data, and identify biophysical and physiological plant features related to crop productivity, resource use, and stress tolerance. Derived data products are uploaded to the Clowder content management system and the BETYdb traits and yields database for querying, supporting research at an experimental plot level. All software is open source2 under a BSD 3-clause or similar license and the data products are open access (currently for evaluation with a full release in fall 2019). In addition, we provide computing environments in which users can explore data and develop new tools. The goal of this system is to enable scientists to evaluate and use data, create new algorithms, and advance the science of digital agriculture and crop improvement.","open science, Phenotyping, Container-based analysis environments","","PEARC '18"
"Conference Paper","Palsberg J,Lopes CV","NJR: A Normalized Java Resource","","2018","","","100–106","Association for Computing Machinery","New York, NY, USA","Companion Proceedings for the ISSTA/ECOOP 2018 Workshops","Amsterdam, Netherlands","2018","9781450359399","","https://doi.org/10.1145/3236454.3236501;http://dx.doi.org/10.1145/3236454.3236501","10.1145/3236454.3236501","We are on the cusp of a major opportunity: software tools that take advantage of Big Code. Specifically, Big Code will enable novel tools in areas such as security enhancers, bug finders, and code synthesizers. What do researchers need from Big Code to make progress on their tools? Our answer is an infrastructure that consists of 100,000 executable Java programs together with a set of working tools and an environment for building new tools. This Normalized Java Resource (NJR) will lower the barrier to implementation of new tools, speed up research, and ultimately help advance research frontiers.Researchers get significant advantages from using NJR. They can write scripts that base their new tool on NJR's already-working tools, and they can search NJR for programs with desired characteristics. They will receive the search result as a container that they can run either locally or on a cloud service. Additionally, they benefit from NJR's normalized representation of each Java program, which enables scalable running of tools on the entire collection. Finally, they will find that NJR's collection of programs is diverse because of our efforts to run clone detection and near-duplicate removal. In this paper we describe our vision for NJR and our current prototype.","software tools, static and dynamic analyses, reproducible results, 100, 000 Java programs, plug-and-play environment","","ISSTA '18"
"Conference Paper","Bragança L,Alves F,Penha JC,Coimbra G,Ferreira R,Nacif JA","Simplifying HW/SW Integration to Deploy Multiple Accelerators for CPU-FPGA Heterogeneous Platforms","","2018","","","97–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation","Pythagorion, Greece","2018","9781450364942","","https://doi.org/10.1145/3229631.3229651;http://dx.doi.org/10.1145/3229631.3229651","10.1145/3229631.3229651","FPGAS became an interesting option for developing hardware accelerators due to their energy efficiency and recent improvements in CPU-FPGA communication speeds. In order to accelerate the development cycle, FPGA high-level synthesis tools have been developed such as Intel HLS, OpenCL, and OpenSPL. These tools aim to free the designer from having to know all the FPGA low-level details. However, in order to achieve high performance processing, the developer should still understand the details of the deeper system layers. Moreover, OpenCL usually consumes more resources/compile time than a design developed directly in RTL. In this work we propose a novel framework to automatically integrate hardware accelerator cores in a final architecture by generating a HW/SW interface for an Intel CPU-FPGA modern platform. Our goal is to simplify the Intel Open Programmable Acceleration Engine (OPAE) by introducing a novel abstraction layer with a simple stream protocol channel. The experimental results for a set of dataflow benchmarks show a performance of up to 131.7 Gops/s, and a power efficiency of up to 353.7 Gops/W even when we bound the memory bandwidth to 12 GB/s.","","","SAMOS '18"
"Conference Paper","Jiang J,Xiong Y,Zhang H,Gao Q,Chen X","Shaping Program Repair Space with Existing Patches and Similar Code","","2018","","","298–309","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis","Amsterdam, Netherlands","2018","9781450356992","","https://doi.org/10.1145/3213846.3213871;http://dx.doi.org/10.1145/3213846.3213871","10.1145/3213846.3213871","Automated program repair (APR) has great potential to reduce bug-fixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both. In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces. We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches.","code differencing, code adaptation, Automated program repair","","ISSTA 2018"
"Journal Article","Braught G,Maccormick J,Bowring J,Burke Q,Cutler B,Goldschmidt D,Krishnamoorthy M,Turner W,Huss-Lederman S,Mackellar B,Tucker A","A Multi-Institutional Perspective on H/FOSS Projects in the Computing Curriculum","ACM Trans. Comput. Educ.","2018","18","2","","Association for Computing Machinery","New York, NY, USA","","","2018-07","","","https://doi.org/10.1145/3145476;http://dx.doi.org/10.1145/3145476","10.1145/3145476","Many computer science programs have capstone experiences or project courses that allow students to integrate knowledge from the full breadth of their major. Such capstone projects may be student-designed, instructor-designed, designed in conjunction with outside companies, or integrated with ongoing free and open source (FOSS) projects. The literature shows that the FOSS approach has attracted a great deal of interest, in particular when implemented with projects that have humanitarian goals (HFOSS). In this article, we describe five unique models from five distinct types of institutions for incorporating sustained FOSS or HFOSS (alternatively H/FOSS) project work into capstone experiences or courses. The goal is to provide instructors wishing to integrate open source experiences into their curriculum with additional perspectives and resources to help in adapting this approach to the specific needs and goals of their institution and students. All of the models presented are based on sustained engagement with H/FOSS projects that last at least one semester and often more. Each model is described in terms of its characteristics and how it fits the needs of the institution using the model. Assessment of each model is also presented. We then discuss the themes that are common across the models, such as project selection, team formation, mentoring, and student assessment. We examine the choices made by each model, as well as the challenges faced. We end with a discussion how the models have leveraged institutional initiatives and collaborations with outside organizations to address some of the challenges associated with these projects.","humanitarian, Open source, capstones, FOSS, projects, HFOSS","",""
"Conference Paper","Zarras AV,Mamalis G,Papamichail A,Kollias P,Vassiliadis P","And the Tool Created a GUI That Was Impure and Without Form: Anti-Patterns in Automatically Generated GUIs","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd European Conference on Pattern Languages of Programs","Irsee, Germany","2018","9781450363877","","https://doi.org/10.1145/3282308.3282333;http://dx.doi.org/10.1145/3282308.3282333","10.1145/3282308.3282333","A basic prerequisite for any daily development task is to understand the source code that we are working with. To this end, the source code should be clean. Usually, it is up to us, the developers, to keep the source code clean. However, often there are parts of the code that are automatically generated. A typical such case are Graphical User Interfaces (GUIs) created via a GUI builder, i.e., a tool that allows the developer to design the GUI by combining graphical control elements, offered in a palette. In this paper, we investigate the quality of the code that is generated by GUI builders. To assist tool-smiths in developing better GUI builders, we report anti-patterns concerning naming, documentation, design and implementation issues, observed in a study that involves four popular GUI builders for Java. The reported anti-patterns can further assist GUI developers/designers in selecting appropriate tools.","Refactoring, Code Clones, GUIs, Responsibilities, Patterns","","EuroPLoP '18"
"Conference Paper","Luxton-Reilly A,Simon,Albluwi I,Becker BA,Giannakos M,Kumar AN,Ott L,Paterson J,Scott MJ,Sheard J,Szabo C","Introductory Programming: A Systematic Literature Review","","2018","","","55–106","Association for Computing Machinery","New York, NY, USA","Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education","Larnaca, Cyprus","2018","9781450362238","","https://doi.org/10.1145/3293881.3295779;http://dx.doi.org/10.1145/3293881.3295779","10.1145/3293881.3295779","As computing becomes a mainstream discipline embedded in the school curriculum and acts as an enabler for an increasing range of academic disciplines in higher education, the literature on introductory programming is growing. Although there have been several reviews that focus on specific aspects of introductory programming, there has been no broad overview of the literature exploring recent trends across the breadth of introductory programming. This paper is the report of an ITiCSE working group that conducted a systematic review in order to gain an overview of the introductory programming literature. Partitioning the literature into papers addressing the student, teaching, the curriculum, and assessment, we explore trends, highlight advances in knowledge over the past 15 years, and indicate possible directions for future research.","review, literature review, systematic review, CS1, novice programming, overview, introductory programming, systematic literature review, ITiCSE working group, SLR","","ITiCSE 2018 Companion"
"Conference Paper","Brown LE,Feltz A,Wallace C","Lab Exercises for a Discrete Structures Course: Exploring Logic and Relational Algebra with Alloy","","2018","","","135–140","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education","Larnaca, Cyprus","2018","9781450357074","","https://doi.org/10.1145/3197091.3197127;http://dx.doi.org/10.1145/3197091.3197127","10.1145/3197091.3197127","Students in computing disciplines need a strong basis in the fundamentals of discrete mathematics, but traditional offline approaches to teaching this material provide limited opportunities for the kind of interactive learning that computing students experience in their programming assignments. We have been using the Alloy language and analyzer to teach concepts in discrete structures (relational algebra, logic, and graphs) in an exploratory, programming-oriented way. Alloy, however, constitutes a new programming paradigm for introductory students, and careful mediation is needed to keep students on track. We use the familiar programming lab format, where students work on small-scope problems co-located with instructors, to provide guidance as students wrestle with the languages of relational algebra and predicate logic through Alloy. We describe selected lab exercises, and report on initial findings based on our experiences with students.","Computer science education, Alloy, Discrete structures, Logic programming","","ITiCSE 2018"
"Conference Paper","Tahir A,Yamashita A,Licorish S,Dietrich J,Counsell S","Can You Tell Me If It Smells? A Study on How Developers Discuss Code Smells and Anti-Patterns in Stack Overflow","","2018","","","68–78","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018","Christchurch, New Zealand","2018","9781450364034","","https://doi.org/10.1145/3210459.3210466;http://dx.doi.org/10.1145/3210459.3210466","10.1145/3210459.3210466","This paper investigates how developers discuss code smells and anti-patterns over Stack Overflow to understand better their perceptions and understanding of these two concepts. Understanding developers' perceptions of these issues are important in order to inform and align future research efforts and direct tools vendors in the area of code smells and anti-patterns. In addition, such insights could lead the creation of solutions to code smells and anti-patterns that are better fit to the realities developers face in practice. We applied both quantitative and qualitative techniques to analyse discussions containing terms associated with code smells and anti-patterns. Our findings show that developers widely use Stack Overflow to ask for general assessments of code smells or anti-patterns, instead of asking for particular refactoring solutions. An interesting finding is that developers very often ask their peers 'to smell their code' (i.e., ask whether their own code 'smells' or not), and thus, utilize Stack Overflow as an informal, crowd-based code smell/anti-pattern detector. We conjecture that the crowd-based detection approach considers contextual factors, and thus, tends to be more trusted by developers over automated detection tools. We also found that developers often discuss the downsides of implementing specific design patterns, and 'flag' them as potential anti-patterns to be avoided. Conversely, we found discussions on why some anti-patterns previously considered harmful should not be flagged as anti-patterns. Our results suggest that there is a need for: 1) more context-based evaluations of code smells and anti-patterns, and 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry.","Code smells, empirical study, anti-patterns, mining software repositories, Stack Overflow","","EASE'18"
"Journal Article","Zheng Y,Cu C,Taylor RN","Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development","ACM Trans. Softw. Eng. Methodol.","2018","27","2","","Association for Computing Machinery","New York, NY, USA","","","2018-06","","1049-331X","https://doi.org/10.1145/3229048;http://dx.doi.org/10.1145/3229048","10.1145/3229048","Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.","Architecture-implementation mapping, architecture-centric feature traceability, architectural evolution, variability conformance, architecture-centric development","",""
"Conference Paper","Khandwala K,Guo PJ","Codemotion: Expanding the Design Space of Learner Interactions with Computer Programming Tutorial Videos","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifth Annual ACM Conference on Learning at Scale","London, United Kingdom","2018","9781450358866","","https://doi.org/10.1145/3231644.3231652;http://dx.doi.org/10.1145/3231644.3231652","10.1145/3231644.3231652","Love them or hate them, videos are a pervasive format for delivering online education at scale. They are especially popular for computer programming tutorials since videos convey expert narration alongside the dynamic effects of editing and running code. However, these screencast videos simply consist of raw pixels, so there is no way to interact with the code embedded inside of them. To expand the design space of learner interactions with programming videos, we developed Codemotion, a computer vision algorithm that automatically extracts source code and dynamic edits from existing videos. Codemotion segments a video into regions that likely contain code, performs OCR on those segments, recognizes source code, and merges together related code edits into contiguous intervals. We used Codemotion to build a novel video player and then elicited interaction design ideas from potential users by running an elicitation study with 10 students followed by four participatory design workshops with 12 additional students. Participants collectively generated ideas for 28 kinds of interactions such as inline code editing, code-based skimming, pop-up video search, and in-video coding exercises.","computer programming, tutorial videos, screencasts","","L@S '18"
"Conference Paper","Elghadhafi HA,Abdelaziz TM,Maatuk AM","A Novel Approach for Improving the Quality of Software Code Using Reverse Engineering","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth International Conference on Engineering & MIS 2018","Istanbul, Turkey","2018","9781450363921","","https://doi.org/10.1145/3234698.3234729;http://dx.doi.org/10.1145/3234698.3234729","10.1145/3234698.3234729","Copying and pasting program code fragments with minor changes is a common practice in software development. Software systems often have similar segments of code, called code clones. Due to many reasons, unintentional smells may also appear in the source code without awareness of program developers. Code smell may violate the principles of software design and negatively impact program design quality, thus making software development and maintenance very costly. This paper presents an enhanced approach to facilitate the process of identification and elimination of code smells. The proposed solution allows the detection and removal of code smells for refinement and improvement of the quality of software system. Code smells are analyzed, restructured and eliminated from the source code using reverse engineering techniques. The solution maintains the external behaviour of software system and judges the efficiency of systems code. An experiment has been conducted using a real software system, which is evaluated before and after using the approach. The results have been encouraging and help in detecting code smells.","Smells Detection, Code Clone, Code Refactoring, Code Smells","","ICEMIS '18"
"Conference Paper","Xue H,Venkataramani G,Lan T","Clone-Hunter: Accelerated Bound Checks Elimination via Binary Code Clone Detection","","2018","","","11–19","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages","Philadelphia, PA, USA","2018","9781450358347","","https://doi.org/10.1145/3211346.3211347;http://dx.doi.org/10.1145/3211346.3211347","10.1145/3211346.3211347","Unsafe pointer usage and illegitimate memory accesses are prevalent bugs in software. To ensure memory safety, conditions for array bound checks are inserted into the code to detect out-of-bound memory accesses. Unfortunately, these bound checks contribute to high runtime overheads, and therefore, redundant array bound checks should be removed to improve application performance. In this paper, we propose Clone-Hunter, a practical and scalable framework for redundant bound check elimination in binary executables. Clone-Hunter first uses binary code clone detection, and then employs bound safety verification mechanism (using binary symbolic execution) to ensure sound removal of redundant bound checks. Our results show the Clone-Hunter can swiftly identify redundant bound checks about 90× faster than pure binary symbolic execution, while ensuring zero false positives.","Binary analysis, Machine learning, Array bound checks, Memory safety","","MAPL 2018"
"Conference Paper","Sachdev S,Li H,Luan S,Kim S,Sen K,Chandra S","Retrieval on Source Code: A Neural Code Search","","2018","","","31–41","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages","Philadelphia, PA, USA","2018","9781450358347","","https://doi.org/10.1145/3211346.3211353;http://dx.doi.org/10.1145/3211346.3211353","10.1145/3211346.3211353","Searching over large code corpora can be a powerful productivity tool for both beginner and experienced developers because it helps them quickly find examples of code related to their intent. Code search becomes even more attractive if developers could express their intent in natural language, similar to the interaction that Stack Overflow supports. In this paper, we investigate the use of natural language processing and information retrieval techniques to carry out natural language search directly over source code, i.e. without having a curated Q&A forum such as Stack Overflow at hand. Our experiments using a benchmark suite derived from Stack Overflow and GitHub repositories show promising results. We find that while a basic word–embedding based search procedure works acceptably, better results can be obtained by adding a layer of supervision, as well as by a customized ranking strategy.","word-embedding, TF-IDF, code search","","MAPL 2018"
"Conference Paper","Gottschlich J,Solar-Lezama A,Tatbul N,Carbin M,Rinard M,Barzilay R,Amarasinghe S,Tenenbaum JB,Mattson T","The Three Pillars of Machine Programming","","2018","","","69–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages","Philadelphia, PA, USA","2018","9781450358347","","https://doi.org/10.1145/3211346.3211355;http://dx.doi.org/10.1145/3211346.3211355","10.1145/3211346.3211355","In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and (iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.","software maintenance, software development, program synthesis, machine programming, intention, invention, adaptation","","MAPL 2018"
"Conference Paper","Begel A,Vrzakova H","Eye Movements in Code Review","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Workshop on Eye Movements in Programming","Warsaw, Poland","2018","9781450357920","","https://doi.org/10.1145/3216723.3216727;http://dx.doi.org/10.1145/3216723.3216727","10.1145/3216723.3216727","In order to ensure sufficient quality, software engineers conduct code reviews to read over one another's code looking for errors that should be fixed before committing to their source code repositories. Many kinds of errors are spotted, from simple spelling mistakes and syntax errors, to architectural flaws that may span several files. However, we know little about how software developers read code when looking for defects. What kinds of code trigger engineers to check more deeply into suspected defects? How long do they take to verify whether a defect is really there? We conducted a study of 35 software engineers performing 40 code reviews while capturing their gaze with an eye tracker. We classified each code defect the developers found and captured the patterns of eye gazes used to deliberate about each one. We report how long it took to confirm defect suspicions for each type of defect and the fraction of time spent skimming the code vs. carefully reading it. This work provides a starting point for automating code reviews that could help engineers spend more time focusing on the difficult task of defect confirmation rather than the tedious task of defect discovery.","code review, eye tracking","","EMIP '18"
"Journal Article","Stawinoga N,Field T","Predictable Thread Coarsening","ACM Trans. Archit. Code Optim.","2018","15","2","","Association for Computing Machinery","New York, NY, USA","","","2018-06","","1544-3566","https://doi.org/10.1145/3194242;http://dx.doi.org/10.1145/3194242","10.1145/3194242","Thread coarsening on GPUs combines the work of several threads into one. We show how thread coarsening can be implemented as a fully automated compile-time optimisation that estimates the optimal coarsening factor based on a low-cost, approximate static analysis of cache line re-use and an occupancy prediction model. We evaluate two coarsening strategies on three different NVidia GPU architectures. For NVidia reduction kernels we achieve a maximum speedup of 5.08x, and for the Rodinia benchmarks we achieve a mean speedup of 1.30x over 8 of 19 kernels that were determined safe to coarsen.","GPU, compiler optimisations","",""
"Conference Paper","Moll S,Hack S","Partial Control-Flow Linearization","","2018","","","543–556","Association for Computing Machinery","New York, NY, USA","Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation","Philadelphia, PA, USA","2018","9781450356985","","https://doi.org/10.1145/3192366.3192413;http://dx.doi.org/10.1145/3192366.3192413","10.1145/3192366.3192413","If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.","Compiler optimizations, SIMD, SPMD","","PLDI 2018"
"Journal Article","Moll S,Hack S","Partial Control-Flow Linearization","SIGPLAN Not.","2018","53","4","543–556","Association for Computing Machinery","New York, NY, USA","","","2018-06","","0362-1340","https://doi.org/10.1145/3296979.3192413;http://dx.doi.org/10.1145/3296979.3192413","10.1145/3296979.3192413","If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.","Compiler optimizations, SIMD, SPMD","",""
"Conference Paper","Alon U,Zilberstein M,Levy O,Yahav E","A General Path-Based Representation for Predicting Program Properties","","2018","","","404–419","Association for Computing Machinery","New York, NY, USA","Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation","Philadelphia, PA, USA","2018","9781450356985","","https://doi.org/10.1145/3192366.3192412;http://dx.doi.org/10.1145/3192366.3192412","10.1145/3192366.3192412","Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.","Programming Languages, Machine Learning, Learning Representations, Big Code","","PLDI 2018"
"Journal Article","Alon U,Zilberstein M,Levy O,Yahav E","A General Path-Based Representation for Predicting Program Properties","SIGPLAN Not.","2018","53","4","404–419","Association for Computing Machinery","New York, NY, USA","","","2018-06","","0362-1340","https://doi.org/10.1145/3296979.3192412;http://dx.doi.org/10.1145/3296979.3192412","10.1145/3296979.3192412","Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.","Programming Languages, Learning Representations, Machine Learning, Big Code","",""
"Conference Paper","Tavares CS,Ferreira F,Figueiredo E","A Systematic Mapping of Literature on Software Refactoring Tools","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XIV Brazilian Symposium on Information Systems","Caxias do Sul, Brazil","2018","9781450365598","","https://doi.org/10.1145/3229345.3229357;http://dx.doi.org/10.1145/3229345.3229357","10.1145/3229345.3229357","Refactoring consists of improving the internal structure of the code without changing the external behavior of a software system. However, the task of refactoring is very costly in the development of an information system. Thus, many tools have been proposed to support refactoring the source code. In order to find tools cited in the literature, this work presents a Systematic Literature Mapping about refactoring. As a result, this paper summarizes the refactoring tools that have been published in the last 5 years in terms of the tool profiles developed, which programming languages have support for refactoring and which are the main refactoring strategies that are handled by tools. It has been identified that publications on refactoring have remained constant over the past 5 years. Also, most of the refactoring works describe tools, being they for systems written in the Java language, that perform code refactoring automatically and the main refactorings are: Move Method, Pull Up Method, Extract Class and Code Clone. Finally, we performed an analysis of the data returned by the DBLP library. As a result, it was observed that the papers returned by the DBLP have a high level of similarity with the other research bases studied.","","","SBSI'18"
"Conference Paper","Cody-Kenny B,O'Neill M,Barrett S","Performance Localisation","","2018","","","27–34","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Genetic Improvement Workshop","Gothenburg, Sweden","2018","9781450357531","","https://doi.org/10.1145/3194810.3194815;http://dx.doi.org/10.1145/3194810.3194815","10.1145/3194810.3194815","Profiling techniques highlight where performance issues manifest and provide a starting point for tracing cause back through a program. While people diagnose and understand the cause of performance to guide formulation of a performance improvement, we seek automated techniques for highlighting performance improvement opportunities to guide search algorithms.We investigate mutation-based approaches for highlighting where a performance improvement is likely to exist. For all modification locations in a program, we make all possible modifications and analyse how often modifications reduce execution count. We compare the resulting code location rankings against rankings derived using a profiler and find that mutation analysis provides the higher accuracy in highlighting performance improvement locations in a set of benchmark problems, though at a much higher execution cost.We see both approaches as complimentary and consider how they may be used to further guide Genetic Programming in finding performance improvements.","","","GI '18"
"Conference Paper","Nori AV,Gaur J,Rai S,Subramoney S,Wang H","Criticality Aware Tiered Cache Hierarchy: A Fundamental Relook at Multi-Level Cache Hierarchies","","2018","","","96–109","IEEE Press","Los Angeles, California","Proceedings of the 45th Annual International Symposium on Computer Architecture","","2018","9781538659847","","https://doi.org/10.1109/ISCA.2018.00019;http://dx.doi.org/10.1109/ISCA.2018.00019","10.1109/ISCA.2018.00019","On-die caches are a popular method to help hide the main memory latency. However, it is difficult to build large caches without substantially increasing their access latency, which in turn hurts performance. To overcome this difficulty, on-die caches are typically built as a multi-level cache hierarchy. One such popular hierarchy that has been adopted by modern microprocessors is the three level cache hierarchy. Building a three level cache hierarchy enables a low average hit latency since most requests are serviced from faster inner level caches. This has motivated recent microprocessors to deploy large level-2 (L2) caches that can help further reduce the average hit latency.In this paper, we do a fundamental analysis of the popular three level cache hierarchy and understand its performance delivery using program criticality. Through our detailed analysis we show that the current trend of increasing L2 cache sizes to reduce average hit latency is, in fact, an inefficient design choice. We instead propose Criticality Aware Tiered Cache Hierarchy (CATCH) that utilizes an accurate detection of program criticality in hardware and using a novel set of inter-cache prefetchers ensures that on-die data accesses that lie on the critical path of execution are served at the latency of the fastest level-1 (L1) cache. The last level cache (LLC) serves the purpose of reducing slow memory accesses, thereby making the large L2 cache redundant for most applications. The area saved by eliminating the L2 cache can then be used to create more efficient processor configurations. Our simulation results show that CATCH outperforms the three level cache hierarchy with a large 1 MB L2 and exclusive LLC by an average of 8.4%, and a baseline with 256 KB L2 and inclusive LLC by 10.3%. We also show that CATCH enables a powerful framework to explore broad chip-level area, performance and power tradeoffs in cache hierarchy design. Supported by CATCH, we evaluate radical architecture directions such as eliminating the L2 altogether and show that such architectures can yield 4.5% performance gain over the baseline at nearly 30% lesser area or improve the performance by 7.3% at the same area while reducing energy consumption by 11%.","prefetching, caching, criticality","","ISCA '18"
"Conference Paper","Hamou-Lhadj W,Nayrolles M","A Project on Software Defect Prevention at Commit-Time: A Success Story of University-Industry Research Collaboration","","2018","","","24–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice","Gothenburg, Sweden","2018","9781450357449","","https://doi.org/10.1145/3195546.3206423;http://dx.doi.org/10.1145/3195546.3206423","10.1145/3195546.3206423","In this talk, we describe a research collaboration project between Concordia University and Ubisoft. The project consists of investigating techniques for defect prevention at commit-time for increased software quality. The outcome of this project is a tool called CLEVER (Combining Levels of Bug Prevention and Resolution techniques) that uses machine learning to automatically detect coding defects as programmers write code. The main novelty of CLEVER is that it relies on code matching techniques to detect coding mistakes based on a database of historical code defects found in multiple related projects. The tool also proposes fixes based on known patterns.","university-industry research project, software maintenance and evolution, machine learning, bug prevention at commit-time","","SER&IP '18"
"Conference Paper","Tufano M,Watson C,Bavota G,Di Penta M,White M,Poshyvanyk D","Deep Learning Similarities from Different Representations of Source Code","","2018","","","542–553","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196431;http://dx.doi.org/10.1145/3196398.3196431","10.1145/3196398.3196431","Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.","code similarities, deep learning, neural networks","","MSR '18"
"Conference Paper","de la Torre G,Robbes R,Bergel A","Imprecisions Diagnostic in Source Code Deltas","","2018","","","492–502","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196404;http://dx.doi.org/10.1145/3196398.3196404","10.1145/3196398.3196404","Beyond a practical use in code review, source code change detection (SCCD) is an important component of many mining software repositories (MSR) approaches. As such, any error or imprecision in the detection may result in a wrong conclusion while mining repositories. We identified, analyzed, and characterized impressions in GumTree, which is the most advanced algorithm for SCCD. After analyzing its detection accuracy over a curated corpus of 107 C# projects, we diagnosed several imprecisions. Many of our findings confirm that a more language-aware perspective of GumTree can be helpful in reporting more precise changes.","source code change detection, gumtree, differencing, quality","","MSR '18"
"Conference Paper","Ott J,Atchison A,Harnack P,Bergh A,Linstead E","A Deep Learning Approach to Identifying Source Code in Images and Video","","2018","","","376–386","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196402;http://dx.doi.org/10.1145/3196398.3196402","10.1145/3196398.3196402","While substantial progress has been made in mining code on an Internet scale, efforts to date have been overwhelmingly focused on data sets where source code is represented natively as text. Large volumes of source code available online and embedded in technical videos have remained largely unexplored, due in part to the complexity of extraction when code is represented with images. Existing approaches to code extraction and indexing in this environment rely heavily on computationally intense optical character recognition. To improve the ease and efficiency of identifying this embedded code, as well as identifying similar code examples, we develop a deep learning solution based on convolutional neural networks and autoencoders. Focusing on Java for proof of concept, our technique is able to identify the presence of typeset and handwritten source code in thousands of video images with 85.6%-98.6% accuracy based on syntactic and contextual features learned through deep architectures. When combined with traditional approaches, this provides a more scalable basis for video indexing that can be incorporated into existing software search and mining tools.","convolutional neural networks, deep learning, programming tutorials, video mining","","MSR '18"
"Conference Paper","Diamantopoulos T,Karagiannopoulos G,Symeonidis AL","Codecatch: Extracting Source Code Snippets from Online Sources","","2018","","","21–27","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering","Gothenburg, Sweden","2018","9781450357234","","https://doi.org/10.1145/3194104.3194107;http://dx.doi.org/10.1145/3194104.3194107","10.1145/3194104.3194107","Nowadays, developers rely on online sources to find example snippets that address the programming problems they are trying to solve. However, contemporary API usage mining methods are not suitable for locating easily reusable snippets, as they provide usage examples for specific APIs, thus requiring the developer to know which library to use beforehand. On the other hand, the approaches that retrieve snippets from online sources usually output a list of examples, without aiding the developer to distinguish among different implementations and without offering any insight on the quality and the reusability of the proposed snippets. In this work, we present CodeCatch, a system that receives queries in natural language and extracts snippets from multiple online sources. The snippets are assessed both for their quality and for their usefulness/preference by the developers, while they are also clustered according to their API calls to allow the developer to select among the different implementations. Preliminary evaluation of CodeCatch in a set of indicative programming problems indicates that it can be a useful tool for the developer.","API usage mining, code reuse, snippet mining","","RAISE '18"
"Conference Paper","Yoshida N,Ishizu T,Edwards B,Inoue K","How Slim Will My System Be? Estimating Refactored Code Size by Merging Clones","","2018","","","352–360","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196353;http://dx.doi.org/10.1145/3196321.3196353","10.1145/3196321.3196353","We have been doing code clone analysis with industry collaborators for a long time, and have been always asked a question, ""OK, I understand my system contains a lot of code clones, but how slim will it be after merging redundant code clones?"" As a software system evolves for long period, it would increasingly contain many code clones due to quick bug fix and new feature addition. Industry collaborators would recognize decay of initial design simplicity, and try to evaluate current system from the view point of maintenance effort and cost. As one of resources for the evaluation, the estimated code size by merging code clone is very important for them. In this paper, we formulate this issue as ""slimming"" problem, and present three different slimming methods, Basic, Complete, and Heuristic Methods, each of which gives a lower bound, upper bound, and modest reduction rates, respectively. Application of these methods to OSS systems written in C/C++ showed that the reduction rate is at most 5.7% of the total size, and to a commercial COBOL system, it is at most 15.4%. For this approach, we have gotten initial but very positive feedback from industry collaborators.","size estimation, refactoring, code clone","","ICPC '18"
"Conference Paper","Nayrolles M,Hamou-Lhadj A","CLEVER: Combining Code Metrics with Clone Detection for Just-in-Time Fault Prevention and Resolution in Large Industrial Projects","","2018","","","153–164","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196438;http://dx.doi.org/10.1145/3196398.3196438","10.1145/3196398.3196438","Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phase process for intercepting risky commits before they reach the central repository. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79% precision and 65% recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7% of the cases.","software evolution, software maintenance, defect predictions, fault fixing","","MSR '18"
"Conference Paper","Doi M,Higo Y,Arima R,Shimonaka K,Kusumoto S","On the Naturalness of Auto-Generated Code: Can We Identify Auto-Generated Code Automatically?","","2018","","","340–343","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196356;http://dx.doi.org/10.1145/3196321.3196356","10.1145/3196321.3196356","Recently, a variety of studies have been conducted on source code analysis. If auto-generated code is included in the target source code, it is usually removed in a preprocessing phase because the presence of auto-generated code may have negative effects on source code analysis. A straightforward way to remove auto-generated code is searching special comments that are included in the files of auto-generated code. However, it becomes impossible to identify auto-generated code with the way if such special comments have disappeared for some reasons. It is obvious that it takes too much effort to see source files one by one manually. In this paper, we propose a new technique to identify auto-generated code by using the naturalness of auto-generated code. We used a golden set that includes thousands of hand-made source files and source files generated by four kinds of compiler-compilers. Through the evaluation with the dataset, we confirmed that our technique was able to identify auto-generated code with over 99% precision and recall for all the cases.","source code analysis, auto-generated code, N-gram language model","","ICPC '18"
"Conference Paper","Hu X,Li G,Xia X,Lo D,Jin Z","Deep Code Comment Generation","","2018","","","200–210","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196334;http://dx.doi.org/10.1145/3196321.3196334","10.1145/3196321.3196334","During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.","deep learning, comment generation, program comprehension","","ICPC '18"
"Conference Paper","Arima R,Higo Y,Kusumoto S","Toward Refactoring Evaluation with Code Naturalness","","2018","","","316–319","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196362;http://dx.doi.org/10.1145/3196321.3196362","10.1145/3196321.3196362","Refactoring evaluation is a challenging research topic because right and wrong of refactoring depend on various aspects of development context such as developers' skills, development cost, deadline and so on. Many techniques have been proposed to evaluate refactoring objectively. However, those techniques do not consider individual contexts of software development. Currently, the authors are trying to evaluate refactoring automatically and objectively with considering development contexts. In this paper, we propose to evaluate refactoring with code naturalness. Our technique is based on a hypothesis: if a given refactoring raises the naturalness of existing code, the refactoring is beneficial. In this paper, we also report our pilot study on open source software.","n-gram language model, refactoring, naturalness","","ICPC '18"
"Conference Paper","Blasi A,Gorla A","Replicomment: Identifying Clones in Code Comments","","2018","","","320–323","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196360;http://dx.doi.org/10.1145/3196321.3196360","10.1145/3196321.3196360","Code comments are the primary means to document implementation and ease program comprehension. Thus, their quality should be a primary concern to improve program maintenance. While a lot of effort has been dedicated to detect bad smell in code, little work focuses on comments. In this paper we start working in this direction by detecting clones in comments. Our initial investigation shows that even well known projects have several comment clones, and just as clones are bad smell in code, they may be for comments. A manual analysis of the clones we identified revealed several issues in real Java projects.","code comments, bad smell, software quality, clones","","ICPC '18"
"Conference Paper","Li S,Niu X,Jia Z,Wang J,He H,Wang T","Logtracker: Learning Log Revision Behaviors Proactively from Software Evolution History","","2018","","","178–188","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Conference on Program Comprehension","Gothenburg, Sweden","2018","9781450357142","","https://doi.org/10.1145/3196321.3196328;http://dx.doi.org/10.1145/3196321.3196328","10.1145/3196321.3196328","Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.","software evolution, log revision, failure diagnose","","ICPC '18"
"Conference Paper","Baltes S,Dumani L,Treude C,Diehl S","SOTorrent: Reconstructing and Analyzing the Evolution of Stack Overflow Posts","","2018","","","319–330","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196430;http://dx.doi.org/10.1145/3196398.3196430","10.1145/3196398.3196430","Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.","stack overflow, open dataset, code snippets, software evolution","","MSR '18"
"Conference Paper","Zampetti F,Serebrenik A,Di Penta M","Was Self-Admitted Technical Debt Removal a Real Removal? An in-Depth Perspective","","2018","","","526–536","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196423;http://dx.doi.org/10.1145/3196398.3196423","10.1145/3196398.3196423","Technical Debt (TD) has been defined as ""code being not quite right yet"", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is ""accidentally"" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers.","","","MSR '18"
"Conference Paper","Majumder S,Balaji N,Brey K,Fu W,Menzies T","500+ Times Faster than Deep Learning: A Case Study Exploring Faster Methods for Text Mining Stackoverflow","","2018","","","554–563","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196424;http://dx.doi.org/10.1145/3196398.3196424","10.1145/3196398.3196424","Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train.This paper extends that recent result by clustering the dataset, then tuning every learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources.More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).","SVM, CNN, local versus global, KNN, deep learning, parameter tuning, DE, K-means","","MSR '18"
"Conference Paper","Mahmoudi M,Nadi S","The Android Update Problem: An Empirical Study","","2018","","","220–230","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196434;http://dx.doi.org/10.1145/3196398.3196434","10.1145/3196398.3196434","Many phone vendors use Android as their underlying OS, but often extend it to add new functionality and to make it compatible with their specific phones. When a new version of Android is released, phone vendors need to merge or re-apply their customizations and changes to the new release. This is a difficult and time-consuming process, which often leads to late adoption of new versions. In this paper, we perform an empirical study to understand the nature of changes that phone vendors make, versus changes made in the original development of Android. By investigating the overlap of different changes, we also determine the possibility of having automated support for merging them. We develop a publicly available tool chain, based on a combination of existing tools, to study such changes and their overlap. As a proxy case study, we analyze the changes in the popular community-based variant of Android, LineageOS, and its corresponding Android versions. We investigate and report the common types of changes that occur in practice. Our findings show that 83% of subsystems modified by LineageOS are also modified in the next release of Android. By taking the nature of overlapping changes into account, we assess the feasibility of having automated tool support to help phone vendors with the Android update problem. Our results show that 56% of the changes in LineageOS have the potential to be safely automated.","merge conflicts, software evolution, software merging, Android","","MSR '18"
"Conference Paper","Rahman MM,Barson J,Paul S,Kayani J,Lois FA,Quezada SF,Parnin C,Stolee KT,Ray B","Evaluating How Developers Use General-Purpose Web-Search for Code Retrieval","","2018","","","465–475","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Mining Software Repositories","Gothenburg, Sweden","2018","9781450357166","","https://doi.org/10.1145/3196398.3196425;http://dx.doi.org/10.1145/3196398.3196425","10.1145/3196398.3196425","Search is an integral part of a software development process. Developers often use search engines to look for information during development, including reusable code snippets, API understanding, and reference examples. Developers tend to prefer general-purpose search engines like Google, which are often not optimized for code related documents and use search strategies and ranking techniques that are more optimized for generic, non-code related information.In this paper, we explore whether a general purpose search engine like Google is an optimal choice for code-related searches. In particular, we investigate whether the performance of searching with Google varies for code vs. non-code related searches. To analyze this, we collect search logs from 310 developers that contains nearly 150,000 search queries from Google and the associated result clicks. To differentiate between code-related searches and non-code-related searches, we build a model which identifies the code intent of queries. Leveraging this model, we build an automatic classifier that detects a code and non-code related query. We confirm the effectiveness of the classifier on manually annotated queries where the classifier achieves a precision of 87%, a recall of 86%, and an F1-score of 87%. We apply this classifier to automatically annotate all the queries in the dataset. Analyzing this dataset, we observe that code related searching often requires more effort (e.g., time, result clicks, and query modifications) than general non-code search, which indicates code search performance with a general search engine is less effective.","","","MSR '18"
"Conference Paper","Schramm C,Wang Y,Bry F","Codekōan: A Source Code Pattern Search Engine Extracting Crowd Knowledge","","2018","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Crowd Sourcing in Software Engineering","Gothenburg, Sweden","2018","9781450357333","","https://doi.org/10.1145/3195863.3195864;http://dx.doi.org/10.1145/3195863.3195864","10.1145/3195863.3195864","Source code search is frequently needed and important in software development. Keyword search for source code is a widely used but a limited approach. This paper presents CodeKōan, a scalable engine for searching millions of online code examples written by the worldwide programmers' community which uses data parallel processing to achieve horizontal scalability. The search engine relies on a token-based, programming language independent algorithm and, as a proof-of-concept, indexes all code examples from Stack Overflow for two programming languages: Java and Python. This paper demonstrates the benefits of extracting crowd knowledge from Stack Overflow by analyzing well-known open source repositories such as OpenNLP and Elasticsearch: Up to one third of the source code in the examined repositories reuses code patterns from Stack Overflow. It also shows that the proposed approach recognizes similar source code and is resilient to modifications such as insertion, deletion and swapping of statements. Furthermore, evidence is given that the proposed approach returns very few false positives among the search results.","mining crowd knowledge, search algorithm, code patterns, stack overflow, source code search, source code similarity","","CSI-SE '18"
"Conference Paper","Wang P,Svajlenko J,Wu Y,Xu Y,Roy CK","CCAligner: A Token Based Large-Gap Clone Detector","","2018","","","1066–1077","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180179;http://dx.doi.org/10.1145/3180155.3180179","10.1145/3180155.3180179","Copying code and then pasting with large number of edits is a common activity in software development, and the pasted code is a kind of complicated Type-3 clone. Due to large number of edits, we consider the clone as a large-gap clone. Large-gap clone can reflect the extension of code, such as change and improvement. The existing state-of-the-art clone detectors suffer from several limitations in detecting large-gap clones. In this paper, we propose a tool, CCAligner, using code window that considers e edit distance for matching to detect large-gap clones. In our approach, a novel e-mismatch index is designed and the asymmetric similarity coefficient is used for similarity measure. We thoroughly evaluate CCAligner both for large-gap clone detection, and for general Type-1, Type-2 and Type-3 clone detection. The results show that CCAligner performs better than other competing tools in large-gap clone detection, and has the best execution time for 10MLOC input with good precision and recall in general Type-1 to Type-3 clone detection. Compared with existing state-of-the-art tools, CCAligner is the best performing large-gap clone detection tool, and remains competitive with the best clone detectors in general Type-1, Type-2 and Type-3 clone detection.","evaluation, large-gap clone, clone detection","","ICSE '18"
"Conference Paper","Upadhyaya G,Rajan H","Collective Program Analysis","","2018","","","620–631","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180252;http://dx.doi.org/10.1145/3180155.3180252","10.1145/3180155.3180252","Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis (CPA), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69% reduction when compared to a baseline and on average a 36% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.","clustering, Boa, source code analysis","","ICSE '18"
"Conference Paper","Kim K,Kim D,Bissyandé TF,Choi E,Li L,Klein J,Traon YL","FaCoY: A Code-to-Code Search Engine","","2018","","","946–957","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180187;http://dx.doi.org/10.1145/3180155.3180187","10.1145/3180155.3180187","Code search is an unavoidable activity in software development. Various approaches and techniques have been explored in the literature to support code search tasks. Most of these approaches focus on serving user queries provided as natural language free-form input. However, there exists a wide range of use-case scenarios where a code-to-code approach would be most beneficial. For example, research directions in code transplantation, code diversity, patch recommendation can leverage a code-to-code search engine to find essential ingredients for their techniques. In this paper, we propose FaCoY, a novel approach for statically finding code fragments which may be semantically similar to user input code. FaCoY implements a query alternation strategy: instead of directly matching code query tokens with code in the search space, FaCoY first attempts to identify other tokens which may also be relevant in implementing the functional behavior of the input code. With various experiments, we show that (1) FaCoY is more effective than online code-to-code search engines; (2) FaCoY can detect more semantic code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; (3) FaCoY, while static, can detect code fragments which are indeed similar with respect to runtime execution behavior; and (4) FaCoY can be useful in code/patch recommendation.","","","ICSE '18"
"Conference Paper","Tsantalis N,Mansouri M,Eshkevari LM,Mazinanian D,Dig D","Accurate and Efficient Refactoring Detection in Commit History","","2018","","","483–494","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180206;http://dx.doi.org/10.1145/3180155.3180206","10.1145/3180155.3180206","Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios.To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.","abstract syntax tree, Oracle, refactoring, Git, commit, accuracy","","ICSE '18"
"Conference Paper","Gu X,Zhang H,Kim S","Deep Code Search","","2018","","","933–944","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180167;http://dx.doi.org/10.1145/3180155.3180167","10.1145/3180155.3180167","To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.","deep learning, code search, joint embedding","","ICSE '18"
"Conference Paper","Bui ND,Jiang L","Hierarchical Learning of Cross-Language Mappings through Distributed Vector Representations for Code","","2018","","","33–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results","Gothenburg, Sweden","2018","9781450356626","","https://doi.org/10.1145/3183399.3183427;http://dx.doi.org/10.1145/3183399.3183427","10.1145/3183399.3183427","Translating a program written in one programming language to another can be useful for software development tasks that need functionality implementations in different languages. Although past studies have considered this problem, they may be either specific to the language grammars, or specific to certain kinds of code elements (e.g., tokens, phrases, API uses). This paper proposes a new approach to automatically learn cross-language representations for various kinds of structural code elements that may be used for program translation. Our key idea is two folded: First, we normalize and enrich code token streams with additional structural and semantic information, and train cross-language vector representations for the tokens (a.k.a. shared embeddings based on word2vec, a neural-network-based technique for producing word embeddings; Second, hierarchically from bottom up, we construct shared embeddings for code elements of higher levels of granularity (e.g., expressions, statements, methods) from the embeddings for their constituents, and then build mappings among code elements across languages based on similarities among embeddings.Our preliminary evaluations on about 40,000 Java and C# source files from 9 software projects show that our approach can automatically learn shared embeddings for various code elements in different languages and identify their cross-language mappings with reasonable Mean Average Precision scores. When compared with an existing tool for mapping library API methods, our approach identifies many more mappings accurately. The mapping results and code can be accessed at https://github.com/bdqnghi/hierarchical-programming-language-mapping. We believe that our idea for learning cross-language vector representations with code structural information can be a useful step towards automated program translation.","language mapping, software maintenance, program translation, syntactic structure, word2vec","","ICSE-NIER '18"
"Conference Paper","Lu Y,Chaudhuri S,Jermaine C,Melski D","Program Splicing","","2018","","","338–349","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180190;http://dx.doi.org/10.1145/3180155.3180190","10.1145/3180155.3180190","We introduce program splicing, a programming methodology that aims to automate the workflow of copying, pasting, and modifying code available online. Here, the programmer starts by writing a ""draft"" that mixes unfinished code, natural language comments, and correctness requirements. A program synthesizer that interacts with a large, searchable database of program snippets is used to automatically complete the draft into a program that meets the requirements. The synthesis process happens in two stages. First, the synthesizer identifies a small number of programs in the database that are relevant to the synthesis task. Next it uses an enumerative search to systematically fill the draft with expressions and statements from these relevant programs. The resulting program is returned to the programmer, who can modify it and possibly invoke additional rounds of synthesis.We present an implementation of program splicing, called Splicer, for the Java programming language. Splicer uses a corpus of over 3.5 million procedures from an open-source software repository. Our evaluation uses the system in a suite of everyday programming tasks, and includes a comparison with a state-of-the-art competing approach as well as a user study. The results point to the broad scope and scalability of program splicing and indicate that the approach can significantly boost programmer productivity.","","","ICSE '18"
"Conference Paper","Hora A,Silva D,Valente MT,Robbes R","Assessing the Threat of Untracked Changes in Software Evolution","","2018","","","1102–1113","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180212;http://dx.doi.org/10.1145/3180155.3180212","10.1145/3180155.3180212","While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies.","mining software repositories, software evolution, refactoring","","ICSE '18"
"Conference Paper","An K,Meng N,Tilevich E","Automatic Inference of Java-to-Swift Translation Rules for Porting Mobile Applications","","2018","","","180–190","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Mobile Software Engineering and Systems","Gothenburg, Sweden","2018","9781450357128","","https://doi.org/10.1145/3197231.3197240;http://dx.doi.org/10.1145/3197231.3197240","10.1145/3197231.3197240","A native cross-platform mobile app has multiple platform-specific implementations. Typically, an app is developed for one platform and then ported to the remaining ones. Translating an app from one language (e.g., Java) to another (e.g., Swift) by hand is tedious and error-prone, while automated translators either require manually defined translation rules or focus on translating APIs. To automate the translation of native cross-platform apps, we present J2SINFERER, a novel approach that iteratively infers syntactic transformation rules and API mappings from Java to Swift. Given a software corpus in both languages, J2SLNFERER first identifies the syntactically equivalent code based on braces and string similarity. For each pair of similar code segments, J2SLNFERER then creates syntax trees of both languages, leveraging the minimalist domain knowledge of language correspondence (e.g., operators and markers) to iteratively align syntax tree nodes, and to infer both syntax and API mapping rules. J2SLNFERER represents inferred rules as string templates, stored in a database, to translate code from Java to Swift. We evaluated J2SLNFERER with four applications, using one part of the data to infer translation rules, and the other part to apply the rules. With 76% in-project accuracy and 65% cross-project accuracy, J2SLNFERER outperforms in accuracy j2swift, a state-of-the-art Java-to-Swift conversion tool. As native cross-platform mobile apps grow in popularity, J2SLNFERER can shorten their time to market by automating the tedious and error prone task of source-to-source translation.","","","MOBILESoft '18"
"Conference Paper","Hassan F,Wang X","HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts","","2018","","","1078–1089","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180181;http://dx.doi.org/10.1145/3180155.3180181","10.1145/3180155.3180181","Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: History-Driven Repair of Build Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes.","patch generation, build logs, software build scripts","","ICSE '18"
"Conference Paper","Wang Y,Wu H,Zhang H,Rountev A","ORLIS: Obfuscation-Resilient Library Detection for Android","","2018","","","13–23","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Mobile Software Engineering and Systems","Gothenburg, Sweden","2018","9781450357128","","https://doi.org/10.1145/3197231.3197248;http://dx.doi.org/10.1145/3197231.3197248","10.1145/3197231.3197248","Android apps often contain third-party libraries. For many program analyses, it is important to identify the library code in a given closed-source Android app. There are several clients of such library detection, including security analysis, clone/repackage detection, and library removal/isolation. However, library detection is complicated significantly by commonly-used code obfuscation techniques for Android. Although some of the state-of-the-art library detection tools are intended to be resilient to obfuscation, there is still room to improve recall, precision, and analysis cost.We propose a new approach to detect third-party libraries in obfuscated apps. The approach relies on obfuscation-resilient code features derived from the interprocedural structure and behavior of the app (e.g., call graphs of methods). The design of our approach is informed by close examination of the code features preserved by typical Android obfuscators. To reduce analysis cost, we use similarity digests as an efficient mechanism for identifying a small number of likely matches. We implemented this approach in the ORLIS library detection tool. As demonstrated by our experimental results, ORLIS advances the state of the art and presents an attractive choice for detection of third-party libraries in Android apps.","Android, obfuscation, static analysis, library detection, library identification","","MOBILESoft '18"
"Conference Paper","Hu Z,Song Y,Gehringer EF","Open-Source Software in Class: Students' Common Mistakes","","2018","","","40–48","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering: Software Engineering Education and Training","Gothenburg, Sweden","2018","9781450356602","","https://doi.org/10.1145/3183377.3183394;http://dx.doi.org/10.1145/3183377.3183394","10.1145/3183377.3183394","Introducing Open Source Software (OSS) projects into a software-engineering course has many advantages, for instance, allowing students to learn good coding practices from real-world projects, and giving students a glimpse of a real project. However, it is not easy for instructors to induce one or more OSS core teams to lend support for course projects. The alternative is to have students work on ""toy features""---features for these projects not specified by OSS core teams, but by teaching staff. However, the project may be unimportant to the OSS project or may disrupt its design, making those code contributions unlikely to be integrated into the OSS code repository. In this paper, we, as both teaching staff and the core team for one OSS project called Expertiza, discuss our experience in supporting 700 students on 313 OSS-based course projects in the past five years. We manually checked these course projects, and summarize 13 common mistakes that frequently occur in students' contributions, such as not following the existing design or messy pull requests. We propose five suggestions to help students reduce the frequency of common mistakes and improve the quality of their OSS pull requests.","open-source curriculum, open-source software, software engineering, expertiza","","ICSE-SEET '18"
"Conference Paper","Lavazza L,Morasca S,Tosi D","Technical Debt as an External Software Attribute","","2018","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 International Conference on Technical Debt","Gothenburg, Sweden","2018","9781450357135","","https://doi.org/10.1145/3194164.3194168;http://dx.doi.org/10.1145/3194164.3194168","10.1145/3194164.3194168","Background: Technical debt is currently receiving increasing attention from practitioners and researchers. Several metaphors, concepts, and indications concerning technical debt have been introduced, but no agreement exists about a solid definition of technical debt.Objective: We aim at providing a solid basis to the definition of technical debt and the way it should be quantified.Method: We view technical debt as a software quality attribute and therefore we use Measurement Theory, the general reference framework for the quantification of attributes, to define technical debt and its characteristics in a rigorous way.Results: We show that technical debt should be defined as an external software quality attribute. Therefore, it should be quantified via statistical and machine-learning models whose independent variables are internal software quality attributes. Different models may exist, depending on the specific needs and goals of the software product and development environment. Also, technical debt is a multifaceted concept, so different kinds of technical debt exist, related to different quality attributes, such as performance, usability, and maintainability. These different kinds of technical debt should be evaluated individually, so one can better focus on the specific quality issues that need to be addressed.Conclusions: We show that, to provide it with a rigorous basis, technical debt should be considered and measured as an external software attribute. Researchers and practitioners should build models for technical debt and use them to (1) assess the extent of the technical debt and (2) investigate and assess different ways of modifying software to repay technical debt.","technical debt, software quality","","TechDebt '18"
"Conference Paper","Kubelka J,Robbes R,Bergel A","The Road to Live Programming: Insights from the Practice","","2018","","","1090–1101","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180200;http://dx.doi.org/10.1145/3180155.3180200","10.1145/3180155.3180200","Live Programming environments allow programmers to get feedback instantly while changing software. Liveness is gaining attention among industrial and open-source communities; several IDEs offer high degrees of liveness. While several studies looked at how programmers work during software evolution tasks, none of them consider live environments. We conduct such a study based on an analysis of 17 programming sessions of practitioners using Pharo, a mature Live Programming environment. The study is complemented by a survey and subsequent analysis of 16 programming sessions in additional languages, e.g., JavaScript. We document the approaches taken by developers during their work. We find that some liveness features are extensively used, and have an impact on the way developers navigate source code and objects in their work.","software evolution, live programming, exploratory study","","ICSE '18"
"Conference Paper","Bai X,Li M,Pei D,Li S,Ye D","Continuous Delivery of Personalized Assessment and Feedback in Agile Software Engineering Projects","","2018","","","58–67","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering: Software Engineering Education and Training","Gothenburg, Sweden","2018","9781450356602","","https://doi.org/10.1145/3183377.3183387;http://dx.doi.org/10.1145/3183377.3183387","10.1145/3183377.3183387","In recent years, Agile development has been adopted in project practices of Software Engineering (SE) courses. However, it is a great challenge to provide timely assessment and feedback to project teams and individual students with a frequency that catches up with iterative, incremental, and cooperative software development with continuous deliveries. Conventional project reviews are mostly dependent upon instructors and teaching assistants in a manual reviewing/mentoring approach, which are simply not scalable.In this paper, we argue that agile projects warrant a ""continuous delivery"" of personalized assessment and feedback. To this end, we propose an online-offline combined approach and built a system upon GitLab. An online platform was built by integrating DevOps tool chains so that personalized reports and assessments are delivered automatically to the teams/students, which serve as the very efficient trigger and basis for the close and targeted offline interactions between students and TAs: daily discussion over instant messaging and weekly in person meeting. This system has been in operation since 2014 for an undergraduate SE course, with over 500 students participating in over 130 project teams in total. Our results show that such a continuous assessment/feedback delivery system is very effective in educating Agile projects in SE courses.","project, devops, assessment, software engineering course, agile","","ICSE-SEET '18"
"Conference Paper","Wang P,Bao Q,Wang L,Wang S,Chen Z,Wei T,Wu D","Software Protection on the Go: A Large-Scale Empirical Study on Mobile App Obfuscation","","2018","","","26–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180169;http://dx.doi.org/10.1145/3180155.3180169","10.1145/3180155.3180169","The prosperity of smartphone markets has raised new concerns about software security on mobile platforms, leading to a growing demand for effective software obfuscation techniques. Due to various differences between the mobile and desktop ecosystems, obfuscation faces both technical and non-technical challenges when applied to mobile software. Although there have been quite a few software security solution providers launching their mobile app obfuscation services, it is yet unclear how real-world mobile developers perform obfuscation as part of their software engineering practices.Our research takes a first step to systematically studying the deployment of software obfuscation techniques in mobile software development. With the help of an automated but coarse-grained method, we computed the likelihood of an app being obfuscated for over a million app samples crawled from Apple App Store. We then inspected the top 6600 instances and managed to identify 601 obfuscated versions of 539 iOS apps. By analyzing this sample set with extensive manual effort, we made various observations that reveal the status quo of mobile obfuscation in the real world, providing insights into understanding and improving the situation of software protection on mobile platforms.","reverse engineering, empirical study, obfuscation, mobile app","","ICSE '18"
"Conference Paper","Bogner J,Fritzsch J,Wagner S,Zimmermann A","Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems","","2018","","","125–133","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 International Conference on Technical Debt","Gothenburg, Sweden","2018","9781450357135","","https://doi.org/10.1145/3194164.3194166;http://dx.doi.org/10.1145/3194164.3194166","10.1145/3194164.3194166","Maintainability assurance techniques are used to control this quality attribute and limit the accumulation of potentially unknown technical debt. Since the industry state of practice and especially the handling of Service- and Microservice-Based Systems in this regard are not well covered in scientific literature, we created a survey to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related treatment of systems based on service-orientation, and c) influences on developer satisfaction w.r.t. maintainability. 60 software professionals responded to our online questionnaire. The results indicate that using explicit and systematic techniques has benefits for maintainability. The more sophisticated the applied methods the more satisfied participants were with the maintainability of their software while no link to a hindrance in productivity could be established. Other important findings were the absence of architecture-level evolvability control mechanisms as well as a significant neglect of service-oriented particularities for quality assurance. The results suggest that industry has to improve its quality control in these regards to avoid problems with long-living service-based software systems.","software quality control, service-based systems, microservice-based systems, maintainability, survey, industry","","TechDebt '18"
"Conference Paper","Peruma A,Krutz DE","Understanding the Relationship between Quality and Security: A Large-Scale Analysis of Android Applications","","2018","","","19–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Workshop on Security Awareness from Design to Deployment","Gothenburg, Sweden","2018","9781450357272","","https://doi.org/10.1145/3194707.3194711;http://dx.doi.org/10.1145/3194707.3194711","10.1145/3194707.3194711","Android applications (apps) are not immune to the problems which also plague conventional software including security vulnerabilities, quality defects, permission misuse, and numerous other issues. Many developers even intentionally create vulnerable or malicious apps (malware) for often highly lucrative purposes. We need to better understand current trends in app quality and security to create higher quality software, and more effectively battle malware. To gather this critical information, we collected and reverse engineered 70,785 Android apps from the Google Play store, along with 1,420 malicious apps from other sources. Each app was analyzed using several static analysis tools to record a variety of information about each of them including requested permissions, size (LOC), possible defects and permission misuse. Our findings conclude that: 1) app categories substantially differ in terms of permissions misuse; 2) at an aggregate level, there is no significant correlation between an app's quality and security; 3) that malware typically requests more permissions and suffers from several quality-related metrics in comparison to benign apps; 4) that malware and benign apps are growing annually both in terms of LOC and requested permissions. We also present an easy to use, robust dataset for the community to replicate or extend this study.","Android development, mobile security, reverse engineering, mobile malware","","SEAD '18"
"Conference Paper","Spadini D,Aniche M,Storey MA,Bruntink M,Bacchelli A","When Testing Meets Code Review: Why and How Developers Review Tests","","2018","","","677–687","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180192;http://dx.doi.org/10.1145/3180155.3180192","10.1145/3180155.3180192","Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.","automated testing, gerrit, software testing, code review","","ICSE '18"
"Conference Paper","Zhang T,Upadhyaya G,Reinhardt A,Rajan H,Kim M","Are Code Examples on an Online Q&A Forum Reliable? A Study of API Misuse on Stack Overflow","","2018","","","886–896","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180260;http://dx.doi.org/10.1145/3180155.3180260","10.1145/3180155.3180260","Programmers often consult an online Q&A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons---missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.","online Q&A forum, code example assessment, API usage pattern","","ICSE '18"
"Conference Paper","Vahabzadeh A,Stocco A,Mesbah A","Fine-Grained Test Minimization","","2018","","","210–221","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180203;http://dx.doi.org/10.1145/3180155.3180203","10.1145/3180155.3180203","As a software system evolves, its test suite can accumulate redundancies over time. Test minimization aims at removing redundant test cases. However, current techniques remove whole test cases from the test suite using test adequacy criteria, such as code coverage. This has two limitations, namely (1) by removing a whole test case the corresponding test assertions are also lost, which can inhibit test suite effectiveness, (2) the issue of partly redundant test cases, i.e., tests with redundant test statements, is ignored. We propose a novel approach for fine-grained test case minimization. Our analysis is based on the inference of a test suite model that enables automated test reorganization within test cases. It enables removing redundancies at the test statement level, while preserving the coverage and test assertions of the test suite. We evaluated our approach, implemented in a tool called Testler, on the test suites of 15 open source projects. Our analysis shows that over 4,639 (24%) of the tests in these test suites are partly redundant, with over 11,819 redundant test statements in total. Our results show that Testler removes 43% of the redundant test statements, reducing the number of partly redundant tests by 52%. As a result, test suite execution time is reduced by up to 37% (20% on average), while maintaining the original statement coverage, branch coverage, test assertions, and fault detection capability.","test redundancy, test reduction, test model, test minimization","","ICSE '18"
"Conference Paper","Hammad M,Garcia J,Malek S","A Large-Scale Empirical Study on the Effects of Code Obfuscations on Android Apps and Anti-Malware Products","","2018","","","421–431","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180228;http://dx.doi.org/10.1145/3180155.3180228","10.1145/3180155.3180228","The Android platform has been the dominant mobile platform in recent years resulting in millions of apps and security threats against those apps. Anti-malware products aim to protect smartphone users from these threats, especially from malicious apps. However, malware authors use code obfuscation on their apps to evade detection by anti-malware products. To assess the effects of code obfuscation on Android apps and anti-malware products, we have conducted a large-scale empirical study that evaluates the effectiveness of the top anti-malware products against various obfuscation tools and strategies. To that end, we have obfuscated 3,000 benign apps and 3,000 malicious apps and generated 73,362 obfuscated apps using 29 obfuscation strategies from 7 open-source, academic, and commercial obfuscation tools. The findings of our study indicate that (1) code obfuscation significantly impacts Android anti-malware products; (2) the majority of anti-malware products are severely impacted by even trivial obfuscations; (3) in general, combined obfuscation strategies do not successfully evade anti-malware products more than individual strategies; (4) the detection of anti-malware products depend not only on the applied obfuscation strategy but also on the leveraged obfuscation tool; (5) anti-malware products are slow to adopt signatures of malicious apps; and (6) code obfuscation often results in changes to an app's semantic behaviors.","","","ICSE '18"
"Conference Paper","Wang K,Zhu C,Celik A,Kim J,Batory D,Gligoric M","Towards Refactoring-Aware Regression Test Selection","","2018","","","233–244","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180254;http://dx.doi.org/10.1145/3180155.3180254","10.1145/3180155.3180254","Regression testing checks that recent project changes do not break previously working functionality. Although important, regression testing is costly when changes are frequent. Regression test selection (RTS) optimizes regression testing by running only tests whose results might be affected by a change. Traditionally, RTS collects dependencies (e.g., on files) for each test and skips the tests, at a new project revision, whose dependencies did not change. Existing RTS techniques do not differentiate behavior-preserving transformations (i.e., refactorings) from other code changes. As a result, tests are run more frequently than necessary.We present the first step towards a refactoring-aware RTS technique, dubbed Reks, which skips tests affected only by behavior-preserving changes. Reks defines rules to update the test dependencies without running the tests. To ensure that Reks does not hide any bug introduced by the refactoring engines, we integrate Reks only in the pre-submit testing phase, which happens on the developers' machines. We evaluate Reks by measuring the savings in the testing effort. Specifically, we reproduce 100 refactoring tasks performed by developers of 37 projects on GitHub. Our results show that Reks would not run, on average, 33% of available tests (that would be run by a refactoring-unaware RTS technique). Additionally, we systematically run 27 refactoring types on ten projects. The results, based on 74,160 refactoring tasks, show that Reks would not run, on average, 16% of tests (max: 97% and SD: 24%). Finally, our results show that the Reks update rules are efficient.","Reks, regression test selection, behavior-preserving changes","","ICSE '18"
"Conference Paper","Choi W,Sen K,Necula G,Wang W","DetReduce: Minimizing Android GUI Test Suites for Regression Testing","","2018","","","445–455","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th International Conference on Software Engineering","Gothenburg, Sweden","2018","9781450356381","","https://doi.org/10.1145/3180155.3180173;http://dx.doi.org/10.1145/3180155.3180173","10.1145/3180155.3180173","In recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing.We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of 16.9× in size and 14.7× in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours.","test minimization, Android, GUI","","ICSE '18"
"Journal Article","Hammari E,Kjeldsberg PG,Catthoor F","Runtime Precomputation of Data-Dependent Parameters in Embedded Systems","ACM Trans. Embed. Comput. Syst.","2018","17","3","","Association for Computing Machinery","New York, NY, USA","","","2018-05","","1539-9087","https://doi.org/10.1145/3191311;http://dx.doi.org/10.1145/3191311","10.1145/3191311","In many modern embedded systems, the available resources (e.g., CPU clock cycles, memory, and energy) are consumed nonuniformly while the system is under exploitation. Typically, the resource requirements in the system change with different input data that the system process. These data trigger different parts of the embedded software, resulting in different operations executed that require different hardware platform resources to be used. A significant research effort has been dedicated to develop mechanisms for runtime resource management (e.g., branch prediction for pipelined processors, prefetching of data from main memory to cache, and scenario-based design methodologies). All these techniques rely on the availability of information at runtime about upcoming changes in resource requirements. In this article, we propose a method for detecting upcoming resource changes based on preliminary calculation of software variables that have the most dynamic impact on resource requirements in the system. We apply the method on a modified real-life biomedical algorithm with real input data and estimate a 40% energy reduction as compared to static DVFS scheduling. Comparing to dynamic DVFS scheduling, an 18% energy reduction is demonstrated.","Dynamic embedded system, run-time resource management, parameter precomputation","",""
"Conference Paper","Díaz J,Almaraz R,Pérez J,Garbajosa J","DevOps in Practice: An Exploratory Case Study","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Agile Software Development: Companion","Porto, Portugal","2018","9781450364225","","https://doi.org/10.1145/3234152.3234199;http://dx.doi.org/10.1145/3234152.3234199","10.1145/3234152.3234199","DevOps is a cultural movement and technical solution that plays a fundamental role for software-intensive organizations whose business greatly depends on how efficient development and operation are. DevOps is relatively recent, and thus little is known about best practices and the real value and barriers associated with DevOps in industry. To conduct an analysis on practicing DevOps in various software development companies in order to provide patterns of DevOps practices and identify their benefits and barriers. An exploratory case study based on the interviews to relevant stakeholders of 11 (multinational) software-intensive companies. The study is currently ongoing. This study aims to help practitioners and researchers to better understand some DevOps improvement practices as well as real DevOps projects and the contexts where the practices worked, and benefits and barriers appeared. This, hopefully, will contribute to strengthening the evidence regarding DevOps and supporting practitioners in making better informed decisions about the ROI of introducing DevOps.","empirical software engineering, exploratory case study, DevOps","","XP '18"
"Conference Paper","Jiang Y,Xu C","Needle: Detecting Code Plagiarism on Student Submissions","","2018","","","27–32","Association for Computing Machinery","New York, NY, USA","Proceedings of ACM Turing Celebration Conference - China","Shanghai, China","2018","9781450364157","","https://doi.org/10.1145/3210713.3210724;http://dx.doi.org/10.1145/3210713.3210724","10.1145/3210713.3210724","Code plagiarism is one of the most prevalent academic dishonesty activities in programming practicums. Automated code plagiarism detection plays an important role in preventing plagiarism and maintaining the academic integrity. This paper describes a novel code plagiarism detection algorithm needle, which is based on the network-flow approximation of editing distances between programs. This paper also presents the effectiveness and efficiency evaluation of the algorithm, the lessons and experiences learned from applying needle in practice, and discussions of the future challenges.","","","TURC '18"
"Conference Paper","Zhou W,Pan Y,Zhou Y,Sun G","The Framework of a New Online Judge System for Programming Education","","2018","","","9–14","Association for Computing Machinery","New York, NY, USA","Proceedings of ACM Turing Celebration Conference - China","Shanghai, China","2018","9781450364157","","https://doi.org/10.1145/3210713.3210721;http://dx.doi.org/10.1145/3210713.3210721","10.1145/3210713.3210721","An OJ (Online Judge) system is a web software for compiling, executing and evaluating programs submitted by users. OJ systems were originally used in programming competitions. Since systems are high-efficiency and suitable for different-level programming learners, they are widely used in programming education at present. We have developed an OJ system and applied it to the course of Programming in C language since the year of 2007. We found that the system did help improve the programming ability of students. However, a traditional OJ system still has some shortcoming for programming education, such as it grade programs only based on the number of test cases passed, regardless of the code quality of programs. In this paper, we propose the framework of a novel OJ system for programming education. The new OJ system contains four modules: personalized feedback for students, code quality checking, code similarity checking, and advising on teaching adjustment.","personalized feedback, programming course, code quality, online judge","","TURC '18"
"Conference Paper","Welton B,Miller BP","Exposing Hidden Performance Opportunities in High Performance GPU Applications","","2018","","","301–310","IEEE Press","Washington, District of Columbia","Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","","2018","9781538658154","","https://doi.org/10.1109/CCGRID.2018.00045;http://dx.doi.org/10.1109/CCGRID.2018.00045","10.1109/CCGRID.2018.00045","Leadership class systems with nodes containing many-core accelerators, such as GPUs, have the potential to increase the performance of applications. Effectively exploiting the parallelism provided by many-core accelerators requires developers to identify where accelerator parallelization would provide benefit and ensuring efficient interaction between the CPU and accelerator. In the abstract, these issues appear straightforward and well understood. However, we have found that significant untapped performance opportunities exist in these areas even in well-known, heavily optimized, real world applications created by experienced GPU developers. These untapped performance opportunities exist because accelerated libraries can create unexpected synchronization delay and memory transfer requests, interaction between accelerated libraries can cause unexpected inefficiencies when combined, and vectorization opportunities can be hidden by the structure of the program. In applications we have studied (Qball, QBox, Hoomd-blue, LAMMPs, and cuIBM), exploiting these opportunities resulted in reduction of their execution time by 18%-87%. In this work, we provide concrete evidence of the existence and impact that these performance issues have on real world applications today. We characterize the missed performance opportunities we have identified by their underlying cause and describe a preliminary design of detection methods that can be used by performance tools to identify these missed opportunities.","distributed computing, performance tools, many-core processing, hidden performance opportunities, graphics processing unit","","CCGrid '18"
"Journal Article","Antenucci D,Cafarella M","Constraint-Based Explanation and Repair of Filter-Based Transformations","Proc. VLDB Endow.","2018","11","9","947–960","VLDB Endowment","","","","2018-05","","2150-8097","https://doi.org/10.14778/3213880.3213886;http://dx.doi.org/10.14778/3213880.3213886","10.14778/3213880.3213886","Data analysts often need to transform an existing dataset, such as with filtering, into a new dataset for downstream analysis. Even the most trivial of mistakes in this phase can introduce bias and lead to the formation of invalid conclusions. For example, consider a researcher identifying subjects for trials of a new statin drug. She might identify patients with a high dietary cholesterol intake as a population likely to benefit from the drug, however, selection of these individuals could bias the test population to those with a generally unhealthy lifestyle, thereby compromising the analysis. Reducing the potential for bias in the dataset transformation process can minimize the need to later engage in the tedious, time-consuming process of trying to eliminate bias while preserving the target dataset.We propose a novel interaction model for explain-and-repair data transformation systems, in which users inter-actively define constraints for transformation code and the resultant data. The system satisfies these constraints as far as possible, and provides an explanation for any problems encountered. We present an algorithm that yields filter-based transformation code satisfying user constraints. We implemented and evaluated a prototype of this architecture, Emeril, using both synthetic and real-world datasets. Our approach finds solutions 34% more often and 77% more quickly than the previous state-of-the-art solution.","","",""
"Conference Paper","Yang Z,Xu F,Chen Z,Sun Y","An Effective Incremental Analysis Algorithm of Open Source Repository","","2018","","","178–182","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 1st International Conference on Internet and E-Business","Singapore, Singapore","2018","9781450363754","","https://doi.org/10.1145/3230348.3230461;http://dx.doi.org/10.1145/3230348.3230461","10.1145/3230348.3230461","Mining code repositories is a significant practice to detect the reuse of open source code in software engineering. As the existing analysis algorithms fail to meet the requirements of efficient large-scale open source analysis, this paper proposes an effective incremental analysis algorithm that can extract incremental text modifications by comparing snapshots in code repositories and then transform the above-mentioned text modifications into incremental functions through the application of mapping algorithm. Meanwhile, a fingerprint algorithm targeting incremental functions is put forward for efficient comparisons of functions. The experiments indicate that when dealing with large-scale code, the incremental analysis algorithm outperforms the traditional ones with less storage space and faster analysis speed.","Open source, incremental analysis, code repository","","ICIEB '18"
"Conference Paper","Rosenblatt L,Carrington P,Hara K,Bigham JP","Vocal Programming for People with Upper-Body Motor Impairments","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Web for All Conference","Lyon, France","2018","9781450356510","","https://doi.org/10.1145/3192714.3192821;http://dx.doi.org/10.1145/3192714.3192821","10.1145/3192714.3192821","Programming heavily relies on entering text using traditional QWERTY keyboards, which poses challenges for people with limited upper-body movement. Developing tools using a publicly available speech recognition API could provide a basis for keyboard free programming. In this paper, we describe our efforts in design, development, and evaluation of a voice-based IDE to support people with limited dexterity. We report on a formative Wizard of Oz (WOz) based design process to gain an understanding of how people would use and what they expect from a speech-based programming environment. Informed by the findings from the WOz, we developed VocalIDE, a prototype speech-based IDE with features such as Context Color Editing that facilitates vocal programming. Finally, we evaluate the utility of VocalIDE with 8 participants who have upper limb motor impairments. The study showed that VocalIDE significantly improves the participants' ability to make navigational edits and select text while programming.","Speech recognition, upper-limb impairment, Cerebral Palsy, programming tools","","W4A '18"
"Conference Paper","Chang M,Guillain LV,Jung H,Hare VM,Kim J,Agrawala M","RecipeScape: An Interactive Tool for Analyzing Cooking Instructions at Scale","","2018","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","Montreal QC, Canada","2018","9781450356206","","https://doi.org/10.1145/3173574.3174025;http://dx.doi.org/10.1145/3173574.3174025","10.1145/3173574.3174025","For cooking professionals and culinary students, understanding cooking instructions is an essential yet demanding task. Common tasks include categorizing different approaches to cooking a dish and identifying usage patterns of particular ingredients or cooking methods, all of which require extensive browsing and comparison of multiple recipes. However, no existing system provides support for such in-depth and at-scale analysis. We present RecipeScape, an interactive system for browsing and analyzing the hundreds of recipes of a single dish available online. We also introduce a computational pipeline that extracts cooking processes from recipe text and calculates a procedural similarity between them. To evaluate how RecipeScape supports culinary analysis at scale, we conducted a user study with cooking professionals and culinary students with 500 recipes for two different dishes. Results show that RecipeScape clusters recipes into distinct approaches, and captures notable usage patterns of ingredients and cooking actions.","culinary analysis, cooking recipes, interactive data mining, naturally crowdsourced data, analysis at scale","","CHI '18"
"Conference Paper","Liu X,Zhang C","DT: An Upgraded Detection Tool to Automatically Detect Two Kinds of Code Smell: Duplicated Code and Feature Envy","","2018","","","6–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Geoinformatics and Data Analysis","Prague, Czech Republic","2018","9781450364454","","https://doi.org/10.1145/3220228.3220245;http://dx.doi.org/10.1145/3220228.3220245","10.1145/3220228.3220245","Code smell is unreasonable programming, and is produced when software developers don't have good habits of development and experience of development and other reasons. Code becomes more and more chaotic, the code structure become bloated. Code smell can make degradation of code quality. It also can make some difficulties for software developers to understand and maintain the source code of projects, and then cause unnecessary maintenance costs.This study presents an evolutionary version of detection tool DT. DT can support detection of two kinds of code smell -Duplicated code and Feature envy. At the same time, two types of code smell are mainly detected by two detection thoughts: dynamic programming algorithm (DP) and abstract grammar tree (AST). DP can be applied in code smell - Duplicated code. DP uses the similarity between comparative lines to determine whether there is duplicated code; AST is used to detect code smell- feature envy, AST use tree structure to represent the source code, the grammatical structure of the source code transforms to each tree node. Through statistical analysis of existence of these nodes, we can determine whether is a kind of code smell or not.In experiment, detection tool DT compares with four famous detection tools Checkstyle, PMD, iPlasma and JDeodorant. Detection accuracy is higher than above detection tools. In addition, these four well-known tools can't support detection of large industrial projects, while DT can support detection of industrial projects. In future work, we want to detect more kinds of code smell, meanwhile we want update detection precision of detection tool.","duplicated code, feature envy, code smell, detection tool","","ICGDA '18"
"Journal Article","Vidal S,Berra I,Zulliani S,Marcos C,Pace JA","Assessing the Refactoring of Brain Methods","ACM Trans. Softw. Eng. Methodol.","2018","27","1","","Association for Computing Machinery","New York, NY, USA","","","2018-04","","1049-331X","https://doi.org/10.1145/3191314;http://dx.doi.org/10.1145/3191314","10.1145/3191314","Code smells are a popular mechanism for identifying structural design problems in software systems. Several tools have emerged to support the detection of code smells and propose some refactorings. However, existing tools do not guarantee that a smell will be automatically fixed by means of refactorings. This article presents Bandago, an automated approach to fix a specific type of code smell called Brain Method. A Brain Method centralizes the intelligence of a class and manifests itself as a long and complex method that is difficult to understand and maintain by developers. For each Brain Method, Bandago recommends several refactoring solutions to remove the smell using a search strategy based on simulated annealing. Our approach has been evaluated with several open-source Java applications, and the results show that Bandago can automatically fix more than 60% of Brain Methods. Furthermore, we conducted a survey with 35 industrial developers that showed evidence about the usefulness of the refactorings proposed by Bandago. Also, we compared the performance of the Bandago against that of a third-party refactoring tool.","long method, brain method, refactoring, Code smells","",""
"Conference Paper","Yao Z,Weld DS,Chen WP,Sun H","StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow","","2018","","","1693–1703","International World Wide Web Conferences Steering Committee","Republic and Canton of Geneva, CHE","Proceedings of the 2018 World Wide Web Conference","Lyon, France","2018","9781450356398","","https://doi.org/10.1145/3178876.3186081;http://dx.doi.org/10.1145/3178876.3186081","10.1145/3178876.3186081","Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of 148K Python and 120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language","deep neural networks, natural language question answering, question-code pairs, stack overflow","","WWW '18"
"Conference Paper","Tsukamoto K,Maezawa Y,Honiden S","AutoPUT: An Automated Technique for Retrofitting Closed Unit Tests into Parameterized Unit Tests","","2018","","","1944–1951","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd Annual ACM Symposium on Applied Computing","Pau, France","2018","9781450351911","","https://doi.org/10.1145/3167132.3167340;http://dx.doi.org/10.1145/3167132.3167340","10.1145/3167132.3167340","Parameterized unit testing is a promising technique for developers to use to facilitate the understanding of test codes. However, as a practical issue, developers might not have sufficient resources to implement parameterized unit tests (PUTs) corresponding to a vast number of closed unit tests (CUTs) in long-term software projects. Although a technique for retrofitting CUTs into PUTs was proposed, it imposes a laborious task on developers to promote parameters in CUTs. In this study, we propose a fully automated CUT-PUT retrofitting technique (called AutoPUT), which detects similar CUTs as PUT candidates by comparing their code structures. It then identifies common procedures and unique parameters to generate PUTs without degradation in terms of code coverage as compared with original CUTs. From the results of our case-study experiments on open-sourced software projects, we found that AutoPUT fully automatically generated 204 PUTs in 8.5 hours. We concluded that AutoPUT can help developers maintain test suites for building reliable software.","Junit, parameterized unit testing, test-suite maintenance","","SAC '18"
"Conference Paper","Spinelli L,Pandey M,Oney S","Attention Patterns for Code Animations: Using Eye Trackers to Evaluate Dynamic Code Presentation Techniques","","2018","","","99–104","Association for Computing Machinery","New York, NY, USA","Companion Proceedings of the 2nd International Conference on the Art, Science, and Engineering of Programming","Nice, France","2018","9781450355131","","https://doi.org/10.1145/3191697.3214338;http://dx.doi.org/10.1145/3191697.3214338","10.1145/3191697.3214338","Programming instructors seek new ways to present code to novice programmers. It is important to understand how these new presentation methods affect students. We prototyped three different ways to animate the presentation of code. We used eye-tracking technology to observe participants as they were presented with animations and completed three activities: code summarization, syntax error correction, and logic error correction. The prototypes, our method for observation, and our analysis methods were each informed by previous research. We observed variation in how participants consumed animations. Our initial results indicate that viewing animations of a single textual representation of source code may affect the attentional processes of novice programmers during subsequent tasks.","Eye Tracking, Programming Education, Program Animation","","Programming '18"
"Conference Paper","Machado M,Choren R","Improving the Detection of Evolutionary Coupling: An Approach Considering Sliding Verification","","2018","","","1410–1416","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd Annual ACM Symposium on Applied Computing","Pau, France","2018","9781450351911","","https://doi.org/10.1145/3167132.3167283;http://dx.doi.org/10.1145/3167132.3167283","10.1145/3167132.3167283","During the evolution of software systems, change tends to make software designs erode over time. Modifications to a system that violate its architectural principles can result in evolutionary coupling between classes. While techniques have been proposed to detect evolutionary coupling, they are difficult to confirm if, indeed, it is a logical dependency acquired. This occurs due to the fact that, normally, the architect uses a single snapshot of the system architecture in the analysis, instead of making sense of the continuous architectural evolution. To address this problem, we argue for using a set of consecutive releases. We consider that using a single unattached release of the system is an inherent obstacle in establishing true evolutionary coupling between architectural elements - the limited view of the evolution process may hinder such analysis. In this paper, we present an approach that considers the concept of sliding verification to calculate the probability two architectural elements change together, improving the detection of evolutionary coupling. A continuous subset of versions are used in such analysis. We also describe the application of the proposed approach in one large Military Command and Control System evolution scenario.","revision history analysis, evolutionary coupling, software architecture, evaluation method","","SAC '18"
"Conference Paper","Sousa BL,Bigonha MA,Ferreira KA","A Systematic Literature Mapping on the Relationship between Design Patterns and Bad Smells","","2018","","","1528–1535","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd Annual ACM Symposium on Applied Computing","Pau, France","2018","9781450351911","","https://doi.org/10.1145/3167132.3167295;http://dx.doi.org/10.1145/3167132.3167295","10.1145/3167132.3167295","Bad Smells are symptoms that appear in the source code of a software system and may indicate a structural problem that requires code refactoring. Design patterns are solutions known as good practices that help building software systems with high quality and flexibility. Intuitively, it is possible to assume that the use of design patterns might avoid bad smells. Intriguingly, some recent studies have pointed out that this assumption is not true. This paper presents a systematic literature mapping of studies that investigate the relationship between design patterns and bad smells. We identified 16 papers which were categorized into three different approaches: impact on software quality, refactoring and co-occurrence. Amongst these three approaches, the co-occurrence relationship is the less explored in the literature. In addition, we identified that studies focusing on co-occurrence between design patterns and bad smells have generally analyzed the relationship between the GOF design patterns and bad smells described by Fowler and Beck. In this context, the Command design pattern was identified as the one with greater relationship with bad smells.","systematic literature mapping, bad smell, design pattern","","SAC '18"
"Conference Paper","Haque MS,Carver J,Atkison T","Causes, Impacts, and Detection Approaches of Code Smell: A Survey","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACMSE 2018 Conference","Richmond, Kentucky","2018","9781450356961","","https://doi.org/10.1145/3190645.3190697;http://dx.doi.org/10.1145/3190645.3190697","10.1145/3190645.3190697","Code smells are anomalies often generated in design, implementation or maintenance phase of software development life cycle. Researchers established several catalogues characterizing the smells. Fowler and Beck developed the most popular catalogue of 22 smells covering varieties of development issues. This literature presents an overview of the existing research conducted on these 22 smells. Our motivation is to represent these smells with an easier interpretation for the software developers, determine the causes that generate these issues in applications and their impact from different aspects of software maintenance. This paper also highlights previous and recent research on smell detection with an effort to categorize the approaches based on the underlying concept.","survey, code smell, software engineering","","ACMSE '18"
"Conference Paper","Beck M,Walden J","Using Software Birthmarks and Clustering to Identify Similar Classes and Major Functionalities","","2018","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACMSE 2018 Conference","Richmond, Kentucky","2018","9781450356961","","https://doi.org/10.1145/3190645.3190677;http://dx.doi.org/10.1145/3190645.3190677","10.1145/3190645.3190677","Software birthmarks are a class of software metrics designed to identify copies of software. An article published in 2006 examined additional applications of software birthmarks. The article described an experiment using software birthmarks to identify similar classes and major functionalities in software applications. This study replicates and extends that experiment, using a modern software birthmark tool and larger dataset, while improving the precision of the research questions and methodologies used in the original article. We found that one of the conclusions of the original article could be replicated while the the other conclusion could not. While software birthmarks provide an effective method for identifying similar class files, they do not offer a reliable, objective, and generalizable method for finding major functionalities in a software release.","software birthmarks, multidimensional scaling","","ACMSE '18"
"Conference Paper","Wang W,Wu J,Gong X,Li T,Yew PC","Improving Dynamically-Generated Code Performance on Dynamic Binary Translators","","2018","","","17–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments","Williamsburg, VA, USA","2018","9781450355797","","https://doi.org/10.1145/3186411.3186413;http://dx.doi.org/10.1145/3186411.3186413","10.1145/3186411.3186413","The recent transition in the software industry toward dynamically generated code poses a new challenge to existing dynamic binary translation (DBT) systems. A significant re-translation overhead could be introduced due to the maintenance of the consistency between the dynamically-generated guest code and the corresponding translated host code. To address this issue, this paper presents a novel approach to optimize DBT systems for guest applications with dynamically-generated code. The proposed approach can maximize the reuse of previously translated host code to mitigate the re-translation overhead. A prototype based on such an approach has been implemented on an existing DBT system HQEMU. Experimental results on a set of JavaScript applications show that it can achieve a 1.24X performance speedup on average compared to the original HQEMU.","JIT, DBT, Binary Code Matching","","VEE '18"
"Journal Article","Wang W,Wu J,Gong X,Li T,Yew PC","Improving Dynamically-Generated Code Performance on Dynamic Binary Translators","SIGPLAN Not.","2018","53","3","17–30","Association for Computing Machinery","New York, NY, USA","","","2018-03","","0362-1340","https://doi.org/10.1145/3296975.3186413;http://dx.doi.org/10.1145/3296975.3186413","10.1145/3296975.3186413","The recent transition in the software industry toward dynamically generated code poses a new challenge to existing dynamic binary translation (DBT) systems. A significant re-translation overhead could be introduced due to the maintenance of the consistency between the dynamically-generated guest code and the corresponding translated host code. To address this issue, this paper presents a novel approach to optimize DBT systems for guest applications with dynamically-generated code. The proposed approach can maximize the reuse of previously translated host code to mitigate the re-translation overhead. A prototype based on such an approach has been implemented on an existing DBT system HQEMU. Experimental results on a set of JavaScript applications show that it can achieve a 1.24X performance speedup on average compared to the original HQEMU.","DBT, JIT, Binary Code Matching","",""
"Conference Paper","Hamid SS,Admodisastro N,Manshor N,Ghani AA,Kamaruddin A","Engagement Prediction in the Adaptive Learning Model for Students with Dyslexia","","2018","","","66–73","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Conference on Human-Computer Interaction and User Experience in Indonesia, CHIuXiD '18","Yogyakarta, Indonesia","2018","9781450364294","","https://doi.org/10.1145/3205946.3205956;http://dx.doi.org/10.1145/3205946.3205956","10.1145/3205946.3205956","Student engagement is one of the most important elements in a likelihood of school failure or dropout. Therefore, it is vital to measure the student engagement as quickly as possible and as often as possible to prevent it occurred in a prolonged situation. There a few ways to assess the engagement that includes self-reporting, teachers rating, interviews and observation. However, these methods are not only takings time but also need a lot of hard work, cost and difficult to conduct for a very short time. Therefore, we a proposing an alternative to predict student engagement through frontal face detection. We apply machine learning approach that utilizes Speed-Up Robust Features (SURF) descriptor to detect key interest point of the images and cluster using different codebook sizes. For classification model, we used Support Vector Machine (SVM) with two different kernels and Naïve Bayes. We managed to get more than 88% of the accuracy results. The model is an important part of our proposed adaptive learning model for dyslexic students.","adaptive learning, Student engagement, Dyslexia","","CHIuXiD '18"
"Conference Paper","Wang W,McCamant S,Zhai A,Yew PC","Enhancing Cross-ISA DBT Through Automatically Learned Translation Rules","","2018","","","84–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems","Williamsburg, VA, USA","2018","9781450349116","","https://doi.org/10.1145/3173162.3177160;http://dx.doi.org/10.1145/3173162.3177160","10.1145/3173162.3177160","This paper presents a novel approach for dynamic binary translation (DBT) to automatically learn translation rules from guest and host binaries compiled from the same source code. The learned translation rules are then verified via binary symbolic execution and used in an existing DBT system, QEMU, to generate more efficient host binary code. Experimental results on SPEC CINT2006 show that the average time of learning a translation rule is less than two seconds. With the rules learned from a collection of benchmark programs excluding the targeted program itself, an average 1.25X performance speedup over QEMU can be achieved for SPEC CINT2006. Moreover, the translation overhead introduced by this rule-based approach is very small even for short-running workloads.","rule learning, symbolic execution, DBT","","ASPLOS '18"
"Journal Article","Wang W,McCamant S,Zhai A,Yew PC","Enhancing Cross-ISA DBT Through Automatically Learned Translation Rules","SIGPLAN Not.","2018","53","2","84–97","Association for Computing Machinery","New York, NY, USA","","","2018-03","","0362-1340","https://doi.org/10.1145/3296957.3177160;http://dx.doi.org/10.1145/3296957.3177160","10.1145/3296957.3177160","This paper presents a novel approach for dynamic binary translation (DBT) to automatically learn translation rules from guest and host binaries compiled from the same source code. The learned translation rules are then verified via binary symbolic execution and used in an existing DBT system, QEMU, to generate more efficient host binary code. Experimental results on SPEC CINT2006 show that the average time of learning a translation rule is less than two seconds. With the rules learned from a collection of benchmark programs excluding the targeted program itself, an average 1.25X performance speedup over QEMU can be achieved for SPEC CINT2006. Moreover, the translation overhead introduced by this rule-based approach is very small even for short-running workloads.","DBT, rule learning, symbolic execution","",""
"Conference Paper","David Y,Partush N,Yahav E","FirmUp: Precise Static Detection of Common Vulnerabilities in Firmware","","2018","","","392–404","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems","Williamsburg, VA, USA","2018","9781450349116","","https://doi.org/10.1145/3173162.3177157;http://dx.doi.org/10.1145/3173162.3177157","10.1145/3173162.3177157","We present a static, precise, and scalable technique for finding CVEs (Common Vulnerabilities and Exposures) in stripped firmware images. Our technique is able to efficiently find vulnerabilities in real-world firmware with high accuracy. Given a vulnerable procedure in an executable binary and a firmware image containing multiple stripped binaries, our goal is to detect possible occurrences of the vulnerable procedure in the firmware image. Due to the variety of architectures and unique tool chains used by vendors, as well as the highly customized nature of firmware, identifying procedures in stripped firmware is extremely challenging. Vulnerability detection requires not only pairwise similarity between procedures but also information about the relationships between procedures in the surrounding executable. This observation serves as the foundation for a novel technique that establishes a partial correspondence between procedures in the two binaries. We implemented our technique in a tool called FirmUp and performed an extensive evaluation over 40 million procedures, over 4 different prevalent architectures, crawled from public vendor firmware images. We discovered 373 vulnerabilities affecting publicly available firmware, 147 of them in the latest available firmware version for the device. A thorough comparison of FirmUp to previous methods shows that it accurately and effectively finds vulnerabilities in firmware, while outperforming the detection rate of the state of the art by 45% on average.","statistical similarity, verification-aided similarity, static binary analysis, partial equivalence","","ASPLOS '18"
"Journal Article","David Y,Partush N,Yahav E","FirmUp: Precise Static Detection of Common Vulnerabilities in Firmware","SIGPLAN Not.","2018","53","2","392–404","Association for Computing Machinery","New York, NY, USA","","","2018-03","","0362-1340","https://doi.org/10.1145/3296957.3177157;http://dx.doi.org/10.1145/3296957.3177157","10.1145/3296957.3177157","We present a static, precise, and scalable technique for finding CVEs (Common Vulnerabilities and Exposures) in stripped firmware images. Our technique is able to efficiently find vulnerabilities in real-world firmware with high accuracy. Given a vulnerable procedure in an executable binary and a firmware image containing multiple stripped binaries, our goal is to detect possible occurrences of the vulnerable procedure in the firmware image. Due to the variety of architectures and unique tool chains used by vendors, as well as the highly customized nature of firmware, identifying procedures in stripped firmware is extremely challenging. Vulnerability detection requires not only pairwise similarity between procedures but also information about the relationships between procedures in the surrounding executable. This observation serves as the foundation for a novel technique that establishes a partial correspondence between procedures in the two binaries. We implemented our technique in a tool called FirmUp and performed an extensive evaluation over 40 million procedures, over 4 different prevalent architectures, crawled from public vendor firmware images. We discovered 373 vulnerabilities affecting publicly available firmware, 147 of them in the latest available firmware version for the device. A thorough comparison of FirmUp to previous methods shows that it accurately and effectively finds vulnerabilities in firmware, while outperforming the detection rate of the state of the art by 45% on average.","statistical similarity, verification-aided similarity, partial equivalence, static binary analysis","",""
"Conference Paper","Katz O,Rinetzky N,Yahav E","Statistical Reconstruction of Class Hierarchies in Binaries","","2018","","","363–376","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems","Williamsburg, VA, USA","2018","9781450349116","","https://doi.org/10.1145/3173162.3173202;http://dx.doi.org/10.1145/3173162.3173202","10.1145/3173162.3173202","We address a fundamental problem in reverse engineering of object-oriented code: the reconstruction of a program's class hierarchy from its stripped binary. Existing approaches rely heavily on structural information that is not always available, e.g., calls to parent constructors. As a result, these approaches often leave gaps in the hierarchies they construct, or fail to construct them altogether. Our main insight is that behavioral information can be used to infer subclass/superclass relations, supplementing any missing structural information. Thus, we propose the first statistical approach for static reconstruction of class hierarchies based on behavioral similarity. We capture the behavior of each type using a statistical language model (SLM), define a metric for pairwise similarity between types based on the Kullback-Leibler divergence between their SLMs, and lift it to determine the most likely class hierarchy. We implemented our approach in a tool called ROCK and used it to automatically reconstruct the class hierarchies of several real-world stripped C++ binaries. Our results demonstrate that ROCK obtained significantly more accurate class hierarchies than those obtained using structural analysis alone.","reverse engineering, class hierarchies, x86, static binary analysis","","ASPLOS '18"
"Journal Article","Katz O,Rinetzky N,Yahav E","Statistical Reconstruction of Class Hierarchies in Binaries","SIGPLAN Not.","2018","53","2","363–376","Association for Computing Machinery","New York, NY, USA","","","2018-03","","0362-1340","https://doi.org/10.1145/3296957.3173202;http://dx.doi.org/10.1145/3296957.3173202","10.1145/3296957.3173202","We address a fundamental problem in reverse engineering of object-oriented code: the reconstruction of a program's class hierarchy from its stripped binary. Existing approaches rely heavily on structural information that is not always available, e.g., calls to parent constructors. As a result, these approaches often leave gaps in the hierarchies they construct, or fail to construct them altogether. Our main insight is that behavioral information can be used to infer subclass/superclass relations, supplementing any missing structural information. Thus, we propose the first statistical approach for static reconstruction of class hierarchies based on behavioral similarity. We capture the behavior of each type using a statistical language model (SLM), define a metric for pairwise similarity between types based on the Kullback-Leibler divergence between their SLMs, and lift it to determine the most likely class hierarchy. We implemented our approach in a tool called ROCK and used it to automatically reconstruct the class hierarchies of several real-world stripped C++ binaries. Our results demonstrate that ROCK obtained significantly more accurate class hierarchies than those obtained using structural analysis alone.","reverse engineering, static binary analysis, x86, class hierarchies","",""
"Conference Paper","Barany G","Finding Missed Compiler Optimizations by Differential Testing","","2018","","","82–92","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th International Conference on Compiler Construction","Vienna, Austria","2018","9781450356442","","https://doi.org/10.1145/3178372.3179521;http://dx.doi.org/10.1145/3178372.3179521","10.1145/3178372.3179521","Randomized differential testing of compilers has had great success in finding compiler crashes and silent miscompilations. In this paper we investigate whether we can use similar techniques to improve the quality of the generated code: Can we compare the code generated by different compilers to find optimizations performed by one but missed by another? We have developed a set of tools for running such tests. We compile C code generated by standard random program generators and use a custom binary analysis tool to compare the output programs. Depending on the optimization of interest, the tool can be configured to compare features such as the number of total instructions, multiply or divide instructions, function calls, stack accesses, and more. A standard test case reduction tool produces minimal examples once an interesting difference has been found. We have used our tools to compare the code generated by GCC, Clang, and CompCert. We have found previously unreported missing arithmetic optimizations in all three compilers, as well as individual cases of unnecessary register spilling, missed opportunities for register coalescing, dead stores, redundant computations, and missing instruction selection patterns.","differential testing, randomized testing, optimization","","CC 2018"
"Conference Paper","Leopoldseder D,Stadler L,Würthinger T,Eisl J,Simon D,Mössenböck H","Dominance-Based Duplication Simulation (DBDS): Code Duplication to Enable Compiler Optimizations","","2018","","","126–137","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 International Symposium on Code Generation and Optimization","Vienna, Austria","2018","9781450356176","","https://doi.org/10.1145/3168811;http://dx.doi.org/10.1145/3168811","10.1145/3168811","Compilers perform a variety of advanced optimizations to improve the quality of the generated machine code. However, optimizations that depend on the data flow of a program are often limited by control-flow merges. Code duplication can solve this problem by hoisting, i.e. duplicating, instructions from merge blocks to their predecessors. However, finding optimization opportunities enabled by duplication is a non-trivial task that requires compile-time intensive analysis. This imposes a challenge on modern (just-in-time) compilers: Duplicating instructions tentatively at every control flow merge is not feasible because excessive duplication leads to uncontrolled code growth and compile time increases. Therefore, compilers need to find out whether a duplication is beneficial enough to be performed. This paper proposes a novel approach to determine which duplication operations should be performed to increase performance. The approach is based on a duplication simulation that enables a compiler to evaluate different success metrics per potential duplication. Using this information, the compiler can then select the most promising candidates for optimization. We show how to map duplication candidates into an optimization cost model that allows us to trade-off between different success metrics including peak performance, code size and compile time. We implemented the approach on top of the GraalVM and evaluated it with the benchmarks Java DaCapo, Scala DaCapo, JavaScript Octane and a micro-benchmark suite, in terms of performance, compilation time and code size increase. We show that our optimization can reach peak performance improvements of up to 40% with a mean peak performance increase of 5.89%, while it generates a mean code size increase of 9.93% and mean compile time increase of 18.44%.","Virtual Machines, Compiler Optimizations, Code Duplication, Tail Duplication, Just-In-Time Compilation","","CGO 2018"
"Conference Paper","Railing BP,Bryant RE","Implementing Malloc: Students and Systems Programming","","2018","","","104–109","Association for Computing Machinery","New York, NY, USA","Proceedings of the 49th ACM Technical Symposium on Computer Science Education","Baltimore, Maryland, USA","2018","9781450351034","","https://doi.org/10.1145/3159450.3159597;http://dx.doi.org/10.1145/3159450.3159597","10.1145/3159450.3159597","This work describes our experience in revising one of the major programming assignments for the second-year course Introduction to Computer Systems, in which students implement a version of the malloc memory allocator. The revisions involved fully supporting a 64-bit address space, promoting a more modern programming style, and creating a set of benchmarks and grading standards that provide an appropriate level of challenge. With this revised assignment, students were able to implement more sophisticated allocators than they had in the past, and they also achieved higher performance on the related questions on the final exam.","systems programming, malloc, programming assignment","","SIGCSE '18"
"Conference Paper","Krüger J,Gu W,Shen H,Mukelabai M,Hebig R,Berger T","Towards a Better Understanding of Software Features and Their Characteristics: A Case Study of Marlin","","2018","","","105–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems","Madrid, Spain","2018","9781450353984","","https://doi.org/10.1145/3168365.3168371;http://dx.doi.org/10.1145/3168365.3168371","10.1145/3168365.3168371","The notion of features is commonly used to describe, structure, and communicate the functionalities of a system. Unfortunately, features and their locations in software artifacts are rarely made explicit and often need to be recovered by developers. To this end, researchers have conceived automated feature-location techniques. However, their accuracy is generally low, and they mostly rely on few information sources, disregarding the richness of modern projects. To improve such techniques, we need to improve the empirical understanding of features and their characteristics, including the information sources that support feature location. Even though, the product-line community has extensively studied features, the focus was primarily on variable features in preprocessor-based systems, largely side-stepping mandatory features, which are hard to identify. We present an exploratory case study on identifying and locating features. We study what information sources reveal features and to what extent, compare the characteristics of mandatory and optional features, and formulate hypotheses about our observations. Among others, we find that locating features in code requires substantial domain knowledge for half of the mandatory features (e.g., to connect keywords) and that mandatory and optional features in fact differ. For instance, mandatory features are less scattered. Other researchers can use our manually created data set of features locations for future research, guided by our formulated hypotheses.","Marlin, preprocessor, case study, Feature location","","VAMOS '18"
"Journal Article","Yang K,Forte D,Tehranipoor M","ReSC: An RFID-Enabled Solution for Defending IoT Supply Chain","ACM Trans. Des. Autom. Electron. Syst.","2018","23","3","","Association for Computing Machinery","New York, NY, USA","","","2018-02","","1084-4309","https://doi.org/10.1145/3174850;http://dx.doi.org/10.1145/3174850","10.1145/3174850","The Internet of Things (IoT), an emerging global network of uniquely identifiable embedded computing devices within the existing Internet infrastructure, is transforming how we live and work by increasing the connectedness of people and things on a scale that was once unimaginable. In addition to facilitated information and service exchange between connected objects, enhanced computing power and analytic capabilities of individual objects, and increased interaction between objects and their environments, the IoT also raises new security and privacy challenges. Hardware trust across the IoT supply chain is the foundation of IoT security and privacy. Two major supply chain issues—disappearance/theft of authentic IoT devices and appearance of inauthentic ones—have to be addressed to secure the IoT supply chain and lay the foundation for further security and privacy-defensive measures. Comprehensive solutions that enable IoT device authentication and traceability across the entire supply chain (i.e., during distribution and after being provisioned) need to be established. Existing hardware, software, and network protection methods, however, do not address IoT supply chain issues. To mitigate this shortcoming, we propose an RFID-enabled solution called ReSC that aims at defending the IoT supply chain. By incorporating three techniques—one-to-one mapping between RFID tag identity and control chip identity; unique tag trace, which records tag provenance and history information; and neighborhood attestation of IoT devices—ReSC is resistant to split attacks (i.e., separating tag from product, swapping tags), counterfeit injection, product theft throughout the entire supply chain, device recycling, and illegal network service access (e.g., Internet, cable TV, online games, remote firmware updates). Simulations, theoretical analysis, and experimental results based on a printed circuit board (PCB) prototype demonstrate the effectiveness of ReSC. Finally, we evaluate the security of our proposed scheme against various attacks.","traceability, authentication, supply chain security, Internet of things (IoT), Radio frequency identification (RFID)","",""
"Journal Article","Alrabaee S,Shirani P,Wang L,Debbabi M","FOSSIL: A Resilient and Efficient System for Identifying FOSS Functions in Malware Binaries","ACM Trans. Priv. Secur.","2018","21","2","","Association for Computing Machinery","New York, NY, USA","","","2018-01","","2471-2566","https://doi.org/10.1145/3175492;http://dx.doi.org/10.1145/3175492","10.1145/3175492","Identifying free open-source software (FOSS) packages on binaries when the source code is unavailable is important for many security applications, such as malware detection, software infringement, and digital forensics. This capability enhances both the accuracy and the efficiency of reverse engineering tasks by avoiding false correlations between irrelevant code bases. Although the FOSS package identification problem belongs to the field of software engineering, conventional approaches rely strongly on practical methods in data mining and database searching. However, various challenges in the use of these methods prevent existing function identification approaches from being effective in the absence of source code. To make matters worse, the introduction of obfuscation techniques, the use of different compilers and compilation settings, and software refactoring techniques has made the automated detection of FOSS packages increasingly difficult. With very few exceptions, the existing systems are not resilient to such techniques, and the exceptions are not sufficiently efficient.To address this issue, we propose FOSSIL, a novel resilient and efficient system that incorporates three components. The first component extracts the syntactical features of functions by considering opcode frequencies and applying a hidden Markov model statistical test. The second component applies a neighborhood hash graph kernel to random walks derived from control-flow graphs, with the goal of extracting the semantics of the functions. The third component applies z-score to the normalized instructions to extract the behavior of instructions in a function. The components are integrated using a Bayesian network model, which synthesizes the results to determine the FOSS function. The novel approach of combining these components using the Bayesian network has produced stronger resilience to code obfuscation.We evaluate our system on three datasets, including real-world projects whose use of FOSS packages is known, malware binaries for which there are security and reverse engineering reports purporting to describe their use of FOSS, and a large repository of malware binaries. We demonstrate that our system is able to identify FOSS packages in real-world projects with a mean precision of 0.95 and with a mean recall of 0.85. Furthermore, FOSSIL is able to discover FOSS packages in malware binaries that match those listed in security and reverse engineering reports. Our results show that modern malware binaries contain 0.10--0.45 of FOSS packages.","Binary code analysis, free software packages, function fingerprinting, malicious code analysis","",""
"Conference Paper","Simon,Sheard J,Morgan M,Petersen A,Settle A,Sinclair J","Informing Students about Academic Integrity in Programming","","2018","","","113–122","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th Australasian Computing Education Conference","Brisbane, Queensland, Australia","2018","9781450363402","","https://doi.org/10.1145/3160489.3160502;http://dx.doi.org/10.1145/3160489.3160502","10.1145/3160489.3160502","In recent years academic integrity has come to be seen as a major concern across the full educational spectrum. The case has been made that in certain ways academic integrity is not the same in computing education as in education more generally, and that as a consequence it is the responsibility of computing educators to explicitly advise their students of the academic integrity requirements of their assessments. As part of a larger project, computing academics around the world were asked a number of questions regarding how they advise their students about academic integrity in programming assessments. Almost all respondents indicated that their students were required to abide by an academic integrity policy, but only about half of them felt that the policy was appropriate for programming assessments. We analyse respondents' descriptions of how they advise students about academic integrity in programming assessments, grouping them into a number of themes, and give excerpts from the guidelines that some respondents provide for their students. A clear finding from the survey is that while most educators agree that externally sourced code or assistance should be acknowledged, fewer than 20% are aware of any standard form for that acknowledgement. We therefore conclude by proposing, for discussion, a standard form for acknowledging externally sourced code or assistance.","programming education, academic integrity, computing education, collusion, plagiarism","","ACE '18"
"Conference Paper","Matsuda M,Fukuda K,Maruyama N","A Portability Layer of an All-Pairs Operation for Hierarchical N-Body Algorithm Framework Tapas","","2018","","","241–250","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region","Chiyoda, Tokyo, Japan","2018","9781450353724","","https://doi.org/10.1145/3149457.3149471;http://dx.doi.org/10.1145/3149457.3149471","10.1145/3149457.3149471","Tapas is a C++ programming framework for developing hierarchical N-body algorithms such as Barnes-Hut and Fast Multipole Method, designed to experiment new implementations including even variations of tree traversals. A pairwise interaction calculation in N-body simulations, or an all-pairs operation, is an important part of Tapas for performance, which enables accelerations with GPUs. However, there is no commonly agreed all-pairs interface appropriate as a primitive, and moreover, it is not supported in existing data-parallel libraries for GPUs such as NVIDIA's Thrust. Thus, we designed an interface for an all-pairs operation that can be easily adopted in libraries and applications. Tapas's all-pairs has an extra function argument for flexibility, which corresponds to a consumer function of the result of an all-pairs that is missing in existing designs. This addition is not an ad hoc one, but it is guided by the consideration of algorithmic skeletons, which indicates the effect of the added argument cannot be substituted by the other arguments in general. The change is just adding an argument, but it gives flexibility to process the result, and the resulting implementation can attain almost the same performance as the tuned N-body implementation in the CUDA examples.","","","HPC Asia 2018"
"Conference Paper","Xue H,Venkataramani G,Lan T","Clone-Slicer: Detecting Domain Specific Binary Code Clones through Program Slicing","","2018","","","27–33","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 Workshop on Forming an Ecosystem Around Software Transformation","Toronto, Canada","2018","9781450359979","","https://doi.org/10.1145/3273045.3273047;http://dx.doi.org/10.1145/3273045.3273047","10.1145/3273045.3273047","In this paper, we presented a novel framework, Clone-Slicer, a domain-specific code clone detector for binary executables, that integrates program slicing and a deep learning based binary code clone modeling framework to improve the number of code clone detected. In particular, we chose pointer analysis for memory safety as our example domain to demonstrate the usefulness of our approach. We evaluated our approach using real-world applications from SPEC 2006 benchmark suite. Our results show Clone-Slicer is able to detect up to 43.64% code clones compared to prior work and further cut the time-to-solution (the time spent to verify memory bound safety) for Clone-Slicer by 32.96% compared to Clone-Hunter. As future work, we plan to apply Clone-Slicer to different domains and tasks, such as vulnerable program path discovery, and further improve the capability for code clone detection through advanced clustering algorithms. We will also study the cost-benefit tradeoffs of using such advanced algorithms.","binary analysis, machine learning, code clones, program slicing","","FEAST '18"
"Conference Paper","Yongting Y,Dongsheng L,Liping Z","Detection Technology and Application of Clone Refactoring","","2018","","","128–133","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences","Wuhan, China","2018","9781450354318","","https://doi.org/10.1145/3180374.3181332;http://dx.doi.org/10.1145/3180374.3181332","10.1145/3180374.3181332","Clone code is a similar part of code, such code may seriously affect the maintainability of the software and reduce the quality of code. In order to eliminate the negative impact of clone code, the researchers offer many methods to eliminate clone code and refactoring is an important part in them. In this paper, several different methods and tools are introduced, the advantages and disadvantages of various methods and tools are summarized, apart from this, we also describe the important application of clone refactoring in maintaining the quality of code, at last, we discussed the challenges of the current clone refactoring detection.","Clone Code, Refactoring, Software Quality, Maintainability","","ICMSS 2018"
"Journal Article","Strüber D,Rubin J,Arendt T,Chechik M,Taentzer G,Plöger J","Variability-Based Model Transformation: Formal Foundation and Application","Form. Asp. Comput.","2018","30","1","133–162","Springer-Verlag","Berlin, Heidelberg","","","2018-01","","0934-5043","https://doi.org/10.1007/s00165-017-0441-3;http://dx.doi.org/10.1007/s00165-017-0441-3","10.1007/s00165-017-0441-3","Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.","Variability, Graph transformation, Category theory, Model transformation","",""
"Journal Article","Lidman J,Mckee SA","Verifying Reliability Properties Using the Hyperball Abstract Domain","ACM Trans. Program. Lang. Syst.","2017","40","1","","Association for Computing Machinery","New York, NY, USA","","","2017-12","","0164-0925","https://doi.org/10.1145/3156017;http://dx.doi.org/10.1145/3156017","10.1145/3156017","Modern systems are increasingly susceptible to soft errors that manifest themselves as bit flips and possibly alter the semantics of an application. We would like to measure the quality degradation on semantics due to such bit flips, and thus we introduce a Hyperball abstract domain that allows us to determine the worst-case distance between expected and actual results. Similar to intervals, hyperballs describe a connected and dense space. The semantics of low-level code in the presence of bit flips is hard to accurately describe in such a space. We therefore combine the Hyperball domain with an existing affine system abstract domain that we extend to handle bit flips, which are introduce as disjunctions. Bit-flips can reduce the precision of our analysis, and we therefor introduce the Scale domain as a disjunctive refinement to minimize precision loss. This domain bounds the number of disjunctive elements by quantifying the over-approximation of different partitions and uses submodular optimization to find a good partitioning (within a bound of optimal). We evaluate these domains to show benefits and potential problems. For the application we examine here, adding the Scale domain to the Hyperball abstraction improves accuracy by up to two orders of magnitude. Our initial results demonstrate the feasibility of this approach, although we would like to further improve execution efficiency.","Reliability analysis, abstract interpretation, disjunctive refinement, numerical abstraction","",""
"Conference Paper","Makady S,Walker RJ","Test Code Reuse from OSS: Current and Future Challenges","","2017","","","31–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd Africa and Middle East Conference on Software Engineering","Cairo, Egypt","2017","9781450355124","","https://doi.org/10.1145/3178298.3178305;http://dx.doi.org/10.1145/3178298.3178305","10.1145/3178298.3178305","We are told of the significant benefits of automated approaches to testing over manual approaches. However, it is unclear what automated testing practices exist, and how efficient or widespread such practices are within open source software. Although some organizations rigorously apply automated testing to their software, this rich pool of test code is not utilized to serve existing source code with poor or no test suites. To investigate how automated testing is performed in practice, we attempted a thorough, large-scale analysis of open source repositories. Alongside this analysis, we propose a novel approach to reuse such existing tests within projects that lack test code, hence leveraging the quality of such projects with minimal developer intervention. While such an analysis seems to be a straightforward task, we report on various practical challenges that hindered applying our proposed approach for tests' reuse. We present the challenges we have addressed so far, and those we expect to appear in the near future, in applying our approach for test reuse with open source projects. We outline potential solutions to the projected future challenges.","source code analysis, Test code reuse, mining software repositories, open source","","AMECSE '17"
"Conference Paper","Marastoni N,Continella A,Quarta D,Zanero S,Preda MD","GroupDroid: Automatically Grouping Mobile Malware by Extracting Code Similarities","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Software Security, Protection, and Reverse Engineering / Software Security and Protection Workshop","Orlando, FL, USA","2017","9781450353878","","https://doi.org/10.1145/3151137.3151138;http://dx.doi.org/10.1145/3151137.3151138","10.1145/3151137.3151138","As shown in previous work, malware authors often reuse portions of code in the development of their samples. Especially in the mobile scenario, there exists a phenomena, called piggybacking, that describes the act of embedding malicious code inside benign apps. In this paper, we leverage such observations to analyze mobile malware by looking at its similarities. In practice, we propose a novel approach that identifies and extracts code similarities in mobile apps. Our approach is based on static analysis and works by computing the Control Flow Graph of each method and encoding it in a feature vector used to measure similarities. We implemented our approach in a tool, GroupDroid, able to group mobile apps together according to their code similarities. Armed with Group-Droid, we then analyzed modern mobile malware samples. Our experiments show that GroupDroid is able to correctly and accurately distinguish different malware variants, and to provide useful and detailed information about the similar portions of malicious code.","Mobile, Similarity, Malware","","SSPREW-7"
"Conference Paper","Kim T,Kim CH,Choi H,Kwon Y,Saltaformaggio B,Zhang X,Xu D","RevARM: A Platform-Agnostic ARM Binary Rewriter for Security Applications","","2017","","","412–424","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd Annual Computer Security Applications Conference","Orlando, FL, USA","2017","9781450353458","","https://doi.org/10.1145/3134600.3134627;http://dx.doi.org/10.1145/3134600.3134627","10.1145/3134600.3134627","ARM is the leading processor architecture in the emerging mobile and embedded market. Unfortunately, there has been a myriad of security issues on both mobile and embedded systems. While many countermeasures of such security issues have been proposed in recent years, a majority of applications still cannot be patched or protected due to run-time and space overhead constraints and the unavailability of source code. More importantly, the rapidly evolving mobile and embedded market makes any platform-specific solution ineffective. In this paper, we propose RevARM, a binary rewriting technique capable of instrumenting ARM-based binaries without limitation on the target platform. Unlike many previous binary instrumentation tools that are designed to instrument binaries based on x86, RevARM must resolve a number of new, ARM-specific binary rewriting challenges. Moreover, RevARM is able to handle stripped binaries, requires no symbolic/semantic information, and supports Mach-O binaries, overcoming the limitations of existing approaches. Finally, we demonstrate the capabilities of RevARM in solving real-world security challenges. Our evaluation results across a variety of platforms, including popular mobile and embedded systems, show that RevARM is highly effective in instrumenting ARM binaries with an average of 3.2% run-time and 1.3% space overhead.","","","ACSAC '17"
"Conference Paper","Bulazel A,Yener B","A Survey On Automated Dynamic Malware Analysis Evasion and Counter-Evasion: PC, Mobile, and Web","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st Reversing and Offensive-Oriented Trends Symposium","Vienna, Austria","2017","9781450353212","","https://doi.org/10.1145/3150376.3150378;http://dx.doi.org/10.1145/3150376.3150378","10.1145/3150376.3150378","Automated dynamic malware analysis systems are important in combating the proliferation of modern malware. Unfortunately, malware can often easily detect and evade these systems. Competition between malware authors and analysis system developers has pushed each to continually evolve their tactics for countering the other.In this paper we systematically review i) ""fingerprint""-based evasion techniques against automated dynamic malware analysis systems for PC, mobile, and web, ii) evasion detection, iii) evasion mitigation, and iv) offensive and defensive evasion case studies. We also discuss difficulties in experimental evaluation, highlight future directions in offensive and defensive research, and briefly survey related topics in anti-analysis.","Dynamic Analysis, Malware Analysis, Anti-Analysis, Anti-Debugging, Evasive Malware, Virtualization, Emulation","","ROOTS"
"Conference Paper","Zhao J,Feng L,Sinha S,Zhang W,Liang Y,He B","COMBA: A Comprehensive Model-Based Analysis Framework for High Level Synthesis of Real Applications","","2017","","","430–437","IEEE Press","Irvine, California","Proceedings of the 36th International Conference on Computer-Aided Design","","2017","","","","","High Level Synthesis (HLS) relies on the use of synthesis pragmas to generate digital designs meeting a set of specifications. However, the selection of a set of pragmas depends largely on designer experience and knowledge of the target architecture and digital design. Existing automated methods of pragma selection are very limited in scope and capability to analyze complex design descriptions in high-level languages to be synthesized using HLS. In this paper, we propose COMBA, a comprehensive model-based analysis framework capable of analyzing the effects of a multitude of pragmas related to functions, loops and arrays in the design description using pluggable analytical models, a recursive data collector (RDC) and a metric-guided design space exploration algorithm (MGDSE). When compared with HLS tools like Vivado HLS, COMBA reports an average error of around 1% in estimating performance, while taking only a few seconds for analysis of Polybench benchmark applications and a few minutes for real-life applications like JPEG, Seidel and Rician. The synthesis pragmas recommended by COMBA result in an average 100x speed-up in performance for the analyzed applications, which establishes COMBA as a superior alternative to current state-of-the-art approaches.","","","ICCAD '17"
"Conference Paper","Choi YK,Zhang P,Li P,Cong J","HLscope+: Fast and Accurate Performance Estimation for FPGA HLS","","2017","","","691–698","IEEE Press","Irvine, California","Proceedings of the 36th International Conference on Computer-Aided Design","","2017","","","","","High-level synthesis (HLS) tools have vastly increased the productivity of field-programmable gate array (FPGA) programmers with design automation and abstraction. However, the side effect is that many architectural details are hidden from the programmers. As a result, programmers who wish to improve the performance of their design often have difficulty identifying the performance bottleneck. It is true that current HLS tools provide some estimate of the performance with a fixed loop count, but they often fail to do so for programs with input-dependent execution behavior. Also, their external memory latency model does not accurately fit the actual bus-based shared memory architecture. This work describes a high-level cycle estimation methodology to solve these problems. To reduce the time overhead, we propose a cycle estimation process that is combined with the HLS software simulation. We also present an automatic code instrumentation technique that finds the reason for stall accurately in on-board execution. The experimental results show that our framework provides a cycle estimate with an average error rate of 1.1% and 5.0% for compute- and DRAM-bound modules, respectively, for ADM-PCIE-7V3 board. The proposed method is about two orders of magnitude faster than the FPGA bitstream generation.","","","ICCAD '17"
"Conference Paper","Chunduri S,Harms K,Parker S,Morozov V,Oshin S,Cherukuri N,Kumaran K","Run-to-Run Variability on Xeon Phi Based Cray XC Systems","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","Denver, Colorado","2017","9781450351140","","https://doi.org/10.1145/3126908.3126926;http://dx.doi.org/10.1145/3126908.3126926","10.1145/3126908.3126926","The increasing complexity of HPC systems has introduced new sources of variability, which can contribute to significant differences in run-to-run performance of applications. With components at various levels of the system contributing variability, application developers and system users are now faced with the difficult task of running and tuning their applications in an environment where run-to-run performance measurements can vary by as much as a factor of two to three. In this study, we classify, quantify, and present ways to mitigate the sources of run-to-run variability on Cray XC systems with Intel Xeon Phi processors and a dragonfly interconnect. We further demonstrate that the code-tuning performance observed in a variability-mitigating environment correlates with the performance observed in production running conditions.","OS noise, variability, performance tuning, system noise","","SC '17"
"Conference Paper","Islam MR,Zibran MF,Nagpal A","Security Vulnerabilities in Categories of Clones and Non-Cloned Code: An Empirical Study","","2017","","","20–29","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.9;http://dx.doi.org/10.1109/ESEM.2017.9","10.1109/ESEM.2017.9","Background: Software security has drawn immense importance in the recent years. While efforts are expected in minimizing security vulnerabilities in source code, the developers' practice of code cloning often causes multiplication of such vulnerabilities and program faults. Although previous studies examined the bug-proneness, stability, and changeability of clones against non-cloned code, the security aspects remained ignored. Aims: The objective of this work is to explore and understand the security vulnerabilities and their severity in different types of clones compared to non-clone code. Method: Using a state-of-the-art clone detector and two reputed security vulnerability detection tools, we detect clones and vulnerabilities in 8.7 million lines of code over 34 software systems. We perform a comparative study of the vulnerabilities identified in different types of clones and non-cloned code. The results are derived based on quantitative analyses with statistical significance. Results: Our study reveals that the security vulnerabilities found in code clones have higher severity of security risks compared to those in non-cloned code. However, the proportion (i.e., density) of vulnerabilities in clones and non-cloned code does not have any significant difference. Conclusion: The findings from this work add to our understanding of the characteristics and impacts of clones, which will be useful in clone-aware software development with improved software security.","","","ESEM '17"
"Conference Paper","Huang Y,Zheng Q,Chen X,Xiong Y,Liu Z,Luo X","Mining Version Control System for Automatically Generating Commit Comment","","2017","","","414–423","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.56;http://dx.doi.org/10.1109/ESEM.2017.56","10.1109/ESEM.2017.56","Commit comments increasingly receive attention as an important complementary component in code change comprehension. To address the comment scarcity issue, a variety of automatic approaches for commit comment generation have been intensively proposed. However, most of these approaches mechanically outline a superficial level summary of the changed software entities, the change intent behind the code changes is lost (e.g., the existing approaches cannot generate such comment: ""fixing null pointer exception""). Considering the comments written by developers often describe the intent behind the code change, we propose a method to automatically generate commit comment by reusing the existing comments in version control system. Specifically, for an input commit, we apply syntax, semantic, pre-syntax, and pre-semantic similarities to discover the similar commits from half a million commits, and recommend the reusable comments to the input commit from the ones of the similar commits. We evaluate our approach on 7 projects. The results show that 9.1% of the generated comments are good, 27.7% of the generated comments need minor fix, and 63.2% are bad, and we also analyze the reasons that make a comment available or unavailable.","commit comment generation, code change comprehension, code syntax similarity, code semantic similarity","","ESEM '17"
"Conference Paper","Sharma T,Fragkoulis M,Spinellis D","House of Cards: Code Smells in Open-Source C# Repositories","","2017","","","424–429","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.57;http://dx.doi.org/10.1109/ESEM.2017.57","10.1109/ESEM.2017.57","Background: Code smells are indicators of quality problems that make a software hard to maintain and evolve. Given the importance of smells in the source code's maintainability, many studies have explored the characteristics of smells and analyzed their effects on the software's quality.Aim: We aim to investigate fundamental characteristics of code smells through an empirical study on frequently occurring smells that examines inter-category and intra-category correlation between design and implementation smells.Method: The study mines 19 design smells and 11 implementation smells in 1988 C# repositories containing more than 49 million lines of code. The mined data are statistically analyzed using methods such as Spearman's correlation and presented through hexbin and scatter plots.Results: We find that unutilized abstraction and magic number smells are the most frequently occurring smells in C# code. Our results also show that implementation and design smells exhibit strong inter-category correlation. The results of co-occurrence analysis imply that whenever unutilized abstraction or magic number smells are found, it is very likely to find other smells from the same smell category in the project.Conclusions: Our experiment shows high average smell density (14.7 and 55.8 for design and implementation smells respectively) for open source C# programs. Such high smell densities turn a software system into a house of cards reflecting the fragility introduced in the system. Our study advocates greater awareness of smells and the adoption of regular refactoring within the developer community to avoid turning software into a house of cards.","design smells, implementation smells, code quality, maintainability, code smells, C#","","ESEM '17"
"Conference Paper","Griffith I,Izurieta C,Huvaere C","An Industry Perspective to Comparing the SQALE and Quamoco Software Quality Models","","2017","","","287–296","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.42;http://dx.doi.org/10.1109/ESEM.2017.42","10.1109/ESEM.2017.42","Context: We investigate the different perceptions of quality provided by leading operational quality models when used to evaluate software systems from an industry perspective. Goal: To compare and evaluate the quality assessments of two competing quality models and to develop an extensible solution to meet the quality assurance measurement needs of an industry stakeholder -The Construction Engineering Research Laboratory (CERL). Method: In cooperation with our industry partner TechLink, we operationalize the Quamoco quality model and employ a multiple case study design comparing the results of Quamoco and SQALE, two implementations of well known quality models. The study is conducted across current versions of several open source software projects sampled from GitHub and commercial software for sustainment management systems implemented in the C# language from our industry partner. Each project represents a separate embedded unit of study in a given context -open source or commercial. We employ inter-rater agreement and correlation analysis to compare the results of both models, focusing on Maintainability, Reliability, and Security assessments. Results: Our observations suggest that there is a significant disconnect between the assessments of quality under both quality models. Conclusion: In order to support industry adoption, additional work is required to bring competing implementations of quality models into alignment. This exploratory case study helps us shed light into this problem.","quality assurance, quality standards, software quality","","ESEM '17"
"Conference Paper","de Mello RM,Oliveira R,Garcia A","On the Influence of Human Factors for Identifying Code Smells: A Multi-Trial Empirical Study","","2017","","","68–77","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.13;http://dx.doi.org/10.1109/ESEM.2017.13","10.1109/ESEM.2017.13","Context: Code smells are symptoms in the source code that represent poor design choices. Professional developers often perceive several types of code smells as indicators of actual design problems. However, the identification of code smells involves multiple steps that are subjective in nature, requiring the engagement of humans. Human factors are likely to play a key role in the precise identification of code smells in industrial settings. Unfortunately, there is limited knowledge about the influence of human factors on smell identification. Goal: We aim at investigating whether the precision of smell identification is influenced by three key human factors, namely reviewer's professional background, reviewer's module knowledge and collaboration of reviewers during the task. We also aim at deriving recommendations for allocating human resources to smell identification tasks. Method: We performed 19 comparisons among different subsamples from two trials of a controlled experiment conducted in the context of an empirical study on code smell identification. One trial was conducted in industrial settings while the other had involved graduate students. The diversity of the samples allowed us to analyze the influence of the three factors in isolation and in conjunction. Results: We found that (i) reviewers' collaboration significantly increases the precision of smell identification, but (ii) some professional background is required from the reviewers to reach high precision. Surprisingly, we also found that: (iii) having previous knowledge of the reviewed module does not affect the precision of reviewers with higher professional background. However, this factor was influential on successful identification of more complex smells. Conclusion: We expect that our findings are helpful to support researchers in conducting proper experimental procedures in the future. Besides, they may also be useful for supporting project managers in allocating resources for smell identification tasks.","context, code smell identification, replication, code review, human factors, collaboration","","ESEM '17"
"Conference Paper","Falessi D,Russo B,Mullen K","What If i Had No Smells?","","2017","","","78–84","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.14;http://dx.doi.org/10.1109/ESEM.2017.14","10.1109/ESEM.2017.14","What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.","machine learning, code smells, technical debt, software estimation","","ESEM '17"
"Conference Paper","Ahmed I,Brindescu C,Mannan UA,Jensen C,Sarma A","An Empirical Examination of the Relationship between Code Smells and Merge Conflicts","","2017","","","58–67","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.12;http://dx.doi.org/10.1109/ESEM.2017.12","10.1109/ESEM.2017.12","Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both ""smelly"" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.","code smell, merge conflict, machine learning, empirical analysis","","ESEM '17"
"Conference Paper","Wang Y","Characterizing Developer Behavior in Cloud Based IDEs","","2017","","","48–57","IEEE Press","Markham, Ontario, Canada","Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","","2017","9781509040391","","https://doi.org/10.1109/ESEM.2017.27;http://dx.doi.org/10.1109/ESEM.2017.27","10.1109/ESEM.2017.27","Background: Cloud based integrated development environments (IDEs) are rapidly gaining popularity for its native support and potential to accelerate DevOps. However, there is little research of how developers behave when interacting with these environments.Aims: To develop empirical knowledge about how developers behave when interacting with cloud based IDEs to deal with programming tasks at various difficulty levels.Method: We conducted a user study using a cloud based IDE, JazzHub. We collected and coded session trace data, self-reported effort and frustration levels, and screen recordings.Results: We built a Markov activity transition model that describes the transitions among common development activities such as coding, debugging, and searching for information. It also captures extended interactions with remote resources. We correlated activity transition with different code growth trajectories. Conclusion: The findings are an early step toward realizing the potential for enhanced interactions in cloud based IDEs. Our study provides empirical evidence that may inspire the future evolution of cloud based IDE designs and features.","cloud based IDE, activity transition, developer behavior, code growth trajectory","","ESEM '17"
"Conference Paper","Maleki H,Rahaeimehr R,van Dijk M","SoK: RFID-Based Clone Detection Mechanisms for Supply Chains","","2017","","","33–41","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 Workshop on Attacks and Solutions in Hardware Security","Dallas, Texas, USA","2017","9781450353977","","https://doi.org/10.1145/3139324.3139332;http://dx.doi.org/10.1145/3139324.3139332","10.1145/3139324.3139332","Clone product injection into supply chains causes serious problems for industry and customers. Many mechanisms have been introduced to detect clone products in supply chains which make use of RFID technologies. This article gives an overview of these mechanisms, categorizes them by hardware change requirements, and compares their attributes.","survey, rfid based supply chain, clone detection","","ASHES '17"
"Conference Paper","Liu H,Li C,Jin X,Li J,Zhang Y,Gu D","Smart Solution, Poor Protection: An Empirical Study of Security and Privacy Issues in Developing and Deploying Smart Home Devices","","2017","","","13–18","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 Workshop on Internet of Things Security and Privacy","Dallas, Texas, USA","2017","9781450353960","","https://doi.org/10.1145/3139937.3139948;http://dx.doi.org/10.1145/3139937.3139948","10.1145/3139937.3139948","The concept of Smart Home drives the upgrade of home devices from traditional mode to an Internet-connected version. Instead of developing the smart devices from scratch, manufacturers often utilize existing smart home solutions released by large IT companies (e.g., Amazon, Google) to help build the smart home network. A smart home solution provides components such as software development kit (SDK) and relevant management system to boost the development and deployment of smart home devices. Nonetheless, the participating of third-party SDKs and management systems complicates the workflow of such devices. If not meticulously assessed, the complex workflow often leads to the violation of privacy and security to both the consumer and the manufacturer. In this paper, we illustrate how the security and privacy of smart home devices are affected by JoyLink, a widely used smart home solution. We demonstrate a concrete analysis combined with network traffic interception, source code audit, and binary code reverse engineering to evince that the design of smart home solution is error-prone. We argue that if the security and privacy issues are not considered, devices using the solution are inevitably vulnerable and thus the privacy and security of smart home are seriously threatened.","smart home solution, privacy, security, iot","","IoTS&P '17"
"Conference Paper","Xu X,Liu C,Feng Q,Yin H,Song L,Song D","Neural Network-Based Graph Embedding for Cross-Platform Binary Code Similarity Detection","","2017","","","363–376","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security","Dallas, Texas, USA","2017","9781450349468","","https://doi.org/10.1145/3133956.3134018;http://dx.doi.org/10.1145/3133956.3134018","10.1145/3133956.3134018","The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph-matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.","similarity detection, neural network, binary code","","CCS '17"
"Conference Paper","Duan R,Bijlani A,Xu M,Kim T,Lee W","Identifying Open-Source License Violation and 1-Day Security Risk at Large Scale","","2017","","","2169–2185","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security","Dallas, Texas, USA","2017","9781450349468","","https://doi.org/10.1145/3133956.3134048;http://dx.doi.org/10.1145/3133956.3134048","10.1145/3133956.3134048","With millions of apps available to users, the mobile app market is rapidly becoming very crowded. Given the intense competition, the time to market is a critical factor for the success and profitability of an app. In order to shorten the development cycle, developers often focus their efforts on the unique features and workflows of their apps and rely on third-party Open Source Software (OSS) for the common features. Unfortunately, despite their benefits, careless use of OSS can introduce significant legal and security risks, which if ignored can not only jeopardize security and privacy of end users, but can also cause app developers high financial loss. However, tracking OSS components, their versions, and interdependencies can be very tedious and error-prone, particularly if an OSS is imported with little to no knowledge of its provenance.We therefore propose OSSPolice, a scalable and fully-automated tool for mobile app developers to quickly analyze their apps and identify free software license violations as well as usage of known vulnerable versions of OSS. OSSPolice introduces a novel hierarchical indexing scheme to achieve both high scalability and accuracy, and is capable of efficiently comparing similarities of app binaries against a database of hundreds of thousands of OSS sources (billions of lines of code). We populated OSSPolice with 60K C/C++ and 77K Java OSS sources and analyzed 1.6M free Google Play Store apps. Our results show that 1) over 40K apps potentially violate GPL/AGPL licensing terms, and 2) over 100K of apps use known vulnerable versions of OSS. Further analysis shows that developers violate GPL/AGPL licensing terms due to lack of alternatives, and use vulnerable versions of OSS despite efforts from companies like Google to improve app security. OSSPolice is available on GitHub.","license violation, code clone detection, application security","","CCS '17"
"Conference Paper","Schuster S,Seidl C,Schaefer I","Towards a Development Process for Maturing Delta-Oriented Software Product Lines","","2017","","","41–50","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development","Vancouver, BC, Canada","2017","9781450355186","","https://doi.org/10.1145/3141848.3141853;http://dx.doi.org/10.1145/3141848.3141853","10.1145/3141848.3141853","A Software Product Line (SPL) exploits reuse-in-the-large to enable customization by explicitly modeling commonalities and variabilities of closely related software systems. Delta-Oriented Programming (DOP) is a flexible implementation approach to SPL engineering, which transforms an existing core product to another desired product by applying transformation operations. By capturing product alterations related to configurable functionality within delta modules, DOP closely resembles a natural process of software development, which proves beneficial in early stages of development. However, increasing complexity for a growing SPL in later development stages caused by the invasiveness of DOP drastically impairs maintenance and extensibility. Hence, a process utilizing the invasiveness of DOP in early development stages and restricting it in later stages would allow developers to mature growing delta-oriented SPLs. Moreover, ever-increasing complexity requires means to migrate into less invasive development approaches that are more suited for large-scale configurable applications. To this end, we propose a development process for delta-oriented SPLs including explicit variability points, metrics and refactorings as well as a semi-automatic reengineering of a delta-oriented SPL into a development approach based on blackbox-components. In this paper, we sketch this development process with its constituents and point out required research essential for successfully maturing a delta-oriented SPL.","Delta-Oriented Programming, Software Product Lines","","FOSD 2017"
"Conference Paper","Wirfs-Brock R","Are Software Patterns Simply a Handy Way to Package Design Heuristics?","","2017","","","","The Hillside Group","USA","Proceedings of the 24th Conference on Pattern Languages of Programs","Vancouver, British Columbia, Canada","2017","9781941652060","","","","Billy Vaughn Koen, in Discussion of the Method: Conducting the Engineer's Approach to Problem Solving, defines a heuristic as anything that provides a plausible direction in the solution of a problem, but in the final analysis is unjustified, incapable of justification, and potentially fallible. Software patterns might be considered nicely packaged heuristics in that they provide a context for the problem, and offer plausible solutions along with forces that the designer needs to consider when implementing a solution. Like any heuristic, software patterns come with no guarantees that they will solve the current problem at hand. A dedicated group of authors in the patterns community continues to write patterns, collections of patterns, and more ambitiously weave patterns into pattern languages that attempt to cover paths to solutions in a particular problem space. Are we deluding ourselves about the utility of these efforts? Or is there something important about both the form and use of patterns in the larger context of design heuristics that we need to understand?","","","PLoP '17"
"Conference Paper","Stijlaart M,Zaytsev V","Towards a Taxonomy of Grammar Smells","","2017","","","43–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering","Vancouver, BC, Canada","2017","9781450355254","","https://doi.org/10.1145/3136014.3136035;http://dx.doi.org/10.1145/3136014.3136035","10.1145/3136014.3136035","Any grammar engineer can tell a good grammar from a bad one, but there is no commonly accepted taxonomy of indicators of required grammar refactorings. One of the consequences of this lack of general smell taxonomy is the scarcity of tools to assess and improve the quality of grammars. By combining two lines of research — on smell detection and on grammar transformation — we have assembled a taxonomy of smells in grammars. As a pilot case, the detectors for identified smells were implemented for grammars in a broad sense and applied to the 641 grammars of the Grammar Zoo.","smell detection, grammar engineering","","SLE 2017"
"Conference Paper","Boronat A","Structural Model Subtyping with OCL Constraints","","2017","","","194–205","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering","Vancouver, BC, Canada","2017","9781450355254","","https://doi.org/10.1145/3136014.3136026;http://dx.doi.org/10.1145/3136014.3136026","10.1145/3136014.3136026","In model-driven engineering (MDE), models abstract the relevant features of software artefacts and model management operations, including model transformations, act on them automating large tasks of the development process. Flexible reuse of such operations is an important factor to improve productivity when developing and maintaining MDE solutions. In this work, we revisit the traditional notion of object subtyping based on subsumption, discarded by other approaches to model subtyping. We refine a type system for object-oriented programming, with multiple inheritance, to support model types in order to analyse its advantages and limitations with respect to reuse in MDE. Specifically, we extend type expressions with referential constraints and with OCL constraints. Our approach has been validated with a tool that extracts model types from (EMF) metamodels, paired with their OCL constraints, automatically and that exploits the extended subtyping relation to reuse model management operations. We show that structural model subtyping is expressive enough to support variants of model subtyping, including multiple, partial and dynamic model subtyping. The tool has received the ACM badge ""Artifacts Evaluated - Functional"".","EMF, OCL, type theory, Model subtyping","","SLE 2017"
"Conference Paper","Caldwell J,Chiba S","Reducing Calling Convention Overhead in Object-Oriented Programming on Embedded ARM Thumb-2 Platforms","","2017","","","146–156","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","Vancouver, BC, Canada","2017","9781450355247","","https://doi.org/10.1145/3136040.3136057;http://dx.doi.org/10.1145/3136040.3136057","10.1145/3136040.3136057","This paper examines the causes and extent of code size overhead caused by the ARM calling convention in Thumb-2 binaries. We show that binaries generated from C++ source files generally have higher amounts of calling convention overhead, and present a binary file optimizer to eliminate some of that overhead. Calling convention overhead can negatively impact power consumption, flash memory costs, and chip size in embedded or otherwise resource-constrained domains. This is particularly true on platforms using ""compressed"" instruction sets, such as the 16-bit ARM Thumb and Thumb-2 instruction sets, used in virtually all smartphones and in many other smaller-scale embedded devices. In this paper, we examine the extent of calling convention overhead in practical software, and compare the results of C and C++ programs, and find that C++ programs generally have a higher percentage of calling-convention overhead. Finally, we demonstrate a tool capable of eliminating some of this overhead, particularly in the case of C++ programs, by modifying the calling conventions on a per-procedure basis.","Thumb, calling conventions, ARM, code size, C++","","GPCE 2017"
"Journal Article","Caldwell J,Chiba S","Reducing Calling Convention Overhead in Object-Oriented Programming on Embedded ARM Thumb-2 Platforms","SIGPLAN Not.","2017","52","12","146–156","Association for Computing Machinery","New York, NY, USA","","","2017-10","","0362-1340","https://doi.org/10.1145/3170492.3136057;http://dx.doi.org/10.1145/3170492.3136057","10.1145/3170492.3136057","This paper examines the causes and extent of code size overhead caused by the ARM calling convention in Thumb-2 binaries. We show that binaries generated from C++ source files generally have higher amounts of calling convention overhead, and present a binary file optimizer to eliminate some of that overhead. Calling convention overhead can negatively impact power consumption, flash memory costs, and chip size in embedded or otherwise resource-constrained domains. This is particularly true on platforms using ""compressed"" instruction sets, such as the 16-bit ARM Thumb and Thumb-2 instruction sets, used in virtually all smartphones and in many other smaller-scale embedded devices. In this paper, we examine the extent of calling convention overhead in practical software, and compare the results of C and C++ programs, and find that C++ programs generally have a higher percentage of calling-convention overhead. Finally, we demonstrate a tool capable of eliminating some of this overhead, particularly in the case of C++ programs, by modifying the calling conventions on a per-procedure basis.","Thumb, ARM, code size, calling conventions, C++","",""
"Conference Paper","Zaytsev V","Open Challenges in Incremental Coverage of Legacy Software Languages","","2017","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM SIGPLAN International Workshop on Programming Experience","Vancouver, BC, Canada","2017","9781450355223","","https://doi.org/10.1145/3167105;http://dx.doi.org/10.1145/3167105","10.1145/3167105","Legacy software systems were often written not just in programming languages typically associated with legacy, such as COBOL, JOVIAL and PL/I, but also in decommissioned or deprecated 4GLs. Writing compilers and other migration and renovation tools for such languages is an active business that requires substantial effort but has proven to be a successful strategy for many cases. However, the process of covering such languages (i.e., parsing their close overapproximation and assigning the right assumed semantics to it) is filled with unconventional requirements and limitations: the lack of useful documentation, large scale of codebases, counter-intuitive language engineering principles, buggy reference implementations, fragile workarounds for them, etc. In this short paper, we motivate the incremental nature of software language engineering when it concerns legacy languages in particular, and outline a few related challenges.","legacy software systems, fourth generation programming languages, compiler construction","","PX/17.2"
"Conference Paper","Leopoldseder D","Simulation-Based Code Duplication for Enhancing Compiler Optimizations","","2017","","","10–12","Association for Computing Machinery","New York, NY, USA","Proceedings Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity","Vancouver, BC, Canada","2017","9781450355148","","https://doi.org/10.1145/3135932.3135935;http://dx.doi.org/10.1145/3135932.3135935","10.1145/3135932.3135935","The scope of compiler optimizations is often limited by control flow, which prohibits optimizations across basic block boundaries. Code duplication can solve this problem by extending basic block sizes, thus enabling subsequent optimizations. However, duplicating code for every optimization opportunity may lead to excessive code growth. Therefore, a holistic approach is required that is capable of finding optimization opportunities and classifying their impact. This paper presents a novel approach to determine which code should be duplicated in order to improve peak performance. The approach analyzes duplication candidates for subsequent optimizations opportunities. It does so by simulating a duplication and analyzing its impact on other optimizations. This allows a compiler to weight up multiple success metrics in order to choose those duplications with the maximum optimization potential. We further show how to map code duplication opportunities to an optimization cost model that allows us to maximize performance while minimizing code size increase.","Code Duplication, Tail Duplication, Compiler Optimizations","","SPLASH Companion 2017"
"Conference Paper","Tan C,Yu L,Leners JB,Walfish M","The Efficient Server Audit Problem, Deduplicated Re-Execution, and the Web","","2017","","","546–564","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Symposium on Operating Systems Principles","Shanghai, China","2017","9781450350853","","https://doi.org/10.1145/3132747.3132760;http://dx.doi.org/10.1145/3132747.3132760","10.1145/3132747.3132760","You put a program on a concurrent server, but you don't trust the server; later, you get a trace of the actual requests that the server received from its clients and the responses that it delivered. You separately get logs from the server; these are untrusted. How can you use the logs to efficiently verify that the responses were derived from running the program on the requests? This is the Efficient Server Audit Problem, which abstracts real-world scenarios, including running a web application on an untrusted provider. We give a solution based on several new techniques, including simultaneous replay and efficient verification of concurrent executions. We implement the solution for PHP web applications. For several applications, our verifier achieves 5.6-10.9x speedup versus simply re-executing, with <10% overhead for the server.","","","SOSP '17"
"Journal Article","Lopes CV,Maj P,Martins P,Saini V,Yang D,Zitny J,Sajnani H,Vitek J","DéJàVu: A Map of Code Duplicates on GitHub","Proc. ACM Program. Lang.","2017","1","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2017-10","","","https://doi.org/10.1145/3133908;http://dx.doi.org/10.1145/3133908","10.1145/3133908","Previous studies have shown that there is a non-trivial amount of duplication in source code. This paper analyzes a corpus of 4.5 million non-fork projects hosted on GitHub representing over 428 million files written in Java, C++, Python, and JavaScript. We found that this corpus has a mere 85 million unique files. In other words, 70% of the code on GitHub consists of clones of previously created files. There is considerable variation between language ecosystems. JavaScript has the highest rate of file duplication, only 6% of the files are distinct. Java, on the other hand, has the least duplication, 60% of files are distinct. Lastly, a project-level analysis shows that between 9% and 31% of the projects contain at least 80% of files that can be found elsewhere. These rates of duplication have implications for systems built on open source software as well as for researchers interested in analyzing large code bases. As a concrete artifact of this study, we have created DéjàVu, a publicly available map of code duplicates in GitHub repositories.","Clone Detection, Source Code Analysis","",""
"Journal Article","Mazinanian D,Ketkar A,Tsantalis N,Dig D","Understanding the Use of Lambda Expressions in Java","Proc. ACM Program. Lang.","2017","1","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2017-10","","","https://doi.org/10.1145/3133909;http://dx.doi.org/10.1145/3133909","10.1145/3133909","Java 8 retrofitted lambda expressions, a core feature of functional programming, into a mainstream object-oriented language with an imperative paradigm. However, we do not know how Java developers have adapted to the functional style of thinking, and more importantly, what are the reasons motivating Java developers to adopt functional programming. Without such knowledge, researchers miss opportunities to improve the state of the art, tool builders use unrealistic assumptions, language designers fail to improve upon their designs, and developers are unable to explore efficient and effective use of lambdas. We present the first large-scale, quantitative and qualitative empirical study to shed light on how imperative programmers use lambda expressions as a gateway into functional thinking. Particularly, we statically scrutinize the source code of 241 open-source projects with 19,770 contributors, to study the characteristics of 100,540 lambda expressions. Moreover, we investigate the historical trends and adoption rates of lambdas in the studied projects. To get a complementary perspective, we seek the underlying reasons on why developers introduce lambda expressions, by surveying 97 developers who are introducing lambdas in their projects, using the firehouse interview method. Among others, our findings revealed an increasing trend in the adoption of lambdas in Java: in 2016, the ratio of lambdas introduced per added line of code increased by 54% compared to 2015. Lambdas were used for various reasons, including but not limited to (i) making existing code more succinct and readable, (ii) avoiding code duplication, and (iii) simulating lazy evaluation of functions. Interestingly, we found out that developers are using Java's built-in functional interfaces inefficiently, i.e., they prefer to use general functional interfaces over the specialized ones, overlooking the performance overheads that might be imposed. Furthermore, developers are not adopting techniques from functional programming, e.g., currying. Finally, we present the implications of our findings for researchers, tool builders, language designers, and developers.","The Firehouse Interview Method, Java 8, Functional Programming, Empirical Studies, Multi-paradigm Programming, Lambda Expressions","",""
"Journal Article","Wu B,Campora III JP,Chen S","Learning User Friendly Type-Error Messages","Proc. ACM Program. Lang.","2017","1","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2017-10","","","https://doi.org/10.1145/3133930;http://dx.doi.org/10.1145/3133930","10.1145/3133930","Type inference is convenient by allowing programmers to elide type annotations, but this comes at the cost of often generating very confusing and opaque type error messages that are of little help to fix type errors. Though there have been many successful attempts at making type error messages better in the past thirty years, many classes of errors are still difficult to fix. In particular, current approaches still generate imprecise and uninformative error messages for type errors arising from errors in grouping constructs like parentheses and brackets. Worse, a recent study shows that these errors usually take more than 10 steps to fix and occur quite frequently (around 45% to 60% of all type errors) in programs written by students learning functional programming. We call this class of errors, nonstructural errors. We solve this problem by developing Learnskell, a type error debugger that uses machine learning to help diagnose and deliver high quality error messages, for programs that contain nonstructural errors. While previous approaches usually report type errors on typing constraints or on the type level, Learnskell generates suggestions on the expression level. We have performed an evaluation on more than 1,500 type errors, and the result shows that Learnskell is quite precise. It can correctly capture 86% of all nonstructural errors and locate the error cause with a precision of 63%/87% with the first 1/3 messages, respectively. This is several times more than the precision of state-of-the-art compilers and debuggers. We have also studied the performance of Learnskell and found out that it scales to large programs.","concrete messages, machine learning, structure- changing errors, Type error debugging","",""
"Journal Article","Kaminski T,Kramer L,Carlson T,Van Wyk E","Reliable and Automatic Composition of Language Extensions to C: The AbleC Extensible Language Framework","Proc. ACM Program. Lang.","2017","1","OOPSLA","","Association for Computing Machinery","New York, NY, USA","","","2017-10","","","https://doi.org/10.1145/3138224;http://dx.doi.org/10.1145/3138224","10.1145/3138224","This paper describes an extensible language framework, ableC, that allows programmers to import new, domain-specific, independently-developed language features into their programming language, in this case C. Most importantly, this framework ensures that the language extensions will automatically compose to form a working translator that does not terminate abnormally. This is possible due to two modular analyses that extension developers can apply to their language extension to check its composability. Specifically, these ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is a distinguishing characteristic of the approach. The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard, to make it more amenable to language extension. These include techniques that make additional extensions pass these modular analyses, refactorings of the host language to support a wider range of extensions, and the addition of semantic extension points to support, for example, operator overloading and non-local code transformations.","attribute grammars, language composition, domain specific languages, extensible compiler frameworks, context-aware scanning","",""
"Journal Article","Liu Q,Feng D,Jiang H,Hu Y,Jiao T","Systematic Erasure Codes with Optimal Repair Bandwidth and Storage","ACM Trans. Storage","2017","13","3","","Association for Computing Machinery","New York, NY, USA","","","2017-09","","1553-3077","https://doi.org/10.1145/3109479;http://dx.doi.org/10.1145/3109479","10.1145/3109479","Erasure codes are widely used in distributed storage systems to prevent data loss. Traditional codes suffer from a typical repair-bandwidth problem in which the amount of data required to reconstruct the lost data, referred to as the repair bandwidth, is often far more than the theoretical minimum. While many novel codes have been proposed in recent years to reduce the repair bandwidth, these codes either require extra storage and computation overhead or are only applicable to some special cases.To address the weaknesses of the existing solutions to the repair-bandwidth problem, we propose Z Codes, a general family of codes capable of achieving the theoretical lower bound of repair bandwidth versus storage. To the best of our knowledge, the Z codes are the first general systematic erasure codes that jointly achieve optimal repair bandwidth and storage. Further, we generalize the Z codes to the GZ codes to gain the Maximum Distance Separable (MDS) property. Our evaluations of a real system indicate that Z/GZ and Reed-Solomon (RS) codes show approximately close encoding and repairing speeds, while GZ codes achieve over 37.5% response time reduction for repairing the same size of data, compared to the RS and Cauchy Reed-Solomon (CRS) codes.","failure tolerance, repair bandwidth, Distributed storage system, erasure codes","",""
"Conference Paper","Krüger J,Nell L,Fenske W,Saake G,Leich T","Finding Lost Features in Cloned Systems","","2017","","","65–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Systems and Software Product Line Conference - Volume B","Sevilla, Spain","2017","9781450351195","","https://doi.org/10.1145/3109729.3109736;http://dx.doi.org/10.1145/3109729.3109736","10.1145/3109729.3109736","Copying and adapting a system, also known as clone-and-own, is a common reuse approach that requires little initial effort. However, the drawbacks of clones are increasing maintenance costs as bug fixes and updates must be propagated. To reduce these costs, migrating cloned legacy systems towards a software product line promises to enable systematic reuse and customization. For both, managing and migrating cloned systems, it remains a challenge to identify and map features in the systems. In this paper, we i) propose a semi-automatic process to identify and map features between legacy systems, ii) suggest a corresponding visualization approach, and iii) assess our process on a case study. The results indicate that our process is suitable to identify features and present commonalities and variability in cloned systems. Our process can be used to enable traceability, prepare refactorings, and extract software product lines.","feature location, Software product line, extractive approach, code clone detection, reverse engineering, legacy system","","SPLC '17"
"Conference Paper","Tenev V,Duszynski S,Becker M","Variant Analysis: Set-Based Similarity Visualization for Cloned Software Systems","","2017","","","22–27","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Systems and Software Product Line Conference - Volume B","Sevilla, Spain","2017","9781450351195","","https://doi.org/10.1145/3109729.3109753;http://dx.doi.org/10.1145/3109729.3109753","10.1145/3109729.3109753","Software product lines are frequently created using an extractive approach, in which a group of existing software products is reengineered to extract their reusable core. To direct that effort, it is necessary to analyze the reuse potential and the code similarity across the products. We present Variant Analysis, a tool visualizing code similarity across a group of software systems. We represent the systems as intersecting sets of content elements, and place the elements similar between any n systems into the intersection of the respective n sets. Using the resulting set model and the system structure hierarchy, we provide similarity visualizations scaling for tens of compared software systems and millions lines of code. The current Variant Analysis tool analyzes similarity of text files such as source code. However, the underlying models and visualizations can also be used for other types of data, even beyond the software domain.","set visualization, tools, set model, software cloning, Similarity","","SPLC '17"
"Conference Paper","Hamza M,Walker RJ,Elaasar M","Unanticipated Evolution in Software Product Lines versus Independent Products: A Case Study","","2017","","","97–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Systems and Software Product Line Conference - Volume B","Sevilla, Spain","2017","9781450351195","","https://doi.org/10.1145/3109729.3109739;http://dx.doi.org/10.1145/3109729.3109739","10.1145/3109729.3109739","Real product families need to evolve in ways that are not always anticipated by a pre-planned design. Any given approach for software product lines will likely lead to both positive and negative consequences during unanticipated software evolution. Unfortunately, we know little about the evolvability characteristics of SPL approaches that concern both modelling and implementation, limiting our ability to make rational and disciplined decisions about adoption. We conduct a case study into the unanticipated evolution of a software product family using two approaches: separate products versus a common codebase using delta-oriented programming (DOP). We compare the ease of change within the two versions through a set of quantitative measurements and qualitative observations. We find that both versions have strengths and weaknesses: complexity and incomplete support from DOP tools versus significant duplication and error-proneness in the separate products.","comparative study, Software product lines, separate products, retrospective study, delta-oriented programming, case study, unanticipated evolution","","SPLC '17"
"Conference Paper","Schlie A,Wille D,Schulze S,Cleophas L,Schaefer I","Detecting Variability in MATLAB/Simulink Models: An Industry-Inspired Technique and Its Evaluation","","2017","","","215–224","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Systems and Software Product Line Conference - Volume A","Sevilla, Spain","2017","9781450352215","","https://doi.org/10.1145/3106195.3106225;http://dx.doi.org/10.1145/3106195.3106225","10.1145/3106195.3106225","Model-based languages such as MATLAB/Simulink play an essential role in the model-driven development of software systems. To comply with new requirements, it is common practice to create new variants by copying existing systems and modifying them. Commonly referred to as clone-and-own, severe problems arise in the long-run when no dedicated variability management is installed. To allow for a documented and structured reuse of systems, their variability information needs to be reverse-engineered. In this paper, we propose an advanced comparison procedure, the Matching Window Technique, and a customizable metric. Both allow us to overcome structural alterations commonly performed during clone-and-own. We analyze related MATLAB/Simulink models and determine, classify and represent their variability information in an understandable way. With our technique, we assist model engineers in maintaining and evolving existing variants. We provide three feasibility studies with real-world models from the automotive domain and show our technique to be fast and precise. Furthermore, we perform semi-structured interviews with domain experts to assess the potential applicability of our technique in practice.","software maintainability, variability mining, MATLAB/Simulink","","SPLC '17"
"Conference Paper","Ramesh S,Mahéo A,Shende S,Malony AD,Subramoni H,Panda DK","MPI Performance Engineering with the MPI Tool Interface: The Integration of MVAPICH and TAU","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th European MPI Users' Group Meeting","Chicago, Illinois","2017","9781450348492","","https://doi.org/10.1145/3127024.3127036;http://dx.doi.org/10.1145/3127024.3127036","10.1145/3127024.3127036","MPI implementations are becoming increasingly complex and highly tunable, and thus scalability limitations can come from numerous sources. The MPI Tools Interface (MPI_T) introduced as part of the MPI 3.0 standard provides an opportunity for performance tools and external software to introspect and understand MPI runtime behavior at a deeper level to detect scalability issues. The interface also provides a mechanism to re-configure the MPI library dynamically at runtime to fine-tune performance. In this paper, we propose an infrastructure that extends existing components - TAU, MVAPICH2 and BEACON to take advantage of the MPI_T interface to offer runtime introspection, online monitoring, recommendation generation and autotuning capabilities. We validate our design by developing optimizations for a combination of production and synthetic applications. We use our infrastructure to implement an autotuning policy for AmberMD[1] that monitors and reduces MVAPICH2 library internal memory footprint by 20% without affecting performance. For applications where collective communication is latency sensitive such as MiniAMR[2], our infrastructure is able to generate recommendations to enable hardware offloading of collectives supported by MVAPICH2. By implementing this recommendation, we see a 5% improvement in application runtime.","MVAPICH2, autotuning, performance engineering, runtime introspection, MPI_T, BEACON, performance recommendations, TAU","","EuroMPI '17"
"Conference Paper","Cao Y,Zou Y,Luo Y,Xie B,Zhao J","Refining Traceability Links between Code and Software Documents","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Asia-Pacific Symposium on Internetware","Shanghai, China","2017","9781450353137","","https://doi.org/10.1145/3131704.3131716;http://dx.doi.org/10.1145/3131704.3131716","10.1145/3131704.3131716","Recovering traceability links between source code and software document can be very helpful for Software Maintenance and Software Reuse. Existing work has already achieved good results in extracting code elements (classes, methods, etc.) from software documents. However, it will lead to a lot of noise links if we link a document to all the code elements existing in it. In this paper, we propose an approach to identify the contextual code elements and the salient code elements in a software document, then we can weight the traceability links between source code and software document so that those noise traceability links can be filtered effectively. We measure the saliency of each code element in a document with four kinds of document-related features and three kinds of code-related features, and we adopt TransR-based code embedding technology to evaluate the distance between code elements. In the experiments, we get a precision of 70.7% in recognizing salient code elements of StackOverflow answer documents, which is more than 12% improvement compared with Rigby's work. At the same time, we can filter about 56.5% 69.3% noise traceability links compared with the RecoDoc approach. It will improve the quality of traceability links between source code and related software documents.","software document, contextual code element, code element, traceability link, salient code element","","Internetware '17"
"Conference Paper","Sousa L,Oliveira R,Garcia A,Lee J,Conte T,Oizumi W,de Mello R,Lopes A,Valentim N,Oliveira E,Lucena C","How Do Software Developers Identify Design Problems? A Qualitative Analysis","","2017","","","54–63","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXI Brazilian Symposium on Software Engineering","Fortaleza, CE, Brazil","2017","9781450353267","","https://doi.org/10.1145/3131151.3131168;http://dx.doi.org/10.1145/3131151.3131168","10.1145/3131151.3131168","When a software design decision has a negative impact on one or more quality attributes, we call it a design problem. For example, the Fat Interface problem indicates that an interface exposes non-cohesive services Thus, clients and implementations of this interface may have to handle with services that they are not interested. A design problem such as this hampers the extensibility and maintainability of a software system. As illustrated by the example, a single design problem often affects several elements in the program. Despite its harmfulness, it is difficult to identify a design problem in a system. It is even more challenging to identify design problems when the source code is the only available artifact. In particular, no study has observed what strategy(ies) developers use in practice to identify design problems when the design documentation is unavailable. In order to address this gap, we conducted a qualitative analysis on how developers identify design problems in two different scenarios: when they are either familiar (Scenario 1) or unfamiliar (Scenario 2) with the analyzed systems. Developers familiar with the systems applied a diverse set of strategies during the identification of each design problem. Some strategies were frequently used to locate code elements for analysis, and other strategies were frequently used to confirm design problems in these elements. Developers unfamiliar with the systems relied only on the use of code smells along the task. Despite some differences among the subjects from both scenarios, we noticed that developers often search for multiple indicators during the identification of each design problem.","strategy, symptoms, software design, design problem","","SBES '17"
"Conference Paper","Chávez A,Ferreira I,Fernandes E,Cedrim D,Garcia A","How Does Refactoring Affect Internal Quality Attributes? A Multi-Project Study","","2017","","","74–83","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXI Brazilian Symposium on Software Engineering","Fortaleza, CE, Brazil","2017","9781450353267","","https://doi.org/10.1145/3131151.3131171;http://dx.doi.org/10.1145/3131151.3131171","10.1145/3131151.3131171","Refactoring is a technique commonly applied by developers along the software maintenance and evolution. Software refactoring is expected to improve the internal quality attributes of a software project, such as coupling and cohesion. However, there is limited understanding on to what extent developers achieve this expectation when refactoring their source code This study investigates how refactoring operations affect five well-known internal quality attributes: cohesion, coupling, complexity, inheritance, and size. For this purpose, we analyze the version history of 23 open source projects with 29,303 refactoring operations. Our analysis revealed interesting observations. First, we noticed that developers apply more than 94% of the refactoring operations to code elements with at least one critical internal quality attribute, as opposed to previous work. Second, 65% of the refactoring operations improve their related internal quality attributes and the remaining 35% operations keep the quality attributes unaffected. Third, whenever pure refactoring operations are applied (the so-called root-canal refactoring), we confirm that internal quality attributes are either frequently improved or at least not worsened. Finally, while refactoring operations often reach other specific aims, such as adding a new feature or fixing a bug, 55% of these operations surprisingly improve internal quality attributes against only 10% of quality decline.","software metrics, code structural quality, Refactoring","","SBES '17"
"Conference Paper","Araújo J,Araújo J,Magalhães C,Andrade J,Mota A","Feasibility of Using Source Code Changes on the Selection of Text-Based Regression Test Cases","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing","Fortaleza, Brazil","2017","9781450353021","","https://doi.org/10.1145/3128473.3128481;http://dx.doi.org/10.1145/3128473.3128481","10.1145/3128473.3128481","This paper investigates the relationship between recently modified source code and the selection of text-based regression test cases. The main reason is that our industrial partner uses release notes documents to perform such a selection, but these documents are not so well-written as the text-based test cases. Therefore, we intend to extract useful text from source code to see whether they can serve as a source of keywords in the selection process. We present an experiment that shows promising results about this hypothesis.","Regression campaign, Source code, Test case selection, Information retrieval","","SAST"
"Conference Paper","Souza PP,Sousa BL,Ferreira KA,Bigonha MA","Applying Software Metric Thresholds for Detection of Bad Smells","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse","Fortaleza, Ceará, Brazil","2017","9781450353250","","https://doi.org/10.1145/3132498.3134268;http://dx.doi.org/10.1145/3132498.3134268","10.1145/3132498.3134268","Software metrics can be an effective measurement tool to assess the quality of software. In the literature, there are a lot of software metrics applicable to systems implemented in different paradigms like Objects Oriented Programming (OOP). To guide the use of these metrics in the evaluation of the quality of software systems, it is important to define their thresholds. The aim of this study is to investigate the effectiveness of the thresholds in the evaluation of the quality of object oriented software. To do that, we used a threshold catalog of 18 software metrics derived from 100 software systems to define detection strategies for five bad smells. They are: Large Class, Long Method, Data Class, Feature Envy and Refused Bequest. We investigate the effectiveness of the thresholds in detection analysis of 12 software systems using these strategies. The results obtained by the proposed strategies were compared with the results obtained by the tools JDeodorant and JSPiRIT, used to identify bad smells. This study shows that the metric thresholds were significantly effective in supporting the detection of bad smells.","software quality, software metrics, thresholds, bad smells detection","","SBCARS '17"
"Conference Paper","Nascimento R,Sant'Anna C","Investigating the Relationship between Bad Smells and Bugs in Software Systems","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse","Fortaleza, Ceará, Brazil","2017","9781450353250","","https://doi.org/10.1145/3132498.3132513;http://dx.doi.org/10.1145/3132498.3132513","10.1145/3132498.3132513","Bad smell is a design choice that can degrade different aspects of the source code quality of a software, such as comprehensibility and changeability. Researchers believe that the occurrence of bad smells can lead to the introduction of bugs during maintenance activities. However, there are few studies concerning the impacts of bad smells. Thus, in order to provide more experimental evidence about the impacts of bad smells on software development and maintenance, we conducted an experimental study aiming at investigating the relationship between the occurrence of bad smells and the occurrence of bugs. In this study we evaluated five open-source systems from the Apache community. The results show that classes with bad smells are slightly more related to the occurrence of bugs than classes without bad smells.","software quality, object-oriented design, bad smells, software repositories, bugs","","SBCARS '17"
"Conference Paper","Selim GM,Cordy JR,Dingel J","How is ATL Really Used? Language Feature Use in the ATL Zoo","","2017","","","34–44","IEEE Press","Austin, Texas","Proceedings of the ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems","","2017","9781538634929","","https://doi.org/10.1109/MODELS.2017.20;http://dx.doi.org/10.1109/MODELS.2017.20","10.1109/MODELS.2017.20","Studies of code repositories have long been used to understand the use of programming languages and to provide insight into how they should evolve. Such studies can highlight features that are rarely used and can safely be removed to simplify the language. Conversely, combinations of features that are frequently used together can be identified and possibly replaced with new features to improve the user experience. Unfortunately, this kind of research has not been as popular in Model Driven Development (MDD). More specifically, using repositories of model transformations (in any language) to understand how the features of these languages are used has not been investigated much, despite its potential benefits. In this paper, we study the use of the ATL model transformation language in an ATL transformation repository. We identify three research questions aimed at providing insight into how ATL's features are actually used. Using the TXL source transformation language, we implement a parser-based analyzer to extract information from the ATL Zoo. We use this information to answer these research questions and provide additional observations based on manual inspection of ATL artifacts.","model transformations, TXL, MDD, ATL","","MODELS '17"
"Conference Paper","Bogner J,Wagner S,Zimmermann A","Towards a Practical Maintainability Quality Model for Service-and Microservice-Based Systems","","2017","","","195–198","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings","Canterbury, United Kingdom","2017","9781450352178","","https://doi.org/10.1145/3129790.3129816;http://dx.doi.org/10.1145/3129790.3129816","10.1145/3129790.3129816","Although current literature mentions a lot of different metrics related to the maintainability of Service-based Systems (SBSs), there is no comprehensive quality model (QM) with automatic evaluation and practical focus. To fill this gap, we propose a Maintainability Model for Services (MM4S), a layered maintainability QM consisting of Service Properties (SPs) related with automatically collectable Service Metrics (SMs). This research artifact created within an ongoing Design Science Research (DSR) project is the first version ready for detailed evaluation and critical feedback. The goal of MM4S is to serve as a simple and practical tool for basic maintainability estimation and control in the context of SBSs and their specialization Microservice-based Systems (μSBSs).","service-based systems, maintainability, microservices, SOA, metrics, quality model","","ECSA '17"
"Conference Paper","Lukács D,Tóth M","Structuring Erlang BEAM Control Flow","","2017","","","31–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM SIGPLAN International Workshop on Erlang","Oxford, UK","2017","9781450351799","","https://doi.org/10.1145/3123569.3123572;http://dx.doi.org/10.1145/3123569.3123572","10.1145/3123569.3123572","As source code dependencies are usually stored in some precompiled executable representation like bytecode, static analysis frameworks for high-level languages have to be specifically adapted so they can meaningfully analyse these libraries too. This adaptation is not trivial, since compilation is in general not injective, the semantics of low-level instruction sets are often not specified adequately, and the structure of the high-level sources and the low-level target is considerably different. This is also true for the functional Erlang programming language and its assembly-like BEAM bytecode. In this paper, we present a structuring algorithm capable of recovering the Erlang syntax tree of functional branching expressions compiled to BEAM. The implementation of the presented algorithm is part of the RefactorErl static analyser framework. Therefore, the tool is able to represent the semantics of the BEAM programs with an Erlang syntax tree and perform further semantic analysis on it to discover the source dependencies.","static analysis, BEAM, decompilation","","Erlang 2017"
"Conference Paper","Berta P,Bystrický M,Krempaský M,Vranić V","Employing Issues and Commits for In-Code Sentence Based Use Case Identification and Remodularization","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifth European Conference on the Engineering of Computer-Based Systems","Larnaca, Cyprus","2017","9781450348430","","https://doi.org/10.1145/3123779.3123792;http://dx.doi.org/10.1145/3123779.3123792","10.1145/3123779.3123792","Use case driven modularization improves code comprehension and maintenance and provides another view on software alongside object-oriented modularization. However, approaches enabling use case driven modularization require to modularize code manually. In this paper, we propose an approach to employing issues and commits for in-code sentence based use case identification and remodularization. The approach aims at providing use case based perspective on the existing code. The sentences of use case steps are compared to sentences of issue descriptions, while the sentences generated from the source code of issue commits are compared to sentences generated from the corresponding methods in source code in order to quantify the similarity between use case steps and methods in source code using different similarity calculation algorithms. The resulting level of similarity is used to remodularize source code according to use cases. We conducted a study on the OpenCart open source e-shop employing 16 use cases. The approach achieved the recall of 3.37% and precision of 75%. The success of the approach strongly depends on issues and commits assigned to them. The results would be better especially for the code that natively employs use case driven modularization.","text similarity, information retrieval, aspect-oriented programming, DCI, natural language processing, modularization, traceability links, use case, remodularization, intent","","ECBS '17"
"Conference Paper","Papp D,Buttyán L,Ma Z","Towards Semi-Automated Detection of Trigger-Based Behavior for Software Security Assurance","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th International Conference on Availability, Reliability and Security","Reggio Calabria, Italy","2017","9781450352574","","https://doi.org/10.1145/3098954.3105821;http://dx.doi.org/10.1145/3098954.3105821","10.1145/3098954.3105821","A program exhibits trigger-based behavior if it performs undocumented, often malicious, functions when the environmental conditions and/or specific input values match some pre-specified criteria. Checking whether such hidden functions exist in the program is important for increasing trustworthiness of software. In this paper, we propose a framework to effectively detect trigger-based behavior at the source code level. Our approach is semi-automated: We use automated source code instrumentation and mixed concrete and symbolic execution to generate potentially suspicious test cases that may trigger hidden, potentially malicious functions. The test cases must be investigated by a human analyst manually to decide which of them are real triggers. While our approach is not fully automated, it greatly reduces manual work by allowing analysts to focus on a few test cases found by our automated tools.","Source Code Analysis, Static Analysis, Mixed Concrete and Symbolic Execution, Trigger-based Behavior, Software Security","","ARES '17"
"Journal Article","Davis M,Meehan W,Shivers O","No-Brainer CPS Conversion (Functional Pearl)","Proc. ACM Program. Lang.","2017","1","ICFP","","Association for Computing Machinery","New York, NY, USA","","","2017-08","","","https://doi.org/10.1145/3110267;http://dx.doi.org/10.1145/3110267","10.1145/3110267","Algorithms that convert direct-style λ-calculus terms to their equivalent terms in continuation-passing style (CPS) typically introduce so-called “administrative redexes:” useless artifacts of the conversion that must be cleaned up by a subsequent pass over the result to reduce them away. We present a simple, linear-time algorithm for CPS conversion that introduces no administrative redexes. In fact, the output term is a normal form in a reduction system that generalizes the notion of “administrative redexes” to what we call “no-brainer redexes,” that is, redexes whose reduction shrinks the size of the term. We state the theorems which establish the algorithm's desireable properties, along with sketches of the full proofs.","compiler, functional language, continuation, lambda calculus, continuation-passing style, CPS","",""
"Conference Paper","Dotzler G,Kamp M,Kreutzer P,Philippsen M","More Accurate Recommendations for Method-Level Changes","","2017","","","798–808","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering","Paderborn, Germany","2017","9781450351058","","https://doi.org/10.1145/3106237.3106276;http://dx.doi.org/10.1145/3106237.3106276","10.1145/3106237.3106276","During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments. We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools.","refactoring, recommendation system, Program transformation","","ESEC/FSE 2017"
"Conference Paper","Fu W,Menzies T","Easy over Hard: A Case Study on Deep Learning","","2017","","","49–60","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering","Paderborn, Germany","2017","9781450351058","","https://doi.org/10.1145/3106237.3106256;http://dx.doi.org/10.1145/3106237.3106256","10.1145/3106237.3106256","While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a) a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b) other researchers to repeat, improve, or even refute that original work. For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method. We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives.","parameter tuning, deep learning, data analytics for software engineering, SVM, Search based software engineering, software analytic, differential evolution","","ESEC/FSE 2017"
"Conference Paper","Gold NE,Binkley D,Harman M,Islam S,Krinke J,Yoo S","Generalized Observational Slicing for Tree-Represented Modelling Languages","","2017","","","547–558","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering","Paderborn, Germany","2017","9781450351058","","https://doi.org/10.1145/3106237.3106304;http://dx.doi.org/10.1145/3106237.3106304","10.1145/3106237.3106304","Model-driven software engineering raises the abstraction level making complex systems easier to understand than if written in textual code. Nevertheless, large complicated software systems can have large models, motivating the need for slicing techniques that reduce the size of a model. We present a generalization of observation-based slicing that allows the criterion to be defined using a variety of kinds of observable behavior and does not require any complex dependence analysis. We apply our implementation of generalized observational slicing for tree-structured representations to Simulink models. The resulting slice might be the subset of the original model responsible for an observed failure or simply the sub-model semantically related to a classic slicing criterion. Unlike its predecessors, the algorithm is also capable of slicing embedded Stateflow state machines. A study of nine real-world models drawn from four different application domains demonstrates the effectiveness of our approach at dramatically reducing Simulink model sizes for realistic observation scenarios: for 9 out of 20 cases, the resulting model has fewer than 25% of the original model's elements.","Simulink, Slicing, ORBS, MATLAB, Observational Slicing","","ESEC/FSE 2017"
"Conference Paper","Glanz L,Amann S,Eichberg M,Reif M,Hermann B,Lerch J,Mezini M","CodeMatch: Obfuscation Won't Conceal Your Repackaged App","","2017","","","638–648","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering","Paderborn, Germany","2017","9781450351058","","https://doi.org/10.1145/3106237.3106305;http://dx.doi.org/10.1145/3106237.3106305","10.1145/3106237.3106305","An established way to steal the income of app developers, or to trick users into installing malware, is the creation of repackaged apps. These are clones of - typically - successful apps. To conceal their nature, they are often obfuscated by their creators. But, given that it is a common best practice to obfuscate apps, a trivial identification of repackaged apps is not possible. The problem is further intensified by the prevalent usage of libraries. In many apps, the size of the overall code base is basically determined by the used libraries. Therefore, two apps, where the obfuscated code bases are very similar, do not have to be repackages of each other. To reliably detect repackaged apps, we propose a two step approach which first focuses on the identification and removal of the library code in obfuscated apps. This approach - LibDetect - relies on code representations which abstract over several parts of the underlying bytecode to be resilient against certain obfuscation techniques. Using this approach, we are able to identify on average 70% more used libraries per app than previous approaches. After the removal of an app's library code, we then fuzzy hash the most abstract representation of the remaining app code to ensure that we can identify repackaged apps even if very advanced obfuscation techniques are used. This makes it possible to identify repackaged apps. Using our approach, we found that ≈ 15% of all apps in Android app stores are repackages","code analysis, repackage detection, obfuscation, library detection","","ESEC/FSE 2017"
"Conference Paper","Baylor D,Breck E,Cheng HT,Fiedel N,Foo CY,Haque Z,Haykal S,Ispir M,Jain V,Koc L,Koo CY,Lew L,Mewald C,Modi AN,Polyzotis N,Ramesh S,Roy S,Whang SE,Wicke M,Wilkiewicz J,Zhang X,Zinkevich M","TFX: A TensorFlow-Based Production-Scale Machine Learning Platform","","2017","","","1387–1395","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","Halifax, NS, Canada","2017","9781450348874","","https://doi.org/10.1145/3097983.3098021;http://dx.doi.org/10.1145/3097983.3098021","10.1145/3097983.3098021","Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt.We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.","large-scale machine learning, continuous training, end-to-end platform","","KDD '17"
"Conference Paper","Preschern C","Patterns for C Iterator Interfaces","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd European Conference on Pattern Languages of Programs","Irsee, Germany","2017","9781450348485","","https://doi.org/10.1145/3147704.3147714;http://dx.doi.org/10.1145/3147704.3147714","10.1145/3147704.3147714","Iterating over a set of elements is a very common operation in any program. Some programming languages provide native constructs to iterate over elements and for object-oriented progamming languages there exists guidance in form of design patterns on how to implement generic iteration functionality. However, there is just very few guidance of this kind for procedural programming languages like C. This paper provides such guidance and presents three patterns on how to design generic iterator interfaces in C.","","","EuroPLoP '17"
"Conference Paper","Koyuncu A,Bissyandé TF,Kim D,Klein J,Monperrus M,Le Traon Y","Impact of Tool Support in Patch Construction","","2017","","","237–248","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis","Santa Barbara, CA, USA","2017","9781450350761","","https://doi.org/10.1145/3092703.3092713;http://dx.doi.org/10.1145/3092703.3092713","10.1145/3092703.3092713","In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches.","Repair, Tools, Empirical, Patch, Debugging, Linux, Automation","","ISSTA 2017"
"Conference Paper","Dennis HE,Ward AS,Balson T,Li Y,Henschel R,Slavin S,Simms S,Brunst H","High Performance Computing Enabled Simulation of the Food-Water-Energy System: Simulation of Intensively Managed Landscapes","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact","New Orleans, LA, USA","2017","9781450352727","","https://doi.org/10.1145/3093338.3093381;http://dx.doi.org/10.1145/3093338.3093381","10.1145/3093338.3093381","Domain science experts are commonly limited by computational efficiency of their code and hardware resources available for execution of desired simulations. Here, we detail a collaboration between domain scientists focused on simulating an ensemble of climate and human management decisions to drive environmental (e.g., water quality) and economic (e.g., crop yield) outcomes. Briefly, the domain scientists developed a message passing interface to execute the formerly serial code across a number of processors, anticipating significant performance improvement by moving to a cluster computing environment from their desktop machines. The code is both too complex to efficiently re-code from scratch and has a shared codebase that must continue to function on desktop machines as well as the parallel implementation. However, inefficiencies in the code caused the LUSTRE filesystem to bottleneck performance for all users. The domain scientists collaborated with Indiana University's Science Applications and Performance Tuning and High Performance File System teams to address the unforeseen performance limitations. The non-linear process of testing software advances and hardware performance is a model of the failures and successes that can be anticipated in similar applications. Ultimately, through a series of iterative software and hardware advances the team worked collaboratively to increase performance of the code, cluster, and file system to enable more than 100-fold increases in performance. As a result, the domain science is able to assess ensembles of climate and human forcing on the model, and sensitivities of ecologically and economically important outcomes of intensively managed agricultural landscapes.","vampir, parallel computing, modeling, hpc, case study, Agro-IBIS, scaling, filesystems, meta-data, benchmarking, performance, mpi, lustre, agro-ecosystem, computer cluster","","PEARC17"
"Conference Paper","Cateté V,Barnes T","Application of the Delphi Method in Computer Science Principles Rubric Creation","","2017","","","164–169","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education","Bologna, Italy","2017","9781450347044","","https://doi.org/10.1145/3059009.3059042;http://dx.doi.org/10.1145/3059009.3059042","10.1145/3059009.3059042","Growing public demand for computer science (CS) education in K-12 schools requires an increase in well-qualified and well-supported computing teachers. To alleviate the lack of K-12 computing teachers, CS education researchers have focused on hosting professional development workshops to prepare in-service teachers from other disciplines to teach introductory level computing courses. In addition to the curriculum knowledge and pedagogical content knowledge taught in the professional development workshops, these new teachers need support in computer science subject matter knowledge throughout the school year. In particular, these new teachers find it difficult to grade programs and labs. This research study uses two variations of the Delphi Method to create learning-oriented rubrics for Computer Science Principles teachers using the Beauty and Joy of Computing curriculum. To perform this study we implemented (1) a heavy-weight, heterogeneous wide-net Delphi, and (2) a lower-weight, homogeneous Delphi composed of master teachers. These methods resulted in the creation of two systematically- and rigorously-created rubrics that produce consistent grading and very similar inter-rater reliabilities.","delphi method, evaluation, bjc, rubrics, ap cs principles","","ITiCSE '17"
"Conference Paper","Dewey K,Conrad P,Craig M,Morozova E","Evaluating Test Suite Effectiveness and Assessing Student Code via Constraint Logic Programming","","2017","","","317–322","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education","Bologna, Italy","2017","9781450347044","","https://doi.org/10.1145/3059009.3059051;http://dx.doi.org/10.1145/3059009.3059051","10.1145/3059009.3059051","A good suite of test inputs is an indispensable tool both for manual and automated assessment of student submissions to programming assignments. Yet, without a way to evaluate our test suites, it is difficult to know how well we are doing, much less improve our practice. We present a technique for evaluating a hand-generated test suite by comparing its ability to find defects against that of a test suite generated automatically using Constraint Logic Programming (CLP). We describe our technique and present a case study using student submissions for an assignment from a second-year programming course. Our results show that a CLP-generated test suite was able to identify significant defects that the instructor-generated suite missed, despite having similar code coverage.","testing, automated test-generation, constraint logic programming","","ITiCSE '17"
"Conference Paper","Hellas A,Leinonen J,Ihantola P","Plagiarism in Take-Home Exams: Help-Seeking, Collaboration, and Systematic Cheating","","2017","","","238–243","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education","Bologna, Italy","2017","9781450347044","","https://doi.org/10.1145/3059009.3059065;http://dx.doi.org/10.1145/3059009.3059065","10.1145/3059009.3059065","Due to the increased enrollments in Computer Science education programs, institutions have sought ways to automate and streamline parts of course assessment in order to be able to invest more time in guiding students' work.This article presents a study of plagiarism behavior in an introductory programming course, where a traditional pen-and-paper exam was replaced with multiple take-home exams. The students who took the take-home exam enabled a software plugin that recorded their programming process. During an analysis of the students' submissions, potential plagiarism cases were highlighted, and students were invited to interviews.The interviews with the candidates for plagiarism highlighted three types of plagiarism behaviors: help-seeking, collaboration, and systematic cheating. Analysis of programming process traces indicates that parts of such behavior are detectable directly from programming process data.","educational data mining, plagiarism, programming process data","","ITiCSE '17"
"Conference Paper","Keuning H,Heeren B,Jeuring J","Code Quality Issues in Student Programs","","2017","","","110–115","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education","Bologna, Italy","2017","9781450347044","","https://doi.org/10.1145/3059009.3059061;http://dx.doi.org/10.1145/3059009.3059061","10.1145/3059009.3059061","Because low quality code can cause serious problems in software systems, students learning to program should pay attention to code quality early. Although many studies have investigated mistakes that students make during programming, we do not know much about the quality of their code. This study examines the presence of quality issues related to program flow, choice of programming constructs and functions, clarity of expressions, decomposition and modularization in a large set of student Java programs. We investigated which issues occur most frequently, if students are able to solve these issues over time and if the use of code analysis tools has an effect on issue occurrence. We found that students hardly fix issues, in particular issues related to modularization, and that the use of tooling does not have much effect on the occurrence of issues.","programming education, code quality","","ITiCSE '17"
"Conference Paper","Sheard J,Simon,Butler M,Falkner K,Morgan M,Weerasinghe A","Strategies for Maintaining Academic Integrity in First-Year Computing Courses","","2017","","","244–249","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education","Bologna, Italy","2017","9781450347044","","https://doi.org/10.1145/3059009.3059064;http://dx.doi.org/10.1145/3059009.3059064","10.1145/3059009.3059064","Safeguarding academic integrity is an issue of concern to all computing academics due to high and rising levels of plagiarism and other cheating in computing courses. There have been many studies of the cheating and plagiarism practices of computing students and the factors that can influence these practices, and a variety of strategies for reducing cheating have been proposed. This national study of first-year computing programs provides insights into what strategies computing academics use to discourage or prevent their students from cheating. Having interviewed 30 academics from 25 universities we found 21 different types of strategy, which we classified into five themes: education; discouraging cheating; making cheating difficult; and empowerment. We also found that academics often employ strategies across all of these themes.","academic integrity, cheating, cs1, assessment, plagiarism","","ITiCSE '17"
"Conference Paper","Besta M,Podstawski M,Groner L,Solomonik E,Hoefler T","To Push or To Pull: On Reducing Communication and Synchronization in Graph Computations","","2017","","","93–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing","Washington, DC, USA","2017","9781450346993","","https://doi.org/10.1145/3078597.3078616;http://dx.doi.org/10.1145/3078597.3078616","10.1145/3078597.3078616","We reduce the cost of communication and synchronization in graph processing by analyzing the fastest way to process graphs: pushing the updates to a shared state or pulling the updates to a private state. We investigate the applicability of this push-pull dichotomy to various algorithms and its impact on complexity, performance, and the amount of used locks, atomics, and reads/writes. We consider 11 graph algorithms, 3 programming models, 2 graph abstractions, and various families of graphs. The conducted analysis illustrates surprising differences between push and pull variants of different algorithms in performance, speed of convergence, and code complexity; the insights are backed up by performance data from hardware counters. We use these findings to illustrate which variant is faster for each algorithm and to develop generic strategies that enable even higher speedups. Our insights can be used to accelerate graph processing engines or libraries on both massively-parallel shared-memory machines as well as distributed-memory systems.","graph computations","","HPDC '17"
"Conference Paper","Shwartz O,Birk Y","SeM: A CPU Architecture Extension for Secure Remote Computing","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Hardware and Architectural Support for Security and Privacy","Toronto, ON, Canada","2017","9781450352666","","https://doi.org/10.1145/3092627.3092631;http://dx.doi.org/10.1145/3092627.3092631","10.1145/3092627.3092631","In shared (multi-user) computing environments, platform software (OS, Hypervisor, VMM etc.) and most of the hardware cannot always be trusted (e.g., public clouds), so ensuring the confidentiality and integrity of a user's program (code and data) is critical. It is highly desirable to do so efficiently while accepting existing application binaries, being able to use the services of untrusted software, not modifying the OS, and with minimal intervention in the system's flow. We present the Secure Machine (SeM), a CPU architecture extension that, unlike previous approaches, does all this. Using novel fine-grained cache and register protection managed by a CPU-resident, publicly identifiable hardware Security Management Unit (SMU), we address both software attacks and off-chip hardware attacks. SeM accepts existing application binaries, which are instrumented automatically, and only incurs negligible performance, power, and area overheads relative to an unprotected platform. SeM is extendable to parallel programs and multiple nodes.","","","HASP '17"
"Conference Paper","Baumann R,Protsenko M,Müller T","Anti-ProGuard: Towards Automated Deobfuscation of Android Apps","","2017","","","7–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th Workshop on Security in Highly Connected IT Systems","Neuchâtel, Switzerland","2017","9781450352710","","https://doi.org/10.1145/3099012.3099020;http://dx.doi.org/10.1145/3099012.3099020","10.1145/3099012.3099020","A wide adoption of obfuscation techniques by Android application developers, and especially malware authors, introduces a high degree of complication into the process of reverse engineering, analysis, and security evaluation of third-party and potentially harmful apps.In this paper we present the early results of our research aiming to provide reliable means for automated deobfuscation of Android apps. According to the underlying approach, deobfuscation of a given app is performed by matching its code parts to the unobfuscated code stored in a database. For this purpose we apply well-known software similarity algorithms, such as SimHash and n-gram based ones. As a source of unobfuscated code can serve open source apps and libraries, as well as previously analyzed and manually deobfuscated code.Although the presented techniques are generic in their nature, our current prototype mainly targets Proguard, as one of the most widely used protection tools for Android performing primarily renaming obfuscation. The evaluation of the presented Anti-ProGuard tool witnesses its effectiveness for the considered task and supports the feasibility of the proposed approach.","Deobfuscation, Reverse Engineering, Android, Software Similarity","","SHCIS '17"
"Conference Paper","Garousi V,Felderer M","Experience-Based Guidelines for Effective and Efficient Data Extraction in Systematic Reviews in Software Engineering","","2017","","","170–179","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering","Karlskrona, Sweden","2017","9781450348041","","https://doi.org/10.1145/3084226.3084238;http://dx.doi.org/10.1145/3084226.3084238","10.1145/3084226.3084238","To systematically collect evidence and to structure a given area in software engineering (SE), Systematic Literature Reviews (SLR) and Systematic Mapping (SM) studies have become common. Data extraction is one of the main phases (activities) when conducting an SM or an SLR, whose objective is to extract required data from the primary studies and to accurately record the information researchers need to answer the questions of the SM/SLR study. Based on experience in a large number of SM/SLR studies, we and many other researchers have found the data extraction in SLRs to be time consuming and error-prone, thus raising the real need for heuristics and guidelines for effective and efficient data extraction in these studies, especially to be learnt by junior and young researchers. As a 'guideline' paper, this paper contributes a synthesized list of challenges usually faced during SLRs' data extraction phase and the corresponding solutions (guidelines). For our synthesis, we consider two data sources: (1) the pool of 16 SLR studies in which the authors have been involved in, as well as (2) a review of challenges and guidelines in the existing literature. Our experience in utilizing the presented guidelines in the near past have helped our junior colleagues to conduct data extractions more effectively and efficiently.","research methodology, SLR, empirical software engineering, Systematic mapping studies, systematic literature reviews, SM, data extraction","","EASE'17"
"Conference Paper","Izquierdo-Cortazar D,Sekitoleko N,Gonzalez-Barahona JM,Kurth L","Using Metrics to Track Code Review Performance","","2017","","","214–223","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering","Karlskrona, Sweden","2017","9781450348041","","https://doi.org/10.1145/3084226.3084247;http://dx.doi.org/10.1145/3084226.3084247","10.1145/3084226.3084247","During 2015, some members of the Xen Project Advisory Board became worried about the performance of their code review process. The Xen Project is a free, open source software project developing one of the most popular virtualization platforms in the industry. They use a pre-commit peer review process similar to that in the Linux kernel, based on email messages. They had observed a large increase over time in the number of messages related to code review, and were worried about how this could be a signal of problems with their code review process.To address these concerns, we designed and conducted, with their continuous feedback, a detailed analysis focused on finding these problems, if any. During the study, we dealt with the methodological problems of Linux-like code review, and with the deeper issue of finding metrics that could uncover the problems they were worried about. For having a benchmark, we run the same analysis on a similar project, which uses very similar code review practices: the Linux Netdev (Netdev) project. As a result, we learned how in fact the Xen Project had some problems, but at the moment of the analysis those were already under control. We found as well how different the Xen and Netdev projects were behaving with respect to code review performance, despite being so similar from many points of view.In this paper we show the results of both analyses, and propose a comprehensive methodology, fully automated, to study Linux-style code review. We discuss also the problems of getting significant metrics to track improvements or detect problems in this kind of code review.","Software development analytics, Data mining, Code review","","EASE'17"
"Conference Paper","David Y,Partush N,Yahav E","Similarity of Binaries through Re-Optimization","","2017","","","79–94","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation","Barcelona, Spain","2017","9781450349888","","https://doi.org/10.1145/3062341.3062387;http://dx.doi.org/10.1145/3062341.3062387","10.1145/3062341.3062387","We present a scalable approach for establishing similarity between stripped binaries (with no debug information). The main challenge in binary similarity, is to establish similarity even when the code has been compiled using different compilers, with different optimization levels, or targeting different architectures. Overcoming this challenge, while avoiding false positives, is invaluable to the process of reverse engineering and the process of locating vulnerable code. We present a technique that is scalable and precise, as it alleviates the need for heavyweight semantic comparison by performing out-of-context re-optimization of procedure fragments. It works by decomposing binary procedures to comparable fragments and transforming them to a canonical, normalized form using the compiler optimizer, which enables finding equivalent fragments through simple syntactic comparison. We use a statistical framework built by analyzing samples collected ""in the wild"" to generate a global context that quantifies the significance of each pair of fragments, and uses it to lift pairwise fragment equivalence to whole procedure similarity. We have implemented our technique in a tool called GitZ and performed an extensive evaluation. We show that GitZ is able to perform millions of comparisons efficiently, and find similarity with high accuracy.","static binary analysis, statistical similarity, binary code search","","PLDI 2017"
"Journal Article","David Y,Partush N,Yahav E","Similarity of Binaries through Re-Optimization","SIGPLAN Not.","2017","52","6","79–94","Association for Computing Machinery","New York, NY, USA","","","2017-06","","0362-1340","https://doi.org/10.1145/3140587.3062387;http://dx.doi.org/10.1145/3140587.3062387","10.1145/3140587.3062387","We present a scalable approach for establishing similarity between stripped binaries (with no debug information). The main challenge in binary similarity, is to establish similarity even when the code has been compiled using different compilers, with different optimization levels, or targeting different architectures. Overcoming this challenge, while avoiding false positives, is invaluable to the process of reverse engineering and the process of locating vulnerable code. We present a technique that is scalable and precise, as it alleviates the need for heavyweight semantic comparison by performing out-of-context re-optimization of procedure fragments. It works by decomposing binary procedures to comparable fragments and transforming them to a canonical, normalized form using the compiler optimizer, which enables finding equivalent fragments through simple syntactic comparison. We use a statistical framework built by analyzing samples collected ""in the wild"" to generate a global context that quantifies the significance of each pair of fragments, and uses it to lift pairwise fragment equivalence to whole procedure similarity. We have implemented our technique in a tool called GitZ and performed an extensive evaluation. We show that GitZ is able to perform millions of comparisons efficiently, and find similarity with high accuracy.","statistical similarity, static binary analysis, binary code search","",""
"Conference Paper","Yessenov K,Kuraj I,Solar-Lezama A","DemoMatch: API Discovery from Demonstrations","","2017","","","64–78","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation","Barcelona, Spain","2017","9781450349888","","https://doi.org/10.1145/3062341.3062386;http://dx.doi.org/10.1145/3062341.3062386","10.1145/3062341.3062386","We introduce DemoMatch, a tool for API discovery that allows the user to discover how to implement functionality using a software framework by demonstrating the functionality in existing applications built with the same framework. DemoMatch matches the demonstrations against a database of execution traces called Semeru and generates code snippets explaining how to use the functionality. We evaluated DemoMatch on several case studies involving Java Swing and Eclipse RCP.","Software engineering tools, Data driven","","PLDI 2017"
"Journal Article","Yessenov K,Kuraj I,Solar-Lezama A","DemoMatch: API Discovery from Demonstrations","SIGPLAN Not.","2017","52","6","64–78","Association for Computing Machinery","New York, NY, USA","","","2017-06","","0362-1340","https://doi.org/10.1145/3140587.3062386;http://dx.doi.org/10.1145/3140587.3062386","10.1145/3140587.3062386","We introduce DemoMatch, a tool for API discovery that allows the user to discover how to implement functionality using a software framework by demonstrating the functionality in existing applications built with the same framework. DemoMatch matches the demonstrations against a database of execution traces called Semeru and generates code snippets explaining how to use the functionality. We evaluated DemoMatch on several case studies involving Java Swing and Eclipse RCP.","Software engineering tools, Data driven","",""
"Conference Paper","Abdelfattah A,Haidar A,Tomov S,Dongarra J","Novel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Supercomputing","Chicago, Illinois","2017","9781450350204","","https://doi.org/10.1145/3079079.3079103;http://dx.doi.org/10.1145/3079079.3079103","10.1145/3079079.3079103","This paper presents a software framework for solving large numbers of relatively small matrix problems using GPUs. Our approach combines novel and existing HPC techniques to methodically apply performance analysis, kernel design, low-level optimizations, and autotuning to exceed in performance proprietary vendor libraries. As a case study, we discuss the fundamental matrix operations defined by the Basic Linear Algebra Subprograms (BLAS) standard. This case study is significantly important for wide range of applications, including astrophysics, tensor contractions, sparse direct solvers, and others. We provide a generic design that is capable of dealing with problems of different sizes, and handling the irregularity arising from size variations. The developed solution adopts a batched computation scheme, where the same operation is concurrently applied to all matrices within a single computational kernel. The paper discusses the data layout, kernel design, and optimization techniques. We also propose a design scheme that is centralized around matrix-matrix multiplication (GEMM) kernel, so that any improvement on this particular kernel propagates automatically to other routines. Our performance results show significant speedups using a Pascal generation GPU (Tesla P100) against state-of-the-art solutions using cuBLAS, as well as against two 10-core Haswell CPUs running the MKL library. This work is part of the MAGMA library.","batched computation, GPU computing, basic linear algebra sub-programs","","ICS '17"
"Journal Article","Mills C,Bavota G,Haiduc S,Oliveto R,Marcus A,Lucia A","Predicting Query Quality for Applications of Text Retrieval to Software Engineering Tasks","ACM Trans. Softw. Eng. Methodol.","2017","26","1","","Association for Computing Machinery","New York, NY, USA","","","2017-05","","1049-331X","https://doi.org/10.1145/3078841;http://dx.doi.org/10.1145/3078841","10.1145/3078841","Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts.Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself.Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery.Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average.Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.","Text retrieval, concept location, artifact traceability","",""
"Conference Paper","Charalampidou S,Ampatzoglou A,Chatzigeorgiou A,Avgeriou P","Assessing Code Smell Interest Probability: A Case Study","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XP2017 Scientific Workshops","Cologne, Germany","2017","9781450352642","","https://doi.org/10.1145/3120459.3120465;http://dx.doi.org/10.1145/3120459.3120465","10.1145/3120459.3120465","An important parameter in deciding to eliminate technical debt (TD) is the probability of a module to generate interest along software evolution. In this study, we explore code smells, which according to practitioners are the most commonly occurring type of TD in industry, by assessing the associated interest probability. As a proxy of smell interest probability we use the frequency of smell occurrences and the change proneness of the modules in which they are identified. To achieve this goal we present a case study on 47,751 methods extracted from two well-known open source projects. The results of the case study suggest that: (a) modules in which ""code smells"" are concentrated are more change-prone than smell-free modules, (b) there are specific types of ""code smells"" that are concentrated in the most change-prone modules, and (c) interest probability of code clones seems to be higher than the other two examined code smells. These results can be useful for both researchers and practitioners, in the sense that the former can focus their research on resolving ""code smells"" that produce more interest, and the latter can improve accordingly the prioritization of their repayment strategy and their training.","interest probability, case study, technical debt, change proneness","","XP '17"
"Conference Paper","Haendler T,Sobernig S,Strembeck M","Towards Triaging Code-Smell Candidates via Runtime Scenarios and Method-Call Dependencies","","2017","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XP2017 Scientific Workshops","Cologne, Germany","2017","9781450352642","","https://doi.org/10.1145/3120459.3120468;http://dx.doi.org/10.1145/3120459.3120468","10.1145/3120459.3120468","Managing technical debt includes the detection and assessment of debt at the code and design levels (such as bad smells). Existing approaches and tools for smell detection primarily use static program data for decision support. While a static analysis allows for identifying smell candidates without executing and instrumenting the system, such approaches also come with the risk of missing candidates or of producing false positives. Moreover, smell candidates might result from a deliberate design decision (e.g., of applying a particular design pattern). Such risks and the general ambivalence of smell detection require a manual design and/or code inspection for reviewing all alleged smells.In this paper, we propose an approach to obtain tailorable design documentation for object-oriented systems based on runtime tests. In particular, the approach supports a tool-supported triaging of code-smell candidates. We use runtime scenario tests to extract execution traces. Based on these execution traces, different (automatically derived) model perspectives on method-call dependencies (e.g., dependency structure matrices, DSMs; UML2 sequence diagrams) are then used as decision support for assessing smell candidates. Our approach is implemented as part of the KaleidoScope tool which is publicly available for download.","unified modeling language (UML2), dependency structure matrix, execution trace, scenario-based testing, code smell, decision support, technical debt, design documentation, software behavior","","XP '17"
"Conference Paper","Hu Y,Zhang Y,Li J,Gu D","Binary Code Clone Detection across Architectures and Compiling Configurations","","2017","","","88–98","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.22;http://dx.doi.org/10.1109/ICPC.2017.22","10.1109/ICPC.2017.22","Binary code clone detection (or similarity comparison) is a fundamental technique for many important applications, such as plagiarism detection, malware analysis, software vulnerability assessment and program comprehension. With the prevailing of smart and IoT (Internet of Things) devices, more and more programs are ported from traditional desktop platforms (e.g., IA-32) to ARM and MIPS architectures. It becomes imperative to detect cloned binary code across architectures. However, because of incomparable instruction sets of different architectures as well as alternative compiling configurations, it is difficult to conduct a binary code clone detection with traditional syntax- or structure-based methods.To address, we propose a semantics-based approach to fulfill the target. We recognize arguments and indirect jump targets of each binary function, and emulate executions of those functions, extracting semantic signatures to measure the similarity of functions. The approach has been implemented in a prototype system named CACompare to detect cloned binary functions across architectures and compiling configurations. It supports comparisons between mainstream architectures (IA-32, ARM and MIPS) and is able to analyse binaries on the Linux platform. The experimental results show that CACompare not only is effective in dealing with binaries of different architectures and variant compiling configurations, but also improves the accuracy of binary code clone detection comparing to state-of-the-art solutions.","reverse engineering, binary program analysis, code clone detection, static analysis","","ICPC '17"
"Conference Paper","Dang Y,Zhang D,Ge S,Huang R,Chu C,Xie T","Transferring Code-Clone Detection and Analysis to Practice","","2017","","","53–62","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track","","2017","9781538627174","","https://doi.org/10.1109/ICSE-SEIP.2017.6;http://dx.doi.org/10.1109/ICSE-SEIP.2017.6","10.1109/ICSE-SEIP.2017.6","During software development, code clones are commonly produced, in the form of a number of the same or similar code fragments spreading within one or many large code bases. Numerous research projects have been carried out on empirical studies or tool support for detecting or analyzing code clones. However, in practice, few such research projects have resulted in substantial industry adoption. In this paper, we report our experiences of transferring XIAO, a code-clone detection and analysis approach and its supporting tool, to broad industrial practices: (1) shipped in Visual Studio 2012, a widely used industrial IDE; (2) deployed and intensively used at the Microsoft Security Response Center. According to our experiences, technology transfer is a rather complicated journey that needs significant efforts from both the technical aspect and social aspect. From the technical aspect, significant efforts are needed to adapt a research prototype to a product-quality tool that addresses the needs of real scenarios, to be integrated into a mainstream product or development process. From the social aspect, there are strong needs to interact with practitioners to identify killer scenarios in industrial settings, figure out the gap between a research prototype and a tool fitting the needs of real scenarios, to understand the requirements of releasing with a mainstream product, being integrated into a development process, understanding their release cadence, etc.","","","ICSE-SEIP '17"
"Conference Paper","Svajlenko J,Roy CK","Fast and Flexible Large-Scale Clone Detection with CloneWorks","","2017","","","27–30","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.3;http://dx.doi.org/10.1109/ICSE-C.2017.3","10.1109/ICSE-C.2017.3","Clone detection in very-large inter-project repositories has numerous applications in software research and development However, existing tools do not provide the flexibility researchers need to explore this emerging domain. We introduce CloneWorks, a fast and flexible clone detector for large-scale clone detection experiments. CloneWorks gives the user full control over the representation of the source code before clone detection, including easy plug-in of custom source transformation, normalization and filtering logic. The user can then perform targeted clone detection for any type or kind of clone of interest CloneWorks uses our fast and scalable partitioned partial indexes approach, which can handle any input size on an average workstation using input partitioning. CloneWorks can detect Type-3 clones in an input as large as 250 million lines of code in just four hours on an average workstation, with good recall and precision as measured by our BigCloneBench.","code clone, scalable, flexible, clone detection, fast","","ICSE-C '17"
"Conference Paper","Svajlenko J,Roy CK","CloneWorks: A Fast and Flexible Large-Scale near-Miss Clone Detection Tool","","2017","","","177–179","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.78;http://dx.doi.org/10.1109/ICSE-C.2017.78","10.1109/ICSE-C.2017.78","Clone detection within large inter-project source-code repositories has numerous rich applications. CloneWorks is a fast and flexible clone detector for large-scale near-miss clone detection experiments. CloneWorks gives the user full control over the processing of the source code before clone detection, enabling the user to target any clone type or perform custom clone detection experiments. Scalable clone detection is achieved, even on commodity hardware, using our partitioned partial indexes approach. CloneWorks scales to 250MLOC in just four hours on an average workstation with good recall and precision.","fast, scalable, code clone, clone detection, flexible","","ICSE-C '17"
"Conference Paper","Krüger J","Lost in Source Code: Physically Separating Features in Legacy Systems","","2017","","","461–462","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.46;http://dx.doi.org/10.1109/ICSE-C.2017.46","10.1109/ICSE-C.2017.46","Feature-oriented programming allows developers to physically separate and reuse features via composition. This promises several benefits compared to other reuse approaches, for instance, easier traceability and maintenance. However, due to their simplicity cloning and annotation-based product lines are established in practice. We aim to reduce risks and costs of migrating towards composition, lowering the adoption barrier. This includes i) processes, ii) migration approaches, and iii) assessing advantages and disadvantages. Overall, we will facilitate integrating physical separation into legacy applications.","software product line, extraction, migration","","ICSE-C '17"
"Conference Paper","Su FH","Uncovering Features in Kindred Programs","","2017","","","477–478","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.176;http://dx.doi.org/10.1109/ICSE-C.2017.176","10.1109/ICSE-C.2017.176","The detection of similar code can support many software engineering tasks such as program understanding and API replacement. Many excellent approaches have been proposed to detect programs having similar syntactic features. However, some programs dynamically or statistically close to each other, which we call kindred programs, may be ignored. We believe the detection of kindred programs can enhance or even automate the tasks relevant to program classification. In this proposal, we will discuss our current approaches to mine kindred programs having similar functional features and behavioral features. We will also roadmap our on-going development that integrates program analysis with machine learning models to extract statistical features from codebases.","simion, program feature, static analysis, code similarity, code clone, dynamic analysis, code relative","","ICSE-C '17"
"Conference Paper","Ishio T,Sakaguchi Y,Ito K,Inoue K","Source File Set Search for Clone-and-Own Reuse Analysis","","2017","","","257–268","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.19;http://dx.doi.org/10.1109/MSR.2017.19","10.1109/MSR.2017.19","Clone-and-own approach is a natural way of source code reuse for software developers. To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one. Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete. In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages). Our method employs an efficient file similarity computation using b-bit minwise hashing technique. We use an aggregated file similarity for ranking components To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code! The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages. Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.","software reuse, source code search, file clone detection, origin analysis","","MSR '17"
"Conference Paper","McAfee P,Mkaouer MW,Krutz DE","CATE: Concolic Android Testing Using Java Pathfinder for Android Applications","","2017","","","213–214","IEEE Press","Buenos Aires, Argentina","Proceedings of the 4th International Conference on Mobile Software Engineering and Systems","","2017","9781538626696","","https://doi.org/10.1109/MOBILESoft.2017.35;http://dx.doi.org/10.1109/MOBILESoft.2017.35","10.1109/MOBILESoft.2017.35","Like all software systems, Android applications are not immune to bugs, security vulnerabilities, and a wide range of other runtime errors. Concolic analysis, a hybrid software verification technique which performs symbolic execution along with a concrete execution path, has been used for a variety of purposes including software testing, code clone detection, and security-related activities. We created a new publicly available concolic analysis tool for analyzing Android applications: Concolic Android TEster (CATE). Building on Java Path Finder (JPF-SPF), this tool performs concolic analysis on a raw Android application file (or source code) and provides output in a useful and easy to understand format.","","","MOBILESoft '17"
"Conference Paper","Yang D,Martins P,Saini V,Lopes C","Stack Overflow in Github: Any Snippets There?","","2017","","","280–290","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.13;http://dx.doi.org/10.1109/MSR.2017.13","10.1109/MSR.2017.13","When programmers look for how to achieve certain programming tasks, Stack Overflow is a popular destination in search engine results. Over the years, Stack Overflow has accumulated an impressive knowledge base of snippets of code that are amply documented. We are interested in studying how programmers use these snippets of code in their projects. Can we find Stack Overflow snippets in real projects? When snippets are used, is this copy literal or does it suffer adaptations? And are these adaptations specializations required by the idiosyncrasies of the target artifact, or are they motivated by specific requirements of the programmer? The large-scale study presented on this paper analyzes 909k non-fork Python projects hosted on Github, which contain 290M function definitions, and 1.9M Python snippets captured in Stack Overflow. Results are presented as quantitative analysis of block-level code cloning intra and inter Stack Overflow and GitHub, and as an analysis of programming behaviors through the qualitative analysis of our findings.","code reuse, code clone, large-scale analysis","","MSR '17"
"Conference Paper","Gharehyazie M,Ray B,Filkov V","Some from Here, Some from There: Cross-Project Code Reuse in GitHub","","2017","","","291–301","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.15;http://dx.doi.org/10.1109/MSR.2017.15","10.1109/MSR.2017.15","Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts.To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub.","","","MSR '17"
"Conference Paper","Hatano T,Matsuo A","Removing Code Clones from Industrial Systems Using Compiler Directives","","2017","","","336–345","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.4;http://dx.doi.org/10.1109/ICPC.2017.4","10.1109/ICPC.2017.4","Refactoring of code clones is an effective method for improving software maintainability. Existing studies have proposed automated techniques and tools for refactoring. However, it is difficult to apply refactoring to our industrial systems in practice because of three main reasons. First, we have many industrial systems written in COBOL which requires a particular refactoring method compared with current techniques because Type-2 clones in COBOL are generated by renaming parts of identifiers. Second, nested clones must be refactored, in which an instance of a clone set is contained within an instance of another clone set. They also make it difficult to estimate the reduction size by refactoring. Third, refactoring requires testing which is time-consuming and laborious. To overcome these problems, we developed an approach for refactoring of Type-2 clones in COBOL programs. Our approach identifies actual refactorable clone sets and includes a string comparison technique to parameterize partial differences in identifier names. The clone sets are extracted as shared code fragments and transformed into the refactored code using compiler directives. It is easy to confirm that refactoring using compiler directives preserves program behavior, because they do not change program structure. We also provide a method that makes it possible to refactor nested clones by ordering their refactoring. This method enables to estimate how many lines can be reduced by refactoring. We applied the approach to four industrial systems to assess how many lines can be reduced. The results show that the lines could be reduced by 10 to 15% and one system was reduced by 27%. We also discuss the parameter number required for our refactoring approach.","","","ICPC '17"
"Conference Paper","Tsantalis N,Mazinanian D,Rostami S","Clone Refactoring with Lambda Expressions","","2017","","","60–70","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.14;http://dx.doi.org/10.1109/ICSE.2017.14","10.1109/ICSE.2017.14","Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means.","code duplication, refactoring, lambda expressions","","ICSE '17"
"Conference Paper","Molderez T,Stevens R,De Roover C","Mining Change Histories for Unknown Systematic Edits","","2017","","","248–256","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.12;http://dx.doi.org/10.1109/MSR.2017.12","10.1109/MSR.2017.12","Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case.","systematic edits, frequent itemset mining, change distilling","","MSR '17"
"Conference Paper","Palomba F,Zaidman A,Oliveto R,De Lucia A","An Exploratory Study on the Relationship between Changes and Refactoring","","2017","","","176–185","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.38;http://dx.doi.org/10.1109/ICPC.2017.38","10.1109/ICPC.2017.38","Refactoring aims at improving the internal structure of a software system without changing its external behavior. Previous studies empirically assessed, on the one hand, the benefits of refactoring in terms of code quality and developers' productivity, and on the other hand, the underlying reasons that push programmers to apply refactoring. Results achieved in the latter investigations indicate that besides personal motivation such as the responsibility concerned with code authorship, refactoring is mainly performed as a consequence of changes in the requirements rather than driven by software quality. However, these findings have been derived by surveying developers, and therefore no software repository study has been carried out to corroborate the achieved findings. To bridge this gap, we provide a quantitative investigation on the relationship between different types of code changes (i.e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) and 28 different refactoring types coming from 3 open source projects. Results showed that developers tend to apply a higher number of refactoring operations aimed at improving maintainability and comprehensibility of the source code when fixing bugs. Instead, when new features are implemented, more complex refactoring operations are performed to improve code cohesion. Most of the times, the underlying reasons behind the application of such refactoring operations are represented by the presence of duplicate code or previously introduced self-admitted technical debts.","empirical studies, refactoring, code changes","","ICPC '17"
"Conference Paper","Shatnawi A,Mili H,Boussaidi GE,Boubaker A,Guéhéneuc YG,Moha N,Privat J,Abdellatif M","Analyzing Program Dependencies in Java EE Applications","","2017","","","64–74","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.6;http://dx.doi.org/10.1109/MSR.2017.6","10.1109/MSR.2017.6","Program dependency artifacts such as call graphs help support a number of software engineering tasks such as software mining, program understanding, debugging, feature location, software maintenance and evolution. Java Enterprise Edition (JEE) applications represent a significant part of the recent legacy applications, and we are interested in modernizing them. This modernization involves, among other things, analyzing dependencies between their various components/tiers. JEE applications tend to be multilanguage, rely on JEE container services, and make extensive use of late binding techniques-all of which makes finding such dependencies difficult. In this paper, we describe some of these difficulties and how we addressed them to build a dependency call graph. We developed our tool called DeJEE (Dependencies in JEE) as an Eclipse plug-in. We applied DeJEE on two open-source JEE applications: Java PetStore and JSP Blog. The results show that DeJEE is able to identify different types of JEE dependencies.","program dependency, code analysis, server pages, Java EE application, modernization, container services","","MSR '17"
"Conference Paper","Mondal M,Roy CK,Schneider KA","Identifying Code Clones Having High Possibilities of Containing Bugs","","2017","","","99–109","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.31;http://dx.doi.org/10.1109/ICPC.2017.31","10.1109/ICPC.2017.31","Code cloning has emerged as a controversial term in software engineering research and practice because of its positive and negative impacts on software evolution and maintenance. Researchers suggest managing code clones through refactoring and tracking. Given the huge number of code clones in a software system's code-base, it is essential to identify the most important ones to manage. In our research, we investigate which clone fragments have high possibilities of containing bugs so that such clones can be prioritized for refactoring and tracking to help minimize future bug-fixing tasks. Existing studies on clone bug-proneness cannot pinpoint code clones that are likely to experience bug-fixes in the future.According to our analysis on thousands of revisions of four diverse subject systems written in Java, change frequency of code clones does not indicate their bug-proneness (i.e., does not indicate their tendencies of experiencing bug-fixes in future). Bug-proneness is mainly related with change recency of code clones. In other words, more recently changed code clones have a higher possibility of containing bugs. Moreover, for the code clones that were not changed previously we observed that clones that were created more recently have higher possibilities of experiencing bug-fixes. Thus, our research reveals the fact that bug-proneness of code clones mainly depends on how recently they were changed or created (for the ones that were not changed before). It invalidates the common intuition regarding the relatedness between high change frequency and bug-proneness. We believe that code clones should be prioritized for management considering their change recency or recency of creation (for the unchanged ones).","","","ICPC '17"
"Conference Paper","Sousa BL,Souza PP,Fernandes E,Ferreira KA,Bigonha MA","FindSmells: Flexible Composition of Bad Smell Detection Strategies","","2017","","","360–363","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.8;http://dx.doi.org/10.1109/ICPC.2017.8","10.1109/ICPC.2017.8","Bad smells are symptoms of problems in the source code of software systems. They may harm the maintenance and evolution of systems on different levels. Thus, detecting smells is essential in order to support the software quality improvement. Since even small systems may contain several bad smell instances, and considering that developers have to prioritize their elimination, its automated detection is a necessary support for developers. Regarding that, detection strategies have been proposed to formalize rules to detect specific bad smells, such as Large Class and Feature Envy. Several tools like JDeodorant and JSpIRIT implement these strategies but, in general, they do not provide full customization of the formal rules that define a detection strategy. In this paper, we propose FindSmells, a tool for detecting bad smells in software systems through software metrics and their thresholds. With FindSmells, the user can compose and manage different strategies, which run without source code analysis. We also provide a running example of the tool.A Video: https://youtu.be/LtomN93y6gg","","","ICPC '17"
"Conference Paper","Silva D,Valente MT","RefDiff: Detecting Refactorings in Version Histories","","2017","","","269–279","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.14;http://dx.doi.org/10.1109/MSR.2017.14","10.1109/MSR.2017.14","Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches.","git, software repositories, refactoring, software evolution","","MSR '17"
"Conference Paper","Rolim R,Soares G,D'Antoni L,Polozov O,Gulwani S,Gheyi R,Suzuki R,Hartmann B","Learning Syntactic Program Transformations from Examples","","2017","","","404–415","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.44;http://dx.doi.org/10.1109/ICSE.2017.44","10.1109/ICSE.2017.44","Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present Refazer, a technique for automatically learning program transformations. Refazer builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, Refazer leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations.We instantiate and evaluate Refazer in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, Refazer learns the intended program transformation in 84% of the cases using only 2.9 examples on average.","program synthesis, tutoring systems, refactoring, program transformation","","ICSE '17"
"Conference Paper","Martinez J,Ziadi T,Bissyandé TF,Klein J,Traon YL","Bottom-up Technologies for Reuse: Automated Extractive Adoption of Software Product Lines","","2017","","","67–70","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.15;http://dx.doi.org/10.1109/ICSE-C.2017.15","10.1109/ICSE-C.2017.15","Adopting Software Product Line (SPL) engineering principles demands a high up-front investment. Bottom-Up Technologies for Reuse (BUT4Reuse) is a generic and extensible tool aimed to leverage existing similar software products in order to help in extractive SPL adoption. The envisioned users are 1) SPL adopters and 2) Integrators of techniques and algorithms to provide automation in SPL adoption activities. We present the methodology it implies for both types of users and we present the validation studies that were already conducted. BUT4Reuse tool and source code are publicly available under the EPL license.Website: http://but4reuse.github.ioVideo: https://www.youtube.com/watch?v=pa62Yc9LWyk","reverse engineering, software product line engineering, variability management, extractive software product line adoption","","ICSE-C '17"
"Conference Paper","Lin B,Ponzanelli L,Mocci A,Bavota G,Lanza M","On the Uniqueness of Code Redundancies","","2017","","","121–131","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.36;http://dx.doi.org/10.1109/ICPC.2017.36","10.1109/ICPC.2017.36","Code redundancy widely occurs in software projects. Researchers have investigated the existence, causes, and impacts of code redundancy, showing that it can be put to good use, for example in the context of code completion. When analyzing source code redundancy, previous studies considered software projects as sequences of tokens, neglecting the role of the syntactic structures enforced by programming languages. However, differences in the redundancy of such structures may jeopardize the performance of applications leveraging code redundancy.We present a study of the redundancy of several types of code constructs in a large-scale dataset of active Java projects mined from GitHub, unveiling that redundancy is not uniform and mainly resides in specific code constructs. We further investigate the implications of the locality of redundancy by analyzing the performance of language models when applied to code completion. Our study discloses the perils of exploiting code redundancy without taking into account its strong locality in specific code constructs.","code redundancy, code completion, empirical study","","ICPC '17"
"Conference Paper","Li M,Wang W,Wang P,Wang S,Wu D,Liu J,Xue R,Huo W","LibD: Scalable and Precise Third-Party Library Detection in Android Markets","","2017","","","335–346","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.38;http://dx.doi.org/10.1109/ICSE.2017.38","10.1109/ICSE.2017.38","With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis.According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.","software mining, android, third-party library","","ICSE '17"
"Conference Paper","Yamashita A,Abtahizadeh SA,Khomh F,Guéhéneuc YG","Software Evolution and Quality Data from Controlled, Multiple, Industrial Case Studies","","2017","","","507–510","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.44;http://dx.doi.org/10.1109/MSR.2017.44","10.1109/MSR.2017.44","A main difficulty to study the evolution and quality of real-life software systems is the effect of moderator factors, such as: programming skill, type of maintenance task, and learning effect. Experimenters must account for moderator factors to identify the relationships between the variables of interest. In practice, controlling for moderator factors in realistic (industrial) settings is expensive and rather difficult. The data presented in this paper has two particularities: First, it involves six professional developers and four real-life, industrial systems. Second, it was obtained from controlled, multiple case studies where the moderator variables: programming skill, maintenance task, and learning effect were controlled for. This data set is relevant to experimenters studying evolution and quality of reallife systems, in particular those interested in studying industrial systems and replicating empirical studies.","code smells, case study, software evolution, software quality, software defects, industrial data, replication, moderator factors, empirical study, software replicability","","MSR '17"
"Conference Paper","Palomba F,Salza P,Ciurumelea A,Panichella S,Gall H,Ferrucci F,De Lucia A","Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews","","2017","","","106–117","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.18;http://dx.doi.org/10.1109/ICSE.2017.18","10.1109/ICSE.2017.18","Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce ChangeAdvisor, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44 683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of ChangeAdvisor in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).","mobile apps, natural language processing, impact analysis, mining user reviews","","ICSE '17"
"Conference Paper","Tang Y,Leung H","Constructing Feature Model by Identifying Variability-Aware Modules","","2017","","","263–274","IEEE Press","Buenos Aires, Argentina","Proceedings of the 25th International Conference on Program Comprehension","","2017","9781538605356","","https://doi.org/10.1109/ICPC.2017.21;http://dx.doi.org/10.1109/ICPC.2017.21","10.1109/ICPC.2017.21","Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.","feature model recovery, product line, configuration, feature modules, variability-aware modularity","","ICPC '17"
"Conference Paper","Chen B,Jiang ZM","Characterizing and Detecting Anti-Patterns in the Logging Code","","2017","","","71–81","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.15;http://dx.doi.org/10.1109/ICSE.2017.15","10.1109/ICSE.2017.15","Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code).In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.","empirical studies, software maintenance, anti-patterns, logging code, logging practices","","ICSE '17"
"Conference Paper","Xu L,Dou W,Gao C,Wang J,Wei J,Zhong H,Huang T","SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering","","2017","","","158–169","IEEE Press","Buenos Aires, Argentina","Proceedings of the 14th International Conference on Mining Software Repositories","","2017","9781538615447","","https://doi.org/10.1109/MSR.2017.28;http://dx.doi.org/10.1109/MSR.2017.28","10.1109/MSR.2017.28","Version information plays an important role in spreadsheet understanding, maintaining and quality improving. However, end users rarely use version control tools to document spreadsheets' version information. Thus, the spreadsheets' version information is missing, and different versions of a spreadsheet coexist as individual and similar spreadsheets. Existing approaches try to recover spreadsheet version information through clustering these similar spreadsheets based on spreadsheet filenames or related email conversation. However, the applicability and accuracy of existing clustering approaches are limited due to the necessary information (e.g., filenames and email conversation) is usually missing.We inspected the versioned spreadsheets in VEnron, which is extracted from the Enron Corporation. In VEnron, the different versions of a spreadsheet are clustered into an evolution group. We observed that the versioned spreadsheets in each evolution group exhibit certain common features (e.g., similar table headers and worksheet names). Based on this observation, we proposed an automatic clustering algorithm, SpreadCluster. SpreadCluster learns the criteria of features from the versioned spreadsheets in VEnron, and then automatically clusters spreadsheets with the similar features into the same evolution group. We applied SpreadCluster on all spreadsheets in the Enron corpus. The evaluation result shows that SpreadCluster could cluster spreadsheets with higher precision (78.5% vs. 59.8%) and recall rate (70.7% vs. 48.7%) than the filename-based approach used by VEnron. Based on the clustering result by SpreadCluster, we further created a new versioned spreadsheet corpus VEnron2, which is much bigger than VEnron (12,254 vs. 7,294 spreadsheets). We also applied SpreadCluster on the other two spreadsheet corpora FUSE and EUSES. The results show that SpreadCluster can cluster the versioned spreadsheets in these two corpora with high precision (91.0% and 79.8%).","spreadsheet, evolution, version, clustering","","MSR '17"
"Conference Paper","Mazinanian D,Tsantalis N","CSSDev: Refactoring Duplication in Cascading Style Sheets","","2017","","","63–66","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering Companion","","2017","9781538615898","","https://doi.org/10.1109/ICSE-C.2017.7;http://dx.doi.org/10.1109/ICSE-C.2017.7","10.1109/ICSE-C.2017.7","Cascading Style Sheets (CSS) is a widely-used language for defining the presentation of structured documents and user interfaces. Despite its popularity, CSS still lacks adequate tool support for everyday maintenance tasks, such as debugging and refactoring. In this paper, we present CSSDev, a tool suite for analyzing CSS code to detect refactoring opportunities. (https://youtu.be/lu3oITi1XrQ)","cascading style sheets, refactoring, preprocessors","","ICSE-C '17"
"Conference Paper","Zhang T,Kim M","Automated Transplantation and Differential Testing for Clones","","2017","","","665–676","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.67;http://dx.doi.org/10.1109/ICSE.2017.67","10.1109/ICSE.2017.67","Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level.In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of Grafter, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.","","","ICSE '17"
"Conference Paper","Sedano T,Ralph P,Péraire C","Software Development Waste","","2017","","","130–140","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering","","2017","9781538638682","","https://doi.org/10.1109/ICSE.2017.20;http://dx.doi.org/10.1109/ICSE.2017.20","10.1109/ICSE.2017.20","Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user.Objective: The purpose of this paper is to identify and describe different types of waste in software development.Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation.Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development.Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types.Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.","extreme programming, lean software development, software engineering waste","","ICSE '17"
"Conference Paper","Wang J,Cui Q,Wang S,Wang Q","Domain Adaptation for Test Report Classification in Crowdsourced Testing","","2017","","","83–92","IEEE Press","Buenos Aires, Argentina","Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track","","2017","9781538627174","","https://doi.org/10.1109/ICSE-SEIP.2017.8;http://dx.doi.org/10.1109/ICSE-SEIP.2017.8","10.1109/ICSE-SEIP.2017.8","In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification.To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly- used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value.","domain adaptation, test report classification, crowdsourced testing, deep learning","","ICSE-SEIP '17"
"Conference Paper","Gao J,Pang B,Lumetta SS","Automated Feedback Framework for Introductory Programming Courses","","2016","","","53–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education","Arequipa, Peru","2016","9781450342315","","https://doi.org/10.1145/2899415.2899440;http://dx.doi.org/10.1145/2899415.2899440","10.1145/2899415.2899440","Using automated grading tools to provide feedback to students is common in Computer Science education. The first step of automated grading is to find defects in the student program. However, finding bugs in code has never been easy. Comparing computation results using a fixed set of test cases is still the most common way to determine correctness among current automated grading tools. It takes time and effort to design a good set of test cases that can test the student code thoroughly. In practice, tests used for grading are often insufficient for accurate diagnosis.In this paper, we present our utilization of industrial automated testing on student assignments in an introductory programming course. We implemented a framework to collect student codes and apply industrial automated testing to their codes. Then we interpreted the results obtained from testing in a way that students can understand easily. We deployed our framework on five different introductory C programming assignments here at the University of Illinois at Urbana-Champaign.The results show that the automated feedback generation framework can discover more errors inside student submissions and can provide timely and useful feedback to both instructors and students. A total of 142 missed bugs were found within 446 submissions. More than 50% of students received their feedback within 3 minutes of submission.","computer science education, concolic testing, auto grader","","ITiCSE '16"
"Conference Paper","Simon,Sheard J,Morgan M,Petersen A,Settle A,Sinclair J,Cross G,Riedesel C","Negotiating the Maze of Academic Integrity in Computing Education","","2016","","","57–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2016 ITiCSE Working Group Reports","Arequipa, Peru","2016","9781450348829","","https://doi.org/10.1145/3024906.3024910;http://dx.doi.org/10.1145/3024906.3024910","10.1145/3024906.3024910","Academic integrity in computing education is a source of much confusion and disagreement. Studies of student and academic approaches to academic integrity in computing indicate considerable variation in practice along with confusion as to what practices are acceptable. The difficulty appears to arise in part from perceived differences between academic practice in computing education and professional practice in the computing industry, which lead to challenges in devising a consistent and meaningful approach to academic integrity. Coding practices in industry rely heavily on teamwork and use of external resources, but when computing educators seek to model industry practice in the classroom these techniques tend to conflict with standard academic integrity policies, which focus on assessing individual achievement.We have surveyed both industry professionals and computing academics about practices relating to academic integrity, and can confirm the uncertainty and variability that permeates the field. We find clear divergence in the views of these two groups, and also a broad range of practices considered acceptable by the academics.Our findings establish a clear need to clarify academic integrity issues in the context of computing education. Educators must carefully consider how academic integrity issues relate to their learning objectives, teaching approaches, and the industry practice for which they are preparing students. To this end we propose a process that fulfils two purposes: to guide academics in the consideration of academic integrity issues when designing assessment items, and to effectively communicate the resulting guidelines to students so as to reduce confusion and improve educational practice.","plagiarism, academic integrity, programming education, collusion","","ITiCSE '16"
"Conference Paper","Preschern C","API Patterns in C","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st European Conference on Pattern Languages of Programs","Kaufbeuren, Germany","2016","9781450340748","","https://doi.org/10.1145/3011784.3011791;http://dx.doi.org/10.1145/3011784.3011791","10.1145/3011784.3011791","Iterating over some elements is a commonly required operation in most programs. In most programming languages there is either a built-in mechanism for iteration, or there is at least some guidance, like the Iterator design pattern for object-oriented programming languages, which describes how to program an iteration mechanism. However, there is no such guidance for procedural programming languages like C. This paper provides such guidance and presents patterns describing the interface of iterators in C.","C programming, software engineering, API, patterns","","EuroPlop '16"
"Conference Paper","Modiba P,Pieterse V,Haskins B","Evaluating Plagiarism Detection Software for Introductory Programming Assignments","","2016","","","37–46","Association for Computing Machinery","New York, NY, USA","Proceedings of the Computer Science Education Research Conference 2016","Pretoria, South Africa","2016","9781450344920","","https://doi.org/10.1145/2998551.2998558;http://dx.doi.org/10.1145/2998551.2998558","10.1145/2998551.2998558","Plagiarism is an issue that all educators have had to deal with. Large numbers of students and assignments have resulted in the development of automated systems to detect code similarities with the aim of identifying cases that may have been plagiarised. These systems are of great value to assessors, allowing them to process submissions automatically. However, these automated systems do present possible disadvantages and drawbacks. In this study we explore and analyse the differences between various systems as well as how their performance compares with manual checking. We consider the different methods students use when committing plagiarism. Then we examine more closely the systems that can aid plagiarism detection, ranging from their characteristics to how they work. In the process, we determine how these systems compare with our own system and their suitability for aiding the identification of submissions which may have been plagiarised in our introductory C++ course.","Automatic detection, Code plagiarism, Introductory programming","","CSERC '16"
"Journal Article","Ouni A,Kessentini M,Sahraoui H,Inoue K,Deb K","Multi-Criteria Code Refactoring Using Search-Based Software Engineering: An Industrial Case Study","ACM Trans. Softw. Eng. Methodol.","2016","25","3","","Association for Computing Machinery","New York, NY, USA","","","2016-06","","1049-331X","https://doi.org/10.1145/2932631;http://dx.doi.org/10.1145/2932631","10.1145/2932631","One of the most widely used techniques to improve the quality of existing software systems is refactoring—the process of improving the design of existing code by changing its internal structure without altering its external behavior. While it is important to suggest refactorings that improve the quality and structure of the system, many other criteria are also important to consider, such as reducing the number of code changes, preserving the semantics of the software design and not only its behavior, and maintaining consistency with the previously applied refactorings. In this article, we propose a multi-objective search-based approach for automating the recommendation of refactorings. The process aims at finding the optimal sequence of refactorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes required to fix those defects, (iii) preserves design semantics, and (iv) maximizes the consistency with the previously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source systems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push down method, push down field, inline class, move class, extract class, extract method, and extract interface) and six commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class, shotgun surgery, and feature envy) through an empirical study conducted with experts. In addition, we performed an industrial validation of our technique, with 10 software engineers, on a large project provided by our industrial partner. We found that the proposed refactorings succeed in preserving the design coherence of the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings applied in the past to similar contexts.","multi-objective optimization, refactoring, software evolution, software maintenance, Search-based software engineering","",""
"Journal Article","Candela I,Bavota G,Russo B,Oliveto R","Using Cohesion and Coupling for Software Remodularization: Is It Enough?","ACM Trans. Softw. Eng. Methodol.","2016","25","3","","Association for Computing Machinery","New York, NY, USA","","","2016-06","","1049-331X","https://doi.org/10.1145/2928268;http://dx.doi.org/10.1145/2928268","10.1145/2928268","Refactoring and, in particular, remodularization operations can be performed to repair the design of a software system and remove the erosion caused by software evolution. Various approaches have been proposed to support developers during the remodularization of a software system. Most of these approaches are based on the underlying assumption that developers pursue an optimal balance between cohesion and coupling when modularizing the classes of their systems. Thus, a remodularization recommender proposes a solution that implicitly provides a (near) optimal balance between such quality attributes. However, there is still no empirical evidence that such a balance is the desideratum by developers. This article aims at analyzing both objectively and subjectively the aforementioned phenomenon. Specifically, we present the results of (1) a large study analyzing the modularization quality, in terms of package cohesion and coupling, of 100 open-source systems, and (2) a survey conducted with 29 developers aimed at understanding the driving factors they consider when performing modularization tasks. The results achieved have been used to distill a set of lessons learned that might be considered to design more effective remodularization recommenders.","software quality, Remodularization","",""
"Conference Paper","Laurenzano MA,Zhang Y,Chen J,Tang L,Mars J","PowerChop: Identifying and Managing Non-Critical Units in Hybrid Processor Architectures","","2016","","","140–152","IEEE Press","Seoul, Republic of Korea","Proceedings of the 43rd International Symposium on Computer Architecture","","2016","9781467389471","","https://doi.org/10.1109/ISCA.2016.22;http://dx.doi.org/10.1109/ISCA.2016.22","10.1109/ISCA.2016.22","On-core microarchitectural structures consume significant portions of a processor's power budget. However, depending on application characteristics, those structures do not always provide (much) performance benefit. While timeout-based power gating techniques have been leveraged for underutilized cores and inactive functional units, these techniques have not directly translated to high-activity units such as vector processing units, complex branch predictors, and caches. The performance benefit provided by these units does not necessarily correspond with unit activity, but instead is a function of application characteristics.This work introduces PowerChop, a novel technique that leverages the unique capabilities of HW/SW co-designed hybrid processors to enact unit-level power management at the application phase level. PowerChop adds two small additional hardware units to facilitate phase identification and triggering different power states, enabling the software layer to cheaply track, predict and take advantage of varying unit criticality across application phases by powering gating units that are not needed for performant execution. Through detailed experimentation, we find that PowerChop significantly decreases power consumption, reducing the leakage power of a hybrid server processor by 9% on average (up to 33%) and a hybrid mobile processor by 19% (up to 40%) while introducing just 2% slowdown.","","","ISCA '16"
"Journal Article","Laurenzano MA,Zhang Y,Chen J,Tang L,Mars J","PowerChop: Identifying and Managing Non-Critical Units in Hybrid Processor Architectures","SIGARCH Comput. Archit. News","2016","44","3","140–152","Association for Computing Machinery","New York, NY, USA","","","2016-06","","0163-5964","https://doi.org/10.1145/3007787.3001152;http://dx.doi.org/10.1145/3007787.3001152","10.1145/3007787.3001152","On-core microarchitectural structures consume significant portions of a processor's power budget. However, depending on application characteristics, those structures do not always provide (much) performance benefit. While timeout-based power gating techniques have been leveraged for underutilized cores and inactive functional units, these techniques have not directly translated to high-activity units such as vector processing units, complex branch predictors, and caches. The performance benefit provided by these units does not necessarily correspond with unit activity, but instead is a function of application characteristics.This work introduces PowerChop, a novel technique that leverages the unique capabilities of HW/SW co-designed hybrid processors to enact unit-level power management at the application phase level. PowerChop adds two small additional hardware units to facilitate phase identification and triggering different power states, enabling the software layer to cheaply track, predict and take advantage of varying unit criticality across application phases by powering gating units that are not needed for performant execution. Through detailed experimentation, we find that PowerChop significantly decreases power consumption, reducing the leakage power of a hybrid server processor by 9% on average (up to 33%) and a hybrid mobile processor by 19% (up to 40%) while introducing just 2% slowdown.","","",""
"Conference Paper","Arjomand M,Kandemir MT,Sivasubramaniam A,Das CR","Boosting Access Parallelism to PCM-Based Main Memory","","2016","","","695–706","IEEE Press","Seoul, Republic of Korea","Proceedings of the 43rd International Symposium on Computer Architecture","","2016","9781467389471","","https://doi.org/10.1109/ISCA.2016.66;http://dx.doi.org/10.1109/ISCA.2016.66","10.1109/ISCA.2016.66","Despite its promise as a DRAM main memory replacement, Phase Change Memory (PCM) has high write latencies which can be a serious detriment to its widespread adoption. Apart from slowing down a write request, the consequent high latency can also keep other chips of the same rank, that are not involved in this write, idle for long times. There are several practical considerations that make it difficult to allow subsequent reads and/or writes to be served concurrently from the same chips during the long latency write. This paper proposes and evaluates several novel mechanisms -- re-constructing data from error correction bits instead of waiting for chips currently busy to serve a read, rotating word mappings across chips of a PCM rank, and rotating the mapping of error detection/correction bits across these chips -- to overlap several reads with an ongoing write (RoW) and even a write with an ongoing write (WoW). The paper also presents the necessary micro-architectural enhancements nee-ded to implement these mechanisms, without significantly changing the current interfaces. The resulting PCM access parallelism (PCMap) system incorporating these enhancements, boosts the intra-rank-level parallelism during such writes from a very low baseline value of 2.4 to an average and maximum values of 4.5 and 7.4, respectively (out of a maximum of 8.0), across a wide spectrum of both multiprogrammed and multithreaded workloads. This boost in parallelism results in an average IPC improvement of 15.6% and 16.7% for the multi-progra-mmed and multi-threaded workloads, respectively.","write performance, phase change memory","","ISCA '16"
"Journal Article","Arjomand M,Kandemir MT,Sivasubramaniam A,Das CR","Boosting Access Parallelism to PCM-Based Main Memory","SIGARCH Comput. Archit. News","2016","44","3","695–706","Association for Computing Machinery","New York, NY, USA","","","2016-06","","0163-5964","https://doi.org/10.1145/3007787.3001211;http://dx.doi.org/10.1145/3007787.3001211","10.1145/3007787.3001211","Despite its promise as a DRAM main memory replacement, Phase Change Memory (PCM) has high write latencies which can be a serious detriment to its widespread adoption. Apart from slowing down a write request, the consequent high latency can also keep other chips of the same rank, that are not involved in this write, idle for long times. There are several practical considerations that make it difficult to allow subsequent reads and/or writes to be served concurrently from the same chips during the long latency write. This paper proposes and evaluates several novel mechanisms -- re-constructing data from error correction bits instead of waiting for chips currently busy to serve a read, rotating word mappings across chips of a PCM rank, and rotating the mapping of error detection/correction bits across these chips -- to overlap several reads with an ongoing write (RoW) and even a write with an ongoing write (WoW). The paper also presents the necessary micro-architectural enhancements nee-ded to implement these mechanisms, without significantly changing the current interfaces. The resulting PCM access parallelism (PCMap) system incorporating these enhancements, boosts the intra-rank-level parallelism during such writes from a very low baseline value of 2.4 to an average and maximum values of 4.5 and 7.4, respectively (out of a maximum of 8.0), across a wide spectrum of both multiprogrammed and multithreaded workloads. This boost in parallelism results in an average IPC improvement of 15.6% and 16.7% for the multi-progra-mmed and multi-threaded workloads, respectively.","phase change memory, write performance","",""
"Journal Article","González-álvarez C,Sartor JB,Álvarez C,Jiménez-González D,Eeckhout L","MInGLE: An Efficient Framework for Domain Acceleration Using Low-Power Specialized Functional Units","ACM Trans. Archit. Code Optim.","2016","13","2","","Association for Computing Machinery","New York, NY, USA","","","2016-06","","1544-3566","https://doi.org/10.1145/2898356;http://dx.doi.org/10.1145/2898356","10.1145/2898356","The end of Dennard scaling leads to new research directions that try to cope with the utilization wall in modern chips, such as the design of specialized architectures. Processor customization utilizes transistors more efficiently, optimizing not only for performance but also for power. However, hardware specialization for each application is costly and impractical due to time-to-market constraints. Domain-specific specialization is an alternative that can increase hardware reutilization across applications that share similar computations. This article explores the specialization of low-power processors with custom instructions (CIs) that run on a specialized functional unit. We are the first, to our knowledge, to design CIs for an application domain and across basic blocks, selecting CIs that maximize both performance and energy efficiency improvements.We present the Merged Instructions Generator for Large Efficiency (MInGLE), an automated framework that identifies and selects CIs. Our framework analyzes large sequences of code (across basic blocks) to maximize acceleration potential while also performing partial matching across applications to optimize for reuse of the specialized hardware. To do this, we convert the code into a new canonical representation, the Merging Diagram, which represents the code’s functionality instead of its structure. This is key to being able to find similarities across such large code sequences from different applications with different coding styles. Groups of potential CIs are clustered depending on their similarity score to effectively reduce the search space. Additionally, we create new CIs that cover not only whole-body loops but also fragments of the code to optimize hardware reutilization further. For a set of 11 applications from the media domain, our framework generates CIs that significantly improve the energy-delay product (EDP) and performance speedup. CIs with the highest utilization opportunities achieve an average EDP improvement of 3.8 × compared to a baseline processor modeled after an Intel Atom. We demonstrate that we can efficiently accelerate a domain with partially matched CIs, and that their design time, from identification to selection, stays within tractable bounds.","canonical representation, acceleration, domain specific, clustering, Customization","",""
"Conference Paper","Chakraborty P,Doshi G,Shekhar S,Kumar V","Opportunity for Compute Partitioning in Pursuit of Energy-Efficient Systems","","2016","","","92–101","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems","Santa Barbara, CA, USA","2016","9781450343169","","https://doi.org/10.1145/2907950.2907956;http://dx.doi.org/10.1145/2907950.2907956","10.1145/2907950.2907956","Performance of computing systems, from handhelds to supercomputers, is increasingly constrained by the energy consumed. A significant and increasing fraction of the energy is consumed in the movement of data. In a compute node, caches have been very effective in reducing data movement by exploiting the available data locality in programs. Program regions with poor data locality, then effect most of the data movement, and consequently consume an ever larger fraction of energy. In this paper we explore the energy efficiency opportunity of minimizing the data movement in precisely such program regions, by first imagining the possibility of compute near memory, and then partitioning the program’s execution between a compute core and the compute near memory (CnM). Due to the emergence of 3D stacked memory, a CnM implementation appears more realistic. Our focus is on evaluating the partitioning opportunity in applications and to do a limit study of systems enabled with CnM capabilities to understand and guide their architectural embodiment. We describe an automated method of analyzing the data access pattern of optimized workload binaries, via a binary-instrumentation tool called SnapCnM, to identify the beneficial program regions (loops) for CnM execution.We also perform a limit study to evaluate the impact of such partitioning over a range of parameters affecting CnM design choices. Our results show that compute partitioning a small (<10%) fraction of a workload can improve its energy efficiency from 3% (for compute-bound applications) to 27% (for memory-bound applications). From the study in this work we discuss the important aspects that help to shape the future CnM design space.","Processing-in-memory, reuse distance, workload characteristics","","LCTES 2016"
"Journal Article","Chakraborty P,Doshi G,Shekhar S,Kumar V","Opportunity for Compute Partitioning in Pursuit of Energy-Efficient Systems","SIGPLAN Not.","2016","51","5","92–101","Association for Computing Machinery","New York, NY, USA","","","2016-06","","0362-1340","https://doi.org/10.1145/2980930.2907956;http://dx.doi.org/10.1145/2980930.2907956","10.1145/2980930.2907956","Performance of computing systems, from handhelds to supercomputers, is increasingly constrained by the energy consumed. A significant and increasing fraction of the energy is consumed in the movement of data. In a compute node, caches have been very effective in reducing data movement by exploiting the available data locality in programs. Program regions with poor data locality, then effect most of the data movement, and consequently consume an ever larger fraction of energy. In this paper we explore the energy efficiency opportunity of minimizing the data movement in precisely such program regions, by first imagining the possibility of compute near memory, and then partitioning the program’s execution between a compute core and the compute near memory (CnM). Due to the emergence of 3D stacked memory, a CnM implementation appears more realistic. Our focus is on evaluating the partitioning opportunity in applications and to do a limit study of systems enabled with CnM capabilities to understand and guide their architectural embodiment. We describe an automated method of analyzing the data access pattern of optimized workload binaries, via a binary-instrumentation tool called SnapCnM, to identify the beneficial program regions (loops) for CnM execution.We also perform a limit study to evaluate the impact of such partitioning over a range of parameters affecting CnM design choices. Our results show that compute partitioning a small (<10%) fraction of a workload can improve its energy efficiency from 3% (for compute-bound applications) to 27% (for memory-bound applications). From the study in this work we discuss the important aspects that help to shape the future CnM design space.","Processing-in-memory, workload characteristics, reuse distance","",""
"Conference Paper","Raghavan B,Pargman D","Refactoring Society: Systems Complexity in an Age of Limits","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Second Workshop on Computing within Limits","Irvine, California","2016","9781450342605","","https://doi.org/10.1145/2926676.2926677;http://dx.doi.org/10.1145/2926676.2926677","10.1145/2926676.2926677","Research in sociology, anthropology, and organizational theory indicates that most societies readily create increasingly complex societal systems. Over long periods of time, accumulated societal complexity bears costs in excess of benefits, and leads to a societal decline. In this paper we attempt to answer a fundamental question: what is the appropriate response to excessive sociotechnical complexity? We argue that the process of refactoring, which is commonplace in computing, is ideally suited to our circumstances today in a global industrial society replete with complex sociotechnical systems. We further consider future directions for computing research and sustainability research with the aim to understand and help decrease sociotechnical complexity.","complexity, refactoring, sustainability","","LIMITS '16"
"Conference Paper","David Y,Partush N,Yahav E","Statistical Similarity of Binaries","","2016","","","266–280","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation","Santa Barbara, CA, USA","2016","9781450342612","","https://doi.org/10.1145/2908080.2908126;http://dx.doi.org/10.1145/2908080.2908126","10.1145/2908080.2908126","We address the problem of finding similar procedures in stripped binaries. We present a new statistical approach for measuring the similarity between two procedures. Our notion of similarity allows us to find similar code even when it has been compiled using different compilers, or has been modified. The main idea is to use similarity by composition: decompose the code into smaller comparable fragments, define semantic similarity between fragments, and use statistical reasoning to lift fragment similarity into similarity between procedures. We have implemented our approach in a tool called Esh, and applied it to find various prominent vulnerabilities across compilers and versions, including Heartbleed, Shellshock and Venom. We show that Esh produces high accuracy results, with few to no false positives -- a crucial factor in the scenario of vulnerability search in stripped binaries.","partial equivalence, static binary analysis, statistical similarity, verification-aided similarity","","PLDI '16"
"Journal Article","David Y,Partush N,Yahav E","Statistical Similarity of Binaries","SIGPLAN Not.","2016","51","6","266–280","Association for Computing Machinery","New York, NY, USA","","","2016-06","","0362-1340","https://doi.org/10.1145/2980983.2908126;http://dx.doi.org/10.1145/2980983.2908126","10.1145/2980983.2908126","We address the problem of finding similar procedures in stripped binaries. We present a new statistical approach for measuring the similarity between two procedures. Our notion of similarity allows us to find similar code even when it has been compiled using different compilers, or has been modified. The main idea is to use similarity by composition: decompose the code into smaller comparable fragments, define semantic similarity between fragments, and use statistical reasoning to lift fragment similarity into similarity between procedures. We have implemented our approach in a tool called Esh, and applied it to find various prominent vulnerabilities across compilers and versions, including Heartbleed, Shellshock and Venom. We show that Esh produces high accuracy results, with few to no false positives -- a crucial factor in the scenario of vulnerability search in stripped binaries.","partial equivalence, static binary analysis, statistical similarity, verification-aided similarity","",""
"Conference Paper","Fernandes E,Oliveira J,Vale G,Paiva T,Figueiredo E","A Review-Based Comparative Study of Bad Smell Detection Tools","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering","Limerick, Ireland","2016","9781450336918","","https://doi.org/10.1145/2915970.2915984;http://dx.doi.org/10.1145/2915970.2915984","10.1145/2915970.2915984","Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.","comparative study, systematic literature review, bad smells, detection tools","","EASE '16"
"Conference Paper","Mi Q,Keung J","An Empirical Analysis of Reopened Bugs Based on Open Source Projects","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering","Limerick, Ireland","2016","9781450336918","","https://doi.org/10.1145/2915970.2915986;http://dx.doi.org/10.1145/2915970.2915986","10.1145/2915970.2915986","Background: Bug fixing is a long-term and time-consuming activity. A software bug experiences a typical life cycle from newly reported to finally closed by developers, but it could be reopened afterwards for further actions due to reasons such as unclear description given by the bug reporter and developer negligence. Bug reopening is neither desirable nor could be completely avoided in practice, and it is more likely to bring unnecessary workloads to already-busy developers. Aims: To the best of our knowledge, there has been a little previous work on software bug reopening. In order to further study in this area, we perform an empirical analysis to provide a comprehensive understanding of this special area. Method: Based on four open source projects from Eclipse product family, they are CDT, JDT, PDE and Platform, we first quantitatively analyze reopened bugs from perspectives of proportion, impacts and time distribution. After initial exploration on their characteristics, we then qualitatively summarize root causes for bug reopening, this is carried out by investigating developer discussions recorded in Eclipse Bugzilla. Results: Results show that 6%--10% of total bugs will lead to reopening eventually. Over 93% of reopened bugs place serious influence on the normal operation of the system being developed. Several key reasons for bug reopening have been identified in our empirical study. Conclusions: Although reopened bugs have significant impacts on both end users and developers, it is quite possible to reduce bug reopening rate through the adoption of appropriate methods, such as promoting effective and efficient communication among bug reporters and developers, which is supported by empirical evidence in this study.","open source projects, bug tracking system, bug reports, reopened bugs, empirical software engineering","","EASE '16"
"Conference Paper","Kumar S,Srinivasan V,Sharifian A,Sumner N,Shriraman A","Peruse and Profit: Estimating the Accelerability of Loops","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2016 International Conference on Supercomputing","Istanbul, Turkey","2016","9781450343619","","https://doi.org/10.1145/2925426.2926269;http://dx.doi.org/10.1145/2925426.2926269","10.1145/2925426.2926269","There exist a multitude of execution models available today for a developer to target. The choices vary from general purpose processors to fixed-function hardware accelerators with a large number of variations in-between. There is a growing demand to assess the potential benefits of porting or rewriting an application to a target architecture in order to fully exploit the benefits of performance and/or energy efficiency offered by such targets. However, as a first step of this process, it is necessary to determine whether the application has characteristics suitable for acceleration.In this paper, we present Peruse, a tool to characterize the features of loops in an application and to help the programmer understand the amenability of loops for acceleration. We consider a diverse set of features ranging from loop characteristics (e.g., loop exit points) and operation mixes (e.g., control vs data operations) to wider code region characteristics (e.g., idempotency, vectorizability). Peruse is language, architecture, and input independent and uses the intermediate representation of compilers to do the characterization. Using static analyses makes Peruse scalable and enables analysis of large applications to identify and extract interesting loops suitable for acceleration. We show analysis results for unmodified applications from the SPEC CPU benchmark suite, Polybench, and HPC workloads.For an end-user it is more desirable to get an estimate of the potential speedup due to acceleration. We use the workload characterization results of Peruse as features and develop a machine-learning based model to predict the potential speedup of a loop when off-loaded to a fixed function hardware accelerator. We use the model to predict the speedup of loops selected by Peruse and achieve an accuracy of 79%.","Accelerator, machine learning, static analysis","","ICS '16"
"Conference Paper","Feng Q,Prakash A,Wang M,Carmony C,Yin H","ORIGEN: Automatic Extraction of Offset-Revealing Instructions for Cross-Version Memory Analysis","","2016","","","11–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security","Xi'an, China","2016","9781450342339","","https://doi.org/10.1145/2897845.2897850;http://dx.doi.org/10.1145/2897845.2897850","10.1145/2897845.2897850","Semantic gap is a prominent problem in raw memory analysis, especially in Virtual Machine Introspection (VMI) and memory forensics. For COTS software, common memory forensics and VMI tools rely on the so-called ""data structure profiles"" -- a mapping between the semantic variables and their relative offsets within the structure in the binary. Construction of such profiles requires the expert knowledge about the internal working of a specified software version. At most time, it requires considerable manual efforts, which often turns out to be a cumbersome process. In this paper, we propose a notion named ""cross-version memory analysis"", wherein our goal is to alleviate the process of profile construction for new versions of a software by transferring the knowledge from the model that has already been trained on its old version. To this end, we first identify such Offset Revealing Instructions (ORI) in a given software and then leverage the code search techniques to label ORIs in an unknown version of the same software. With labeled ORIs, we can localize the profile for the new version. We provide a proof-of-concept implementation called ORIGEN. The efficacy and efficiency of ORIGEN have been empirically verified by a number of softwares. The experimental results show that by conducting the ORI search within Windows XP SP0 and Linux 3.5.0, we can successfully recover the data structure profiles for Windows XP SP2, Vista, Win 7, and Linux 2.6.32, 3.8.0, 3.13.0, respectively. The systematical evaluation on 40 versions of OpenSSH demonstrates ORIGEN can achieve a precision of more than 90%. As a case study, we integrate ORIGEN into a VMI tool to automatically extract semantic information required for VMI. We develop two plugins to the Volatility memory forensic framework, one for OpenSSH session key extraction, the other for encrypted filesystem key extraction. Both of them can achieve the cross-version analysis by ORIGEN.","program analysis, memory analysis, code search","","ASIA CCS '16"
"Journal Article","Weinstein MJ,Rao AV","A Source Transformation via Operator Overloading Method for the Automatic Differentiation of Mathematical Functions in MATLAB","ACM Trans. Math. Softw.","2016","42","2","","Association for Computing Machinery","New York, NY, USA","","","2016-05","","0098-3500","https://doi.org/10.1145/2699456;http://dx.doi.org/10.1145/2699456","10.1145/2699456","A source transformation via operator overloading method is presented for computing derivatives of mathematical functions defined by MATLAB computer programs. The transformed derivative code that results from the method of this article computes a sparse representation of the derivative of the function defined in the original code. As in all source transformation automatic differentiation techniques, an important feature of the method is that any flow control in the original function code is preserved in the derivative code. Furthermore, the resulting derivative code relies solely upon the native MATLAB library. The method is useful in applications where it is required to repeatedly evaluate the derivative of the original function. The approach is demonstrated on several examples and is found to be highly efficient when compared to well-known MATLAB automatic differentiation programs.","applied mathematics, Scientific computation","",""
"Conference Paper","Fontana FA,Roveda R,Vittori S,Metelli A,Saldarini S,Mazzei F","On Evaluating the Impact of the Refactoring of Architectural Problems on Software Quality","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Scientific Workshop Proceedings of XP2016","Edinburgh, Scotland, UK","2016","9781450341349","","https://doi.org/10.1145/2962695.2962716;http://dx.doi.org/10.1145/2962695.2962716","10.1145/2962695.2962716","We can improve software quality in different ways and by removing different kinds of problems. In this paper, we focus our attention on architectural problems, as architectural smells or antipatterns represent, we remove some of these problems through refactoring steps and we check the impact that the refactoring has on different quality metrics. In particular, we focus our attention on some Quality Indexes computed by four tools. These tools are used also for the detection of the architectural problems. We present the results and outline different issues related to the impact of the refactoring of these architectural problems on the Quality Indexes and the difficulties in the choice of the problems to be refactored.","Refactoring, Antipatterns, Software Quality Evaluation, Architectural Smells","","XP '16 Workshops"
"Conference Paper","Elezi L,Sali S,Demeyer S,Murgia A,Pèrez J","A Game of Refactoring: Studying the Impact of Gamification in Software Refactoring","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Scientific Workshop Proceedings of XP2016","Edinburgh, Scotland, UK","2016","9781450341349","","https://doi.org/10.1145/2962695.2962718;http://dx.doi.org/10.1145/2962695.2962718","10.1145/2962695.2962718","Software refactoring is an essential skill for developers. It improves the quality of the software and reduces maintenance costs in the long run. In this paper, we investigate the impact that gamification can have on the refactoring process and on the usability of the existing refactoring tools. For this reason we created CodeArena, a gamification system that tracks and rewards refactorings during development. We used CodeArena in an exploratory study which involved 12 students of computer science. Then, we collected the feedback provided by these students via a survey. Although the gamification had less effect than expected, our findings can be useful to practitioners interested in promoting refactoring tools via gamification.","Gamification, CodeArena, Survey, Eclipse, Refactoring","","XP '16 Workshops"
"Conference Paper","Sajnani H,Saini V,Svajlenko J,Roy CK,Lopes CV","SourcererCC: Scaling Code Clone Detection to Big-Code","","2016","","","1157–1168","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884877;http://dx.doi.org/10.1145/2884781.2884877","10.1145/2884781.2884877","Despite a decade of active research, there has been a marked lack in clone detection techniques that scale to large repositories for detecting near-miss clones. In this paper, we present a token-based clone detector, SourcererCC, that can detect both exact and near-miss clones from large inter-project repositories using a standard workstation. It exploits an optimized inverted-index to quickly query the potential clones of a given code block. Filtering heuristics based on token ordering are used to significantly reduce the size of the index, the number of code-block comparisons needed to detect the clones, as well as the number of required token-comparisons needed to judge a potential clone. We evaluate the scalability, execution time, recall and precision of SourcererCC, and compare it to four publicly available and state-of-the-art tools. To measure recall, we use two recent benchmarks: (1) a big benchmark of real clones, BigCloneBench, and (2) a Mutation/Injection-based framework of thousands of fine-grained artificial clones. We find SourcererCC has both high recall and precision, and is able to scale to a large inter-project repository (25K projects, 250MLOC) using a standard workstation.","","","ICSE '16"
"Conference Paper","Saini V,Sajnani H,Kim J,Lopes C","SourcererCC and SourcererCC-I: Tools to Detect Clones in Batch Mode and during Software Development","","2016","","","597–600","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889165;http://dx.doi.org/10.1145/2889160.2889165","10.1145/2889160.2889165","Given the availability of large source-code repositories, there has been a large number of applications for large-scale clone detection. Unfortunately, despite a decade of active research, there is a marked lack in clone detectors that scale to big software systems or large repositories, specifically for detecting near-miss (Type 3) clones where significant editing activities may take place in the cloned code.This paper demonstrates: (i) SourcererCC, a token-based clone detector that targets the first three clone types, and exploits an index to achieve scalability to large inter-project repositories using a standard workstation. It uses an optimized inverted-index to quickly query the potential clones of a given code block. Filtering heuristics based on token ordering are used to significantly reduce the size of the index, the number of code-block comparisons needed to detect the clones, as well as the number of required token-comparisons needed to judge a potential clone; and (ii) SourcererCC-I, an Eclipse plug-in, that uses SourcererCC's core engine to identify and navigate clones (both inter and intra project) in real-time during software development.In our experiments, comparing SourcererCC with the state-of-the-art tools 1, we found that it is the only clone detection tool to successfully scale to 250 MLOC on a standard workstation with 12 GB RAM and efficiently detect the first three types of clones (precision 86% and recall 86-100%). Link to the demo: https://youtu.be/17F_9Qp-ks4","","","ICSE '16"
"Conference Paper","Ishio T,Kula RG,Kanda T,German DM,Inoue K","Software Ingredients: Detection of Third-Party Component Reuse in Java Software Release","","2016","","","339–350","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901773;http://dx.doi.org/10.1145/2901739.2901773","10.1145/2901739.2901773","A software product is often dependent on a large number of third-party components. To assess potential risks, such as security vulnerabilities and license violations, a list of components and their versions in a product is important for release engineers and security analysts. Since such a list is not always available, a code comparison technique named Software Bertillonage has been proposed to test whether a product likely includes a copy of a particular component or not. Although the technique can extract candidates of reused components, a user still has to manually identify the original components among the candidates. In this paper, we propose a method to automatically select the most likely origin of components reused in a product, based on an assumption that a product tends to include an entire copy of a component rather than a partial copy. More concretely, given a Java product and a repository of jar files of existing components, our method selects jar files that can provide Java classes to the product in a greedy manner. To compare the method with the existing technique, we have conducted an evaluation using randomly created jar files including up to 1,000 components. The Software Bertillonage technique reports many candidates; the precision and recall are 0.357 and 0.993, respectively. Our method reports a list of original components whose precision and recall are 0.998 and 0.997.","software reuse, reverse engineering, origin analysis","","MSR '16"
"Conference Paper","Nguyen AT,Nguyen HA,Nguyen TN","A Large-Scale Study on Repetitiveness, Containment, and Composability of Routines in Open-Source Projects","","2016","","","362–373","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901759;http://dx.doi.org/10.1145/2901739.2901759","10.1145/2901739.2901759","Source code in software systems has been shown to have a good degree of repetitiveness at the lexical, syntactical, and API usage levels. This paper presents a large-scale study on the repetitiveness, containment, and composability of source code at the semantic level. We collected a large dataset consisting of 9,224 Java projects with 2.79M class flies, 17.54M methods with 187M SLOCs. For each method in a project, we build the program dependency graph (PDG) to represent a routine, and compare PDGs with one another as well as the subgraphs within them. We found that within a project, 12.1% of the routines are repeated, and most of them repeat from 2--7 times. As entirety, the routines are quite project-specific with only 3.3% of them exactly repeating in 1--1 other projects with at most 8 times. We also found that 26.1% and 7.27% of the routines are contained in other routine(s), i.e., implemented as part of other routine(s) elsewhere within a project and in other projects, respectively. Except for trivial routines, their repetitiveness and containment is independent of their complexity. Defining a subroutine via a per-variable slicing subgraph in a PDG, we found that 14.3% of all routines have all of their subroutines repeated. A high percentage of subroutines in a routine can be found/reused elsewhere. We collected 8,764,971 unique subroutines (with 323,564 unique JDK subroutines) as basic units for code searching/synthesis. We also provide practical implications of our findings to automated tools.","repetitiveness, composability, containment, code reuse","","MSR '16"
"Conference Paper","Kreutzer P,Dotzler G,Ring M,Eskofier BM,Philippsen M","Automatic Clustering of Code Changes","","2016","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901749;http://dx.doi.org/10.1145/2901739.2901749","10.1145/2901739.2901749","Several research tools and projects require groups of similar code changes as input. Examples are recommendation and bug finding tools that can provide valuable information to developers based on such data. With the help of similar code changes they can simplify the application of bug fixes and code changes to multiple locations in a project. But despite their benefit, the practical value of existing tools is limited, as users need to manually specify the input data, i.e., the groups of similar code changes.To overcome this drawback, this paper presents and evaluates two syntactical similarity metrics, one of them is specifically designed to run fast, in combination with two carefully selected and self-tuning clustering algorithms to automatically detect groups of similar code changes.We evaluate the combinations of metrics and clustering algorithms by applying them to several open source projects and also publish the detected groups of similar code changes online as a reference dataset. The automatically detected groups of similar code changes work well when used as input for LASE, a recommendation system for code changes.","clustering, code changes, software repositories","","MSR '16"
"Conference Paper","Sharma T,Mishra P,Tiwari R","Designite: A Software Design Quality Assessment Tool","","2016","","","1–4","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Workshop on Bringing Architectural Design Thinking into Developers' Daily Activities","Austin, Texas","2016","9781450341530","","https://doi.org/10.1145/2896935.2896938;http://dx.doi.org/10.1145/2896935.2896938","10.1145/2896935.2896938","Poor design quality and huge technical debt are common issues perceived in real-life software projects. Design smells are indicators of poor design quality and the volume of design smells found could be treated as the design debt of the software system. The existing smell detection tools focus largely on implementation smells and do not reveal a comprehensive set of smells that arise at design level. In this paper, we present Designite - a software design quality assessment tool. It not only supports comprehensive design smells detection but also provides a detailed metrics analysis. Further, it offers various features to help identify issues contributing to design debt and improve the design quality of the analyzed software system.","technical debt, DSM, design smells, refactoring, design debt","","BRIDGE '16"
"Conference Paper","Mazinanian D,Tsantalis N,Stein R,Valenta Z","JDeodorant: Clone Refactoring","","2016","","","613–616","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889168;http://dx.doi.org/10.1145/2889160.2889168","10.1145/2889160.2889168","Code duplication is widely recognized as a potentially harmful code smell for the maintenance of software systems. In this demonstration, we present a tool, developed as part of the JDeodorant Eclipse plug-in, which offers cutting-edge features for the analysis and refactoring of clones found in Java projects. https://youtu.be/K_xAEqIEJ-4","code duplication, refactoring, refactorability analysis","","ICSE '16"
"Conference Paper","Cadar C,Donaldson AF","Analysing the Program Analyser","","2016","","","765–768","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889206;http://dx.doi.org/10.1145/2889160.2889206","10.1145/2889160.2889206","The reliability of program analysis tools is clearly important if such tools are to play a serious role in improving the quality and integrity of software systems, and the confidence which users place in such systems. Yet our experience is that, currently, little attention is paid to analysing the correctness of program analysers themselves, beyond regression testing. In this position paper we present our vision that, by 2025, the use of more rigorous analyses to check the reliability of program analysers will be commonplace. Inspired by recent advances in compiler testing, we set out initial steps towards this vision, building upon techniques such as cross-checking, program transformation and program generation.","program generators, program analysis, cross-checking, testing, program transformations","","ICSE '16"
"Conference Paper","Vendome C,Poshyvanyk D","Assisting Developers with License Compliance","","2016","","","811–814","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889259;http://dx.doi.org/10.1145/2889160.2889259","10.1145/2889160.2889259","Software licensing determines how open source systems are reused, distributed, and modified from a legal perspective. While it facilitates rapid development, it can present difficulty for developers in understanding due to the legal language of these licenses. Because of misunderstandings, systems can incorporate licensed code in a way that violates the terms of the license. Our research first aimed at understanding the rationale of developers in choosing and changing licenses. We also investigated the problem of traceability of license changes. These two studies are fundamental components for understanding problems that developers face so that we can better support developers with licensing.Subsequently, we present our proposed research plan of ensuring license compliance of a system. Our research incorporates techniques from information retrieval, code search, mining software repositories, and previous work on licensing. Our work focuses on the development of a license compliance engine. This engine will not only identify license incompatibilities but it will also recommend strategies for developers to fix the incompatible components to ensure license compliance. Our research aims then to address both the analysis of licenses across dependencies and creating license traceability of byte-code (i.e. provenance). The component will allow us to extend our compliance to include binaries.","software licenses, empirical studies","","ICSE '16"
"Conference Paper","Li S,Xiao X,Bassett B,Xie T,Tillmann N","Measuring Code Behavioral Similarity for Programming and Software Engineering Education","","2016","","","501–510","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889204;http://dx.doi.org/10.1145/2889160.2889204","10.1145/2889160.2889204","In recent years, online programming and software engineering education via information technology has gained a lot of popularity. Typically, popular courses often have hundreds or thousands of students but only a few course staff members. Tool automation is needed to maintain the quality of education. In this paper, we envision that the capability of quantifying behavioral similarity between programs is helpful for teaching and learning programming and software engineering, and propose three metrics that approximate the computation of behavioral similarity. Specifically, we leverage random testing and dynamic symbolic execution (DSE) to generate test inputs, and run programs on these test inputs to compute metric values of the behavioral similarity. We evaluate our metrics on three real-world data sets from the Pex4Fun platform (which so far has accumulated more than 1.7 million game-play interactions). The results show that our metrics provide highly accurate approximation to the behavioral similarity. We also demonstrate a number of practical applications of our metrics including hint generation, progress indication, and automatic grading.","","","ICSE '16"
"Conference Paper","Ciancarini P,Russo D,Sillitti A,Succi G","A Guided Tour of the Legal Implications of Software Cloning","","2016","","","563–572","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889220;http://dx.doi.org/10.1145/2889160.2889220","10.1145/2889160.2889220","Software Cloning is the typical example where an interdisciplinary approach may bring additional elements into the community's discussion. In fact, little research has been done in its analysis from an Intellectual Propriety Rights (IPRs) perspective, even if it is a widely studied aspect of software engineering. An interdisciplinary approach is crucial to better understand the legal implications of software in the IPR context. Interestingly, the academic community of software and systems deals much more with such IPR issues than courts themselves. In this paper, we analyze some recent legal decisions in using software clones from a software engineering perspective. In particular, we survey the behavior of some major courts about cloning issues. As a major outcome of our research, it seems that legal fora do not have major concerns regarding copyright infringements in software cloning. The major contribution of this work is a case by case analysis of more than one hundred judgments by the US courts and the European Court of Justice. We compare the US and European courts case laws and discuss the impact of a recent European ruling. The US and EU contexts are quite different, since in the US software is patentable while in the EU it is not. Hence, European courts look more permissive regarding cloning, since ""principles,"" or ""ideas,"" are not copyrightable by themselves.","IPR, software reuse, copyright, software cloning","","ICSE '16"
"Conference Paper","Raghothaman M,Wei Y,Hamadi Y","SWIM: Synthesizing What i Mean: Code Search and Idiomatic Snippet Synthesis","","2016","","","357–367","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884808;http://dx.doi.org/10.1145/2884781.2884808","10.1145/2884781.2884808","Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as ""generate md5 hash code"". The query does not need to contain framework-specific trivia such as the type names or methods of interest.We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce structured call sequences to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis.We evaluated swim with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.","","","ICSE '16"
"Conference Paper","Zhang B,Duszynski S,Becker M","Variability Mechanisms and Lessons Learned in Practice","","2016","","","14–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Workshop on Variability and Complexity in Software Design","Austin, Texas","2016","9781450341769","","https://doi.org/10.1145/2897045.2897048;http://dx.doi.org/10.1145/2897045.2897048","10.1145/2897045.2897048","In the design of complex and variable software systems, one of the key steps is to select the variability mechanism that defines how variable features are realized on the design and code level. Although different variability mechanisms were invented and applied in practice for decades, there are not many studies that compare these mechanisms based on practical experiences. This paper characterizes and compares seven variability mechanisms in terms of their techniques, binding time, granularity, and further aspects. It provides experiences of their usage, the practical benefits and challenges, as well as discusses existing solutions to the challenges based on related studies and our practice in industry.","variability mechanisms, variability design, practical experience","","VACE '16"
"Conference Paper","Mannan UA,Ahmed I,Almurshed RA,Dig D,Jensen C","Understanding Code Smells in Android Applications","","2016","","","225–234","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Mobile Software Engineering and Systems","Austin, Texas","2016","9781450341783","","https://doi.org/10.1145/2897073.2897094;http://dx.doi.org/10.1145/2897073.2897094","10.1145/2897073.2897094","Code smells are associated with poor coding practices that cause long-term maintainability problems and mask bugs. Despite mobile being a fast growing software sector, code smells in mobile applications have been understudied. We do not know how code smells in mobile applications compare to those in desktop applications, and how code smells are affecting the design of mobile applications. Without such knowledge, application developers, tool builders, and researchers cannot improve the practice and state of the art of mobile development.We first reviewed the literature on code smells in Android applications and found that there is a significant gap between the most studied code smells in literature and most frequently occurring code smells in real world applications. Inspired by this finding, we conducted a large scale empirical study to compare the type, density, and distribution of code smells in mobile vs. desktop applications. We analyze an open-source corpus of 500 Android applications (total of 6.7M LOC) and 750 desktop Java applications (total of 16M LOC), and compare 14,553 instances of code smells in Android applications to 117,557 instances of code smells in desktop applications. We find that, despite mobile applications having different structure and workflow than desktop applications, the variety and density of code smells is similar. However, the distribution of code smells is different - some code smells occur more frequently in mobile applications. We also found that different categories of Android applications have different code smell distributions. We highlight several implications of our study for application developers, tool builders, and researchers.","","","MOBILESoft '16"
"Conference Paper","Yang D,Hussain A,Lopes CV","From Query to Usable Code: An Analysis of Stack Overflow Code Snippets","","2016","","","391–402","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901767;http://dx.doi.org/10.1145/2901739.2901767","10.1145/2901739.2901767","Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries.With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets?A total of 3M code snippets are analyzed across four languages: C#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.","code mining, automatic program generation","","MSR '16"
"Conference Paper","Ringlstetter A,Scherzinger S,Bissyandé TF","Data Model Evolution Using Object-NoSQL Mappers: Folklore or State-of-the-Art?","","2016","","","33–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Workshop on BIG Data Software Engineering","Austin, Texas","2016","9781450341523","","https://doi.org/10.1145/2896825.2896827;http://dx.doi.org/10.1145/2896825.2896827","10.1145/2896825.2896827","In big data software engineering, the schema flexibility of NoSQL document stores is a major selling point: When the document store itself does not actively manage a schema, the data model is maintained within the application. Just like object-relational mappers for relational databases, object-NoSQL mappers are part of professional software development with NoSQL document stores. Some mappers go beyond merely loading and storing Java objects: Using dedicated evolution annotations, developers may conveniently add, remove, or rename attributes from stored objects, and also conduct more complex transformations. In this paper, we analyze the dissemination of this technology in Java open source projects. While we find evidence on GitHub that evolution annotations are indeed being used, developers do not employ them so much for evolving the data model, but to solve different tasks instead. Our observations trigger interesting questions for further research.","object-NoSQL mappers, data model evolution","","BIGDSE '16"
"Conference Paper","Luo L,Zeng Q","SolMiner: Mining Distinct Solutions in Programs","","2016","","","481–490","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889202;http://dx.doi.org/10.1145/2889160.2889202","10.1145/2889160.2889202","Given a programming problem, because of a variety of data structures and algorithms that can be applied and different tradeoffs, such as space-time, to be considered, there may be many distinct solutions. By comparing his/her solution against others' and learning from the distinct solutions, a learner may quickly improve programming skills and gain experience in making trade-offs. Meanwhile, on the Internet many websites provide venues for programming practice and contests. Popular websites receive hundreds of thousands of submissions daily from novices as well as advanced learners. While these websites can automatically judge the correctness of a submission, none extracts distinct solutions from the submissions and provides them to learners. How to automatically identify distinct solutions from a large number of submissions is a challenging and unresolved problem. Due to diverse coding styles and high programming flexibility, submissions implementing the same solution may appear very different from each other; in addition, dealing with submissions at scale imposes extra challenges. We propose SolMiner, a solution miner, that automatically mines distinct solutions from a large number of submissions. SolMiner leverages static program analysis, data mining, and machine learning to automatically measure the similarity between submissions and identify distinct solutions. We have built a prototype of SolMiner and evaluated it. The evaluation shows that the technique is effective and efficient.","static program analysis, clustering, data mining, online judge","","ICSE '16"
"Conference Paper","Ye X,Shen H,Ma X,Bunescu R,Liu C","From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering","","2016","","","404–415","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884862;http://dx.doi.org/10.1145/2884781.2884862","10.1145/2884781.2884862","The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are first trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly defined task of linking API documents to computer programming questions.","skip-gram model, bug reports, word embeddings, API documents, bug localization","","ICSE '16"
"Conference Paper","Ma Z,Wang H,Guo Y,Chen X","LibRadar: Fast and Accurate Detection of Third-Party Libraries in Android Apps","","2016","","","653–656","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889178;http://dx.doi.org/10.1145/2889160.2889178","10.1145/2889160.2889178","We present LibRadar, a tool that is able to detect third-party libraries used in an Android app accurately and instantly. As third-party libraries are widely used in Android apps, program analysis on Android apps typically needs to detect or remove third-party libraries first in order to function correctly or provide accurate results. However, most previous studies employ a whitelist of package names of known libraries, which is incomplete and unable to deal with obfuscation. In contrast, LibRadar detects libraries based on stable API features that are obfuscation resilient in most cases. After analyzing one million free Android apps from Google Play, we have identified possible libraries and collected their unique features. Based on these features, LibRadar can detect third-party libraries in a given Android app within seconds, as it only requires simple static analysis and fast comparison. LibRadar is available for public use at http://radar.pkuos.org. The demo video is available at: https://youtu.be/GoMYjYxsZnI","","","ICSE '16"
"Conference Paper","Bellomo S,Nord RL,Ozkaya I,Popeck M","Got Technical Debt? Surfacing Elusive Technical Debt in Issue Trackers","","2016","","","327–338","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901754;http://dx.doi.org/10.1145/2901739.2901754","10.1145/2901739.2901754","Concretely communicating technical debt and its consequences is of common interest to both researchers and software engineers. In the absence of validated tools and techniques to achieve this goal with repeatable results, developers resort to ad hoc practices. Most commonly they report using issue trackers or their existing backlog management practices to capture and track technical debt. In a manual examination of 1,264 issues from four issue trackers from open source industry and government projects, we identified 109 examples of technical debt. Our study reveals that technical debt and its related concepts have entered the vernacular of developers as they discuss development tasks through issue trackers. Even when issues are not explicitly tagged as technical debt, it is possible to identify technical debt items in these issue trackers using a categorization method we developed. We use our results and data to motivate an improved definition and an approach to explicitly report technical debt in issue trackers.","text categorization, software anomalies, software design, issue tracking, technical debt","","MSR '16"
"Conference Paper","Dou W,Xu L,Cheung SC,Gao C,Wei J,Huang T","VEnron: A Versioned Spreadsheet Corpus and Related Evolution Analysis","","2016","","","162–171","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889238;http://dx.doi.org/10.1145/2889160.2889238","10.1145/2889160.2889238","Like most conventional software, spreadsheets are subject to software evolution. However, spreadsheet evolution is rarely assisted by version management tools. As a result, the version information across evolved spreadsheets is often missing or highly fragmented. This makes it difficult for users to notice the evolution issues arising from their spreadsheets.In this paper, we propose a semi-automated approach that leverages spreadsheets' contexts (e.g., attached emails) and contents to identify evolved spreadsheets and recover the embedded version information. We apply it to the released email archive of the Enron Corporation and build an industrial-scale, versioned spreadsheet corpus VEnron. Our approach first clusters spreadsheets that likely evolved from one to another into evolution groups based on various fragmented information, such as spreadsheet filenames, spreadsheet contents, and spreadsheet-attached emails. Then, it recovers the version information of the spreadsheets in each evolution group. VEnron enables us to identify interesting issues that can arise from spreadsheet evolution. For example, the versioned spreadsheets popularly exist in the Enron email archive; changes in formulas are common; and some groups (16.9%) can introduce new errors during evolution.According to our knowledge, VEnron is the first spreadsheet corpus with version information. It provides a valuable resource to understand issues arising from spreadsheet evolution.","evolution, version, spreadsheet","","ICSE '16"
"Conference Paper","Palomba F,Di Nucci D,Panichella A,Oliveto R,De Lucia A","On the Diffusion of Test Smells in Automatically Generated Test Code: An Empirical Study","","2016","","","5–14","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Workshop on Search-Based Software Testing","Austin, Texas","2016","9781450341660","","https://doi.org/10.1145/2897010.2897016;http://dx.doi.org/10.1145/2897010.2897016","10.1145/2897010.2897016","The role of software testing in the software development process is widely recognized as a key activity for successful projects. This is the reason why in the last decade several automatic unit test generation tools have been proposed, focusing particularly on high code coverage. Despite the effort spent by the research community, there is still a lack of empirical investigation aimed at analyzing the characteristics of the produced test code. Indeed, while some studies inspected the effectiveness and the usability of these tools in practice, it is still unknown whether test code is maintainable. In this paper, we conducted a large scale empirical study in order to analyze the diffusion of bad design solutions, namely test smells, in automatically generated unit test classes. Results of the study show the high diffusion of test smells as well as the frequent co-occurrence of different types of design problems. Finally we found that all test smells have strong positive correlation with structural characteristics of the systems such as size or number of classes.","empirical studies, software quality, automatically generated test classes, mining software repositories, test smells","","SBST '16"
"Conference Paper","Xiao L,Cai Y,Kazman R,Mo R,Feng Q","Identifying and Quantifying Architectural Debt","","2016","","","488–498","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884822;http://dx.doi.org/10.1145/2884781.2884822","10.1145/2884781.2884822","Our prior work showed that the majority of error-prone source files in a software system are architecturally connected. Flawed architectural relations propagate defects among these files and accumulate high maintenance costs over time, just like debts accumulate interest. We model groups of architecturally connected files that accumulate high maintenance costs as architectural debts. To quantify such debts, we formally define architectural debt, and show how to automatically identify debts, quantify their maintenance costs, and model these costs over time. We describe a novel history coupling probability matrix for this purpose, and identify architecture debts using 4 patterns of architectural flaws shown to correlate with reduced software quality. We evaluate our approach on 7 large-scale open source projects, and show that a significant portion of total project maintenance effort is consumed by paying interest on architectural debts. The top 5 architectural debts, covering a small portion (8% to 25%) of each project's error-prone files, capture a significant portion (20% to 61%) of each project's maintenance effort. Finally, we show that our approach reveals how architectural issues evolve into debts over time.","software quality, software architecture, technical debt","","ICSE '16"
"Conference Paper","Rahman MM,Roy CK,Collins JA","CoRReCT: Code Reviewer Recommendation in GitHub Based on Cross-Project and Technology Experience","","2016","","","222–231","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889244;http://dx.doi.org/10.1145/2889160.2889244","10.1145/2889160.2889244","Peer code review locates common coding rule violations and simple logical errors in the early phases of software development, and thus reduces overall cost. However, in GitHub, identifying an appropriate code reviewer for a pull request is a non-trivial task given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation technique that considers not only the relevant cross-project work history (e.g., external library experience) but also the experience of a developer in certain specialized technologies associated with a pull request for determining her expertise as a potential code reviewer. We first motivate our technique using an exploratory study with 10 commercial projects and 10 associated libraries external to those projects. Experiments using 17,115 pull requests from 10 commercial projects and six open source projects show that our technique provides 85%-- 92% recommendation accuracy, about 86% precision and 79%--81% recall in code reviewer recommendation, which are highly promising. Comparison with the state-of-the-art technique also validates the empirical findings and the superiority of our recommendation technique.","code reviewer recommendation, pull request, GitHub, specialized technology experience, cross-project experience","","ICSE '16"
"Conference Paper","Bavota G,Russo B","A Large-Scale Empirical Study on Self-Admitted Technical Debt","","2016","","","315–326","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901742;http://dx.doi.org/10.1145/2901739.2901742","10.1145/2901739.2901742","Technical debt is a metaphor introduced by Cunningham to indicate ""not quite right code which we postpone making it right"". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects.This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding.Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.","technical debt, empirical software engineering, mining software repositories","","MSR '16"
"Conference Paper","Sharma T,Fragkoulis M,Spinellis D","Does Your Configuration Code Smell?","","2016","","","189–200","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901761;http://dx.doi.org/10.1145/2901739.2901761","10.1145/2901739.2901761","Infrastructure as Code (IaC) is the practice of specifying computing system configurations through code, and managing them through traditional software engineering methods. The wide adoption of configuration management and increasing size and complexity of the associated code, prompt for assessing, maintaining, and improving the configuration code's quality. In this context, traditional software engineering knowledge and best practices associated with code quality management can be leveraged to assess and manage configuration code quality. We propose a catalog of 13 implementation and 11 design configuration smells, where each smell violates recommended best practices for configuration code. We analyzed 4,621 Puppet repositories containing 8.9 million lines of code and detected the cataloged implementation and design configuration smells. Our analysis reveals that the design configuration smells show 9% higher average co-occurrence among themselves than the implementation configuration smells. We also observed that configuration smells belonging to a smell category tend to co-occur with configuration smells belonging to another smell category when correlation is computed by volume of identified smells. Finally, design configuration smell density shows negative correlation whereas implementation configuration smell density exhibits no correlation with the size of a configuration management system.","code quality, infrastructure as code, configuration smells, technical debt, maintainability","","MSR '16"
"Conference Paper","Acharya MP,Parnin C,Kraft NA,Dagnino A,Qu X","Code Drones","","2016","","","785–788","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889211;http://dx.doi.org/10.1145/2889160.2889211","10.1145/2889160.2889211","We propose and explore a new paradigm called Code Drones in which every software artifact such as a class is an intelligent and socially active entity. In this paradigm, humanized artifacts take the lead and choreograph (socially, in collaboration with other intelligent software artifacts and humans) automated software engineering solutions to a myriad of development and maintenance challenges, including API migration, reuse, documentation, testing, patching, and refactoring. We discuss the implications of having social and intelligent/cognitive software artifacts that guide their own self-improvement.","","","ICSE '16"
"Conference Paper","Imminni SK,Hasan MA,Duckett M,Sachdeva P,Karmakar S,Kumar P,Haiduc S","SPYSE: A Semantic Search Engine for Python Packages and Modules","","2016","","","625–628","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889174;http://dx.doi.org/10.1145/2889160.2889174","10.1145/2889160.2889174","Code reuse is a common practice among software developers, whether novices or experts. Developers often rely on online resources in order to find code to reuse. For Python, the Python Package Index (PyPI) contains all packages developed for the community and is the largest catalog of reusable, open source packages developers can consult. While a valuable resource, the state of the art PyPI search has very limited capabilities, making it hard for developers to find useful, high quality Python code to use for their task at hand.We introduce SPYSE (Semantic PYthon Search Engine), a web-based search engine that overcomes the limitations of the state of the art, making it easier for developers to find useful code. The power of SPYSE lays in the combination of three different aspects meant to provide developers with relevant, and at the same time high quality code: code semantics, popularity, and code quality. SPYSE also allows searching for modules, in addition to packages, which opens new reuse opportunities for developers, currently not supported. TOOL URL: https://pypi.compgeom.com VIDEO URL: https://youtu.be/Praglw-vS50","search engine, Python, code reuse, recommender systems","","ICSE '16"
"Conference Paper","Kononenko O,Baysal O,Godfrey MW","Code Review Quality: How Developers See It","","2016","","","1028–1038","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884840;http://dx.doi.org/10.1145/2884781.2884840","10.1145/2884781.2884840","In a large, long-lived project, an effective code review process is key to ensuring the long-term quality of the code base. In this work, we study code review practices of a large, open source project, and we investigate how the developers themselves perceive code review quality. We present a qualitative study that summarizes the results from a survey of 88 Mozilla core developers. The results provide developer insights into how they define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. We found that the review quality is primarily associated with the thoroughness of the feedback, the reviewer's familiarity with the code, and the perceived quality of the code itself. Also, we found that while different factors are perceived to contribute to the review quality, reviewers often find it difficult to keep their technical skills up-to-date, manage personal priorities, and mitigate context switching.","survey, review quality, developer perception, code review","","ICSE '16"
"Conference Paper","Dig D,Johnson R,Marinov D,Bailey B,Batory D","COPE: Vision for a Change-Oriented Programming Environment","","2016","","","773–776","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889208;http://dx.doi.org/10.1145/2889160.2889208","10.1145/2889160.2889208","Software engineering involves a lot of change as code artifacts are not only created once but maintained over time. In the last 25 years, major paradigms of program development have arisen -- agile development with refactorings, software product lines, moving sequential code to multicore or cloud, etc. Each is centered on particular kinds of change; their conceptual foundations rely on transformations that (semi-) automate these changes.We are exploring how transformations can be placed at the center of software development in future IDEs, and when such a view can provide benefits over the traditional view. COPE, a Change-Oriented Programming Environment, looks at 5 activities: (1) analyze what changes programmers typically make and how they perceive, recall, and communicate changes, (2) automate transformations to make it easier to apply and script changes, (3) develop tools that compose and manipulate transformations to make it easier to reuse them, (4) integrate transformations with version control to provide better ways for archiving and understanding changes, and (5) develop tools that infer higher-level transformations from lower-level changes. Characterizing software development in terms of transformations is an essential step to take software engineering from manual development to (semi-) automated development of software.","","","ICSE '16"
"Conference Paper","Hossein Abad ZS,Karimpour R,Ho J,Didar-Al-Alam SM,Ruhe G,Tse E,Barabash K,Hargreaves I","Understanding the Impact of Technical Debt in Coding and Testing: An Exploratory Case Study","","2016","","","25–31","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Workshop on Software Engineering Research and Industrial Practice","Austin, Texas","2016","9781450341707","","https://doi.org/10.1145/2897022.2897023;http://dx.doi.org/10.1145/2897022.2897023","10.1145/2897022.2897023","Technical Debt (TD) refers to the long-term consequences of shortcuts taken during different phases of software development life cycle. Lack of attention to monitoring and managing testing and development debt can contribute to unexpectedly large cost overruns and severe quality issues in software development projects. This paper describes a case study conducted with an industry partner to explore the impact of TD in coding and testing. By conducting (i) a semi-structured interview, and (ii) a quantitative survey, we found that (1) the status of TD is largely project-independent, (2) we could not reject that there is no significant difference between the percentage of existing TD and the required time for reducing this TD in testing and development teams, (3) there is a statistically significant difference between the perceived influence of reducing TD on productivity increase in testing and development teams, (4) team member's experience has impact on the existing percentage of TD and influences productivity increase that is caused by reducing TD, (5) allocating more resources such as time, budget, and infrastructure is considered a potential solution for reducing TD.","development technical debt, testing technical debt, technical debt, case study","","SER&IP '16"
"Conference Paper","Treude C,Robillard MP","Augmenting API Documentation with Insights from Stack Overflow","","2016","","","392–403","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering","Austin, Texas","2016","9781450339001","","https://doi.org/10.1145/2884781.2884800;http://dx.doi.org/10.1145/2884781.2884800","10.1145/2884781.2884800","Software developers need access to different kinds of information which is often dispersed among different documentation sources, such as API documentation or Stack Overflow. We present an approach to automatically augment API documentation with ""insight sentences"" from Stack Overflow---sentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach that uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found that SISE resulted in the highest number of sentences that were considered to add useful information not found in the API documentation. These results indicate that taking into account the meta data available on Stack Overflow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overflow data.","stack overflow, insight sentences, API documentation","","ICSE '16"
"Conference Paper","Perry DE","Theories, Theories Everywhere","","2016","","","8–14","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Theory-Oriented Software Engineering","Austin, Texas","2016","9781450341745","","https://doi.org/10.1145/2897134.2897138;http://dx.doi.org/10.1145/2897134.2897138","10.1145/2897134.2897138","A general theory of software engineering has that there are two logical parts: design and evaluation, D and E, each of which as a theory T. The interesting question is ""what is the relationship between these two theories in D and E"". I first delineate a rich variety of theories related to D and E and consider these to be sub-theories critical to understanding the relationships between the theories in D and E. I then delineate these relationships in terms of different types of empirical evaluations. Doing so explains why empirical evaluations are a dominant part of software engineering projects and why these evaluations are so complex.","theories in evaluations, theories in designs, theory relationships in designs and evaluations","","TOSE '16"
"Conference Paper","Quintela-Pumares M,Cabral B,Fernandez-Lanvin D,Fernandez-Alvarez AM","Integrating Automatic Backward Error Recovery in Asynchronous Rich Clients","","2016","","","192–201","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th International Conference on Software Engineering Companion","Austin, Texas","2016","9781450342056","","https://doi.org/10.1145/2889160.2889241;http://dx.doi.org/10.1145/2889160.2889241","10.1145/2889160.2889241","Rich Web Clients allow developers to manage data locally and update it from the server by means of asynchronous requests, thus providing more interactive interfaces and an improved user experience. On the other hand, they face concerning challenges regarding error management. When there is a need to update the local data through multiple asynchronous requests and it is required that all them succeed, an error on a single call can lead to having incorrect information shown to the user. Consequently, developers need to explicitly implement proper recovery mechanisms, a task that most of times is complex and highly error prone, leading to tangled code and harder maintenance, especially in an asynchronous environment. These problems could be lessened through automatic error recovery techniques, but the existing state of the art for Rich Web Client development does not support recovery from asynchronous scenarios. To cope with this problem we extended the existing error recovery technique of Reconstructors, adding to it the capability of recovering the state in the presence of several asynchronous requests. We applied this technology in a widely used open source project for rendering interactive charts, ChartJs, thus allowing the developer to effortlessly guarantee that the data shown to the user, even when it results from multiple asynchronous requests, is never inconsistent. We compare our proposal to other solutions using state of the art approaches and verify that by using Reconstructors the overall implementation requires 39.16% less lines of code, and 5.66 times less lines of code are dedicated specifically to error management, while avoiding code tangling completely. Execution time showed by reconstructors is between 5.2% and 9.3% slower than other solutions, a cost we believe is worth its benefits, and feasible for using these techniques in real world client applications.","rich web clients, chart libraries, automatic error recovery, asynchronous events","","ICSE '16"
"Conference Paper","Izquierdo-Cortazar D,Kurth L,Gonzalez-Barahona JM,Dueñas S,Sekitoleko N","Characterization of the Xen Project Code Review Process: An Experience Report","","2016","","","386–390","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Mining Software Repositories","Austin, Texas","2016","9781450341868","","https://doi.org/10.1145/2901739.2901778;http://dx.doi.org/10.1145/2901739.2901778","10.1145/2901739.2901778","Many software development projects have introduced mandatory code review for every change to the code. This means that the project needs to devote a significant effort to review all proposed changes, and that their merging into the code base may get considerably delayed. Therefore, all those projects need to understand how code review is working, and the delays it is causing in time to merge.This is the case in the Xen project, which performs peer review using mailing lists. During the first half of 2015, some people in the project observed a large and sustained increase in the number of messages related to code review, which had started some years before. This observation led to concerns on whether the code review process was having some trouble, and too large an impact on the overall development process.Those concerns were addressed with a quantitative study, which is presented in this paper. Based on the information in code review messages, some metrics were defined to infer delays imposed by code review. The study produced quantitative data suitable for informed discussion, which the project is using to understand its code review process, and to take decisions to improve it.","software process, data mining, code review","","MSR '16"
"Journal Article","Avgeriou P,Ernst NA,Nord RL,Kruchten P","Technical Debt: Broadening Perspectives Report on the Seventh Workshop on Managing Technical Debt (MTD 2015)","SIGSOFT Softw. Eng. Notes","2016","41","2","38–41","Association for Computing Machinery","New York, NY, USA","","","2016-05","","0163-5948","https://doi.org/10.1145/2894784.2894800;http://dx.doi.org/10.1145/2894784.2894800","10.1145/2894784.2894800","Increasingly software engineers use the metaphor of technical debt to communicate issues related to the growing cost of change. In this article, we report on the Seventh Workshop on Managing Technical Debt (MTD 2015), held in Bremen, Germany, on October 2, 2015, collocated with the International Conference on Software Maintenance and Evolution (ICSME). The 30 workshop participants from industry and academia engaged in lively discussions, which helped clarify issues, refine questions, and promote common understanding about technical debt in software.","","",""
"Conference Paper","Loksa D,Ko AJ,Jernigan W,Oleson A,Mendez CJ,Burnett MM","Programming, Problem Solving, and Self-Awareness: Effects of Explicit Guidance","","2016","","","1449–1461","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","San Jose, California, USA","2016","9781450333627","","https://doi.org/10.1145/2858036.2858252;http://dx.doi.org/10.1145/2858036.2858252","10.1145/2858036.2858252","More people are learning to code than ever, but most learning opportunities do not explicitly teach the problem solving skills necessary to succeed at open-ended programming problems. In this paper, we present a new approach to impart these skills, consisting of: 1) explicit instruction on programming problem solving, which frames coding as a process of translating mental representations of problems and solutions into source code, 2) a method of visualizing and monitoring progression through six problem solving stages, 3) explicit, on-demand prompts for learners to reflect on their strategies when seeking help from instructors, and 4) context-sensitive help embedded in a code editor that reinforces the problem solving instruction. We experimentally evaluated the effects of our intervention across two 2-week web development summer camps with 48 high school students, finding that the intervention increased productivity, independence, programming self-efficacy, metacognitive awareness, and growth mindset. We discuss the implications of these results on learning technologies and classroom instruction.","problem-solving, computer science education, metacognition, programming","","CHI '16"
"Conference Paper","Kwiatkowska M","Measuring the Difficulty of Test Items in Computing Science Education","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st Western Canadian Conference on Computing Education","Kamloops, BC, Canada","2016","9781450343558","","https://doi.org/10.1145/2910925.2910950;http://dx.doi.org/10.1145/2910925.2910950","10.1145/2910925.2910950","This paper describes a communication-based approach for measuring the difficulty of test items used in computing science education, specifically, in first course in computer programming language. The assessment process is viewed here as a bi-directional and cyclical communication process, which consists of participants assuming the roles of senders and receivers, who are sending messages through channels. Thus, the difficulty of test items can be described from four perspectives: (1) message sender (test designer/teacher), (2) message receiver (student), (3) message (set of linguistic components), and (4) channel (medium for the message). These perspectives and related types of measurements were explored in an empirical study, and the preliminary results are presented in this paper. Analysis of the results shows that a communication-centered approach offers a broader view of the assessment process, reveals its multi-dimensionality, and captures new aspects of the tests' difficulty -- for example, time constraints.","measurements, difficulty level, Assessment","","WCCCE '16"
"Conference Paper","Brunnlieb M,Poetzsch-Heffter A","Application of Architecture Implementation Patterns by Incremental Code Generation","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Travelling Conference on Pattern Languages of Programs","Leerdam, AA, Netherlands","2016","9781450342001","","https://doi.org/10.1145/3022636.3022647;http://dx.doi.org/10.1145/3022636.3022647","10.1145/3022636.3022647","Reference architectures have been established in many domains of software engineering enabling reuse of expert knowledge of design, guidelines and much more. Nevertheless, reference architectures might also lead to boilerplate code and implementation overhead. This work addresses this drawback by introducing architecture implementation patterns (AIM Patterns), which are automatically applicable to the application code by an incremental generation approach. AIM Patterns are restricted to and developed for a specific reference architecture as the solution part of an AIM Pattern will be enriched by architecture prescribed implementation details to enable code generation at all. Thus, we gain from the modularized structure of patterns already occurring in pattern-based reference architectures used in industries. By the use of an incremental code generation approach we establish seamless integration of architectural-driven code generation into the developers individual development process on the basis of strict code conventions. The incremental code generation approach has already been successfully used in practice giving a first impression of how the approach can be successfully applied in today's software development. In contrast to that, the specification of the AIM Patterns describes a new extension to incremental code generation establishing a more structured way of reusing architectural knowledge also on code level.","Incremental Code Generation, Pattern-based Reference Architecture, Architecture Implementation Patterns","","VikingPLoP '16"
"Conference Paper","Lahtinen S,Leppänen M","Refactoring Patterns, Practices for Daily Work","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Travelling Conference on Pattern Languages of Programs","Leerdam, AA, Netherlands","2016","9781450342001","","https://doi.org/10.1145/3022636.3022642;http://dx.doi.org/10.1145/3022636.3022642","10.1145/3022636.3022642","In this paper, we describe patterns that can be used to find ways to integrate refactoring into everyday work in a software project. They are a part of a larger refactoring patterns pattern collection. We introduce the pattern collection and the background and describe three patterns in detail. Revision control logging pattern helps to maintain the rationale of the refactoring operations and separates refactoring from, for instance, feature development and bug fixes. Embed small refactorings encourages you to do the minor tweaks, fixes, and changes during your daily routines. Finally, Establish safety net guides you to setup unit tests and revision control before starting refactoring.","software process, software engineering, Refactoring","","VikingPLoP '16"
"Journal Article","Badamo M,Casarona J,Zhao M,Yeung D","Identifying Power-Efficient Multicore Cache Hierarchies via Reuse Distance Analysis","ACM Trans. Comput. Syst.","2016","34","1","","Association for Computing Machinery","New York, NY, USA","","","2016-04","","0734-2071","https://doi.org/10.1145/2851503;http://dx.doi.org/10.1145/2851503","10.1145/2851503","To enable performance improvements in a power-efficient manner, computer architects have been building CPUs that exploit greater amounts of thread-level parallelism. A key consideration in such CPUs is properly designing the on-chip cache hierarchy. Unfortunately, this can be hard to do, especially for CPUs with high core counts and large amounts of cache. The enormous design space formed by the combinatorial number of ways in which to organize the cache hierarchy makes it difficult to identify power-efficient configurations. Moreover, the problem is exacerbated by the slow speed of architectural simulation, which is the primary means for conducting such design space studies.A powerful tool that can help architects optimize CPU cache hierarchies is reuse distance (RD) analysis. Recent work has extended uniprocessor RD techniques-i.e., by introducing concurrent RD and private-stack RD profiling—to enable analysis of different types of caches in multicore CPUs. Once acquired, parallel locality profiles can predict the performance of numerous cache configurations, permitting highly efficient design space exploration. To date, existing work on multicore RD analysis has focused on developing the profiling techniques and assessing their accuracy. Unfortunately, there has been no work on using RD analysis to optimize CPU performance or power consumption.This article investigates applying multicore RD analysis to identify the most power efficient cache configurations for a multicore CPU. First, we develop analytical models that use the cache-miss counts from parallel locality profiles to estimate CPU performance and power consumption. Although future scalable CPUs will likely employ multithreaded (and even out-of-order) cores, our current study assumes single-threaded in-order cores to simplify the models, allowing us to focus on the cache hierarchy and our RD-based techniques. Second, to demonstrate the utility of our techniques, we apply our models to optimize a large-scale tiled CPU architecture with a two-level cache hierarchy. We show that the most power efficient configuration varies considerably across different benchmarks, and that our locality profiles provide deep insights into why certain configurations are power efficient. We also show that picking the best configuration can provide significant gains, as there is a 2.01x power efficiency spread across our tiled CPU design space. Finally, we validate the accuracy of our techniques using detailed simulation. Among several simulated configurations, our techniques can usually pick the most power efficient configuration, or one that is very close to the best. In addition, across all simulated configurations, we can predict power efficiency with 15.2% error.","design space exploration, chip multiprocessors, Cache performance, reuse distance","",""
"Conference Paper","Yokoyama H,Higo Y,Hotta K,Ohta T,Okano K,Kusumoto S","Toward Improving Ability to Repair Bugs Automatically: A Patch Candidate Location Mechanism Using Code Similarity","","2016","","","1364–1370","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851770;http://dx.doi.org/10.1145/2851613.2851770","10.1145/2851613.2851770","Automated program repair is a promising way to reduce costs on program debugging dramatically. Some repair techniques with genetic algorithm have been proposed, and they were able to fix several dozen of actual bugs in open source software. However, existing techniques occasionally take a long time to generate a repaired version of a given program. The dominant factor for that is generating so many programs that do not pass given test cases. In this research, we are trying to generate a repaired program, which passes all the test cases, in less time. Our key idea is using code similarity to select code lines to be inserted into a given program. More concretely, we propose to select code lines in code regions similar to the faulty code regions. In existing techniques, code lines for insertion are randomly selected from a given program. Currently, we are still in an early stage of this research. In this paper, we report some promising results of our pilot study, which show that using code similarity is very effective to generate repaired programs in less time.","source code analysis, automated program repair, genetic programming","","SAC '16"
"Conference Paper","Al-omari F,Roy CK","Is Code Cloning in Games Really Different?","","2016","","","1512–1519","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851792;http://dx.doi.org/10.1145/2851613.2851792","10.1145/2851613.2851792","Since there are a tremendous number of similar functionalities related to images, 3D graphics, sounds, and script in games software, there is a common wisdom that there might be more cloned code in games compared to traditional software. Also, there might be more cloned code across games since many of these games share similar strategies and libraries. In this study, we attempt to investigate whether such statements are true by conducting a large empirical study using 32 games and 9 non-games software, written in three different programming languages C, Java, and C#, for the case of both exact and near-miss clones. Using a hybrid clone detection tool NiCad and a visualization tool VisCad, we examine and compare the cloning status in them and compare it to the non-games, and examine the cloned methods across game engines. The results show that code reuse in open source games is much different from that of other software systems. Specifically, in contrast to the common wisdom, there are fewer function clones in game open source comparing to non-game open source software systems. Similar to non-games open source, we observed that cloning status changes between different programming languages of the games. In addition, there are very fewer clones across games and mostly no clones (no code reuse) across different game engines. But clones exist heavily across recreated (cloned) games.","open source games, software clones, game clones","","SAC '16"
"Conference Paper","Krutz DE,Mirakhorl M","Architectural Clones: Toward Tactical Code Reuse","","2016","","","1480–1485","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851787;http://dx.doi.org/10.1145/2851613.2851787","10.1145/2851613.2851787","Architectural tactics are the building blocks of software architecture. They describe solutions for addressing specific quality concerns, and are prevalent across many software systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for implementing the tactic in the code. Unfortunately, this is a non-trivial task for even experienced developers. Developers often resort to using search engines, crowd-sourcing websites, or discussion forums to find sample code snippets. A robust Tactic Search Engine can replace this manual, internet-based search process and help developers to reuse proper architectural knowledge and accurately implement tactics and patterns from a wide range of open source systems. In this paper we analyze several implementations of architectural tactics in the open source community and identify the foundation for building a practical Tactic Search Engine. We also introduce the concept of tactical-clones which may be used as the basic element of a tactic search engine.","code reuse, software architecture, tactical code clone","","SAC '16"
"Conference Paper","Schmorleiz T,Lämmel R","Similarity Management of 'cloned and Owned' Variants","","2016","","","1466–1471","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851785;http://dx.doi.org/10.1145/2851613.2851785","10.1145/2851613.2851785","The 'clone and own' approach to software product lines assumes that variants are created by cloning and evolve more or less independently afterwards. In this paper, we describe a process to manage similarity of such 'cloned and owned' variants along the timeline. The process uses annotations for recording developer intentions and it leverages automatic change propagation. We describe a case study where we manage similarity for clowned-and-owned Haskell-based variants of a simple human-resources management system.","change propagation, similarity analysis, clone detection, annotation, software product lines, similarity management, variability","","SAC '16"
"Conference Paper","Romano S,Scanniello G,Sartiani C,Risi M","A Graph-Based Approach to Detect Unreachable Methods in Java Software","","2016","","","1538–1541","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851968;http://dx.doi.org/10.1145/2851613.2851968","10.1145/2851613.2851968","In this paper, we have defined a static approach named DUM (Detecting Unreachable Methods) that works on Java byte-code and detects unreachable methods by traversing a graph-based representation of the software to be analyzed. To assess the validity of our approach, we have implemented it in a prototype software system. Both our approach and prototype have been validated on four open-source software. Results have shown the correctness, the completeness, and the accuracy of the methods that our solution detected as unreachable. We have also compared our solution with: JTombstone and Google CodePro AnalytiX. This comparison suggested that DUM outperforms baselines.","bad smells, software maintenance, unreachable methods","","SAC '16"
"Conference Paper","Gómez-Abajo P,Guerra E,de Lara J","Wodel: A Domain-Specific Language for Model Mutation","","2016","","","1968–1973","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851751;http://dx.doi.org/10.1145/2851613.2851751","10.1145/2851613.2851751","Model-Driven Engineering (MDE) is a software engineering paradigm that uses models as main assets in all development phases. While many languages for model manipulation exist (e.g., for model transformation or code generation), there is a lack of frameworks to define and apply model mutations.A model mutant is a variation of an original model, created by specific model mutation operations. Model mutation has many applications, for instance, in the areas of model transformation testing, model-based testing or education.In this paper, we present a domain-specific language, called Wodel, for the specification and generation of model mutants. Wodel is domain-independent, as it can be used to generate mutants of models conforming to arbitrary metamodels. Its development environment is extensible, permitting the incorporation of post-processors for different applications. As an example, we show an application consisting on the automated generation of exercises for particular domains (automata, class diagrams, electronic circuits, etc.).","education, domain-specific languages, model-driven engineering, mutation, model","","SAC '16"
"Conference Paper","Behringer B,Rothkugel S","Integrating Feature-Based Implementation Approaches Using a Common Graph-Based Representation","","2016","","","1504–1511","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851791;http://dx.doi.org/10.1145/2851613.2851791","10.1145/2851613.2851791","Based on the structured document algebra, we propose the model of structured document graphs: a common, generic graph-based representation for compositional and annotative feature implementations. The core elements in the graph are modules that allow adding, removing and replacing feature details in a fine-granular fashion. To show the feasibility and generality of our model, we prototypically implemented our concepts and imported five FeatureHouse examples (written in Java) and three CIDE projects (written in Java, Haskell and HTML) into our prototype. To test the validity of the resulting graph, we randomly produced different products, semi-automatically compared our results to the ones produced by the original tool, and found no semantic differences. As a result, our model lays the foundation for projections that allow moving fluidly between compositional, annotative and mixed versions of the product line.","compositional approach, annotations, variability, structured documents, feature-oriented software product lines","","SAC '16"
"Conference Paper","de F. Farias MA,Novais R,Júnior MC,da Silva Carvalho LP,Mendonça M,Spínola RO","A Systematic Mapping Study on Mining Software Repositories","","2016","","","1472–1479","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851786;http://dx.doi.org/10.1145/2851613.2851786","10.1145/2851613.2851786","Background: Software repositories provide large amount of data encompassing software changes throughout its evolution. Those repositories can be effectively used to extract and analyze pertinent information and derive conclusions related to the software history or its current snapshot. Objective: This work aims to investigate recent studies on Mining Software Repositories (MSR) approaches collecting evidences about software analysis goals (purpose, focus, and object of analysis), data sources, evaluation methods, tools, and how the area is evolving. Method: A systematic mapping study was performed to identify and analyze research on mining software repositories by analyzing five editions of Working Conference on Mining Software Repositories -- the main conference on this area. Results: MSR approaches have been used for many different goals, mainly for comprehension of defects, analysis of the contribution and behavior of developers, and software evolution comprehension. Besides, some gaps were identified with respect to their goals, focus, and data source type (e.g. lack of usage of comments to identify smells, refactoring, and issues of software quality). Regarding the evaluation method, our analysis pointed out to an extensive usage of some types of empirical evaluation. Conclusion: Studies of the MSR have focused on different goals, however there are still many research opportunities to be explored and issues associated with MSR that should be considered.","empirical software engineering, mining software repository, secondary study, systematic mapping study","","SAC '16"
"Conference Paper","Tavakoli M,Heydarnoori A,Ghafari M","Improving the Quality of Code Snippets in Stack Overflow","","2016","","","1492–1497","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual ACM Symposium on Applied Computing","Pisa, Italy","2016","9781450337397","","https://doi.org/10.1145/2851613.2851789;http://dx.doi.org/10.1145/2851613.2851789","10.1145/2851613.2851789","Question and answer (Q&A) websites like Stack Overflow are one of the important sources of code examples in which developers can ask their questions and leave their answers about programming issues. Since the number of programmers who use these websites are increasing and a large number of questions and answers are being posted there by them, verifying the quality of all the answers and particularly the code snippets in them is impossible. Consequently, some code snippets might be of low quality and/or with faults. To mitigate this issue, we introduce ExRec (Example Recommender), an Eclipse plugin with which programmers can contribute in improving the quality of code snippets in the answers to questions in the Stack Overow website. Using ExRec, a programmer can transfer a code snippet in the answer to a desired question in Stack Overow to her own program, apply her desired modifications to that code snippet, and then contribute the improved code snippet to Stack Overow as an answer to that particular question. Our evaluations of ExRec in a user study with 10 skilled programmers indicate that it is effective in practice and can help in improving the quality of code snippets in Stack Overow.","Q&A websites, recommender systems, eclipse plugin, stack overflow, improved code snippets","","SAC '16"
"Journal Article","Char B","Automatic Feedback Systems for Code: What Can They Tell the Busy Instructor?","J. Comput. Sci. Coll.","2016","31","4","87–93","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2016-04","","1937-4771","","","CodeLab, an automatic exercise and coding practice system was introduced into a university's CS1-style introductory programming course across several terms. CodeLab question bank items, and instructor-authored exercises and programming assignments were used, complementing conventional face-to-face class lectures, labs, and quizzes. Grading data from CodeLab describes for each problem and student how many attempts were needed before correct completion. Analysis indicates students persevered with the exercises, with many doing more problems than they received grade credit for. Observations were also obtained on problem suitability and relationships between exercise effort and course grades. Such findings show how autograder data can be used to facilitate awareness and improvement efforts within the time-resource envelope of typical instruction.","","",""
"Conference Paper","Mao J,Chen Y,Xiao Q,Shi Y","RID: Finding Reference Count Bugs with Inconsistent Path Pair Checking","","2016","","","531–544","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems","Atlanta, Georgia, USA","2016","9781450340915","","https://doi.org/10.1145/2872362.2872389;http://dx.doi.org/10.1145/2872362.2872389","10.1145/2872362.2872389","Reference counts are widely used in OS kernels for resource management. However, reference counts are not trivial to be used correctly in large scale programs because it is left to developers to make sure that an increment to a reference count is always paired with a decrement. This paper proposes inconsistent path pair checking, a novel technique that can statically discover bugs related to reference counts without knowing how reference counts should be changed in a function. A prototype called RID is implemented and evaluations show that RID can discover more than 80 bugs which were confirmed by the developers in the latest Linux kernel. The results also show that RID tends to reveal bugs caused by developers' misunderstanding on API specifications or error conditions that are not handled properly.","inconsistency, static analysis, reference counting","","ASPLOS '16"
"Journal Article","Mao J,Chen Y,Xiao Q,Shi Y","RID: Finding Reference Count Bugs with Inconsistent Path Pair Checking","SIGARCH Comput. Archit. News","2016","44","2","531–544","Association for Computing Machinery","New York, NY, USA","","","2016-03","","0163-5964","https://doi.org/10.1145/2980024.2872389;http://dx.doi.org/10.1145/2980024.2872389","10.1145/2980024.2872389","Reference counts are widely used in OS kernels for resource management. However, reference counts are not trivial to be used correctly in large scale programs because it is left to developers to make sure that an increment to a reference count is always paired with a decrement. This paper proposes inconsistent path pair checking, a novel technique that can statically discover bugs related to reference counts without knowing how reference counts should be changed in a function. A prototype called RID is implemented and evaluations show that RID can discover more than 80 bugs which were confirmed by the developers in the latest Linux kernel. The results also show that RID tends to reveal bugs caused by developers' misunderstanding on API specifications or error conditions that are not handled properly.","reference counting, inconsistency, static analysis","",""
"Journal Article","Mao J,Chen Y,Xiao Q,Shi Y","RID: Finding Reference Count Bugs with Inconsistent Path Pair Checking","SIGPLAN Not.","2016","51","4","531–544","Association for Computing Machinery","New York, NY, USA","","","2016-03","","0362-1340","https://doi.org/10.1145/2954679.2872389;http://dx.doi.org/10.1145/2954679.2872389","10.1145/2954679.2872389","Reference counts are widely used in OS kernels for resource management. However, reference counts are not trivial to be used correctly in large scale programs because it is left to developers to make sure that an increment to a reference count is always paired with a decrement. This paper proposes inconsistent path pair checking, a novel technique that can statically discover bugs related to reference counts without knowing how reference counts should be changed in a function. A prototype called RID is implemented and evaluations show that RID can discover more than 80 bugs which were confirmed by the developers in the latest Linux kernel. The results also show that RID tends to reveal bugs caused by developers' misunderstanding on API specifications or error conditions that are not handled properly.","static analysis, reference counting, inconsistency","",""
"Conference Paper","Hoegg T,Fiedler G,Koehler C,Kolb A","Flow Driven GPGPU Programming Combining Textual and Graphical Programming","","2016","","","88–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Workshop on Programming Models and Applications for Multicores and Manycores","Barcelona, Spain","2016","9781450341967","","https://doi.org/10.1145/2883404.2883412;http://dx.doi.org/10.1145/2883404.2883412","10.1145/2883404.2883412","GPGPUs (General Purpose Computation on Graphics Processing Unit) have become the most important invention in the last years in computer graphics and the vision domain. Despite improvement of the two main programming platforms, CUDA (Compute Unified Device Architecture) and OpenCL (Open Computing Language), GPGPU programming and development is still a complex, time consuming and error-prone task. To overcome these problems for general software engineering, the graphical modeling language UML (Unified Modeling Language) was introduced and became the first choice for designing software systems. However, its generic design causes representations of algorithmic problem descriptions to be either limited or too complicated. We present GU-DSL, a novel domain-specific language (DSL), including novel modeling concepts (new activity-diagram node types and special language constructs), based on Eclipse Xtext and GMF, adopting and extending class- and activity-diagrams in a textual and graphical form. Furthermore, we present a C++ and OpenCL code generation framework in combination with a heterogeneous C++ GPGPU computing framework allowing for a smooth connection with our DSL and graphical editors.","Domain-specific languages, GPGPU, Model-Driven Development, Modeling, Computer Graphics and Vision","","PMAM'16"
"Conference Paper","Ishii Y,Watanabe T,Akiyama M,Mori T","Clone or Relative? Understanding the Origins of Similar Android Apps","","2016","","","25–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2016 ACM on International Workshop on Security And Privacy Analytics","New Orleans, Louisiana, USA","2016","9781450340779","","https://doi.org/10.1145/2875475.2875480;http://dx.doi.org/10.1145/2875475.2875480","10.1145/2875475.2875480","Since it is not hard to repackage an Android app, there are many cloned apps, which we call clones in this work. As previous studies have reported, clones are generated for bad purposes by malicious parties, e.g., adding malicious functions, injecting/replacing advertising modules, and piracy. Besides such clones, there are legitimate, similar apps, which we call ""relatives"" in this work. These relatives are not clones but are similar in nature; i.e., they are generated by the same app-building service or by the same developer using a same template. Given these observations, this paper aims to answer the following two research questions: (RQ1) How can we distinguish between clones and relatives? (RQ2) What is the breakdown of clones and relatives in the official and third-party marketplaces? To answer the first research question, we developed a scalable framework called APPraiser that systematically extracts similar apps and classifies them into clones and relatives. We note that our key algorithms, which leverage sparseness of the data, have the time complexity of O(n) in practice. To answer the second research question, we applied the APPraiser framework to the over 1.3 millions of apps collected from official and third-party marketplaces. Our analysis revealed the following findings: In the official marketplace, 79% of similar apps were attributed to relatives while, in the third-party marketplace, 50% of similar apps were attributed to clones. The majority of relatives are apps developed by prolific developers in both marketplaces. We also found that in the third-party market, of the clones that were originally published in the official market, 76% of them are malware.To the best of our knowledge, this is the first work that clarified the breakdown of ""similar"" Android apps, and quantified their origins using a huge dataset equivalent to the size of official market.","repackaging, mobile security, large-scale data, android","","IWSPA '16"
"Conference Paper","Stevens R,Crussell J,Chen H","On the Origin of Mobile Apps: Network Provenance for Android Applications","","2016","","","160–171","Association for Computing Machinery","New York, NY, USA","Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy","New Orleans, Louisiana, USA","2016","9781450339353","","https://doi.org/10.1145/2857705.2857712;http://dx.doi.org/10.1145/2857705.2857712","10.1145/2857705.2857712","Many mobile services consist of two components: a server providing an API, and an application running on smartphones and communicating with the API. An unresolved problem in this design is that it is difficult for the server to authenticate which app is accessing the API. This causes many security problems. For example, the provider of a private network API has to embed secrets in its official app to ensure that only this app can access the API; however, attackers can uncover the secret by reverse-engineering. As another example, malicious apps may send automatic requests to ad servers to commit ad fraud.In this work, we propose a system that allows network API to authenticate the mobile app that sends each request so that the API can make an informed access control decision. Our system, the Mobile Trusted-Origin Policy, consists of two parts: 1) an app provenance mechanism that annotates outgoing HTTP(S) requests with information about which app generated the network traffic, and 2) a code isolation mechanism that separates code within an app that should have different app provenance signatures into mobile origin. As motivation for our work, we present two previously-unknown families of apps that perform click fraud, and examine how the lack of mobile origin information enables the attacks. Based on our observations, we propose Trusted Cross-Origin Requests to handle point (1), which automatically includes mobile origin information in outgoing HTTP requests. Servers may then decide, based on the mobile origin data, whether to process the request or not. We implement a prototype of our system for Android and evaluate its performance, security, and deployability. We find that our system can achieve our security and utility goals with negligible overhead.","advertising security, app authentication, mobile security, mobile advertising","","CODASPY '16"
"Journal Article","Gregg B","The Flame Graph: This Visualization of Software Execution is a New Necessity for Performance Profiling and Debugging","Queue","2016","14","2","91–110","Association for Computing Machinery","New York, NY, USA","","","2016-03","","1542-7730","https://doi.org/10.1145/2927299.2927301;http://dx.doi.org/10.1145/2927299.2927301","10.1145/2927299.2927301","An everyday problem in our industry is understanding how software is consuming resources, particularly CPUs. What exactly is consuming how much, and how did this change since the last software version? These questions can be answered using software profilers, tools that help direct developers to optimize their code and operators to tune their environment. The output of profilers can be verbose, however, making it laborious to study and comprehend. The flame graph provides a new visualization for profiler output and can make for much faster comprehension, reducing the time for root cause analysis.","","",""
"Conference Paper","Gupta RK,Manikreddy P,Naik S,Arya K","Pragmatic Approach for Managing Technical Debt in Legacy Software Project","","2016","","","170–176","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th India Software Engineering Conference","Goa, India","2016","9781450340182","","https://doi.org/10.1145/2856636.2856655;http://dx.doi.org/10.1145/2856636.2856655","10.1145/2856636.2856655","Tackling the issues of technical debt in a large system in parallel with continuing to enable it to evolve is a challenging problem.In this paper, we are describing a case study of managing technical debt on a legacy project referred here as Global Configurator Project (GCP) using pragmatic approach. The paper presents holistic lifecycle approach with four stages and various practices in each stage for managing technical debt. Given life cycle approach and practices will be useful for any software project. In particular, these practices will be significant to any legacy project towards repaying debt. These methods can also be applied to continuously improve code quality and product quality. This paper also focus on technical debt user stories to gain business buy-in and share few 'best in market' tools that we used in repaying technical debt. It also focuses on sensitizing developers to the concept of debt and improving their skills. This paper describes the process used by a separate team formed to reduce technical debt in a large legacy system.The paper targets to the Project Managers, Test Managers architects and Scrum Masters in agile software development.","Product Quality, Pragmatic, Code Quality, Technical Debt, Static Analysis, Lifecycle Approach","","ISEC '16"
"Conference Paper","Sripada SK,Reddy YR,Khandelwal S","Architecting an Extensible Framework for Gamifying Software Engineering Concepts","","2016","","","119–130","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th India Software Engineering Conference","Goa, India","2016","9781450340182","","https://doi.org/10.1145/2856636.2856649;http://dx.doi.org/10.1145/2856636.2856649","10.1145/2856636.2856649","Software engineering activities like code reviews, change management, knowledge management, issue tracking, etc. tend to be heavily process oriented. Gamification of such activities by composing the core activities with game design elements like badges and points can increase developers' interest in performing such activities. While there are various frameworks/applications that assist in gamification, extending the frameworks to add any/all desired game design elements has not been adequately addressed. In this paper, we propose an extensible architectural framework for gamification of software engineering activities where in the game design elements are modeled as services. We create an example instance of our framework by building a prototype for code review activity and note the challenges of designing such an extensible architectural framework. The example instance uses python's Flask micro framework and has five game design elements implemented as services, and exposed using restful APIs.","Architecture, REST API, Game Design Elements, Web Hooks, Gamification, Code review, Services","","ISEC '16"
"Conference Paper","Prior J,Ferguson S,Leaney J","Reflection is Hard: Teaching and Learning Reflective Practice in a Software Studio","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Australasian Computer Science Week Multiconference","Canberra, Australia","2016","9781450340427","","https://doi.org/10.1145/2843043.2843346;http://dx.doi.org/10.1145/2843043.2843346","10.1145/2843043.2843346","We have observed that it is a non-trivial exercise for undergraduate students to learn how to reflect. Reflective practice is now recognised as important for software developers and has become a key part of software studios in universities, but there is limited empirical investigation into how best to teach and learn reflection. In the literature on reflection in software studios, there are many papers that claim that reflection in the studio is mandatory. However, there is inadequate guidance about teaching early stage students to reflect in that literature. The essence of the work presented in this paper is a beginning to the consideration of how the teaching of software development can best be combined with teaching reflective practice for early stage software development students. We started on a research programme to understand how to encourage students to learn to reflect. As we were unsure about teaching reflection, and we wished to change our teaching as we progressively understood better what to do, we chose action research as the most suitable approach. Within the action research cycles we used ethnography to understand what was happening with the students when they attempted to reflect. This paper reports on the first 4 semesters of research.We have developed and tested a reflection model and process that provide scaffolding for students beginning to reflect. We have observed three patterns in how our students applied this process in writing their reflections, which we will use to further understand what will help them learn to reflect. We have also identified two themes, namely, motivation and intervention, which highlight where the challenges lie in teaching and learning reflection.","software studios, software development, reflective practice, reflection","","ACSW '16"
"Conference Paper","Simon,Sheard J","Academic Integrity and Computing Assessments","","2016","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Australasian Computer Science Week Multiconference","Canberra, Australia","2016","9781450340427","","https://doi.org/10.1145/2843043.2843060;http://dx.doi.org/10.1145/2843043.2843060","10.1145/2843043.2843060","A recent Australian project has investigated academics' and students' understandings of and attitudes to academic integrity in computing assessments. We explain the project and summarise some of its findings, which have been presented in a number of prior papers. In an extended discussion section we then raise a number of questions that we believe must be addressed by the computing education community if it is to be seen to take academic integrity seriously. We question the value and the validity of a number of current educational practices, and urge the community to work towards resolutions of the unanswered questions.","computing assessment, academic integrity, programming assessment","","ACSW '16"
"Conference Paper","Barry T,Couroussé D,Robisson B","Compilation of a Countermeasure Against Instruction-Skip Fault Attacks","","2016","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third Workshop on Cryptography and Security in Computing Systems","Prague, Czech Republic","2016","9781450340656","","https://doi.org/10.1145/2858930.2858931;http://dx.doi.org/10.1145/2858930.2858931","10.1145/2858930.2858931","Physical attacks especially fault attacks represent one the major threats against embedded systems. In the state of the art, software countermeasures against fault attacks are either applied at the source code level where it will very likely be removed at compilation time, or at assembly level where several transformations need to be performed on the assembly code and lead to significant overheads both in terms of code size and execution time. This paper presents the use of compiler techniques to efficiently automate the application of software countermeasures against instruction-skip fault attacks. We propose a modified LLVM compiler that considers our security objectives throughout the compilation process. Experimental results illustrate the effectiveness of this approach on AES implementations running on an ARM-based microcontroller in terms of security overhead compared to existing solutions.","AES, Countermeasures, Compiler, LLVM, Fault Attacks","","CS2 '16"
"Conference Paper","Katz O,El-Yaniv R,Yahav E","Estimating Types in Binaries Using Predictive Modeling","","2016","","","313–326","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","St. Petersburg, FL, USA","2016","9781450335492","","https://doi.org/10.1145/2837614.2837674;http://dx.doi.org/10.1145/2837614.2837674","10.1145/2837614.2837674","Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model (SLM) for each type.We then use the resulting ensemble of SLMs over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer.","x86, static binary analysis, reverse engineering","","POPL '16"
"Journal Article","Katz O,El-Yaniv R,Yahav E","Estimating Types in Binaries Using Predictive Modeling","SIGPLAN Not.","2016","51","1","313–326","Association for Computing Machinery","New York, NY, USA","","","2016-01","","0362-1340","https://doi.org/10.1145/2914770.2837674;http://dx.doi.org/10.1145/2914770.2837674","10.1145/2914770.2837674","Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model (SLM) for each type.We then use the resulting ensemble of SLMs over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer.","reverse engineering, static binary analysis, x86","",""
"Journal Article","Cheramangalath U,Nasre R,Srikant YN","Falcon: A Graph Manipulation Language for Heterogeneous Systems","ACM Trans. Archit. Code Optim.","2015","12","4","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1544-3566","https://doi.org/10.1145/2842618;http://dx.doi.org/10.1145/2842618","10.1145/2842618","Graph algorithms have been shown to possess enough parallelism to keep several computing resources busy—even hundreds of cores on a GPU. Unfortunately, tuning their implementation for efficient execution on a particular hardware configuration of heterogeneous systems consisting of multicore CPUs and GPUs is challenging, time consuming, and error prone. To address these issues, we propose a domain-specific language (DSL), Falcon, for implementing graph algorithms that (i) abstracts the hardware, (ii) provides constructs to write explicitly parallel programs at a higher level, and (iii) can work with general algorithms that may change the graph structure (morph algorithms). We illustrate the usage of our DSL to implement local computation algorithms (that do not change the graph structure) and morph algorithms such as Delaunay mesh refinement, survey propagation, and dynamic SSSP on GPU and multicore CPUs. Using a set of benchmark graphs, we illustrate that the generated code performs close to the state-of-the-art hand-tuned implementations.","local computation algorithms, domain specific languages, GPU, multi-core CPU, OpenMP, CUDA, morph algorithms, Graph manipulation languages","",""
"Journal Article","Äijö T,Jääskeläinen P,Elomaa T,Kultala H,Takala J","Integer Linear Programming-Based Scheduling for Transport Triggered Architectures","ACM Trans. Archit. Code Optim.","2015","12","4","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1544-3566","https://doi.org/10.1145/2845082;http://dx.doi.org/10.1145/2845082","10.1145/2845082","Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler. This is motivated by the ability to support high degrees of instruction-level parallelism without requiring complicated scheduling logic in the processor hardware. The simpler-control hardware results in reduced area and power consumption, but leads to a challenge of engineering a compiler with good code-generation quality.Transport triggered architectures (TTA), and other so-called exposed datapath architectures, take the compiler-oriented philosophy even further by pushing more details of the datapath under software control. The main benefit of this is the reduced register file pressure, with a drawback of adding even more complexity to the compiler side.In this article, we propose an Integer Linear Programming (ILP)-based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program. The model is validated and measured by compiling application kernels to various TTAs with a different number of datapath components and connectivity. In the best case, the cycle count is reduced to 52% when compared to a heuristic scheduler. In addition to producing shorter schedules, the number of register accesses in the compiled programs is generally notably less than those with the heuristic scheduler; in the best case, the ILP scheduler reduced the number of register file reads to 33% of the heuristic results and register file writes to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more time to produce a schedule than the heuristic scheduler, but the compilation time is within tolerable limits for production-code generation.","transport triggered architectures, instruction-level parallelism, integer linear programming, Code generation, exposed datapath","",""
"Journal Article","Winterstein FJ,Bayliss SR,Constantinides GA","Separation Logic for High-Level Synthesis","ACM Trans. Reconfigurable Technol. Syst.","2015","9","2","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1936-7406","https://doi.org/10.1145/2836169;http://dx.doi.org/10.1145/2836169","10.1145/2836169","High-Level Synthesis (HLS) promises a significant shortening of the FPGA design cycle by raising the abstraction level of the design entry to high-level languages such as C/C++. However, applications using dynamic, pointer-based data structures and dynamic memory allocation remain difficult to implement well, yet such constructs are widely used in software. Automated optimizations that leverage the memory bandwidth of FPGAs by distributing the application data over separate banks of on-chip memory are often ineffective in the presence of dynamic data structures due to the lack of an automated analysis of pointer-based memory accesses. In this work, we take a step toward closing this gap. We present a static analysis for pointer-manipulating programs that automatically splits heap-allocated data structures into disjoint, independent regions. The analysis leverages recent advances in separation logic, a theoretical framework for reasoning about heap-allocated data that has been successfully applied in recent software verification tools. Our algorithm focuses on dynamic data structures accessed in loops and is accompanied by automated source-to-source transformations that enable automatic loop parallelization and memory partitioning by off-the-shelf HLS tools. We demonstrate the successful loop parallelization and memory partitioning by our tool flow using three real-life applications that build, traverse, update, and dispose of dynamically allocated data structures. Our case studies, comparing the automatically parallelized to the direct HLS implementations, show an average latency reduction by a factor of 2 × across our benchmarks.","FPGA, high-level synthesis, static analysis, memory system, separation logic","",""
"Journal Article","Lu J,Bai K,Shrivastava A","Efficient Code Assignment Techniques for Local Memory on Software Managed Multicores","ACM Trans. Embed. Comput. Syst.","2015","14","4","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1539-9087","https://doi.org/10.1145/2738039;http://dx.doi.org/10.1145/2738039","10.1145/2738039","Scaling the memory hierarchy is a major challenge when we scale the number of cores in a multicore processor. Software Managed Multicore (SMM) architectures come up as one of the promising solutions. In an SMM architecture, there are no caches, and each core has only a local scratchpad memory [Banakar et al. 2002]. As the local memory usually is small, large applications cannot be directly executed on it. Code and data of the task mapped to each core need to be managed between global memory and local memory. This article solves the problem of efficiently managing code on an SMM architecture. The primary requirement of generating efficient code assignments is a correct management cost model. In this article, we address this problem by proposing a cost calculation graph. In addition, we develop two heuristics CMSM (Code Mapping for Software Managed multicores) and CMSM_advanced that result in efficient code management execution on the local scratchpad memory. Experimental results collected after executing applications from the MiBench suite [Guthaus et al. 2001] demonstrate that merely by adopting the correct management cost calculation, even using previous code assignment schemes, we can improve performance by an average of 12%. Combining the correct management cost model and a more optimized code mapping algorithm together, our heuristics can reduce runtime in more than 80% of the cases, and by up to 20% on our set of benchmarks, compared to the state-of-the-art code assignment approach [Jung et al. 2010]. When compared with Instruction-level Parallelism (ILP) results, CMSM_advanced performs an average of 5% worse. We also simulate the benchmarks on a cache-based system, and find that the code management overhead on SMM core with our code management is much less than memory latency of a cache-based system.","local memory, multicore processor, embedded systems, instruction, scratchpad memory, Code","",""
"Journal Article","Anderson A,Malik A,Gregg D","Automatic Vectorization of Interleaved Data Revisited","ACM Trans. Archit. Code Optim.","2015","12","4","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1544-3566","https://doi.org/10.1145/2838735;http://dx.doi.org/10.1145/2838735","10.1145/2838735","Automatically exploiting short vector instructions sets (SSE, AVX, NEON) is a critically important task for optimizing compilers. Vector instructions typically work best on data that is contiguous in memory, and operating on non-contiguous data requires additional work to gather and scatter the data. There are several varieties of non-contiguous access, including interleaved data access. An existing approach used by GCC generates extremely efficient code for loops with power-of-2 interleaving factors (strides). In this paper we propose a generalization of this approach that produces similar code for any compile-time constant interleaving factor. In addition, we propose several novel program transformations, which were made possible by our generalized representation of the problem. Experiments show that our approach achieves significant speedups for both power-of-2 and non--power-of-2 interleaving factors. Our vectorization approach results in mean speedups over scalar code of 1.77x on Intel SSE and 2.53x on Intel AVX2 in real-world benchmarking on a selection of BLAS Level 1 routines. On the same benchmark programs, GCC 5.0 achieves mean improvements of 1.43x on Intel SSE and 1.30x on Intel AVX2. In synthetic benchmarking on Intel SSE, our maximum improvement on data movement is over 4x for gathering operations and over 6x for scattering operations versus scalar code.","SIMD, data permutation, interleaving, Vectorization","",""
"Conference Paper","Copos B,Murthy P","InputFinder: Reverse Engineering Closed Binaries Using Hardware Performance Counters","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th Program Protection and Reverse Engineering Workshop","Los Angeles, CA, USA","2015","9781450336420","","https://doi.org/10.1145/2843859.2843865;http://dx.doi.org/10.1145/2843859.2843865","10.1145/2843859.2843865","The effectiveness of many dynamic program analysis techniques depends heavily on the completeness of the test suite applied during the analysis process. Test suites are often composed by developers and aim at testing all of the functionality of a software system. However, test suites may not be complete, if they exist at all. To date, only two methods exist for automatically generating test input for closed binaries: fuzzing and symbolic execution. Despite previous successes of these methods in identifying bugs, both techniques have limitations. In this paper, we propose a new method for autonomously generating valid input and identifying protocols for closed x86 binaries. The method presented can be used as a standalone tool or can be combined with other techniques for improved results. To assess its effectiveness, we test InputFinder, the implementation of our method, against binaries from the DARPA Cyber Grand Challenge example set. Our evaluations show that our method is not only effective in finding input and determining whether a protocol is expected but can also find unexpected control flow paths.","","","PPREW-5"
"Conference Paper","Wang M,Yin H,Bhaskar AV,Su P,Feng D","Binary Code Continent: Finer-Grained Control Flow Integrity for Stripped Binaries","","2015","","","331–340","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st Annual Computer Security Applications Conference","Los Angeles, CA, USA","2015","9781450336826","","https://doi.org/10.1145/2818000.2818017;http://dx.doi.org/10.1145/2818000.2818017","10.1145/2818000.2818017","Control Flow Integrity (CFI) is an effective technique to mitigate threats such as code-injection and code-reuse attacks in programs by protecting indirect transfers. For stripped binaries, a CFI policy has to be made conservatively due to the lack of source code level semantics. Existing binary-only CFI solutions such as BinCFI and CCFIR demonstrate the ability to protect stripped binaries, but the policies they apply are too permissive, allowing sophisticated code-reuse attacks. In this paper, we propose a new binary-only CFI protection scheme called BinCC, which applies static binary rewriting to provide finer-grained protection for x86 stripped ELF binaries. Through code duplication and static analysis, we divide the binary code into several mutually exclusive code continents. We further classify each indirect transfer within a code continent as either an Intra-Continent transfer or an Inter-Continent transfer, and apply separate, strict CFI polices to constrain these transfers. To evaluate BinCC, we introduce new metrics to estimate the average amount of legitimate targets of each kind of indirect transfer as well as the difficulty to leverage call preceded gadgets to generate ROP exploits. Compared to the state of the art binary-only CFI, BinCFI, the experimental results show that BinCC significantly reduces the legitimate transfer targets by 81.34% and increases the difficulty for adversaries to bypass CFI restriction to launch sophisticated ROP attacks. Also, BinCC achieves a reasonable performance, around 14% of the space overhead decrease and only 4% runtime overhead increase as compared to BinCFI.","Control Flow Integrity","","ACSAC '15"
"Conference Paper","Zhu Y,Richins D,Halpern M,Reddi VJ","Microarchitectural Implications of Event-Driven Server-Side Web Applications","","2015","","","762–774","Association for Computing Machinery","New York, NY, USA","Proceedings of the 48th International Symposium on Microarchitecture","Waikiki, Hawaii","2015","9781450340342","","https://doi.org/10.1145/2830772.2830792;http://dx.doi.org/10.1145/2830772.2830792","10.1145/2830772.2830792","Enterprise Web applications are moving towards server-side scripting using managed languages. Within this shifting context, event-driven programming is emerging as a crucial programming model to achieve scalability. In this paper, we study the microarchitectural implications of server-side scripting, JavaScript in particular, from a unique event-driven programming model perspective. Using the Node.js framework, we come to several critical microarchitectural conclusions. First, unlike traditional server-workloads such as CloudSuite and BigDataBench that are based on the conventional thread-based execution model, event-driven applications are heavily single-threaded, and as such they require significant single-thread performance. Second, the single-thread performance is severely limited by the front-end inefficiencies of today's server processor microarchitecture, ultimately leading to overall execution inefficiencies. The front-end inefficiencies stem from the unique combination of limited intra-event code reuse and large inter-event reuse distance. Third, through a deep understanding of event-specific characteristics, architects can mitigate the front-end inefficiencies of the managed-language-based event-driven execution via a combination of instruction cache insertion policy and prefetcher.","event-driven, microarchitecture, prefetcher, JavaScript","","MICRO-48"
"Conference Paper","Shevgoor M,Koladiya S,Balasubramonian R,Wilkerson C,Pugsley SH,Chishti Z","Efficiently Prefetching Complex Address Patterns","","2015","","","141–152","Association for Computing Machinery","New York, NY, USA","Proceedings of the 48th International Symposium on Microarchitecture","Waikiki, Hawaii","2015","9781450340342","","https://doi.org/10.1145/2830772.2830793;http://dx.doi.org/10.1145/2830772.2830793","10.1145/2830772.2830793","Prior work in hardware prefetching has focused mostly on either predicting regular streams with uniform strides, or predicting irregular access patterns at the cost of large hardware structures. This paper introduces the Variable Length Delta Prefetcher (VLDP), which builds up delta histories between successive cache line misses within physical pages, and then uses these histories to predict the order of cache line misses in new pages. One of VLDP's distinguishing features is its use of multiple prediction tables, each of which stores predictions based on a different length of input history. For example, the first prediction table takes as input only the single most recent delta between cache misses within a page, and attempts to predict the next cache miss in that page. The second prediction table takes as input a sequence of the two most recent deltas between cache misses within a page, and also attempts to predict the next cache miss in that page, and so on with additional tables. Longer histories generally yield more accurate predictions, so VLDP prefers to make predictions based on the longest history table that has a matching entry.Using a global history of patterns it has seen in the past, VLDP is able to issue prefetches without having to wait for additional per-page confirmation, and it is even able to prefetch patterns that show no repetition within a physical page. VLDP does not use the program counter (PC) to make its predictions, but our evaluation shows that it out-performs the highest-performing PC-based prefetcher by 7.1%, and the highest performing prefetcher that doesn't employ the PC by 5.8%.","prefetching","","MICRO-48"
"Journal Article","Mahmoud A,Bradshaw G","Estimating Semantic Relatedness in Source Code","ACM Trans. Softw. Eng. Methodol.","2015","25","1","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1049-331X","https://doi.org/10.1145/2824251;http://dx.doi.org/10.1145/2824251","10.1145/2824251","Contemporary software engineering tools exploit semantic relations between individual code terms to aid in code analysis and retrieval tasks. Such tools employ word similarity methods, often used in natural language processing (nlp), to analyze the textual content of source code. However, the notion of similarity in source code is different from natural language. Source code often includes unnatural domain-specific terms (e.g., abbreviations and acronyms), and such terms might be related due to their structural relations rather than linguistic aspects. Therefore, applying natural language similarity methods to source code without adjustment can produce low-quality and error-prone results. Motivated by these observations, we systematically investigate the performance of several semantic-relatedness methods in the context of software. Our main objective is to identify the most effective semantic schemes in capturing association relations between source code terms. To provide an unbiased comparison, different methods are compared against human-generated relatedness information using terms from three software systems. Results show that corpus-based methods tend to outperform methods that exploit external sources of semantic knowledge. However, due to inherent code limitations, the performance of such methods is still suboptimal. To address these limitations, we propose Normalized Software Distance (nsd), an information-theoretic method that captures semantic relatedness in source code by exploiting the distributional cues of code terms across the system. nsd overcomes data sparsity and lack of context problems often associated with source code, achieving higher levels of resemblance to the human perception of relatedness at the term and the text levels of code.","Semantic relatedness, latent semantics, information retrieval, information theory, clustering","",""
"Journal Article","Dyer R,Nguyen HA,Rajan H,Nguyen TN","Boa: Ultra-Large-Scale Software Repository and Source-Code Mining","ACM Trans. Softw. Eng. Methodol.","2015","25","1","","Association for Computing Machinery","New York, NY, USA","","","2015-12","","1049-331X","https://doi.org/10.1145/2803171;http://dx.doi.org/10.1145/2803171","10.1145/2803171","In today's software-centric world, ultra-large-scale software repositories, such as SourceForge, GitHub, and Google Code, are the new library of Alexandria. They contain an enormous corpus of software and related information. Scientists and engineers alike are interested in analyzing this wealth of information. However, systematic extraction and analysis of relevant data from these repositories for testing hypotheses is hard, and best left for mining software repository (MSR) experts! Specifically, mining source code yields significant insights into software development artifacts and processes. Unfortunately, mining source code at a large scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse grained, or sacrifice studying the history of the code. In this article we address mining source code: (a) at a very large scale; (b) at a fine-grained level of detail; and (c) with full history information. To address these challenges, we present domain-specific language features for source-code mining in our language and infrastructure called Boa. The goal of Boa is to ease testing MSR-related hypotheses. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also show drastic improvements in scalability.","domain-specific language, mining software repositories, scalable, ease of use, lower barrier to entry, Boa","",""
"Conference Paper","Simon,Sheard J","In Their Own Words: Students and Academics Write about Academic Integrity","","2015","","","97–106","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th Koli Calling Conference on Computing Education Research","Koli, Finland","2015","9781450340205","","https://doi.org/10.1145/2828959.2828977;http://dx.doi.org/10.1145/2828959.2828977","10.1145/2828959.2828977","We report on a survey of Australian computing students and academics that was designed to explore their thoughts about academic integrity with regard to the assessments undertaken in computing degrees. A number of questions on the survey permitted free-text responses, and we have conducted a qualitative analysis of those responses to identify concerns that were not covered in the quantitative part of the survey and to uncover new perspectives on issues that were covered in the survey.In response to specific questions, we identified a perception that copying program code without reference can be legitimate; a perception that copying of program code cannot be detected because all correct answers will effectively be the same; and some suggestions, possibly hitherto unreported, as to why students might engage in academic misconduct.In response to a completely open question, we identified four themes, concerning the implementation and the applicability of academic integrity policy and procedure, possible reasons for breaching academic integrity, and justifications for breaching academic integrity.We conclude by discussing what can be done, by universities and by individual academics, to bring the academic discipline of computing closer to consensus with regard to the meaning of academic integrity and its practice in computing assessments.","academic integrity, computing education, non-text-based assessment","","Koli Calling '15"
"Conference Paper","Blair S,Albing C,Grund A,Jocksch A","Accelerating an MPI Lattice Boltzmann Code Using OpenACC","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Second Workshop on Accelerator Programming Using Directives","Austin, Texas","2015","9781450340144","","https://doi.org/10.1145/2832105.2832111;http://dx.doi.org/10.1145/2832105.2832111","10.1145/2832105.2832111","This paper describes our OpenACC porting efforts for a code that we have developed that solves the isothermal incompressible Navier Stokes equation via the lattice Boltzmann method (LBM). Implemented initially as a hybrid MPI/OpenMP parallel program, we ported our code to use OpenACC in order to obtain the benefit of accelerators in a high performance computing (HPC) environment. We describe the elements of parallelism inherent in the LBM algorithm and the way in which that parallelism can be expressed using OpenACC directives. By setting compile-time flags during the build process, the program alternatively can be compiled for, and continue to run without the benefit of, accelerators. Through this porting process we were able to accelerate our code in an incremental fashion without extensive code transformation. We point out some additional efforts that were required to expose C++ class data members to the OpenACC compiler and describe difficulties encountered in this process. We show an instance where similar code segments decorated with the same OpenACC directives can result in different compiler outputs with significantly different performance properties. The result of the code porting process, which occurred primarily during OpenACC EuroHack, a 5-day intensive computing workshop, was a 5.5x speedup over the non-accelerated version of the code.","parallel computing, OpenACC, Lattice Boltzmann methods, GPU, directives, performance analysis, accelerator computing","","WACCPD '15"
"Conference Paper","Janetschek M,Prodan R,Benedict S","A Workflow Runtime Environment for Manycore Parallel Architectures","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Workshop on Workflows in Support of Large-Scale Science","Austin, Texas","2015","9781450339896","","https://doi.org/10.1145/2822332.2822333;http://dx.doi.org/10.1145/2822332.2822333","10.1145/2822332.2822333","We introduce a new Manycore Workflow Runtime Environment (MWRE) to efficiently enact traditional scientific workflows on modern manycore computing architectures. In contrast to existing engines that enact workflows acting as external services, MWRE is compiler-based and translates workflows specified in the XML-based Interoperable Workflow Intermediate Representation (IWIR) into an equivalent C++-based program. This program efficiently enacts the workflow as a stand-alone executable by means of a new callback mechanism that resolves dependencies, transfers data, and handles composite activities. Experimental results on a number of real-world workflows demonstrate that MWRE clearly outperforms existing Java-based workflow engines designed for distributed (Grid/Cloud) computing infrastructures in terms of enactment time, is generally better than an existing script-based engine for manycore architectures (Swift), and sometimes gets even close to an artificial baseline implementation of the workflows in the standard OpenMP language for shared memory systems.","heterogeneous manycore parallel architectures, source-to-source compiler, enactment engine, scientific workflows","","WORKS '15"
"Journal Article","Yeh JF,Chen WY,Su MC","Chinese Spelling Checker Based on an Inverted Index List with a Rescoring Mechanism","ACM Trans. Asian Low-Resour. Lang. Inf. Process.","2015","14","4","","Association for Computing Machinery","New York, NY, USA","","","2015-11","","2375-4699","https://doi.org/10.1145/2826235;http://dx.doi.org/10.1145/2826235","10.1145/2826235","An approach is proposed for Chinese spelling error detection and correction, in which an inverted index list with a rescoring mechanism is used. The inverted index list is a structure for mapping from word to desired sentence, and for representing nodes in lattices constructed through character expansion (according to predefined phonologically and visually similar character sets). Pruning based on a contextual dependency confidence measure was used to markedly reduce the search space and computational complexity. Relevant mapping relations between the original input and desired input were obtained using a scoring mechanism composed of class-based language and maximum entropy correction models containing character, word, and contextual features. The proposed method was evaluated using data sets provided by SigHan 7 bakeoff. The experimental results show that the proposed method achieved acceptable performance in terms of recall rate or precision rate in error sentence detection and error location detection, and it outperformed other approaches in error location detection and correction.","correction model, inverted index list, maximum entropy, language model, Spelling checker, contextual information","",""
"Conference Paper","Narasimhan K,Reichenbach C","Copy and Paste Redeemed","","2015","","","630–640","IEEE Press","Lincoln, Nebraska","Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering","","2015","9781509000241","","https://doi.org/10.1109/ASE.2015.39;http://dx.doi.org/10.1109/ASE.2015.39","10.1109/ASE.2015.39","Modern software development relies on code reuse, which software engineers typically realise through hand-written abstractions, such as functions, methods, or classes. However, such abstractions can be challenging to develop and maintain. One alternative form of re-use is copy-paste-modify, a methodology in which developers explicitly duplicate source code to adapt the duplicate for a new purpose. We observe that copy-paste-modify can be substantially faster to use than manual abstraction, and past research strongly suggests that it is a popular technique among software developers.We therefore propose that software engineers should forego hand-written abstractions in favour of copying and pasting. However, empirical evidence also shows that copy-paste-modify complicates software maintenance and increases the frequency of bugs. To address this concern, we propose a software tool that merges together similar pieces of code and automatically creates suitable abstractions. This allows software developers to get the best of both worlds: custom abstraction together with easy re-use.To demonstrate the feasibility of our approach, we have implemented and evaluated a prototype merging tool for C++ on a number of near-miss clones (clones with some modifications) in popular Open Source packages. We found that maintainers find our algorithmically created abstractions to be largely preferable to existing duplicated code.","","","ASE '15"
"Conference Paper","Ke Y,Stolee KT,Goues CL,Brun Y","Repairing Programs with Semantic Code Search","","2015","","","295–306","IEEE Press","Lincoln, Nebraska","Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering","","2015","9781509000241","","https://doi.org/10.1109/ASE.2015.60;http://dx.doi.org/10.1109/ASE.2015.60","10.1109/ASE.2015.60","Automated program repair can potentially reduce debugging costs and improve software quality but recent studies have drawn attention to shortcomings in the quality of automatically generated repairs. We propose a new kind of repair that uses the large body of existing open-source code to find potential fixes. The key challenges lie in efficiently finding code semantically similar (but not identical) to defective code and then appropriately integrating that code into a buggy program. We present SearchRepair, a repair technique that addresses these challenges by (1) encoding a large database of human-written code fragments as SMT constraints on input-output behavior, (2) localizing a given defect to likely buggy program fragments and deriving the desired input-output behavior for code to replace those fragments, (3) using state-of-the-art constraint solvers to search the database for fragments that satisfy that desired behavior and replacing the likely buggy code with these potential patches, and (4) validating that the patches repair the bug against program test suites. We find that SearchRepair repairs 150 (19%) of 778 benchmark C defects written by novice students, 20 of which are not repaired by GenProg, TrpAutoRepair, and AE. We compare the quality of the patches generated by the four techniques by measuring how many independent, not-used-during-repair tests they pass, and find that SearchRepair-repaired programs pass 97.3% of the tests, on average, whereas GenProg-, TrpAutoRepair-, and AE-repaired programs pass 68.7%, 72.1%, and 64.2% of the tests, respectively. We conclude that SearchRepair produces higher-quality repairs than GenProg, TrpAutoRepair, and AE, and repairs some defects those tools cannot.","","","ASE '15"
"Conference Paper","Wölfl A,Siegmund N,Apel S,Kosch H,Krautlager J,Weber-Urbina G","Generating Qualifiable Avionics Software: An Experience Report","","2015","","","726–736","IEEE Press","Lincoln, Nebraska","Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering","","2015","9781509000241","","https://doi.org/10.1109/ASE.2015.35;http://dx.doi.org/10.1109/ASE.2015.35","10.1109/ASE.2015.35","We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.","","","ASE '15"
"Conference Paper","Kölling M,Brown NC,Altadmri A","Frame-Based Editing: Easing the Transition from Blocks to Text-Based Programming","","2015","","","29–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the Workshop in Primary and Secondary Computing Education","London, United Kingdom","2015","9781450337533","","https://doi.org/10.1145/2818314.2818331;http://dx.doi.org/10.1145/2818314.2818331","10.1145/2818314.2818331","Block-based programming systems, such as Scratch or Alice, are the most popular environments for introducing young children to programming. However, mastery of text-based programming continues to be the educational goal for students who continue to program into their teenage years and beyond. Transitioning across the significant gap between the two editing styles presents a difficult challenge in school-level teaching of programming. We propose a new style of program manipulation to bridge the gap: frame-based editing. Frame-based editing has the resistance to errors and approachability of block-based programming while retaining the flexibility and more conventional programming semantics of text-based programming languages. In this paper, we analyse the issues involved in the transition from blocks to text and argue that they can be overcome by using frame-based editing as an intermediate step. A design and implementation of a frame-based editor is provided.","Frame-based editing, Editing, Novice programming","","WiPSCE '15"
"Conference Paper","Guo PJ","Codeopticon: Real-Time, One-To-Many Human Tutoring for Computer Programming","","2015","","","599–608","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology","Charlotte, NC, USA","2015","9781450337793","","https://doi.org/10.1145/2807442.2807469;http://dx.doi.org/10.1145/2807442.2807469","10.1145/2807442.2807469","One-on-one tutoring from a human expert is an effective way for novices to overcome learning barriers in complex domains such as computer programming. But there are usually far fewer experts than learners. To enable a single expert to help more learners at once, we built Codeopticon, an interface that enables a programming tutor to monitor and chat with dozens of learners in real time. Each learner codes in a workspace that consists of an editor, compiler, and visual debugger. The tutor sees a real-time view of each learner's actions on a dashboard, with each learner's workspace summarized in a tile. At a glance, the tutor can see how learners are editing and debugging their code, and what errors they are encountering. The dashboard automatically reshuffles tiles so that the most active learners are always in the tutor's main field of view. When the tutor sees that a particular learner needs help, they can open an embedded chat window to start a one-on-one conversation. A user study showed that 8 first-time Codeopticon users successfully tutored anonymous learners from 54 countries in a naturalistic online setting. On average, in a 30-minute session, each tutor monitored 226 learners, started 12 conversations, exchanged 47 chats, and helped 2.4 learners.","computer programming, remote tutoring, learning at scale","","UIST '15"
"Conference Paper","Uddin MS,Roy CK,Schneider KA","Towards Convenient Management of Software Clone Codes in Practice: An Integrated Approach","","2015","","","211–220","IBM Corp.","USA","Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering","Markham, Canada","2015","","","","","Software code cloning is inevitable during software development and unmanaged cloning practice can create substantial problems for software maintenance and evolution. Current research in the area software clones includes, but is not limited to: finding ways to manage clones; gaining more control over clone generation; and, studying clone evolution and its effects on the evolution of software. In this study, we investigate tools and techniques for detecting, managing, and understanding the evolution of clones, as well as design a convenient tool to make those techniques available to a developer's software development environment. Towards the goal of promoting the practical use of code clone research and to provide better support for managing clones in software systems, we first developed SimEclipse: a clone-aware software development platform, and then, using the tool, we performed a study to investigate the usefulness of using a number clone based technologies in an integrated platform rather than using those discretely. Finally, a small scale user study is performed to evaluate SimEclipse's effectiveness, usability and information management with respect to some pre-defined clone management activities. We believe that both researchers and developers would enjoy and utilize the benefits of using SimEclipse for different aspects of code clone research as well as for managing cloned code in software systems.","integrated clone management, software clone, IDE plugin","","CASCON '15"
"Conference Paper","Maki S,Kpodjedo S,El Boussaidi G","Context Extraction in Recommendation Systems in Software Engineering: A Preliminary Survey","","2015","","","151–160","IBM Corp.","USA","Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering","Markham, Canada","2015","","","","","Recommendation System in Software Engineering (RSSE) represents a new promising research area, whose goal is to help software developers in their tasks by providing them with context-dependent insights extracted from their current project or taken from best practices. A key challenge here is to retrieve the context from the programming task in order to provide useful recommendations. In this paper, we conduct a survey of RSSEs with a particular focus on different approaches used to extract the context. We propose a feature model to represent some important characteristics of such extraction and identify some open issues.","context extraction, recommendation systems, software engineering","","CASCON '15"
"Conference Paper","Mondal M,Roy CK,Schneider KA","An Empirical Study on Change Recommendation","","2015","","","141–150","IBM Corp.","USA","Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering","Markham, Canada","2015","","","","","Recommending changes to programmers by exploiting their repetition tendencies during system evolution has been investigated by a number of studies. In our research we perform a change type (additions, deletions, and modifications) based analysis of the efficiency of change recommendation. We also investigate the programmer sensitivity of the repeated changes (i.e., the extent the same changes are repeated by the same programmers) of different change types. The existing studies did not perform such investigations. However, these investigations can be important for efficient ranking (i.e., prioritizing) and filtering of recommendations. According to our investigation on thousands of commits of five diverse subject systems we observe that modifications have a very low tendency (around 1.3%) of being repeated. We should primarily focus on recommending additions, and deletions. More importantly, overall 71% of the repeated changes are programmer sensitive. We believe that a change recommendation system that prioritizes recommendations considering programmer sensitivity can help programmers reuse previous changes in a time-efficient manner.","","","CASCON '15"
"Conference Paper","Rahman MM,Roy CK","Recommending Relevant Sections from a Webpage about Programming Errors and Exceptions","","2015","","","181–190","IBM Corp.","USA","Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering","Markham, Canada","2015","","","","","Programming errors or exceptions are inherent in software development and maintenance, and given today's Internet era, software developers often look at web for finding working solutions. They make use of a search engine for retrieving relevant pages, and then look for the appropriate solutions by manually going through the pages one by one. However, both the manual checking of a page's content against a given exception (and its context) and then working an appropriate solution out are non-trivial tasks. They are even more complex and time-consuming with the bulk of irrelevant (i.e., off-topic) and noisy (e.g., advertisements) content in the web page. In this paper, we propose an IDE-based and context-aware page content recommendation technique that locates and recommends relevant sections from a given web page by exploiting the technical details, in particular, the context of an encountered exception in the IDE. An evaluation with 250 web pages related to 80 programming exceptions, comparison with the only available closely related technique, and a case study involving comparison with VSM and LSA techniques show that the proposed technique is highly promising in terms of precision, recall and F1-measure.","content relevance, content recommendation, traceability","","CASCON '15"
"Conference Paper","Reguly IZ,Keita AK,Giles MB","Benchmarking the IBM Power8 Processor","","2015","","","61–69","IBM Corp.","USA","Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering","Markham, Canada","2015","","","","","This paper discusses the performance of IBM's Power8 CPUs, on a number of skeleton, financial and CFD benchmarks and applications. Implicitly, the performance of the software toolchain is also tested - the bare-bones Little-Endian Ubuntu, the XL compilers and OpenMP runtimes. First, we aim to establish some roofline numbers on bandwidth and compute throughput, then move on to benchmark explicit and implicit one-/three-factor Black-Scholes computations, and CFD applications based on the OP2 and OPS frameworks, such as Airfoil, CloverLeaf, CloverLeaf 3D. These applications all exhibit different characteristics in terms of computations, communications, memory access patterns, etc. Both absolute and relative performance metrics are computed and compared to NVIDIA GPUs and Intel Xeon CPUs.","POWER8, benchmarking","","CASCON '15"
"Conference Paper","Walls RJ,Kilmer ED,Lageman N,McDaniel PD","Measuring the Impact and Perception of Acceptable Advertisements","","2015","","","107–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 Internet Measurement Conference","Tokyo, Japan","2015","9781450338486","","https://doi.org/10.1145/2815675.2815703;http://dx.doi.org/10.1145/2815675.2815703","10.1145/2815675.2815703","In 2011, Adblock Plus---the most widely-used ad blocking software---began to permit some advertisements as part of their Acceptable Ads program. Under this program, some ad networks and content providers pay to have their advertisements shown to users. Such practices have been controversial among both users and publishers. In a step towards informing the discussion about these practices, we present the first comprehensive study of the Acceptable Ads program. Specifically, we characterize which advertisements are allowed and how the whitelisting has changed since its introduction in 2011. We show that the list of filters used to whitelist acceptable advertisements has been updated on average every 1.5 days and grew from 9 filters in 2011 to over 5,900 in the Spring of 2015. More broadly, the current whitelist triggers filters on 59% of the top 5,000 websites. Our measurements also show that the program allows advertisements on 2.6 million parked domains. Lastly, we take the lessons learned from our analysis and suggest ways to improve the transparency of the whitelisting process.","ad avoidance, adblock plus, acceptable ads","","IMC '15"
"Conference Paper","Alshara Z,Seriai AD,Tibermacine C,Bouziane HL,Dony C,Shatnawi A","Migrating Large Object-Oriented Applications into Component-Based Ones: Instantiation and Inheritance Transformation","","2015","","","55–64","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","Pittsburgh, PA, USA","2015","9781450336871","","https://doi.org/10.1145/2814204.2814223;http://dx.doi.org/10.1145/2814204.2814223","10.1145/2814204.2814223","Large object-oriented applications have complex and numerous dependencies, and usually do not have explicit software architectures. Therefore they are hard to maintain, and parts of them are difficult to reuse. Component-based development paradigm emerged for improving these aspects and for supporting effective maintainability and reuse. It provides better understandability through a high-level architecture view of the application. Thereby migrating object-oriented applications to component-based ones will contribute to improve these characteristics (maintainability and reuse). In this paper, we propose an approach to automatically transform object-oriented applications to component-based ones. More particularly, the input of the approach is the result provided by software architecture recovery: a component-based architecture description. Then, our approach transforms the object-oriented source code in order to produce deployable components. We focus in this paper on the transformation of source code related to instantiation and inheritance dependencies between classes that are in different components. We experimented the proposed solution in the transformation of a collection of Java applications into the OSGi framework. The experimental results are discussed in this paper.","Object, Component, Class Instantiation, Inheritance, OSGi, Encapsulation, Code Transformation, Java, Refactoring","","GPCE 2015"
"Journal Article","Alshara Z,Seriai AD,Tibermacine C,Bouziane HL,Dony C,Shatnawi A","Migrating Large Object-Oriented Applications into Component-Based Ones: Instantiation and Inheritance Transformation","SIGPLAN Not.","2015","51","3","55–64","Association for Computing Machinery","New York, NY, USA","","","2015-10","","0362-1340","https://doi.org/10.1145/2936314.2814223;http://dx.doi.org/10.1145/2936314.2814223","10.1145/2936314.2814223","Large object-oriented applications have complex and numerous dependencies, and usually do not have explicit software architectures. Therefore they are hard to maintain, and parts of them are difficult to reuse. Component-based development paradigm emerged for improving these aspects and for supporting effective maintainability and reuse. It provides better understandability through a high-level architecture view of the application. Thereby migrating object-oriented applications to component-based ones will contribute to improve these characteristics (maintainability and reuse). In this paper, we propose an approach to automatically transform object-oriented applications to component-based ones. More particularly, the input of the approach is the result provided by software architecture recovery: a component-based architecture description. Then, our approach transforms the object-oriented source code in order to produce deployable components. We focus in this paper on the transformation of source code related to instantiation and inheritance dependencies between classes that are in different components. We experimented the proposed solution in the transformation of a collection of Java applications into the OSGi framework. The experimental results are discussed in this paper.","Component, Refactoring, Object, Code Transformation, Java, OSGi, Inheritance, Class Instantiation, Encapsulation","",""
"Conference Paper","Felgentreff T,Lincke J,Hirschfeld R,Thamsen L","Lively Groups: Shared Behavior in a World of Objects without Classes or Prototypes","","2015","","","15–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the Workshop on Future Programming","Pittsburgh, PA, USA","2015","9781450339056","","https://doi.org/10.1145/2846656.2846659;http://dx.doi.org/10.1145/2846656.2846659","10.1145/2846656.2846659","Development environments which aim to provide short feedback loops to developers must strike a balance between immediacy and the ability to abstract and reuse behavioral modules. The Lively Kernel, a self-supporting, browser-based environment for explorative development supports standard object-oriented programming with classes or prototypes, but also a more immediate, object-centric approach for modifying and programming visible objects directly. This allows users to quickly create graphical prototypes with concrete objects. However, when developing with the object-centric approach, sharing behavior between similar objects becomes cumbersome. Developers must choose to either abstract behavior into classes, scatter code across collaborating objects, or to manually copy code between multiple objects. That is, they must choose between less concrete development, reduced maintainability, or code duplication. In this paper, we propose Lively Groups, an extension to the object-centric development tools of Lively to work on multiple concrete objects. In our approach, developers may dynamically group live objects that share behavior using tags. They can then modify and program such groups as if they were single objects. Our approach scales the Lively Kernel’s explorative development approach from one to many objects, while preserving the maintainability of abstractions and the immediacy of concrete objects.","Web Applications, Interactive Systems, Explorative Development, Lively Kernel","","FPW 2015"
"Conference Paper","Della Toffola L,Pradel M,Gross TR","Performance Problems You Can Fix: A Dynamic Analysis of Memoization Opportunities","","2015","","","607–622","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications","Pittsburgh, PA, USA","2015","9781450336895","","https://doi.org/10.1145/2814270.2814290;http://dx.doi.org/10.1145/2814270.2814290","10.1145/2814270.2814290","Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.","profiling, performance bugs, caching, Memoization","","OOPSLA 2015"
"Journal Article","Della Toffola L,Pradel M,Gross TR","Performance Problems You Can Fix: A Dynamic Analysis of Memoization Opportunities","SIGPLAN Not.","2015","50","10","607–622","Association for Computing Machinery","New York, NY, USA","","","2015-10","","0362-1340","https://doi.org/10.1145/2858965.2814290;http://dx.doi.org/10.1145/2858965.2814290","10.1145/2858965.2814290","Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.","Memoization, profiling, caching, performance bugs","",""
"Conference Paper","Alves P,Gruber F,Doerfert J,Lamprineas A,Grosser T,Rastello F,Pereira FM","Runtime Pointer Disambiguation","","2015","","","589–606","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications","Pittsburgh, PA, USA","2015","9781450336895","","https://doi.org/10.1145/2814270.2814285;http://dx.doi.org/10.1145/2814270.2814285","10.1145/2814270.2814285","To optimize code effectively, compilers must deal with memory dependencies. However, the state-of-the-art heuristics available in the literature to track memory dependencies are inherently imprecise and computationally expensive. Consequently, the most advanced code transformations that compilers have today are ineffective when applied on real-world programs. The goal of this paper is to solve this conundrum through dynamic disambiguation of pointers. We provide different ways to determine at runtime when two memory locations can overlap. We then produce two versions of a code region: one that is aliasing-free - hence, easy to optimize - and another that is not. Our checks let us safely branch to the optimizable region. We have applied these ideas on Polly-LLVM, a loop optimizer built on top of the LLVM compilation infrastructure. Our experiments indicate that our method is precise, effective and useful: we can disambiguate every pair of pointer in the loop intensive Polybench benchmark suite. The result of this precision is code quality: the binaries we generate are 10% faster than those that Polly-LLVM produces without our optimization, at the -O3 optimization level of LLVM.","dynamic guards, optimization, Alias analysis","","OOPSLA 2015"
"Journal Article","Alves P,Gruber F,Doerfert J,Lamprineas A,Grosser T,Rastello F,Pereira FM","Runtime Pointer Disambiguation","SIGPLAN Not.","2015","50","10","589–606","Association for Computing Machinery","New York, NY, USA","","","2015-10","","0362-1340","https://doi.org/10.1145/2858965.2814285;http://dx.doi.org/10.1145/2858965.2814285","10.1145/2858965.2814285","To optimize code effectively, compilers must deal with memory dependencies. However, the state-of-the-art heuristics available in the literature to track memory dependencies are inherently imprecise and computationally expensive. Consequently, the most advanced code transformations that compilers have today are ineffective when applied on real-world programs. The goal of this paper is to solve this conundrum through dynamic disambiguation of pointers. We provide different ways to determine at runtime when two memory locations can overlap. We then produce two versions of a code region: one that is aliasing-free - hence, easy to optimize - and another that is not. Our checks let us safely branch to the optimizable region. We have applied these ideas on Polly-LLVM, a loop optimizer built on top of the LLVM compilation infrastructure. Our experiments indicate that our method is precise, effective and useful: we can disambiguate every pair of pointer in the loop intensive Polybench benchmark suite. The result of this precision is code quality: the binaries we generate are 10% faster than those that Polly-LLVM produces without our optimization, at the -O3 optimization level of LLVM.","optimization, Alias analysis, dynamic guards","",""
"Conference Paper","Akhin M,Suhinin A","Discovering Clones in Software: From Complex Algorithms to Everyday Desktop Tool","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Central & Eastern European Software Engineering Conference in Russia","Moscow, Russia","2015","9781450341301","","https://doi.org/10.1145/2855667.2855676;http://dx.doi.org/10.1145/2855667.2855676","10.1145/2855667.2855676","Software clone detection is an actively researched area which has spawned numerous approaches to the problem of duplicated code. These approaches, however, are of little practical use as most of them are ill-suited for incremental analysis required by the developers in their day-to-day activities. This paper tackles the problem of clone detection from the practical point of view, in the context of IDE integration, and presents an online clone detection algorithm based on extended suffix trees. The approach has been tested in a prototype plugin for IntelliJ IDEA and proved its applicability in the industrial setting.","suffix trie, IDE, clone detection","","CEE-SECR '15"
"Conference Paper","Charalampidou S,Ampatzoglou A,Avgeriou P","Size and Cohesion Metrics as Indicators of the Long Method Bad Smell: An Empirical Study","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering","Beijing, China","2015","9781450337151","","https://doi.org/10.1145/2810146.2810155;http://dx.doi.org/10.1145/2810146.2810155","10.1145/2810146.2810155","Source code bad smells are usually resolved through the application of well-defined solutions, i.e., refactorings. In the literature, software metrics are used as indicators of the existence and prioritization of resolving bad smells. In this paper, we focus on the long method smell (i.e. one of the most frequent and persistent bad smells) that can be resolved by the extract method refactoring. Until now, the identification of long methods or extract method opportunities has been performed based on cohesion, size or complexity metrics. However, the empirical validation of these metrics has exhibited relatively low accuracy with regard to their capacity to indicate the existence of long methods or extract method opportunities. Thus, we empirically explore the ability of size and cohesion metrics to predict the existence and the refactoring urgency of long method occurrences, through a case study on java open-source methods. The results of the study suggest that one size and four cohesion metrics are capable of characterizing the need and urgency for resolving the long method bad smell, with a higher accuracy compared to the previous studies. The obtained results are discussed by providing possible interpretations and implications to practitioners and researchers.","size, case study, metrics, Long method, cohesion","","PROMISE '15"
"Conference Paper","Polito G,Ducasse S,Bouraqadi N,Fabresse L","A Bootstrapping Infrastructure to Build and Extend Pharo-like Languages","","2015","","","183–196","Association for Computing Machinery","New York, NY, USA","2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)","Pittsburgh, PA, USA","2015","9781450336888","","https://doi.org/10.1145/2814228.2814236;http://dx.doi.org/10.1145/2814228.2814236","10.1145/2814228.2814236","Bootstrapping is well known in the context of compilers, where a bootstrapped compiler can compile its own source code. Bootstrapping is a beneficial engineering practice because it raises the level of abstraction of a program making it easier to understand, optimize, evolve, etc. Bootstrapping a reflective object-oriented language is however more challenging, as we need also to initialize the runtime of the language with its initial objects and classes besides writing its compiler. In this paper, we present a novel bootstrapping infrastructure for Pharo-like languages that allows us to easily extend and modify such languages. Our bootstrapping process relies on a first-class runtime. A first-class runtime is a meta-object that represents a program’s runtime and provides a MOP to easily load code into it and manipulate its objects. It decouples the virtual machine (VM) and language concerns by introducing a clear VM-language interface. Using this process, we show how we succeeded to bootstrap a Smalltalk-based language named Candle and then extend it with traits in less than 250 lines of high-level Smalltalk code. We also show how we can bootstrap with minimal effort two other languages (Pharo and MetaTalk) with similar execution semantics but different object models.","Bootstrapping, Metaprogramming, Pharo, Traits, OOP","","Onward! 2015"
"Conference Paper","Feeley M","Compiling for Multi-Language Task Migration","","2015","","","63–77","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Symposium on Dynamic Languages","Pittsburgh, PA, USA","2015","9781450336901","","https://doi.org/10.1145/2816707.2816713;http://dx.doi.org/10.1145/2816707.2816713","10.1145/2816707.2816713","Task migration allows a running program to continue its execution in a different destination environment. Increasingly, execution environments are defined by combinations of cultural and technological constraints, affecting the choice of host language, libraries and tools. A compiler supporting multiple target environments and task migration must be able to marshal continuations and then unmarshal and continue their execution, ideally, even if the language of the destination environment is different. In this paper, we propose a compilation approach based on a virtual machine that strikes a balance between implementation portability and efficiency. We explain its implementation within a Scheme compiler targeting JavaScript, PHP, Python, Ruby and Java -- some of the most popular host languages for web applications. As our experiments show, this approach compares well with other Scheme compilers targeting high-level languages in terms of execution speed, being sometimes up to 3 orders of magnitude faster.","JavaScript, virtual machine, Task migration, trampoline, Scheme, marshalling, tail call, continuation","","DLS 2015"
"Journal Article","Feeley M","Compiling for Multi-Language Task Migration","SIGPLAN Not.","2015","51","2","63–77","Association for Computing Machinery","New York, NY, USA","","","2015-10","","0362-1340","https://doi.org/10.1145/2936313.2816713;http://dx.doi.org/10.1145/2936313.2816713","10.1145/2936313.2816713","Task migration allows a running program to continue its execution in a different destination environment. Increasingly, execution environments are defined by combinations of cultural and technological constraints, affecting the choice of host language, libraries and tools. A compiler supporting multiple target environments and task migration must be able to marshal continuations and then unmarshal and continue their execution, ideally, even if the language of the destination environment is different. In this paper, we propose a compilation approach based on a virtual machine that strikes a balance between implementation portability and efficiency. We explain its implementation within a Scheme compiler targeting JavaScript, PHP, Python, Ruby and Java -- some of the most popular host languages for web applications. As our experiments show, this approach compares well with other Scheme compilers targeting high-level languages in terms of execution speed, being sometimes up to 3 orders of magnitude faster.","trampoline, Task migration, JavaScript, continuation, marshalling, virtual machine, tail call, Scheme","",""
"Conference Paper","Teruel C,Ducasse S,Cassou D,Denker M","Access Control to Reflection with Object Ownership","","2015","","","168–176","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Symposium on Dynamic Languages","Pittsburgh, PA, USA","2015","9781450336901","","https://doi.org/10.1145/2816707.2816721;http://dx.doi.org/10.1145/2816707.2816721","10.1145/2816707.2816721","Reflection is a powerful programming language feature that enables language extensions, generic code, dynamic analyses, development tools, etc. However, uncontrolled reflection breaks object encapsulation and considerably increases the attack surface of programs e.g., malicious libraries can use reflection to attack their client applications. To bring reflection and object encapsulation back together, we use dynamic object ownership to design an access control policy to reflective operations. This policy grants objects full reflective power over the objects they own but limited reflective power over other objects. Code is still able to use advanced reflective operations but reflection cannot be used as an attack vector anymore.","encapsulation, reflection, object ownership","","DLS 2015"
"Journal Article","Teruel C,Ducasse S,Cassou D,Denker M","Access Control to Reflection with Object Ownership","SIGPLAN Not.","2015","51","2","168–176","Association for Computing Machinery","New York, NY, USA","","","2015-10","","0362-1340","https://doi.org/10.1145/2936313.2816721;http://dx.doi.org/10.1145/2936313.2816721","10.1145/2936313.2816721","Reflection is a powerful programming language feature that enables language extensions, generic code, dynamic analyses, development tools, etc. However, uncontrolled reflection breaks object encapsulation and considerably increases the attack surface of programs e.g., malicious libraries can use reflection to attack their client applications. To bring reflection and object encapsulation back together, we use dynamic object ownership to design an access control policy to reflective operations. This policy grants objects full reflective power over the objects they own but limited reflective power over other objects. Code is still able to use advanced reflective operations but reflection cannot be used as an attack vector anymore.","object ownership, reflection, encapsulation","",""
"Conference Paper","Ming J,Xu D,Wang L,Wu D","LOOP: Logic-Oriented Opaque Predicate Detection in Obfuscated Binary Code","","2015","","","757–768","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security","Denver, Colorado, USA","2015","9781450338325","","https://doi.org/10.1145/2810103.2813617;http://dx.doi.org/10.1145/2810103.2813617","10.1145/2810103.2813617","Opaque predicates have been widely used to insert superfluous branches for control flow obfuscation. Opaque predicates can be seamlessly applied together with other obfuscation methods such as junk code to turn reverse engineering attempts into arduous work. Previous efforts in detecting opaque predicates are far from mature. They are either ad hoc, designed for a specific problem, or have a considerably high error rate. This paper introduces LOOP, a Logic Oriented Opaque Predicate detection tool for obfuscated binary code. Being different from previous work, we do not rely on any heuristics; instead we construct general logical formulas, which represent the intrinsic characteristics of opaque predicates, by symbolic execution along a trace. We then solve these formulas with a constraint solver. The result accurately answers whether the predicate under examination is opaque or not. In addition, LOOP is obfuscation resilient and able to detect previously unknown opaque predicates. We have developed a prototype of LOOP and evaluated it with a range of common utilities and obfuscated malicious programs. Our experimental results demonstrate the efficacy and generality of LOOP. By integrating LOOP with code normalization for matching metamorphic malware variants, we show that LOOP is an appealing complement to existing malware defenses.","logical formulas, control flow obfuscation, opaque predicate detection, dynamic symbolic execution, obfuscated binary code, deobfuscation","","CCS '15"
"Conference Paper","Kirat D,Vigna G","MalGene: Automatic Extraction of Malware Analysis Evasion Signature","","2015","","","769–780","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security","Denver, Colorado, USA","2015","9781450338325","","https://doi.org/10.1145/2810103.2813642;http://dx.doi.org/10.1145/2810103.2813642","10.1145/2810103.2813642","Automated dynamic malware analysis is a common approach for detecting malicious software. However, many malware samples identify the presence of the analysis environment and evade detection by not performing any malicious activity. Recently, an approach to the automated detection of such evasive malware was proposed. In this approach, a malware sample is analyzed in multiple analysis environments, including a bare-metal environment, and its various behaviors are compared. Malware whose behavior deviates substantially is identified as evasive malware. However, a malware analyst still needs to re-analyze the identified evasive sample to understand the technique used for evasion. Different tools are available to help malware analysts in this process. However, these tools in practice require considerable manual input along with auxiliary information. This manual process is resource-intensive and not scalable. In this paper, we present MalGene, an automated technique for extracting analysis evasion signatures. MalGene leverages algorithms borrowed from bioinformatics to automatically locate evasive behavior in system call sequences. Data flow analysis and data mining techniques are used to identify call events and data comparison events used to perform the evasion. These events are used to construct a succinct evasion signature, which can be used by an analyst to quickly understand evasions. Finally, evasive malware samples are clustered based on their underlying evasive techniques. We evaluated our techniques on 2810 evasive samples. We were able to automatically extract their analysis evasion signatures and group them into 78 similar evasion techniques.","bioinformatics, malware analysis, sequence alignment, evasive malware, computer security","","CCS '15"
"Conference Paper","Soni P,Budianto E,Saxena P","The SICILIAN Defense: Signature-Based Whitelisting of Web JavaScript","","2015","","","1542–1557","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security","Denver, Colorado, USA","2015","9781450338325","","https://doi.org/10.1145/2810103.2813710;http://dx.doi.org/10.1145/2810103.2813710","10.1145/2810103.2813710","Whitelisting has become a common practice to ensure the execution of trusted applications. However, its effectiveness in protecting client-side web application code has not yet been established. In this paper, we seek to study the efficacy of signature-based whitelisting approach in preventing script injection attacks. This includes a recently-proposed W3C recommendation called Subresource Integrity (SRI), which is based on raw text signatures of scripts. Our 3-month long measurement study shows that applying such raw signatures require signature updates at an impractical rate. We then present SICILIAN, a novel multi-layered approach for whitelisting scripts that can tolerate changes in them without sacrificing the security. Our solution comes with a deployment model called progressive lockdown, which lets browsers assist the server in composing the whitelist. Such assistance from the browser minimizes the burden of building the signature-based whitelist. Our evaluation on Alexa's top 500 sites and 15 popular PHP applications shows that SICILIAN can be fully applied to 84.7% of the sites and all the PHP applications with updates to the whitelist required roughly once in a month.SICILIAN incurs an average performance overhead of 7.02%.","web security, whitelisting, script injection attacks","","CCS '15"
"Conference Paper","Park J,Son D,Kang D,Choi J,Jeon G","Software Similarity Analysis Based on Dynamic Stack Usage Patterns","","2015","","","285–290","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems","Prague, Czech Republic","2015","9781450337380","","https://doi.org/10.1145/2811411.2811508;http://dx.doi.org/10.1145/2811411.2811508","10.1145/2811411.2811508","Analysing software similarity is actively utilized for various purposes such as plagiarism detection, malware classification and software refactoring. In this study, we observe that the runtime behavior of stack, which manipulates arguments and local variables during function calls, is one of the viable indicators to identify similar software and to detect plagiarized programs. This observation drives us to design a novel software similarity analysis scheme that has the following two strong points. First, it examines the stack usage patterns collected during the execution of binary codes, without requiring source codes. Therefore, it can be used for the software theft trial case where involved companies or individuals are not willing to uncover their source codes. Second, the stack usage patterns are invulnerable to syntactic alterations such as renaming, statements reordering and control flow restructuring. The proposed scheme provides four functionalities; trace collection, dynamic call sequence graph generation, stack usage pattern refinement, and similarity comparison. Real implementation based experimental results show that our proposal identifies similar software appropriately, reporting similarity scores comparable to MOSS (Measure Of Software Similarity). In addition, it has a capability to discover whether binaries use the same core logics.","software plagiarized, stack usage pattern, dynamic analysis, similarity comparison","","RACS"
"Conference Paper","Kim J,Kim TG,Im EG","Structural Information Based Malicious App Similarity Calculation and Clustering","","2015","","","314–318","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems","Prague, Czech Republic","2015","9781450337380","","https://doi.org/10.1145/2811411.2811545;http://dx.doi.org/10.1145/2811411.2811545","10.1145/2811411.2811545","Depending on expansion of supply of smartphone, development of mobile application is more active using various mobile platform. As a result of malicious applications, but also targeting the mobile it is rapidly increasing. In this paper, method of Android malware similarity and clustering. First, there is a need for a process for extracting the control flow graph in an Android application. By extract the control flow graph, we form structural information of methods in Android application called '4-tuple'. After we create the structural information extracted from the control flow graph it is necessary to compare the matching process. Matching process we propose has two steps, 'initial matching' and 'second matching'. Initial matching step is the process of matching the '4-tuple' information but not exactly same with each other only a single in Android application. Second matching step is process of matching in the same way as the initial matching target method that calling its method and method that is invoked. Finally, it measure the ratio of the total number of method in Android application and matched method after initial matching and second matching. Finally, it measure the ratio of the total number of method in Android application and matched method after initial matching and second matching. We proceeds clustering using the above process. Based on previous studies, we used the DBSCAN algorithm for clustering. It was 65.8% average using the structural information of the result of the clustering.","clustering, function matching, malware analysis, android","","RACS"
"Conference Paper","Kang B,Yerima S,McLaughlin K,Sezer S","PageRank in Malware Categorization","","2015","","","291–295","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems","Prague, Czech Republic","2015","9781450337380","","https://doi.org/10.1145/2811411.2811514;http://dx.doi.org/10.1145/2811411.2811514","10.1145/2811411.2811514","In this paper, we propose a malware categorization method that models malware behavior in terms of instructions using PageRank. PageRank computes ranks of web pages based on structural information and can also compute ranks of instructions that represent the structural information of the instructions in malware analysis methods. Our malware categorization method uses the computed ranks as features in machine learning algorithms. In the evaluation, we compare the effectiveness of different PageRank algorithms and also investigate bagging and boosting algorithms to improve the categorization accuracy.","dynamic analysis, malware categorization, pagerank, malware classification","","RACS"
"Conference Paper","Miné A,Delmas D","Towards an Industrial Use of Sound Static Analysis for the Verification of Concurrent Embedded Avionics Software","","2015","","","65–74","IEEE Press","Amsterdam, The Netherlands","Proceedings of the 12th International Conference on Embedded Software","","2015","9781467380799","","","","Formal methods, and in particular sound static analyses, have been recognized by Certification Authorities as reliable methods to certify embedded avionics software. For sequential C software, industrial static analyzers, such as Astrée, already exist and are deployed. This is not the case for concurrent C software. This article discusses the requirements for sound static analysis of concurrent embedded software at Airbus and presents AstréeA, an extension of Astrée with the potential to address these requirements: it is scalable and reports soundly all run-time errors with few false positives. We illustrate this potential on a variety of case studies targeting different avionics software components, including large ARINC 653 and POSIX threads applications, and a small part of an operating system. While the experiments on some case studies were conducted in an academic setting, others were conducted in an industrial setting by engineers, hinting at the maturity of our approach.","static analysis, embedded software, abstract interpretation, concurrent software","","EMSOFT '15"
"Conference Paper","Stephan M,Cordy JR","Identification of Simulink Model Antipattern Instances Using Model Clone Detection","","2015","","","276–285","IEEE Press","Ottawa, Ontario, Canada","Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems","","2015","9781467369084","","","","One challenge facing the Model-Driven Engineering community is the need for model quality assurance. Specifically, there should be better facilities for analyzing models automatically. One measure of quality is the presence or absence of good and bad properties, such as patterns and antipatterns, respectively. We elaborate on and validate our earlier idea of detecting patterns in model-based systems using model clone detection by devising a Simulink antipattern instance detector. We chose Simulink because it is prevalent in industry, has mature model clone detection techniques, and interests our industrial partners. We demonstrate our technique using near-miss cross-clone detection to find instances of Simulink antipatterns derived from the literature in four sets of public Simulink projects. We present our detection results, highlight interesting examples, and discuss potential improvements to our approach. We hope this work provides a first step in helping practitioners improve Simulink model quality and further research in the area.","","","MODELS '15"
"Conference Paper","Hesse R,Jerger NE","Improving DVFS in NoCs with Coherence Prediction","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Symposium on Networks-on-Chip","Vancouver, BC, Canada","2015","9781450333962","","https://doi.org/10.1145/2786572.2786595;http://dx.doi.org/10.1145/2786572.2786595","10.1145/2786572.2786595","As Networks-on-Chip (NoCs) continue to consume a large fraction of the total chip power budget, dynamic voltage and frequency scaling (DVFS) has evolved into an integral part of NoC designs. Efficient DVFS relies on accurate predictions of future network state. Most previous approaches are reactive and based on network-centric metrics, such as buffer occupation and channel utilization. However, we find that there is little correlation between those metrics and subsequent NoC traffic, which leads to suboptimal DVFS decisions. In this work, we propose to utilize highly predictable properties of cache-coherence communication to derive more specific and reliable NoC traffic predictions. A DVFS mechanism based on our traffic predictions, reduces power by 41% compared to a baseline without DVFS and by 21% on average when compared to a state-of-the-art DVFS implementation, while only degrading performance by 3%.","chip multiprocessor, cache coherence, dynamic voltage/frequency scaling, Networks-on-chip","","NOCS '15"
"Conference Paper","Ekblad A","Foreign Exchange at Low, Low Rates a Lightweight FFI for Web-Targeting Haskell Dialects","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th Symposium on the Implementation and Application of Functional Programming Languages","Koblenz, Germany","2015","9781450342735","","https://doi.org/10.1145/2897336.2897338;http://dx.doi.org/10.1145/2897336.2897338","10.1145/2897336.2897338","We present a novel yet simple foreign function interface, designed for web-targeting Haskell dialects but also applicable to a wider range of high-level target languages. The interface automates marshalling, eliminates boilerplate code, allows increased sanity checking of external data, allows the import of functions as well as arbitrary expressions of JavaScript code, and is implementable as a plain Haskell '98 library without any modification to the Haskell compiler or environment.We give an implementation of this interface for the JavaScript-targeting Haste compiler, and show how the basic implementation can be further optimized with minimal effort to perform on par with Haskell's vanilla foreign function interface, as well as extended to support automatic marshalling of higher-order functions and automatic marshalling of host language exceptions. We also discuss how the interface may be extended beyond the web domain and implemented across a larger range of host environments and target languages.","web, compilers, interoperability","","IFL '15"
"Conference Paper","Fang ZF,Lam P","Identifying Test Refactoring Candidates with Assertion Fingerprints","","2015","","","125–137","Association for Computing Machinery","New York, NY, USA","Proceedings of the Principles and Practices of Programming on The Java Platform","Melbourne, FL, USA","2015","9781450337120","","https://doi.org/10.1145/2807426.2807437;http://dx.doi.org/10.1145/2807426.2807437","10.1145/2807426.2807437","Test cases constitute around 30% of the codebase of a number of large software systems. Poor design of test suites hinders test comprehension and maintenance. Developers often copy-paste existing tests and reproduce both boilerplate and essential environment setup code as well as assertions. Test case refactoring would be valuable for developers aiming to control technical debt arising due to copy-pasted test cases.In the context of test code, identifying candidates for refactoring requires tedious manual effort. In this work, we specifically tailor static analysis techniques for test analysis. We present a novel technique, assertion fingerprints, for finding similar test cases based on the set of assertion calls in test methods. Assertion fingerprints encode the control flow around the ordered set of assertions in methods.We have implemented similar test case detection using assertion fingerprints and applied it to 10 test suites for open-source Java programs. We provide an empirical study and a qualitative analysis of our results. Assertion fingerprints enable the discovery of tests that exhibit strong structural similarities and are amenable to refactoring. Our technique delivers an overall 75% true positive rate on our benchmarks and reports that 40% of the benchmark test methods are potentially refactorable.","","","PPPJ '15"
"Conference Paper","Raphel J,Vinod P","Information Theoretic Method for Classification of Packed and Encoded Files","","2015","","","296–303","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th International Conference on Security of Information and Networks","Sochi, Russia","2015","9781450334532","","https://doi.org/10.1145/2799979.2800015;http://dx.doi.org/10.1145/2799979.2800015","10.1145/2799979.2800015","Malware authors make use of some anti-reverse engineering and obfuscation techniques like packing and encoding in-order to conceal their malicious payload. These techniques succeeded in evading the traditional signature based AV scanners. Packed or encoded malware samples are difficult to be analysed directly by the AV scanners. So, such samples must be initially unpacked or decoded for efficient analysis of the malicious code. This paper illustrates a static information theoretic method for the classification of packed and encoded files. The proposed method extracts fragments of fixed size from the files and calculates the entropy scores of the fragments. These entropy scores are then used for computing the Similarity Distance Matrix for fragments in a file-pair. The proposed system classifies all the encoded and packed samples properly, thereby obtaining improved detection. The proposed system is also capable of differentiating the type of packers used for the packing or encoding process.","malware, encoders, obfuscation, entropy, classification, packers","","SIN '15"
"Conference Paper","Budimac Z,Rakić G","Consistent Static Analysis in Multilingual Software Products Development","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Balkan Conference on Informatics Conference","Craiova, Romania","2015","9781450333351","","https://doi.org/10.1145/2801081.2801082;http://dx.doi.org/10.1145/2801081.2801082","10.1145/2801081.2801082","The paper points out some problems that are existing in the field of static analysis of software quality. Building tools that are language independent and based on a common intermediate structure are seen as a way to overcome most of the observed problems. As an example, an original, language-independent infrastructure containing a set of static quality software analyzers (SSQSA) is introduced.","Static analysis tools, Programming languages, Software quality","","BCI '15"
"Conference Paper","Lin Y,Peng X,Xing Z,Zheng D,Zhao W","Clone-Based and Interactive Recommendation for Modifying Pasted Code","","2015","","","520–531","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","Bergamo, Italy","2015","9781450336758","","https://doi.org/10.1145/2786805.2786871;http://dx.doi.org/10.1145/2786805.2786871","10.1145/2786805.2786871","Developers often need to modify pasted code when programming with copy-and-paste practice. Some modifications on pasted code could involve lots of editing efforts, and any missing or wrong edit could incur bugs. In this paper, we propose a clone-based and interactive approach to recommending where and how to modify the pasted code. In our approach, we regard clones of the pasted code as the results of historical copy-and-paste operations and their differences as historical modifications on the same piece of code. Our approach first retrieves clones of the pasted code from a clone repository and detects syntactically complete differences among them. Then our approach transfers each clone difference into a modification slot on the pasted code, suggests options for each slot, and further mines modifying regulations from the clone differences. Based on the mined modifying regulations, our approach dynamically updates the suggested options and their ranking in each slot according to developer's modifications on the pasted code. We implement a proof-of-concept tool CCDemon based on our approach and evaluate its effectiveness based on code clones detected from five open source projects. The results show that our approach can identify 96.9% of the to-be-modified positions in pasted code and suggest 75.0% of the required modifications. Our human study further confirms that CCDemon can help developers to accomplish their modifications of pasted code more efficiently.","reuse, code clone, copy and paste, differencing, recommendation","","ESEC/FSE 2015"
"Conference Paper","Chen F,Kim S","Crowd Debugging","","2015","","","320–332","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","Bergamo, Italy","2015","9781450336758","","https://doi.org/10.1145/2786805.2786819;http://dx.doi.org/10.1145/2786805.2786819","10.1145/2786805.2786819","Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none.","Crowd Sourcing, Debugging, Crowd Debugging","","ESEC/FSE 2015"
"Conference Paper","Zheng Q,Mockus A,Zhou M","A Method to Identify and Correct Problematic Software Activity Data: Exploiting Capacity Constraints and Data Redundancies","","2015","","","637–648","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","Bergamo, Italy","2015","9781450336758","","https://doi.org/10.1145/2786805.2786866;http://dx.doi.org/10.1145/2786805.2786866","10.1145/2786805.2786866","Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10% and rapidly rising. We found the corrected values to be 50% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.","data quality, data redundancy, mining software repositories, capacity constraint","","ESEC/FSE 2015"
"Conference Paper","Soetens QD,Pérez J,Demeyer S,Zaidman A","Circumventing Refactoring Masking Using Fine-Grained Change Recording","","2015","","","9–18","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Workshop on Principles of Software Evolution","Bergamo, Italy","2015","9781450338165","","https://doi.org/10.1145/2804360.2804362;http://dx.doi.org/10.1145/2804360.2804362","10.1145/2804360.2804362","Today, refactoring reconstruction techniques are snapshot-based: they compare two revisions from a source code management system and calculate the shortest path of edit operations to go from the one to the other. An inherent risk with snapshot-based approaches is that a refactoring may be concealed by later edit operations acting on the same source code entity, a phenomenon we call refactoring masking. In this paper, we performed an experiment to find out at which point refactoring masking occurs and confirmed that a snapshot-based technique misses refactorings when several edit operations are performed on the same source code entity. We present a way of reconstructing refactorings using fine grained changes that are recorded live from an integrated development environment and demonstrate on two cases ---PMD and Cruisecontrol--- that our approach is more accurate in a significant number of situations than the state-of-the-art snapshot-based technique RefFinder.","Fine Grained Changes, Refactoring Masking, Refactoring Reconstruction, Software Evolution","","IWPSE 2015"
"Conference Paper","Hayase Y,Kanda T,Ishio T","Estimating Product Evolution Graph Using Kolmogorov Complexity","","2015","","","66–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Workshop on Principles of Software Evolution","Bergamo, Italy","2015","9781450338165","","https://doi.org/10.1145/2804360.2804368;http://dx.doi.org/10.1145/2804360.2804368","10.1145/2804360.2804368","This paper proposes a method of estimating a product evolution graph based on Kolmogorov complexity. The method EEGL applies lossless compression to the source code of products, then, presumes a derivation relationship between two products when the increase of information between the two products is small. An evaluation experiment confirms that EEGL and an existing method PRET tends to produce different errors when estimating evolution graph results.","Software evolution, estimation, Kolmogorov complexity, evolution graph, lossless compression","","IWPSE 2015"
"Conference Paper","Matsuda J,Hayashi S,Saeki M","Hierarchical Categorization of Edit Operations for Separately Committing Large Refactoring Results","","2015","","","19–27","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Workshop on Principles of Software Evolution","Bergamo, Italy","2015","9781450338165","","https://doi.org/10.1145/2804360.2804363;http://dx.doi.org/10.1145/2804360.2804363","10.1145/2804360.2804363","In software configuration management using a version control system, developers have to follow the commit policy of the project. However, preparing changes according to the policy are sometimes cumbersome and time-consuming, in particular when applying large refactoring consisting of multiple primitive refactoring instances. In this paper, we propose a technique for re-organizing changes by recording editing operations of source code. Editing operations including refactoring operations are hierarchically managed based on their types provided by an integrated development environment. Using the obtained hierarchy, developers can easily configure the granularity of changes and obtain the resulting changes based on the configured granularity. We confirmed the feasibility of the technique by applying it to the recorded changes in a large refactoring process.","refactoring, edit history, tangled changes","","IWPSE 2015"
"Conference Paper","Prause CR,Jarke M","Gamification for Enforcing Coding Conventions","","2015","","","649–660","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","Bergamo, Italy","2015","9781450336758","","https://doi.org/10.1145/2786805.2786806;http://dx.doi.org/10.1145/2786805.2786806","10.1145/2786805.2786806","Software is a knowledge intensive product, which can only evolve if there is effective and efficient information exchange between developers. Complying to coding conventions improves information exchange by improving the readability of source code. However, without some form of enforcement, compliance to coding conventions is limited. We look at the problem of information exchange in code and propose gamification as a way to motivate developers to invest in compliance. Our concept consists of a technical prototype and its integration into a Scrum environment. By means of two experiments with agile software teams and subsequent surveys, we show that gamification can effectively improve adherence to coding conventions.","gamification, experiment, code style, software quality","","ESEC/FSE 2015"
"Conference Paper","Nagappan M,Robbes R,Kamei Y,Tanter É,McIntosh S,Mockus A,Hassan AE","An Empirical Study of Goto in C Code from GitHub Repositories","","2015","","","404–414","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering","Bergamo, Italy","2015","9781450336758","","https://doi.org/10.1145/2786805.2786834;http://dx.doi.org/10.1145/2786805.2786834","10.1145/2786805.2786834","It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is ‘harmful’ enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21±5%) and cleaning up resources at the end of a procedure (40.36 ± 5%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice.","Use of goto statements, Empirical SE, Github, Dijkstra","","ESEC/FSE 2015"
"Conference Paper","Madsen FM,Clifton-Everest R,Chakravarty MM,Keller G","Functional Array Streams","","2015","","","23–34","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM SIGPLAN Workshop on Functional High-Performance Computing","Vancouver, BC, Canada","2015","9781450338073","","https://doi.org/10.1145/2808091.2808094;http://dx.doi.org/10.1145/2808091.2808094","10.1145/2808091.2808094","Regular array languages for high performance computing based on aggregate operations provide a convenient parallel programming model, which enables the generation of efficient code for SIMD architectures, such as GPUs. However, the data sets that can be processed with current implementations are severely constrained by the limited amount of main memory available in these architectures. In this paper, we propose an extension of the embedded array language Accelerate with a notion of sequences, resulting in a two level hierarchy which allows the programmer to specify a partitioning strategy which facilitates automatic resource allocation. Depending on the available memory, the runtime system processes the overall data set in streams of chunks appropriate to the hardware parameters. In this paper, we present the language design for the sequence operations, as well as the compilation and runtime support, and demonstrate with a set of benchmarks the feasibility of this approach.","Arrays, Data parallelism, Embedded language, GPGPU, Haskell, Streams","","FHPC 2015"
"Conference Paper","Karachalias G,Schrijvers T,Vytiniotis D,Jones SP","GADTs Meet Their Match: Pattern-Matching Warnings That Account for GADTs, Guards, and Laziness","","2015","","","424–436","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming","Vancouver, BC, Canada","2015","9781450336697","","https://doi.org/10.1145/2784731.2784748;http://dx.doi.org/10.1145/2784731.2784748","10.1145/2784731.2784748","For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.","pattern matching, Generalized Algebraic Data Types, Haskell, OutsideIn(X)","","ICFP 2015"
"Journal Article","Karachalias G,Schrijvers T,Vytiniotis D,Jones SP","GADTs Meet Their Match: Pattern-Matching Warnings That Account for GADTs, Guards, and Laziness","SIGPLAN Not.","2015","50","9","424–436","Association for Computing Machinery","New York, NY, USA","","","2015-08","","0362-1340","https://doi.org/10.1145/2858949.2784748;http://dx.doi.org/10.1145/2858949.2784748","10.1145/2858949.2784748","For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.","OutsideIn(X), pattern matching, Haskell, Generalized Algebraic Data Types","",""
"Conference Paper","Burnap P,Javed A,Rana OF,Awan MS","Real-Time Classification of Malicious URLs on Twitter Using Machine Activity Data","","2015","","","970–977","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015","Paris, France","2015","9781450338547","","https://doi.org/10.1145/2808797.2809281;http://dx.doi.org/10.1145/2808797.2809281","10.1145/2808797.2809281","Massive online social networks with hundreds of millions of active users are increasingly being used by Cyber criminals to spread malicious software (malware) to exploit vulnerabilities on the machines of users for personal gain. Twitter is particularly susceptible to such activity as, with its 140 character limit, it is common for people to include URLs in their tweets to link to more detailed information, evidence, news reports and so on. URLs are often shortened so the endpoint is not obvious before a person clicks the link. Cyber criminals can exploit this to propagate malicious URLs on Twitter, for which the endpoint is a malicious server that performs unwanted actions on the person's machine. This is known as a drive-by-download. In this paper we develop a machine classification system to distinguish between malicious and benign URLs within seconds of the URL being clicked (i.e. 'real-time'). We train the classifier using machine activity logs created while interacting with URLs extracted from Twitter data collected during a large global event -- the Superbowl -- and test it using data from another large sporting event -- the Cricket World Cup. The results show that machine activity logs produce precision performances of up to 0.975 on training data from the first event and 0.747 on a test data from a second event. Furthermore, we examine the properties of the learned model to explain the relationship between machine activity and malicious software behaviour, and build a learning curve for the classifier to illustrate that very small samples of training data can be used with only a small detriment to performance.","","","ASONAM '15"
"Journal Article","Laguna I,Ahn DH,de Supinski BR,Gamblin T,Lee GL,Schulz M,Bagchi S,Kulkarni M,Zhou B,Chen Z,Qin F","Debugging High-Performance Computing Applications at Massive Scales","Commun. ACM","2015","58","9","72–81","Association for Computing Machinery","New York, NY, USA","","","2015-08","","0001-0782","https://doi.org/10.1145/2667219;http://dx.doi.org/10.1145/2667219","10.1145/2667219","Dynamic analysis techniques help programmers find the root cause of bugs in large-scale parallel applications.","","",""
"Journal Article","Faneca C,Vieira J,Zúquete A,Cano J,Moreira A,Almeida L","Towards Dynamic Adaptation in Broadcasting with Hybrid Rateless Codes","SIGBED Rev.","2015","12","3","45–48","Association for Computing Machinery","New York, NY, USA","","","2015-08","","","https://doi.org/10.1145/2815482.2815491;http://dx.doi.org/10.1145/2815482.2815491","10.1145/2815482.2815491","There are many situations, such as in training and education, in which there is a frequent need to distribute large files to many clients, e.g., operating system boot images or raw data files. To carry out such distribution efficiently, we use wireless broadcast and a hybrid coding technique that combines forward coding using weak LT Codes with a feedback phase at the end, which allows concluding the process faster with lower computing cost than traditional LT codes. However, for the sake of scalability, in the feedback phase a scheduler bounds the maximum number of clients that can communicate feedback in each given cycle and schedules them. Moreover, using a shorter or longer feedback phase also impacts on the number of simultaneous clients in feedback mode, resulting in more or less impact of the scheduler and more or less effectiveness of the feedback itself. In this short paper we briefly describe a recently developed prototype and we summarize some preliminary results confirming the advantage of using scheduled feedback. Moreover, we discuss the interplay between duration of the feedback phase and clients scheduling as a line for future research in scheduling-coding co-design.","weak-LT codes, fountain codes, traffic scheduling, LT codes, ad-hoc networks, wireless broadcast","",""
"Conference Paper","Sharma M,Kumari M,Singh VB","Post Release Versions Based Code Change Quality Metrics","","2015","","","235–243","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third International Symposium on Women in Computing and Informatics","Kochi, India","2015","9781450333610","","https://doi.org/10.1145/2791405.2791466;http://dx.doi.org/10.1145/2791405.2791466","10.1145/2791405.2791466","Software Metric is a quantitative measure of the degree to which a system, component or process possesses a given attribute. Bug fixing, new features (NFs) introduction and feature improvements (IMPs) are the key factors in deciding the next version of software. For fixing an issue (bug/new feature/feature improvement), a lot of changes have to be incorporated into the source code of the software. These code changes need to be understood by software engineers and managers when performing their daily development and maintenance tasks. In this paper, we have proposed four new metrics namely code change quality, code change density, file change quality and file change density to understand the quality of code changes across the different versions of five open source software products, namely Avro, Pig, Hive, jUDDI and Whirr of Apache project. Results show that all the products get better code change quality over a period of time. We have also observed that all the five products follow the similar code change trend.","Open Source Software, Feature improvement, New feature, Software Repositories, Entropy","","WCI '15"
"Conference Paper","Deitrick E,Shapiro RB,Ahrens MP,Fiebrink R,Lehrman PD,Farooq S","Using Distributed Cognition Theory to Analyze Collaborative Computer Science Learning","","2015","","","51–60","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eleventh Annual International Conference on International Computing Education Research","Omaha, Nebraska, USA","2015","9781450336307","","https://doi.org/10.1145/2787622.2787715;http://dx.doi.org/10.1145/2787622.2787715","10.1145/2787622.2787715","Research on students' learning in computing typically investigates how to enable individuals to develop concepts and skills, yet many forms of computing education, from peer instruction to robotics competitions, involve group work in which understanding may not be entirely locatable within individuals' minds. We need theories and methods that allow us to understand learning in cognitive systems: culturally and historically situated groups of students, teachers, and tools. Accordingly, we draw on Hutchins' Distributed Cognition [16] theory to present a qualitative case study analysis of interaction and learning within a small group of middle school students programming computer music. Our analysis shows how a system of students, teachers, and tools, working in a music classroom, is able to accomplish conceptually demanding computer music programming. We show how the system does this by 1) collectively drawing on individuals' knowledge, 2) using the physical and virtual affordances of different tools to organize work, externalize knowledge, and create new demands for problem solving, and 3) reconfiguring relationships between individuals and tools over time as the focus of problem solving changes. We discuss the implications of this perspective for research on teaching, learning and assessment in computing.","research methods, learning, music","","ICER '15"
"Conference Paper","Martinez J,Ziadi T,Bissyandé TF,Klein J,Le Traon Y","Bottom-up Adoption of Software Product Lines: A Generic and Extensible Approach","","2015","","","101–110","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Software Product Line","Nashville, Tennessee","2015","9781450336130","","https://doi.org/10.1145/2791060.2791086;http://dx.doi.org/10.1145/2791060.2791086","10.1145/2791060.2791086","Although Software Product Lines are recurrently praised as an efficient paradigm for systematic reuse, practical adoption remains challenging. For bottom-up Software Product Line adoption, where a set of artefact variants already exists, practitioners lack end-to-end support for chaining (1) feature identification, (2) feature location, (3) feature constraints discovery, as well as (4) reengineering approaches. This challenge can be overcome if there exists a set of principles for building a framework to integrate various algorithms and to support different artefact types. In this paper, we propose the principles of such a framework and we provide insights on how it can be extended with adapters, algorithms and visualisations enabling their use in different scenarios. We describe its realization in BUT4Reuse (Bottom--Up Technologies for Reuse) and we assess its generic and extensible properties by implementing a variety of extensions. We further empirically assess the complexity of integration by reproducing case studies from the literature. Finally, we present an experiment where users realize a bottom-up Software Product Line adoption building on the case study of Eclipse variants.","reverse engineering, mining existing assets, software product line engineering","","SPLC '15"
"Conference Paper","Rumpe B,Schulze C,von Wenckstern M,Ringert JO,Manhart P","Behavioral Compatibility of Simulink Models for Product Line Maintenance and Evolution","","2015","","","141–150","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Software Product Line","Nashville, Tennessee","2015","9781450336130","","https://doi.org/10.1145/2791060.2791077;http://dx.doi.org/10.1145/2791060.2791077","10.1145/2791060.2791077","Embedded software systems, e.g. automotive, robotic or automation systems are highly configurable and consist of many software components being available in different variants and versions. To identify the degree of reusability between these different occurrences of a component, it is necessary to determine the functional backward and forward compatibility between them. Based on this information it is possible to identify in which system context a component can be replaced safely by another version, e.g. exchanging an older component, or variant, e.g. introducing new features, to achieve the same functionality.This paper presents a model checking approach to determine behavioral compatibility of Simulink models, obtained from different component variants or during evolution. A prototype for automated compatibility checking demonstrates its feasibility. In addition implemented optimizations make the analysis more efficient, when the compared variants or versions are structurally similar.A case study on a driver assistance system provided by Daimler AG shows the effectiveness of the approach to automatically compare Simulink components.","","","SPLC '15"
"Journal Article","Grosser T,Verdoolaege S,Cohen A","Polyhedral AST Generation Is More Than Scanning Polyhedra","ACM Trans. Program. Lang. Syst.","2015","37","4","","Association for Computing Machinery","New York, NY, USA","","","2015-07","","0164-0925","https://doi.org/10.1145/2743016;http://dx.doi.org/10.1145/2743016","10.1145/2743016","Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations.We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.","Presburger relations, code generation, Polyhedral compilation, index set splitting, unrolling","",""
"Conference Paper","Pollet D,Ducasse S","A First Analysis of String APIs: The Case of Pharo","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on Smalltalk Technologies","Brescia, Italy","2015","9781450338578","","https://doi.org/10.1145/2811237.2811298;http://dx.doi.org/10.1145/2811237.2811298","10.1145/2811237.2811298","Most programming languages natively provide an abstraction of character strings. However, it is difficult to assess the design or the API of a string library. There is no comprehensive analysis of the needed operations and their different variations. There are no real guidelines about the different forces in presence and how they structure the design space of string manipulation. In this article, we harvest and structure a set of criteria to describe a string API. We propose an analysis of the Pharo 4 String library as a first experience on the topic.","Design, Library, Style, Strings, API","","IWST '15"
"Conference Paper","Wang H,Guo Y,Ma Z,Chen X","WuKong: A Scalable and Accurate Two-Phase Approach to Android App Clone Detection","","2015","","","71–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Symposium on Software Testing and Analysis","Baltimore, MD, USA","2015","9781450336208","","https://doi.org/10.1145/2771783.2771795;http://dx.doi.org/10.1145/2771783.2771795","10.1145/2771783.2771795","Repackaged Android applications (app clones) have been found in many third-party markets, which not only compromise the copyright of original authors, but also pose threats to security and privacy of mobile users. Both fine-grained and coarse-grained approaches have been proposed to detect app clones. However, fine-grained techniques employing complicated clone detection algorithms are difficult to scale to hundreds of thousands of apps, while coarse-grained techniques based on simple features are scalable but less accurate. This paper proposes WuKong, a two-phase detection approach that includes a coarse-grained detection phase to identify suspicious apps by comparing light-weight static semantic features, and a fine-grained phase to compare more detailed features for only those apps found in the first phase. To further improve the detection speed and accuracy, we also introduce an automated clustering-based preprocessing step to filter third-party libraries before conducting app clone detection. Experiments on more than 100,000 Android apps collected from five Android markets demonstrate the effectiveness and scalability of our approach.","Clone detection, Android, repackaging, mobile applications, third-party library","","ISSTA 2015"
"Conference Paper","Xue Y,Wang J,Liu Y,Xiao H,Sun J,Chandramohan M","Detection and Classification of Malicious JavaScript via Attack Behavior Modelling","","2015","","","48–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Symposium on Software Testing and Analysis","Baltimore, MD, USA","2015","9781450336208","","https://doi.org/10.1145/2771783.2771814;http://dx.doi.org/10.1145/2771783.2771814","10.1145/2771783.2771814","Existing malicious JavaScript (JS) detection tools and commercial anti-virus tools mostly use feature-based or signature-based approaches to detect JS malware. These tools are weak in resistance to obfuscation and JS malware variants, not mentioning about providing detailed information of attack behaviors. Such limitations root in the incapability of capturing attack behaviors in these approches. In this paper, we propose to use Deterministic Finite Automaton (DFA) to abstract and summarize common behaviors of malicious JS of the same attack type. We propose an automatic behavior learning framework, named JS*, to learn DFAs from dynamic execution traces of JS malware, where we implement an effective online teacher by combining data dependency analysis, defense rules and trace replay mechanism. We evaluate JS* using real world data of 10000 benign and 276 malicious JS samples to cover 8 most-infectious attack types. The results demonstrate the scalability and effectiveness of our approach in the malware detection and classification, compared with commercial anti-virus tools. We also show how to use our DFAs to detect variants and new attacks.","behavior modelling, malware detection, malicious JavaScript, L*","","ISSTA 2015"
"Conference Paper","Barr ET,Harman M,Jia Y,Marginean A,Petke J","Automated Software Transplantation","","2015","","","257–269","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Symposium on Software Testing and Analysis","Baltimore, MD, USA","2015","9781450336208","","https://doi.org/10.1145/2771783.2771796;http://dx.doi.org/10.1145/2771783.2771796","10.1145/2771783.2771796","Automated transplantation would open many exciting avenues for software development: suppose we could autotransplant code from one system into another, entirely unrelated, system. This paper introduces a theory, an algorithm, and a tool that achieve this. Leveraging lightweight annotation, program analysis identifies an organ (interesting behavior to transplant); testing validates that the organ exhibits the desired behavior during its extraction and after its implantation into a host. While we do not claim automated transplantation is now a solved problem, our results are encouraging: we report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H.264 video encoding functionality from the x264 system to the VLC media player; compare this to upgrading x264 within VLC, a task that we estimate, from VLC's version history, took human programmers an average of 20 days of elapsed, as opposed to dedicated, time.","genetic improvement, Automated software transplantation, autotransplantation","","ISSTA 2015"
"Conference Paper","Muşlu K,Brun Y,Meliou A","Preventing Data Errors with Continuous Testing","","2015","","","373–384","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Symposium on Software Testing and Analysis","Baltimore, MD, USA","2015","9781450336208","","https://doi.org/10.1145/2771783.2771792;http://dx.doi.org/10.1145/2771783.2771792","10.1145/2771783.2771792","Today, software systems that rely on data are ubiquitous, and ensuring the data's quality is an increasingly important challenge as data errors result in annual multi-billion dollar losses. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: identifying system errors caused by well-formed but incorrect data. We present continuous data testing (CDT), a low-overhead, delay-free technique that quickly identifies likely data errors. CDT continuously executes domain-specific test queries; when a test fails, CDT unobtrusively warns the user or administrator. We implement CDT in the ConTest prototype for the PostgreSQL database management system. A feasibility user study with 96 humans shows that ConTest was extremely effective in a setting with a data entry application at guarding against data errors: With ConTest, users corrected 98.4% of their errors, as opposed to 40.2% without, even when we injected 40% false positives into ConTest's output. Further, when using ConTest, users corrected data entry errors 3.2 times faster than when using state-of-the-art methods.","data testing, continuous testing, data debugging","","ISSTA 2015"
"Conference Paper","Wang X,Zhang L,Tanofsky P","Experience Report: How is Dynamic Symbolic Execution Different from Manual Testing? A Study on KLEE","","2015","","","199–210","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Symposium on Software Testing and Analysis","Baltimore, MD, USA","2015","9781450336208","","https://doi.org/10.1145/2771783.2771818;http://dx.doi.org/10.1145/2771783.2771818","10.1145/2771783.2771818","Software testing has been the major approach to software quality assurance for decades, but it typically involves intensive manual efforts. To reduce manual efforts, researchers have proposed numerous approaches to automate test-case generation, which is one of the most time-consuming tasks in software testing. One most recent achievement in the area is Dynamic Symbolic Execution (DSE), and tools based on DSE, such as KLEE, have been reported to generate test suites achieving higher code coverage than manually developed test suites. However, besides the competitive code coverage, there have been few studies to compare DSE-based test suites with manually developed test suites more thoroughly on various metrics to understand the detailed differences between the two testing methodologies. In this paper, we revisit the experimental study on the KLEE tool and GNU CoreUtils programs, and compare KLEE-based test suites with manually developed test suites on various aspects. We further carried out a qualitative study to investigates the reasons behind the differences in statistical results. The results of our studies show that while KLEE-based test suites are able to generate test cases with higher code coverage, they are relatively less effective on covering hard-to-cover code and killing mutants. Furthermore, our qualitative study reveals that KLEE-based test suites have advantages in exploring error-handling code and exhausting options, but are less effective on generating valid string inputs and exploring meaningful program behaviors.","Manual Testing, Dynamic Symbolic Execution, Empirical Study","","ISSTA 2015"
"Conference Paper","Lopez-Herrejon RE,Linsbauer L,Assunção WK,Fischer S,Vergilio SR,Egyed A","Genetic Improvement for Software Product Lines: An Overview and a Roadmap","","2015","","","823–830","Association for Computing Machinery","New York, NY, USA","Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation","Madrid, Spain","2015","9781450334884","","https://doi.org/10.1145/2739482.2768422;http://dx.doi.org/10.1145/2739482.2768422","10.1145/2739482.2768422","Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.","variability, genetic programming, genetic improvement, evolutionary algorithms, software product lines","","GECCO Companion '15"
"Conference Paper","Höller A,Rauter T,Iber J,Kreiner C","Patterns for Automated Software Diversity to Support Security and Reliability","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th European Conference on Pattern Languages of Programs","Kaufbeuren, Germany","2015","9781450338479","","https://doi.org/10.1145/2855321.2855360;http://dx.doi.org/10.1145/2855321.2855360","10.1145/2855321.2855360","Many approaches for increasing the security and/or fault tolerance of software systems are based on introducing ""diversity in execution"". This means that different versions of the same software show different characteristics during the execution while retaining the same functionality. For example, they can have diverse timing behaviors, access different memory regions, etc.In this paper, we present two patterns describing commonly used practices of realizing automated software diversity. The first pattern is static randomization that creates multiple execution variants of the same source code before distributing these variants. The second one is dynamic randomization, where only one single version of an executable program is able to perform its executions differently.","software diversity fault tolerance","","EuroPLoP '15"
"Conference Paper","Vilar R,Oliveira D,Almeida H","Rendering Patterns for Enterprise Applications","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th European Conference on Pattern Languages of Programs","Kaufbeuren, Germany","2015","9781450338479","","https://doi.org/10.1145/2855321.2855344;http://dx.doi.org/10.1145/2855321.2855344","10.1145/2855321.2855344","Graphical User Interface (GUI) coding represents a significant amount of effort in Enterprise applications development. Due to the coupling between GUI screens and domain entities, GUI code is hard to reuse and has lots of duplicated chunks. We propose five rendering patterns to organize GUI artifacts based on domain model meta data of Enterprise applications. We have validated our approach by using evolution scenarios to compare the use of the proposed patterns with existing solutions, such as Scaffolding frameworks and AOM Rendering patterns.","rendering patterns, enterprise applications, scaffolding frameworks, graphical user interface","","EuroPLoP '15"
"Conference Paper","Cholakov T,Birov D","Duplicate Code Detection Algorithm","","2015","","","104–111","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Computer Systems and Technologies","Dublin, Ireland","2015","9781450333573","","https://doi.org/10.1145/2812428.2812449;http://dx.doi.org/10.1145/2812428.2812449","10.1145/2812428.2812449","In this paper we propose an algorithm for detecting duplicate fragments of source code based on call graphs. The complexity of the proposed algorithm is estimated and the practical performance is tested using several executions of the algorithm on three different versions of open source Ant. An analysis is made of the influence of the algorithm parameters on its results. Additionally a visualization approach for displaying the results is represented, demonstrated by a tool implementing the algorithm.","source code analysis and manipulation, duplicated code detection, reengineering, clone detection, refactoring","","CompSysTech '15"
"Conference Paper","Pulkkinen P,Holvitie J,Nevalainen OS,Leppänen V","Reusability Based Program Clone Detection: Case Study on Large Scale Healthcare Software System","","2015","","","90–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Computer Systems and Technologies","Dublin, Ireland","2015","9781450333573","","https://doi.org/10.1145/2812428.2812471;http://dx.doi.org/10.1145/2812428.2812471","10.1145/2812428.2812471","A novel process for identifying reusable program clones is described in this work. The process is applied to a very demanding context, a large-scale closed source healthcare software system. As results, the applicability and maturity of the clone detection system are discussed but also the results of the study are summarized. In this, especial care has been taken in order to retain result comparability with other hallmark studies on clone detection.","refactoring, reusability, clone detection","","CompSysTech '15"
"Conference Paper","Zhou Y,Wu L,Wang Z,Jiang X","Harvesting Developer Credentials in Android Apps","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM Conference on Security & Privacy in Wireless and Mobile Networks","New York, New York","2015","9781450336239","","https://doi.org/10.1145/2766498.2766499;http://dx.doi.org/10.1145/2766498.2766499","10.1145/2766498.2766499","Developers often integrate third-party services into their apps. To access a service, an app must authenticate itself to the service with a credential. However, credentials in apps are often not properly or adequately protected, and might be easily extracted by attackers. A leaked credential could pose serious privacy and security threats to both the app developer and app users.In this paper, we propose CredMiner to systematically study the prevalence of unsafe developer credential uses in Android apps. CredMiner can programmatically identify and recover (obfuscated) developer credentials unsafely embedded in Android apps. Specifically, it leverages data flow analysis to identify the raw form of the embedded credential, and selectively executes the part of the program that builds the credential to recover it. We applied CredMiner to 36,561 apps collected from various Android markets to study the use of free email services and Amazon AWS. There were 237 and 196 apps that used these two services, respectively. CredMiner discovered that 51.5% (121/237) and 67.3% (132/196) of them were vulnerable. In total, CredMiner recovered 302 unique email login credentials and 58 unique Amazon AWS credentials, and verified that 252 and 28 of these credentials were still valid at the time of the experiments, respectively.","Amazon AWS, information flow, CredMiner, static analysis","","WiSec '15"
"Conference Paper","Aloor R,Nandivada VK","Unique Worker Model for OpenMP","","2015","","","47–56","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM on International Conference on Supercomputing","Newport Beach, California, USA","2015","9781450335591","","https://doi.org/10.1145/2751205.2751238;http://dx.doi.org/10.1145/2751205.2751238","10.1145/2751205.2751238","In OpenMP, because of the underlying efficient 'team of workers' model, each worker is given a chunk of tasks (iterations of a parallel-for-loop, or sections in a parallel-sections block), and a barrier construct is used to synchronize the workers (not the tasks). Naturally, the practitioners are discouraged from invoking barriers in these tasks; as otherwise, depending on the number of workers the behavior of the program can vary. Such a restrictive practice can adversely impact programmability and program readability. To overcome such a restrictive practice, in this paper, inspired from the more intuitive interaction of tasks and barriers found in newer task parallel languages like X10, HJ, Chapel and so on, we present an extension to OpenMP (called UW-OpenMP).UW-OpenMP gives the programmer an impression that each parallel task has been assigned a unique worker, and importantly these parallel tasks can be synchronized using a barrier construct. Consequently, the semantics of the programs (using parallel-for-loops, sections and barriers) remains independent of the actual number of worker threads, at runtime. We argue that such a scheme allows the programmer to conveniently think and express his/her logic in parallel. We propose a source to source transformation scheme to translate UW-OpenMP C programs to equivalent OpenMP C programs that are guaranteed to not invoke barriers in any task. We have implemented our proposed translation scheme in the ROSE compiler framework. Our preliminary evaluation shows that the proposed extension leads to programs that are concise and arguably easier to understand. Importantly, the efficiency resulting from the 'team of workers' model is not compromised.","multi-core, openmp, parallel-for loops, barrier synchronization","","ICS '15"
"Conference Paper","Sidiroglou-Douskos S,Lahtinen E,Long F,Rinard M","Automatic Error Elimination by Horizontal Code Transfer across Multiple Applications","","2015","","","43–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation","Portland, OR, USA","2015","9781450334686","","https://doi.org/10.1145/2737924.2737988;http://dx.doi.org/10.1145/2737924.2737988","10.1145/2737924.2737988","We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.","automatic code transfer, data structure translation, program repair","","PLDI '15"
"Journal Article","Sidiroglou-Douskos S,Lahtinen E,Long F,Rinard M","Automatic Error Elimination by Horizontal Code Transfer across Multiple Applications","SIGPLAN Not.","2015","50","6","43–54","Association for Computing Machinery","New York, NY, USA","","","2015-06","","0362-1340","https://doi.org/10.1145/2813885.2737988;http://dx.doi.org/10.1145/2813885.2737988","10.1145/2813885.2737988","We present Code Phage (CP), a system for automatically transferring correct code from donor applications into recipient applications that process the same inputs to successfully eliminate errors in the recipient. Experimental results using seven donor applications to eliminate ten errors in seven recipient applications highlight the ability of CP to transfer code across applications to eliminate out of bounds access, integer overflow, and divide by zero errors. Because CP works with binary donors with no need for source code or symbolic information, it supports a wide range of use cases. To the best of our knowledge, CP is the first system to automatically transfer code across multiple applications.","data structure translation, program repair, automatic code transfer","",""
"Conference Paper","Cardoso B,Figueiredo E","Co-Occurrence of Design Patterns and Bad Smells in Software Systems: An Exploratory Study","","2015","","","347–354","Brazilian Computer Society","Porto Alegre, BRA","Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1","Goiania, Goias, Brazil","2015","","","","","A design pattern is a general reusable solution to a recurring problem in software design. Bad smells are symptoms that may indicate something wrong in the system design or code. Therefore, design patterns and bad smells represent antagonistic structures. They are subject of recurring research and typically appear in software systems. Although design patterns represent good design, their use is often inadequate because their implementation is not always trivial or they may be unnecessarily employed. The inadequate use of design patterns may lead to a bad smell. Therefore, this paper performs an exploratory study in order to identify instances of co-occurrences of design patterns and bad smells. This study is performed over five systems and discovers some co-occurrences between design patterns and bad smells. For instance, we observed the co-occurrences of Command with God Class and Template Method with Duplicated Code. The results of this study make it possible to understand in which situations design patterns are misused or overused and establish guidelines for their better use.","Design Patterns, Bad Smells","","SBSI 2015"
"Conference Paper","Brandao BC,Santoro FM,Azevedo LG","Towards Aspects Identification in Business Process Through Process Mining","","2015","","","741–748","Brazilian Computer Society","Porto Alegre, BRA","Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1","Goiania, Goias, Brazil","2015","","","","","In business process models, elements can be scattered (repeated) within different processes, making it difficult to handle changes, analyze process for improvements, or check crosscutting impacts. These scattered elements are named as Aspects. Similar to the aspect-oriented paradigm in programming languages, in BPM, aspect handling has the goal to modularize the crosscutting concerns spread across the models. This process modularization facilitates the management of the process (reuse, maintenance and understanding). The current approaches for aspect identification are made manually; thus, resulting in the problem of subjectivity and lack of systematization. This paper proposes a method to automatically identify aspects in business process from its event logs. The method is based on mining techniques and it aims to solve the problem of the subjectivity identification made by specialists. The initial results from a preliminary evaluation showed evidences that the method identified correctly the aspects present in the process model.","Process Mining, Business Process Management, Aspects","","SBSI 2015"
"Conference Paper","Fontana FA,Mangiacavalli M,Pochiero D,Zanoni M","On Experimenting Refactoring Tools to Remove Code Smells","","2015","","","","Association for Computing Machinery","New York, NY, USA","Scientific Workshop Proceedings of the XP2015","Helsinki, Finland","2015","9781450334099","","https://doi.org/10.1145/2764979.2764986;http://dx.doi.org/10.1145/2764979.2764986","10.1145/2764979.2764986","When we develop a software project of a certain complexity, source code maintainability could become a problem, in particular if developers do not use a consolidate development process that simplifies the management of the entire project. When source code becomes very complex, it is difficult for developers to share and modify it. We can improve internal software qualities such as reusability, maintainability and readability through refactoring. Refactoring can be applied to remove possible problems in the code, as code smells. Identifying code smells and removing them through refactoring results in better code maintainability, but it can be an overwhelming task. In this paper, we describe our experimentation on using four refactoring tools to remove code smells in four systems, with the aim to outline advantages and disadvantages of the tools with respect to the accomplishment of this task, and to identify the smells easier to be removed among the ones we considered in this paper.","code smells, refactoring","","XP '15 workshops"
"Conference Paper","Stephan M,Cordy JR","Identifying Instances of Model Design Patterns and Antipatterns Using Model Clone Detection","","2015","","","48–53","IEEE Press","Florence, Italy","Proceedings of the Seventh International Workshop on Modeling in Software Engineering","","2015","","","","","A hurdle in the growth of model driven software engineering is our ability to evaluate the quality of models automatically. One perspective is that software quality is a function of the existence, or lack thereof, of good and bad properties, also known as patterns and antipatterns, respectively. In this paper, we introduce the notion of using model clone detection to detect model pattern and antipattern instances by looking for models that are cross clones of pattern models. By detecting patterns at the model level, analysis is accomplished earlier in the engineering process, can be applied to primarily model-based projects, and remains at the same level of abstraction that engineers are used to. We outline the process of using model clone detection for this purpose, including representing the patterns and detection of instances. We present some Simulink examples of pattern representations and discuss future work and research in the area.","","","MiSE '15"
"Conference Paper","McBurney PW","Automatic Documentation Generation via Source Code Summarization","","2015","","","903–906","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Programmers need software documentation. However, documentation is expensive to produce and maintain, and often becomes outdated over time. Programmers often lack the time and resources to write documentation. Therefore, automated solutions are desirable. Designers of automatic documentation tools are limited because there is not yet a clear understanding of what characteristics are important to generating high quality summaries. I propose three specific research objectives to improving automatic documentation generation. I propose to study the similarity between source code and summary. Second, I propose studying whether or not including contextual information about source code improves summary quality. Finally, I propose to study the problem of similarity in source code structure and source code documentation. This paper discusses my work on these three objectives towards my Ph.D. dissertation, including my preliminary and proposed work.","","","ICSE '15"
"Conference Paper","Ribeiro TV,Travassos GH","On the Alignment of Source Code Quality Perspectives through Experimentation: An Industrial Case","","2015","","","26–33","IEEE Press","Florence, Italy","Proceedings of the Third International Workshop on Conducting Empirical Studies in Industry","","2015","","","","","Alignment is a key factor for success in many software development projects. Aligned teams are capable of bringing collaboration and positive results to companies; whereas misalignment among developers can make a conflicted environment and even lead the project to failure. OBJECTIVE. To assist developers in an embedded software development company in their conceptual alignment regarding source code quality. METHOD. In the organizational context, plan and perform a series of studies such as surveys, systematic literature review (SLR), qualitative data analysis and focus group to support the identification of conceptual misalignments among developers and establish common terminology and guidance concerning source code quality. RESULTS. The results from a survey conducted in one company showed a conceptual misalignment among developers regarding the source code quality that was triggering continuous rework during software evolution activities. Through an SLR and a qualitative analysis of code snippets, a set of evidence-based coding guidelines for readability and understandability of source code were formulated. These guidelines were evaluated and used as an instrument for aligning source code perspectives during a focus group, showing their feasibility and adequacy to the company's context. CONCLUSIONS. The use of all contextual information observed -- e.g. teams' locations, software development context, and time constraints -- along with the information gathered during the industry-academia collaboration was particularly important to help us appropriately chose research methods to be used, and formulate evidence-based coding guidelines that matched the company's needs and expectations. Further evaluations have to be carried out to ensure the quality impact of some guidelines proposed before using them all over the company.","code quality, conceptual alignment, experimental studies, industry-academia collaboration","","CESI '15"
"Conference Paper","Higo Y,Ohtani A,Hayashi S,Hata H,Shinji K","Toward Reusing Code Changes","","2015","","","372–376","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","Existing techniques have succeeded to help developers implement new code. However, they are insufficient to help to change existing code. Previous studies have proposed techniques to support bug fixes but other kinds of code changes such as function enhancements and refactorings are not supported by them. In this paper, we propose a novel system that helps developers change existing code. Unlike existing techniques, our system can support any kinds of code changes if similar code changes occurred in the past. Our research is still on very early stage and we have not have any implementation or any prototype yet. This paper introduces our research purpose, an outline of our system, and how our system is different from existing techniques.","change reuse, source code analysis, code clone","","MSR '15"
"Conference Paper","Soh C,Tan HB,Arnatovich YL,Wang L","Detecting Clones in Android Applications through Analyzing User Interfaces","","2015","","","163–173","IEEE Press","Florence, Italy","Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","","","The blooming mobile smart phone device industry has attracted a large number of application developers. However, due to the availability of reverse engineering tools for Android applications, it also caught the attention of plagiarists and malware writers. In recent years, application cloning has become a serious threat to the Android market. In previous work, mobile application clone detection mainly focuses on code-based analysis. Such an approach lacks resilient to advanced obfuscation techniques. Their efficiency is also questionable, as billions of opcodes need to be processed for cross-market clone detection. In this paper, we propose a novel technique of detecting Android application clones based on the analysis of user interface (UI) information collected at runtime. By leveraging on the multiple entry points feature of Android applications, the UI information can be collected easily without the need to generate relevant inputs and execute the entire application. Another advantage of our technique is obfuscation resilient since semantics preserving obfuscation technique do not affect runtime behaviors. We evaluated our approach on a set of real-world dataset and it has a low false positive rate and false negative rate. Furthermore, the results also show that our approach is effective in detecting different types of repackaging attacks.","Android, user interface, repackaging, obfuscation resilient, clone detection","","ICPC '15"
"Conference Paper","Moreno L,Bavota G,Di Penta M,Oliveto R,Marcus A","How Can I Use This Method?","","2015","","","880–890","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Code examples are small source code fragments whose purpose is to illustrate how a programming language construct, an API, or a specific function/method works. Since code examples are not always available in the software documentation, researchers have proposed techniques to automatically extract them from existing software or to mine them from developer discussions. In this paper we propose muse (Method USage Examples), an approach for mining and ranking actual code examples that show how to use a specific method. muse combines static slicing (to simplify examples) with clone detection (to group similar examples), and uses heuristics to select and rank the best examples in terms of reusability, understandability, and popularity. muse has been empirically evaluated using examples mined from six libraries, by performing three studies involving a total of 140 developers to: (i) evaluate the selection and ranking heuristics, (ii) provide their perception on the usefulness of the selected examples, and (iii) perform specific programming tasks using the muse examples. The results indicate that muse selects and ranks examples close to how humans do, most of the code examples (82%) are perceived as useful, and they actually help when performing programming tasks.","","","ICSE '15"
"Conference Paper","Sabi Y,Murakami H,Higo Y,Kusumoto S","Reordering Results of Keyword-Based Code Search for Supporting Simultaneous Code Changes","","2015","","","289–290","IEEE Press","Florence, Italy","Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","","","Many research studies have been conducted to help simultaneous code changes on multiple code fragments. Code clones and logical couplings are often utilized in such research studies. However, most of them have been evaluated on only open source projects or students' software. In this paper, we report our academic-industrial collaboration with a software company. The collaboration is intended to suggest multiple code fragments to be changed simultaneously when a developer specifies a keyword such as variable names on source code. In the collaboration, we propose to use code clones and logical couplings information to reorder the code fragments. We confirmed that code clones and logical couplings worked well on helping simultaneous code changes on three projects that have being developed in the company.","","","ICPC '15"
"Conference Paper","Ahmed TM,Shang W,Hassan AE","An Empirical Study of the Copy and Paste Behavior during Development","","2015","","","99–110","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","Developers frequently employ Copy and Paste. However, little is known about the copy and paste behavior during development. To better understand the copy and paste behavior, automated approaches are proposed to identify cloned code. However, such automated approaches can only identify the location of the code that has been copied and pasted, but little is known about the context of the copy and paste. On the other hand, prior research studying actual copy and paste behavior is based on a small number of users in an experimental setup.In this paper, we study the behavior of developers copying and pasting code while using the Eclipse IDE. We mine the usage data of over 20,000 Eclipse users. We aim to explore the different patterns of Copy and Paste (C&P) that are used by Eclipse users during development. We compare such usage patterns to the regular users' usage of copy and paste during non-development tasks reported in earlier studies. Our findings instruct builders of future IDEs. We find that developers' C&P behavior is considerably different from the behavior of regular users. For example, developers tend to perform more frequent C&P in the same file contrary to regular users, who tend to perform C&P across different windows. Moreover, we find that C&P across different programming languages is a common behavior as we extracted more than 75,000 C&P incidents across different programming languages. Such a finding highlights the need for clone detection techniques that can detect code clones across different programming languages.","","","MSR '15"
"Conference Paper","Wu Y,Manabe Y,Kanda T,German DM,Inoue K","A Method to Detect License Inconsistencies in Large-Scale Open Source Projects","","2015","","","324–333","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","The reuse of free and open source software (FOSS) components is becoming more and more popular. They usually contain one or more software licenses describing the requirements and conditions which should be followed when been reused. Licenses are usually written in the header of source code files as program comments. Removing or modifying the license header by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. But to the best of our knowledge, no research has been devoted to investigate such kind of license infringements nor license inconsistencies. In this paper, we describe and categorize different types of license inconsistencies and propose a feasible method to detect them. Then we apply this method to Debian 7.5 and present the license inconsistencies found in it. With a manual analysis, we summarized various reasons behind these license inconsistencies, some of which imply license infringement and require the attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.","","","MSR '15"
"Conference Paper","Tamai T","Software Engineering View of a Large-Scale System Failure and the Following Lawsuit","","2015","","","18–24","IEEE Press","Florence, Italy","Proceedings of the Second International Workshop on Software Engineering Research and Industrial Practice","","2015","9781467370851","","","","In 2005, a system trouble occurred in the Tokyo Stock Exchange Order System, which caused a loss of more than 40 billion yen to Mizuho Securities and Mizuho filed a lawsuit demanding compensation for the loss to the Tokyo Stock Exchange (TSE).The judgment was pronounced in December 2009, ordering TSE to pay Mizuho Securities about 10 billion yen in compensation for the loss. Mizuho Securities found the judgment unacceptable and appealed to the Tokyo High Court. The judgment of the appellate court was issued in July 2013, basically upholding the initial verdict.From the software engineering point of view, the most important incident during the appellate court was partial disclosure of the source code of the system. We report the incident from the viewpoint of software engineering, making use of the partially disclosed source code. We discuss various issues, focusing on the role of SE researchers and engineers.","","","SER&IP '15"
"Conference Paper","Burlet G,Hindle A","An Empirical Study of End-User Programmers in the Computer Music Community","","2015","","","292–302","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.","","","MSR '15"
"Conference Paper","Saha RK,Zhang L,Khurshid S,Perry DE","An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes","","2015","","","268–279","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.","regression testing, information retrieval, test prioritization","","ICSE '15"
"Conference Paper","Ray B,Nagappan M,Bird C,Nagappan N,Zimmermann T","The Uniqueness of Changes: Characteristics and Applications","","2015","","","34–44","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways; they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.","","","MSR '15"
"Conference Paper","Meng N,Hua L,Kim M,McKinley KS","Does Automated Refactoring Obviate Systematic Editing?","","2015","","","392–402","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","When developers add features and fix bugs, they often make systematic edits---similar edits to multiple locations. Systematic edits may indicate that developers should instead refactor to eliminate redundancy. This paper explores this question by designing and implementing a fully automated refactoring tool called Rase, which performs clone removal. Rase (1) extracts common code guided by a systematic edit; (2) creates new types and methods as needed; (3) parameterizes differences in types, methods, variables, and expressions; and (4) inserts return objects and exit labels based on control and data flow. To our knowledge, this functionality makes Rase the most advanced refactoring tool for automated clone removal.We evaluate Rase with real-world systematic edits and compare to method based clone removal. Rase successfully performs clone removal in 30 of 56 method pairs (n=2) and 20 of 30 method groups (n≥3) with systematic edits. We find that scoping refactoring based on systematic edits (58%), rather than the entire method (33%), increases the applicability of automated clone removal. Automated refactoring is not feasible in the other 42% cases, which indicates that automated refactoring does not obviate the need for systematic editing.","","","ICSE '15"
"Conference Paper","Nguyen AT,Nguyen TN","Graph-Based Statistical Language Model for Code","","2015","","","858–868","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan also has high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.","","","ICSE '15"
"Conference Paper","Weiss C,Rubio-González C,Liblit B","Database-Backed Program Analysis for Scalable Error Propagation","","2015","","","586–597","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Software is rapidly increasing in size and complexity. Static analyses must be designed to scale well if they are to be usable with realistic applications, but prior efforts have often been limited by available memory. We propose a database-backed strategy for large program analysis based on graph algorithms, using a Semantic Web database to manage representations of the program under analysis. Our approach is applicable to a variety of interprocedural finite distributive subset (IFDS) dataflow problems; we focus on error propagation as a motivating example. Our implementation analyzes multi-million-line programs quickly and in just a fraction of the memory required by prior approaches. When memory alone is insufficient, our approach falls back on disk using several hybrid configurations tuned to put all available resources to good use.","","","ICSE '15"
"Conference Paper","Stevens R","A Declarative Foundation for Comprehensive History Querying","","2015","","","907–910","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Researchers in the field of Mining Software Repositories perform studies about the evolution of software projects. To this end, they use the version control system storing the changes made to a single software project. Such studies are concerned with the source code characteristics in one particular revision, the commit data for that revision, how the code evolves over time and what concrete, fine-grained changes were applied to the source code between two revisions. Although tools exist to analyse an individual concern, scripts and manual work is required to combine these tools to perform a single experiment. We present a general-purpose history querying tool named QwalKeko that enables expressing these concerns in a single uniform language, and having them detected in a git repository. We have validated our work by means of replication studies as well as through MSR studies of our own.","","","ICSE '15"
"Conference Paper","Alexandru CV,Gall HC","Rapid Multi-Purpose, Multi-Commit Code Analysis","","2015","","","635–638","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.","","","ICSE '15"
"Conference Paper","Zhang T,Song M,Pinedo J,Kim M","Interactive Code Review for Systematic Changes","","2015","","","111–122","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes---similar, related changes to multiple contexts. We present Critics, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, Critics allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated Critics using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used Critics to investigate diff patches authored by their own team. After using Critics, all six participants indicated that they would like Critics to be integrated into their current code review environment. This also attests to the fact that Critics scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using Critics and Eclipse diff. The results show that human subjects using Critics answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that Critics should improve developer productivity in inspecting systematic changes during peer code reviews.","","","ICSE '15"
"Conference Paper","Tao Y,Kim S","Partitioning Composite Code Changes to Facilitate Code Review","","2015","","","180–190","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","Developers expend significant effort on reviewing source code changes. Hence, the comprehensibility of code changes directly affects development productivity. Our prior study has suggested that composite code changes, which mix multiple development issues together, are typically difficult to review. Unfortunately, our manual inspection of 453 open source code changes reveals a non-trivial occurrence (up to 29%) of such composite changes.In this paper, we propose a heuristic-based approach to automatically partition composite changes, such that each subchange in the partition is more cohesive and self-contained. Our quantitative and qualitative evaluation results are promising in demonstrating the potential benefits of our approach for facilitating code review of composite code changes.","","","MSR '15"
"Conference Paper","Beller M,Zaidman A,Karpov A","The Last Line Effect","","2015","","","240–243","IEEE Press","Florence, Italy","Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","","","Micro-clones are tiny duplicated pieces of code; they typically comprise only a few statements or lines. In this paper, we expose the ""last line effect,"" the phenomenon that the last line or statement in a micro-clone is much more likely to contain an error than the previous lines or statements. We do this by analyzing 208 open source projects and reporting on 202 faulty micro-clones.","","","ICPC '15"
"Conference Paper","Wang W,Poo-Caamaño G,Wilde E,German DM","What is the Gist? Understanding the Use of Public Gists on GitHub","","2015","","","314–323","IEEE Press","Florence, Italy","Proceedings of the 12th Working Conference on Mining Software Repositories","","2015","9780769555942","","","","GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers' collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as ""a simple way to share snippets and pastes with others."" This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and composed of a single file. However, Gists serve a wide variety of uses, from saving snippets of code, to creating reusable components for web pages.","","","MSR '15"
"Conference Paper","Mirakhorli M,Chen HM,Kazman R","Mining Big Data for Detecting, Extracting and Recommending Architectural Design Concepts","","2015","","","15–18","IEEE Press","Florence, Italy","Proceedings of the First International Workshop on BIG Data Software Engineering","","2015","","","","","An architecture recommender system can help programmers make better design choices to address their architectural quality attribute concerns while doing their daily programming tasks. We mine big data to detect and extract a large set of architectural design concepts, such as design patterns, design tactics, architecture styles, etc., to be used in our architecture recommender system called ARS. However, mining big data poses many practical challenges for system implementation. The volume, velocity and variety of our data set, like all other big data systems, requires careful planning. This first challenge is to select appropriate technologies from the large number of available products for our system implementation. Building on these technologies our greatest challenge is to custom-fit our algorithms to the parallel processing platform we have selected for ARS, to meet our performance goals.","design knowledge, open architecture, tactics, mining internet scale software repositories, patterns","","BIGDSE '15"
"Conference Paper","Avgustinov P,Baars AI,Henriksen AS,Lavender G,Menzel G,de Moor O,Schäfer M,Tibble J","Tracking Static Analysis Violations over Time to Capture Developer Characteristics","","2015","","","437–447","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Many interesting questions about the software quality of a code base can only be answered adequately if fine-grained information about the evolution of quality metrics over time and the contributions of individual developers is known. We present an approach for tracking static analysis violations (which are often indicative of defects) over the revision history of a program, and for precisely attributing the introduction and elimination of these violations to individual developers. As one application, we demonstrate how this information can be used to compute ""fingerprints"" of developers that reflect which kinds of violations they tend to introduce or to fix. We have performed an experimental study on several large open-source projects written in different languages, providing evidence that these fingerprints are well-defined and capture characteristic information about the coding habits of individual developers.","","","ICSE '15"
"Conference Paper","Arnaoudova V,Haiduc S,Marcus A,Antoniol G","The Use of Text Retrieval and Natural Language Processing in Software Engineering","","2015","","","949–950","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.","natural language processing, text retrieval","","ICSE '15"
"Conference Paper","Gray J,Chechik M,Kulkarni V,Paige RF","7th International Workshop on Modeling in Software Engineering (MiSE 2015)","","2015","","","985–986","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Models are an important tool in conquering the increasing complexity of modern software systems. Key industries are strategically directing their development environments towards more extensive use of modeling techniques. MiSE 2015 aimed to understand, through critical analysis, the current and future uses of models in the engineering of software-intensive systems. The MiSE workshop series has proven to be an effective forum for discussing modeling techniques from both the MDE and software engineering perspectives. An important goal of this workshop is to foster exchange between these two communities. In 2015 the focus was on considering the current state of tool support and the challenges that need to be addressed to improve the maturity of tools. There was also analysis of successful applications of modeling techniques in specific application domains, with attempts to determine how the participants' experiences can be carried over to other domains.","","","ICSE '15"
"Conference Paper","Olsson T,Toll D,Wingkvist A,Ericsson M","Evolution and Evaluation of the Model-View-Controller Architecture in Games","","2015","","","8–14","IEEE Press","Florence, Italy","Proceedings of the Fourth International Workshop on Games and Software Engineering","","2015","","","","","In game software it is important to separate gameplay code from rendering code to ease transitions to new technologies or different platforms. The architectural pattern Model-View-Controller (MVC) is commonly used to achieve such separation. We investigate how the MVC architectural pattern is implemented in five game projects from a small development studio. We define a metrics-based quality model to assess software quality goals such as portability and rendering engine independence and perform an architectural analysis. The analysis reveals three different evolutions of the pattern. We also assess the quality and find that 1. the evolutions of the architecture differ in quality and 2. an architectural refactoring to a newer version of the architecture increases the software quality.","","","GAS '15"
"Conference Paper","Vendome C","A Large Scale Study of License Usage on GitHub","","2015","","","772–774","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from GitHub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.","mining software repositories, empirical studies, software licenses","","ICSE '15"
"Conference Paper","Hauptmann B,Juergens E,Woinke V","Generating Refactoring Proposals to Remove Clones from Automated System Tests","","2015","","","115–124","IEEE Press","Florence, Italy","Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","","","Automated system tests often have many clones, which make them complex to understand and costly to maintain. Unfortunately, removing clones is challenging as there are numerous possibilities of how to refactor them to reuse components such as subroutines. Additionally, clones often overlap partly which makes it particularly difficult to decide which parts to extract. If done wrongly, reuse potential is not leveraged optimally and structures between tests and reuse components will become unnecessarily complex. We present a method to support test engineers in extracting overlapping clones. Using grammar inference algorithms, we generate a refactoring proposal that demonstrates test engineers how overlapping clones can be extracted. Furthermore, we visualize the generated refactoring proposal to make it easily understandable for test engineers. An industrial case study demonstrates that our approach helps test engineers to gain information of the reuse potential of test suites and guides them to perform refactorings.","automated testing, test clones, refactoring","","ICPC '15"
"Conference Paper","Vendome C,Linares-Vásquez M,Bavota G,Di Penta M,German D,Poshyvanyk D","License Usage and Changes: A Large-Scale Study of Java Projects on GitHub","","2015","","","218–228","IEEE Press","Florence, Italy","Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","","","Software licenses determine, from a legal point of view, under which conditions software can be integrated, used, and above all, redistributed. Licenses evolve over time to meet the needs of development communities and to cope with emerging legal issues and new development paradigms. Such evolution of licenses is likely to be accompanied by changes in the way how software uses such licenses, resulting in some licenses being adopted while others are abandoned. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developer change software licenses. Specifically, we first identify licenses' changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis---following a grounded theory approach---of commit notes and issue tracker discussions concerning licensing topics and, whenever possible, try to build traceability links between discussions and changes. Our results point out a lack of traceability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it.","software licenses, mining software repositories, empirical studies","","ICPC '15"
"Conference Paper","Tahir A,MacDonell SG","Combining Dynamic Analysis and Visualization to Explore the Distribution of Unit Test Suites","","2015","","","21–30","IEEE Press","Florence, Italy","Proceedings of the Sixth International Workshop on Emerging Trends in Software Metrics","","2015","","","","","As software systems have grown in scale and complexity the test suites built alongside those systems have also become increasingly complex. Understanding key aspects of test suites, such as their coverage of production code, is important when maintaining or reengineering systems. This work investigates the distribution of unit tests in Open Source Software (OSS) systems through the visualization of data obtained from both dynamic and static analysis. Our long-term aim is to support developers in their understanding of test distribution and the relationship of tests to production code. We first obtain dynamic coupling information from five selected OSS systems and we then map the test and production code results. The mapping is shown in graphs that depict both the dependencies between classes and static test information. We analyze these graphs using Centrality metrics derived from graph theory and SNA. Our findings suggest that, for these five systems at least, unit test and dynamic coupling information 'do not match', in that unit tests do not appear to be distributed in line with the systems' dynamic coupling. We contend that, by mapping dynamic coupling data onto unit test information, and through the use of software metrics and visualization, we can locate central system classes and identify to which classes unit testing effort has (or has not) been dedicated.","program comprehension, dynamic metrics, unit testing, test analysis, visualization, dynamic analysis","","WETSoM '15"
"Conference Paper","Linsbauer L,Fischer S,Lopez-Herrejon RE,Egyed A","Using Traceability for Incremental Construction and Evolution of Software Product Portfolios","","2015","","","57–60","IEEE Press","Florence, Italy","Proceedings of the 8th International Symposium on Software and Systems Traceability","","2015","","","","","Software reuse has become mandatory for companies to compete and a wide range of reuse techniques are available today. Despite the great benefits of these techniques, they also have the disadvantage that they do not necessarily support the creation and evolution of closely related products -- products that are customized to different infrastructures, ecosystems, machinery, or customers. In this paper we outline an approach for incrementally constructing and evolving software product portfolios of similar product variants. An initial proof of concept demonstrates its feasibility.","","","SST '15"
"Conference Paper","Ma L,Zhang C,Yu B,Sato H","An Empirical Study on Effects of Code Visibility on Code Coverage of Software Testing","","2015","","","80–84","IEEE Press","Florence, Italy","Proceedings of the 10th International Workshop on Automation of Software Test","","2015","","","","","Software testability is the degree of difficulty to test a program. Code visibility is important to support design principles, such as information hiding. It is widely believed that code visibility has effects on testability. However, little empirical evidence has been shown to clarify whether and how software testability is influenced by code visibility. We have performed an empirical study to shed light on this problem.Our study focuses on test code coverage, in particular that of automatic testing tools. Code coverage is commonly used for various purposes, such as evaluating test adequacy, assessing test quality, and analyzing testability. Our study uses code coverage as the concrete measurement of testability. By analyzing code coverage of two state-of-the-art tools, in comparison with that of developer-written tests, we have discovered that code visibility does not necessarily have effects on its code coverage, but significantly affects automatic testing tools. Low code visibility often leads to low code coverage for automatic tools. In addition, different treatments on code visibility can result in significant differences in overall code coverage for automatic tools. Using a tool enhancement specific to code visibility, we demonstrate the great potential to improve existing tools.","code coverage, automatic testing, software testability, code accessibility, code visibility","","AST '15"
"Conference Paper","Delaitre A,Stivalet B,Fong E,Okun V","Evaluating Bug Finders: Test and Measurement of Static Code Analyzers","","2015","","","14–20","IEEE Press","Florence, Italy","Proceedings of the First International Workshop on Complex FaUlts and Failures in LargE Software Systems","","2015","","","","","Software static analysis is one of many options for finding bugs in software. Like compilers, static analyzers take a program as input. This paper covers tools that examine source codewithout executing itand output bug reports. Static analysis is a complex and generally undecidable problem. Most tools resort to approximation to overcome these obstacles and it sometimes leads to incorrect results. Therefore, tool effectiveness needs to be evaluated. Several characteristics of the tools should be examined. First, what types of bugs can they find? Second, what proportion of bugs do they report? Third, what percentage of findings is correct? These questions can be answered by one or more metrics. But to calculate these, we need test cases having certain characteristics: statistical significance, ground truth, and relevance. Test cases with all three attributes are out of reach, but we can use combinations of only two to calculate the metrics.The results in this paper were collected during Static Analysis Tool Exposition (SATE) V, where participants ran 14 static analyzers on the test sets we provided and submitted their reports to us for analysis. Tools had considerably different support for most bug classes. Some tools discovered significantly more bugs than others or generated mostly accurate warnings, while others reported wrong findings more frequently. Using the metrics, an evaluator can compare candidates and select the tool that aligns best with his or her objectives. In addition, our results confirm that the bugs most commonly found by tools are among the most common and important bugs in software. We also observed that code complexity is a major hindrance for static analyzers and detailed which code constructs tools handle well and which impede their analysis.","static analysis tools, software faults, software vulnerability, software assurance","","COUFLESS '15"
"Conference Paper","Rajan H,Nguyen TN,Leavens GT,Dyer R","Inferring Behavioral Specifications from Large-Scale Repositories by Leveraging Collective Intelligence","","2015","","","579–582","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Despite their proven benefits, useful, comprehensible, and efficiently checkable specifications are not widely available. This is primarily because writing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available. Furthermore, the lack of specifications for widely-used libraries and frameworks, caused by the high cost of writing specifications, tends to have a snowball effect. Core libraries lack specifications, which makes specifying applications that use them expensive. To contain the skyrocketing development and maintenance costs of high assurance systems, this self-perpetuating cycle must be broken. The labor cost of specifying programs can be significantly decreased via advances in specification inference and synthesis, and this has been attempted several times, but with limited success. We believe that practical specification inference and synthesis is an idea whose time has come. Fundamental breakthroughs in this area can be achieved by leveraging the collective intelligence available in software artifacts from millions of open source projects. Finegrained access to such data sets has been unprecedented, but is now easily available. We identify research directions and report our preliminary results on advances in specification inference that can be had by using such data sets to infer specifications.","","","ICSE '15"
"Conference Paper","Gan J,Kok R,Kohli P,Ding Y,Mah B","Using Virtual Machine Protections to Enhance Whitebox Cryptography","","2015","","","17–23","IEEE Press","Florence, Italy","Proceedings of the 1st International Workshop on Software Protection","","2015","","","","","Since attackers can gain full control of the mobile execution environment, they are able to examine the inputs, outputs, and, with the help of a disassembler/debugger the result of every intermediate computation a cryptographic algorithm carries out. Essentially, attackers have total visibility into the cryptographic operation.Whitebox cryptography aims at protecting keys from disclosed in software implementation. With theoretically unbounded resources a determined attacker is able to recover any confidential keys and data. A strong whitebox cipher implementation as the cornerstone of security is essential for the overall security in mobile environments.Our goal is to provide an increased degree of protection given the constraints of a software-solution and the resource-constrained, hostile-host environments. We seek neither perfect protection nor long-term guarantees, but rather a practical level of protection to balance cost, security and usability. Regular software updates can be applied such that the protection will need to withstand a limited period of time. V-OS operates as a virtual machine (VM) within the native mobile operating system to provide a secure software environment within which to perform critical processes and computations for a mobile app.","data obfuscation, whitebox cryptography (WBC), virtual machine protections (VMP), software licensin, software renewability, anti-debugging, fingerprinting, mobile code, code obfuscation, anti-reverse engineering, software tamper resistance","","SPRO '15"
"Conference Paper","Kazman R,Cai Y,Mo R,Feng Q,Xiao L,Haziyev S,Fedak V,Shapochka A","A Case Study in Locating the Architectural Roots of Technical Debt","","2015","","","179–188","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces (DRSpaces). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.","","","ICSE '15"
"Conference Paper","Beyer S","DIETs: Recommender Systems for Mobile API Developers","","2015","","","859–862","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 2","","2015","","","","","The increasing number of posts related to mobile app development indicates unaddressed problems in the usage of mobile APIs. Arguing that these problems result from inadequate documentation and shortcomings in the design and implementation of the APIs, the goal of this research is to develop and evaluate two developers' issues elimination tools (DIETs) for mobile API developers to diminish the problems of mobile applications (apps) development.After categorizing the problems, we investigate their causes, by exploring the relationships between the topics and trends of posts on Stack Overflow, the app developers' experience, the API and test code, and its changes. The results of these studies will be used to develop two DIETs that support API developers to improve the documentation, design, and implementation of their APIs.","","","ICSE '15"
"Conference Paper","Barnett M,Bird C,Brunet J,Lahiri SK","Helping Developers Help Themselves: Automatic Decomposition of Code Review Changesets","","2015","","","134–144","IEEE Press","Florence, Italy","Proceedings of the 37th International Conference on Software Engineering - Volume 1","","2015","9781479919345","","","","Code Reviews, an important and popular mechanism for quality assurance, are often performed on a changeset, a set of modified files that are meant to be committed to a source repository as an atomic action. Understanding a code review is more difficult when the changeset consists of multiple, independent, code differences. We introduce ClusterChanges, an automatic technique for decomposing changesets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study.","","","ICSE '15"
"Journal Article","Carzaniga A,Gorla A,Perino N,Pezzè M","Automatic Workarounds: Exploiting the Intrinsic Redundancy of Web Applications","ACM Trans. Softw. Eng. Methodol.","2015","24","3","","Association for Computing Machinery","New York, NY, USA","","","2015-05","","1049-331X","https://doi.org/10.1145/2755970;http://dx.doi.org/10.1145/2755970","10.1145/2755970","Despite the best intentions, the competence, and the rigorous methods of designers and developers, software is often delivered and deployed with faults. To cope with imperfect software, researchers have proposed the concept of self-healing for software systems. The ambitious goal is to create software systems capable of detecting and responding “autonomically” to functional failures, or perhaps even preempting such failures, to maintain a correct functionality, possibly with acceptable degradation. We believe that self-healing can only be an expression of some form of redundancy, meaning that, to automatically fix a faulty behavior, the correct behavior must be already present somewhere, in some form, within the software system either explicitly or implicitly. One approach is to deliberately design and develop redundant systems, and in fact this kind of deliberate redundancy is the essential ingredient of many fault tolerance techniques. However, this type of redundancy is also generally expensive and does not always satisfy the time and cost constraints of many software projects.With this article we take a different approach. We observe that modern software systems naturally acquire another type of redundancy that is not introduced deliberately but rather arises intrinsically as a by-product of modern modular software design. We formulate this notion of intrinsic redundancy and we propose a technique to exploit it to achieve some level of self-healing. We first demonstrate that software systems are indeed intrinsically redundant. Then we develop a way to express and exploit this redundancy to tolerate faults with automatic workarounds. In essence, a workaround amounts to replacing some failing operations with alternative operations that are semantically equivalent in their intended effect, but that execute different code and ultimately avoid the failure. The technique we propose finds such workarounds automatically. We develop this technique in the context of Web applications. In particular, we implement this technique within a browser extension, which we then use in an evaluation with several known faults and failures of three popular Web libraries. The evaluation demonstrates that automatic workarounds are effective: out of the nearly 150 real faults we analyzed, 100 could be overcome with automatic workarounds, and half of these workarounds found automatically were not publicly known before.","Web API, Automatic workarounds, Web applications","",""
"Journal Article","Mkaouer W,Kessentini M,Shaout A,Koligheu P,Bechikh S,Deb K,Ouni A","Many-Objective Software Remodularization Using NSGA-III","ACM Trans. Softw. Eng. Methodol.","2015","24","3","","Association for Computing Machinery","New York, NY, USA","","","2015-05","","1049-331X","https://doi.org/10.1145/2729974;http://dx.doi.org/10.1145/2729974","10.1145/2729974","Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.","Search-based software engineering, software maintenance, remodularization, software quality","",""
"Conference Paper","Gupta S,Gupta BB","PHP-Sensor: A Prototype Method to Discover Workflow Violation and XSS Vulnerabilities in PHP Web Applications","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th ACM International Conference on Computing Frontiers","Ischia, Italy","2015","9781450333580","","https://doi.org/10.1145/2742854.2745719;http://dx.doi.org/10.1145/2742854.2745719","10.1145/2742854.2745719","As the usage of web applications for security-sensitive facilities has enlarged, the quantity and cleverness of web-based attacks against the web applications have grown-up as well. Several annual cyber security reports revealed that modern web applications suffer from two main categories of attacks: Workflow Violation Attacks and Cross-Site Scripting (XSS) attacks. Presently, in comparison to XSS attacks, there have been actual restricted work carried out that discover workflow violation attacks, as web application logic errors are particular to the expected functionality of a specific web application.This paper presents PHP-Sensor, a novel defensive model that discovers both the vulnerabilities of workflow violation attack and XSS attack concurrently in the real world PHP web applications. For the workflow violation attack, we extract a certain set of axioms by monitoring the sequences of HTTP request/responses and their corresponding session variables during the offline mode. The set of axioms is then utilized for evaluating the HTTP request/response in online mode. Any HTTP request/ response that bypass the corresponding axiom is recognized as a workflow violation attack in PHP web application. For the XSS attack, PHP-Sensor discovers the self-propagating features of XSS worms by monitoring the outgoing HTTP web request with the scripts that are injected in the currently HTTP response web page. We develop prototype of our proposed defensive model on the web proxy as well as on the client-side for the recognition of workflow violation and XSS attacks respectively. We evaluate the detection capability of PHP-Sensor on open source real-world PHP web applications and the simulation outcomes reveal that our defensive model is efficient and feasible at discovering workflow violation attacks, XSS attacks and experiences tolerable performance overhead.","web application security, PHP, workflow violation attacks, XSS attacks, axioms, HTTP","","CF '15"
"Conference Paper","Charpentier A,Falleri JR,Lo D,Réveillère L","An Empirical Assessment of Bellon's Clone Benchmark","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering","Nanjing, China","2015","9781450333504","","https://doi.org/10.1145/2745802.2745821;http://dx.doi.org/10.1145/2745802.2745821","10.1145/2745802.2745821","Context: Clone benchmarks are essential to the assessment and improvement of clone detection tools and algorithms. Among existing benchmarks, Bellon's benchmark is widely used by the research community. However, a serious threat to the validity of this benchmark is that reference clones it contains have been manually validated by Bellon alone. Other persons may disagree with Bellon's judgment. Objective: In this paper, we perform an empirical assessment of Bellon's benchmark. Method: We seek the opinion of eighteen participants on a subset of Bellon's benchmark to determine if researchers should trust the reference clones it contains. Results: Our experiment shows that a significant amount of the reference clones are debatable, and this phenomenon can introduce noise in results obtained using this benchmark.","code clone, empirical study, software metrics","","EASE '15"
"Conference Paper","Elish MO,Al-Ghamdi Y","Fault Density Analysis of Object-Oriented Classes in Presence of Code Clones","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering","Nanjing, China","2015","9781450333504","","https://doi.org/10.1145/2745802.2745811;http://dx.doi.org/10.1145/2745802.2745811","10.1145/2745802.2745811","Code cloning has been a typical practice during software development, by which code fragments are reused with or without changes by copying and pasting. It has been a questionable issue whether cloning has a destructive impact or not on software development and the quality of the delivered software. This paper empirically investigates the relationship between code clones and fault density of object-oriented classes. More than 3000 classes from five open source software systems were analyzed. The results suggest that classes that have clones were less fault dense on average than the classes that do not have clones. However, there was no association between intra/inter-class clone fragments within a class and its fault density. The results also indicate that among the groups of classes that have only one type of clones, the group of classes with Type III clones was the least fault dense. Minor, although statistically significant, correlations were observed between many code clone metrics and class fault density. Furthermore, the fault density predictive powers of these metrics were found to be almost the same. However, no improvement in the accuracy of fault density prediction models was observed when these metrics were used as inputs.","code cloning, software quality, fault density, software metrics","","EASE '15"
"Conference Paper","Javed MA,Zdun U","On the Effects of Traceability Links in Differently Sized Software Systems","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering","Nanjing, China","2015","9781450333504","","https://doi.org/10.1145/2745802.2745812;http://dx.doi.org/10.1145/2745802.2745812","10.1145/2745802.2745812","Context: None of the published empirical studies on software traceability have comparatively examined the support for differently sized systems. Objective: This paper reports on two controlled experiments performed with two Enterprise Service Bus (ESB) systems that are comparable in terms of support features and system structure, but are different in their size, in particular, UltraESB Version 2.3.0 and PetalsESB Version 4.2.0, to investigate the effects of system size on the use of traceability links. Method: We conducted two controlled experiments in which the same impact evaluation activities were performed and measured how the control groups (provided with no traceability information) and the experiment groups (provided with traceability information) performed these activities in terms of the quantity and quality of retrieved elements. Results: Our findings show that the 133.71% larger size of one of ESBs does not have a significant influence on the quantity and quality of retrieved elements in the experiment groups. In the control groups, in contrast, this increase in system size significantly increases the quantity of incorrect elements and reduces the overall quality of elements retrieved, while no conclusive evidence concerning the quantity of missing elements was found. Conclusion: It is concluded that traceability is more important in larger software systems.","traceability, impact analysis, differently sized systems, controlled experiment, empirical software engineering","","EASE '15"
"Conference Paper","Zhou Y,Zhang H,Huang X,Yang S,Babar MA,Tang H","Quality Assessment of Systematic Reviews in Software Engineering: A Tertiary Study","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering","Nanjing, China","2015","9781450333504","","https://doi.org/10.1145/2745802.2745815;http://dx.doi.org/10.1145/2745802.2745815","10.1145/2745802.2745815","Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.","software engineering, quality assessment, systematic (literature) review","","EASE '15"
"Conference Paper","Gonzalez H,Kadir AA,Stakhanova N,Alzahrani AJ,Ghorbani AA","Exploring Reverse Engineering Symptoms in Android Apps","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighth European Workshop on System Security","Bordeaux, France","2015","9781450334792","","https://doi.org/10.1145/2751323.2751330;http://dx.doi.org/10.1145/2751323.2751330","10.1145/2751323.2751330","The appearance of the Android platform and its popularity has resulted in a sharp rise in the number of reported vulnerabilities and consequently in the number of mobile threats. Leveraging openness of Android app markets and the lack of security testing, malware authors commonly plagiarize Android applications (e.g., through code reuse and repackaging) boosting the amount of malware on the markets and consequently the infection rate. In this study, we present AndroidSOO, a lightweight approach for the detection of repackaging symptoms on Android apps. In this work, we introduce and explore novel and easily extractable attribute called String Offset Order. Extractable from string identifiers list in the .dex file, the method is able to pinpoint symptoms of reverse engineered Android apps without the need for complex further analysis. We performed extensive evaluation of String Order metric to assess its capabilities on datasets made available by three recent studies: Android Malware Genome Project, DroidAnalytics and Drebin. We also performed a large-scale study of over 5,000 Android applications extracted from Google Play market and over 80 000 samples from Virus Total service.","malware, privacy, Android","","EuroSec '15"
"Conference Paper","Huang P,Bolosky WJ,Singh A,Zhou Y","ConfValley: A Systematic Configuration Validation Framework for Cloud Services","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth European Conference on Computer Systems","Bordeaux, France","2015","9781450332385","","https://doi.org/10.1145/2741948.2741963;http://dx.doi.org/10.1145/2741948.2741963","10.1145/2741948.2741963","Studies and many incidents in the headlines suggest misconfigurations remain a major cause of unavailability in large systems despite the large amount of work put into detecting, diagnosing and repairing them. In part, this is because many of the solutions are either post-mortem or too expensive to use in production cloud-scale systems. Configuration validation is the process of explicitly defining specifications and proactively checking configurations against those specifications to prevent misconfigurations from entering production.We propose a generic framework, ConfValley, to make configuration validation easy, systematic and efficient, and to allow configuration validation as an ordinary part of system deployment. ConfValley consists of a declarative language for practitioners to express configuration specifications, an inference engine that automatically generates specifications, and a checker that determines if a given configuration obeys its specifications. Our specification language expressed the configuration validation code from Microsoft Azure in 10x fewer lines, many of which were automatically inferred. Using expert-written and inferred specifications, we detected a number of configuration errors in the latest configurations deployed in Microsoft Azure.","","","EuroSys '15"
"Conference Paper","Lestringant P,Guihéry F,Fouque PA","Automated Identification of Cryptographic Primitives in Binary Code with Data Flow Graph Isomorphism","","2015","","","203–214","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security","Singapore, Republic of Singapore","2015","9781450332453","","https://doi.org/10.1145/2714576.2714639;http://dx.doi.org/10.1145/2714576.2714639","10.1145/2714576.2714639","Softwares use cryptographic algorithms to secure their communications and to protect their internal data. However the algorithm choice, its implementation design and the generation methods of its input parameters may have dramatic consequences on the security of the data it was initially supposed to protect. Therefore to assess the security of a binary program involving cryptography, analysts need to check that none of these points will cause a system vulnerability. It implies, as a first step, to precisely identify and locate the cryptographic code in the binary program. Since binary analysis is a difficult and cumbersome task, it is interesting to devise a method to automatically retrieve cryptographic primitives and their parameters.In this paper, we present a novel approach to automatically identify symmetric cryptographic algorithms and their parameters inside binary code. Our approach is static and based on DFG isomorphism. To cope with binary codes produced from different source codes and by different compilers and options, the DFG is normalized using code rewrite mechanisms. Our approach differs from previous works, that either use statistical criteria leading to imprecise results, or rely on heavy dynamic instrumentation. To validate our approach, we present experimental results on a set of synthetic samples including several cryptographic algorithms, binary code of well-known cryptographic libraries and reference source implementation compiled using different compilers and options.","reverse engineering, static binary analysis, cryptography","","ASIA CCS '15"
"Conference Paper","Krutz DE,Malachowsky SA,Shihab E","Examining the Effectiveness of Using Concolic Analysis to Detect Code Clones","","2015","","","1610–1615","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Annual ACM Symposium on Applied Computing","Salamanca, Spain","2015","9781450331968","","https://doi.org/10.1145/2695664.2695929;http://dx.doi.org/10.1145/2695664.2695929","10.1145/2695664.2695929","During the initial construction and subsequent maintenance of an application, duplication of functionality is common, whether intentional or otherwise. This replicated functionality, known as a code clone, has a diverse set of causes and can have moderate to severe adverse effects on a software project in a variety of ways. A code clone is defined as multiple code fragments that produce similar results when provided the same input. While there is an array of powerful clone detection tools, most suffer from a variety of drawbacks including, most importantly, the inability to accurately and reliably detect the more difficult clone types.This paper presents a new technique for detecting code clones based on concolic analysis, which uses a mixture of concrete and symbolic values to traverse a large and diverse portion of the source code. By performing concolic analysis on the targeted source code and then examining the holistic output for similarities, code clone candidates can be consistently identified. We found that concolic analysis was able to accurately and reliably discover all four types of code clones with an average precision of .8, recall of .91, F-score of .85 and an accuracy of .99.","software engineering, concolic analysis, code clones","","SAC '15"
"Conference Paper","Almeida D,Campos JC,Saraiva J,Silva JC","Towards a Catalog of Usability Smells","","2015","","","175–181","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Annual ACM Symposium on Applied Computing","Salamanca, Spain","2015","9781450331968","","https://doi.org/10.1145/2695664.2695670;http://dx.doi.org/10.1145/2695664.2695670","10.1145/2695664.2695670","This paper presents a catalog of smells in the context of interactive applications. These so-called usability smells are indicators of poor design on an application's user interface, with the potential to hinder not only its usability but also its maintenance and evolution. To eliminate such usability smells we discuss a set of program/usability refactorings. In order to validate the presented usability smells catalog, and the associated refactorings, we present a preliminary empirical study with software developers in the context of a real open source hospital management application. Moreover, a tool that computes graphical user interface behavior models, giving the applications' source code, is used to automatically detect usability smells at the model level.","code smells, empirical studies, graphical user interfaces","","SAC '15"
"Conference Paper","Santos JA,de Mendonça MG","Exploring Decision Drivers on God Class Detection in Three Controlled Experiments","","2015","","","1472–1479","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Annual ACM Symposium on Applied Computing","Salamanca, Spain","2015","9781450331968","","https://doi.org/10.1145/2695664.2695682;http://dx.doi.org/10.1145/2695664.2695682","10.1145/2695664.2695682","Context: Code smells define potential problems in design of software. However, some empirical studies on the topic have shown findings in opposite direction. The misunderstanding is mainly caused by lack of works focusing on human role on code smell detection. Objective: Our aim is to build empirical support to exploration of the human role on code smell detection. specifically, we investigated what issues in code make a human identify a class as a code smell. We called these issues decision drivers. Method: We performed a controlled experiment and replicated it twice. We asked participants to detect god class (one of the most known smell) on different software, indicating what decision drivers they adopted. Results: The stronger drivers were ""class is high complex"" and ""method is misplaced"". We also found the agreement on drivers' choice is low. Another finding is: some important drivers are dependent of alternative support. In our case, ""dependency"" was an important driver only when visual resources were permitted. Conclusion: This study contributes with the comprehension of the human role on smell detection through the exploration of decision drivers. This perception contributes to characterize what we called the ""code smell conceptualization problem"".","decision drivers, controlled experiment, code smell, god class, empirical study","","SAC '15"
"Conference Paper","Chakroun I,Vander Aa T,De Fraine B,Haber T,Wuyts R,Demeuter W","ExaShark: A Scalable Hybrid Array Kit for Exascale Simulation","","2015","","","41–48","Society for Computer Simulation International","San Diego, CA, USA","Proceedings of the Symposium on High Performance Computing","Alexandria, Virginia","2015","9781510801011","","","","Many problems in high-performance computing, such as stencil applications in iterative solvers or in particle-based simulations, have a need for regular distributed grids. Libraries offering such n-dimensional regular grids need to take advantage of the latest high-performance computing technologies and scale to exascale systems, but also need to be usable by non HPC experts. This paper presents ExaShark: a library for handling n-dimensional distributed data structures that strikes a balance between performance and usability. ExaShark is an open source middleware, offered as a library, targeted at reducing the increasing programming burden on heterogeneous current and future exascale architectures. It offers its users a global-array-like usability while its runtime can be configured to use shared memory threading techniques (Pthreads, OpenMP, TBB), inter-node distribution techniques (MPI, GPI), or combinations of both. ExaShark has been used to develop applications such as a Jacobian 2D heat simulation and advanced pipelined conjugate gradient solvers. These applications are used to demonstrate the performance and usability of ExaShark.","numerical solvers, N-dimensional grids, middleware library, partitioned global address space, exascale simulation","","HPC '15"
"Conference Paper","Denil J,Meyers B,De Meulenaere P,Vangheluwe H","Explicit Semantic Adaptation of Hybrid Formalisms for FMI Co-Simulation","","2015","","","99–106","Society for Computer Simulation International","San Diego, CA, USA","Proceedings of the Symposium on Theory of Modeling & Simulation: DEVS Integrative M&S Symposium","Alexandria, Virginia","2015","9781510801059","","","","With the advent of Software-Intensive and Cyber-Physical Systems, hybrid formalisms can be used to intuitively model the interactions of different models in different formalisms. Hybrid formalisms combine discrete (time/event) model constructs with continuous-time model constructs. These hybrid formalisms usually require a dedicated simulator. In this work we explicitly model the interfaces involved in the semantic adaptation of different formalisms and implement the execution using the Functional Mock-up Interface standard for co-simulation. The interfaces and co-simulation units are automatically generated using transformations. On the one hand, this allows tool builders to reuse the existing simulation tools without the need to create a new simulation kernel for the hybrid formalism. On the other hand, our approach supports the generation of different bus architectures to address different constraints, such as the use of hardware in the loop, the API of the legacy simulator, bus or processor load performance, and real-time constraints. We apply our approach to the modelling and (co-)simulation of an automotive power window.","functional mock-up interface, co-simulation, MDE, semantic adaptation, heterogeneous modelling","","DEVS '15"
"Conference Paper","Guimarães E,Garcia A,Cai Y","Architecture-Sensitive Heuristics for Prioritizing Critical Code Anomalies","","2015","","","68–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th International Conference on Modularity","Fort Collins, CO, USA","2015","9781450332491","","https://doi.org/10.1145/2724525.2724567;http://dx.doi.org/10.1145/2724525.2724567","10.1145/2724525.2724567","The progressive insertion of code anomalies in evolving software systems might lead to architecture degradation symptoms. Code anomalies are particularly harmful when they contribute to the architecture degradation. Although several approaches have been proposed aiming to detect anomalies in the source code, most of them fail to assist developers when prioritizing code anomalies critical to the architectural design. Blueprints of the architecture design are artifacts often available in industry software projects. However, such blueprints are rarely explored to support the prioritization of code anomalies in terms of their architecture relevance. This paper proposes and evaluates 2 sets of blueprint-based heuristics for supporting the prioritization of critical code anomalies. The prioritization is based on their potential impact on revealing architectural drift problems. The heuristics allow developers prioritizing critical code anomalies by exploiting architectural information provided in the blueprint. The contributions of this paper include: (i) a set of architecture sensitive heuristics to support developers when prioritizing critical code anomalies; (ii) an evaluation of the proposed heuristics in terms of their prioritization accuracy in 3 systems; and (iii) an empirical analysis on how the blueprints’ information might enhance the prioritization of critical code anomalies, as opposed to existing heuristics strictly based on source code analysis.","Heuristics, Component, Software Architecture, Empirical Evaluation, Code Anomalies, Blueprints","","MODULARITY 2015"
"Journal Article","Blem E,Menon J,Vijayaraghavan T,Sankaralingam K","ISA Wars: Understanding the Relevance of ISA Being RISC or CISC to Performance, Power, and Energy on Modern Architectures","ACM Trans. Comput. Syst.","2015","33","1","","Association for Computing Machinery","New York, NY, USA","","","2015-03","","0734-2071","https://doi.org/10.1145/2699682;http://dx.doi.org/10.1145/2699682","10.1145/2699682","RISC versus CISC wars raged in the 1980s when chip area and processor design complexity were the primary constraints and desktops and servers exclusively dominated the computing landscape. Today, energy and power are the primary design constraints and the computing landscape is significantly different: Growth in tablets and smartphones running ARM (a RISC ISA) is surpassing that of desktops and laptops running x86 (a CISC ISA). Furthermore, the traditionally low-power ARM ISA is entering the high-performance server market, while the traditionally high-performance x86 ISA is entering the mobile low-power device market. Thus, the question of whether ISA plays an intrinsic role in performance or energy efficiency is becoming important again, and we seek to answer this question through a detailed measurement-based study on real hardware running real applications. We analyze measurements on seven platforms spanning three ISAs (MIPS, ARM, and x86) over workloads spanning mobile, desktop, and server computing. Our methodical investigation demonstrates the role of ISA in modern microprocessors’ performance and energy efficiency. We find that ARM, MIPS, and x86 processors are simply engineering design points optimized for different levels of performance, and there is nothing fundamentally more energy efficient in one ISA class or the other. The ISA being RISC or CISC seems irrelevant.","Power, technology scaling, energy efficiency","",""
"Conference Paper","Wood Z,Keen A","Building Worlds: Bridging Imperative-First and Object-Oriented Programming in CS1-CS2","","2015","","","144–149","Association for Computing Machinery","New York, NY, USA","Proceedings of the 46th ACM Technical Symposium on Computer Science Education","Kansas City, Missouri, USA","2015","9781450329668","","https://doi.org/10.1145/2676723.2677249;http://dx.doi.org/10.1145/2676723.2677249","10.1145/2676723.2677249","When teaching introductory computing courses, we are often guilty of writing rudimentary programming assignments - those meant to illustrate one simple language feature, comprised mostly of code that will never be used beyond the assignment. Admittedly, first-year computing students must navigate a myriad of challenges, sometimes learning both imperative and object-oriented programming, in addition to mastering syntax, logic,debugging, and testing. To tackle the difficulties of developing CS 1 and CS 2 courses that engage students in learning while addressing the numerous course objectives, we chose to challenge students to create virtual worlds in one large comprehensive two-quarter long programming project. Students were granted creative freedom within a framework that gradually introduced many programming skills and that required the mastery of object-oriented programming and some engaging algorithms. We present the curriculum, performance comparisons, and observations. Overall, we consider the experimental courses a success that will have an impact on our department's future curricular offerings.","object-oriented programming, introductory programming","","SIGCSE '15"
"Conference Paper","Wirth M","A Descent into the Maelstrom: Teaching Legacy Programming and Re-Engineering","","2015","","","156–161","Association for Computing Machinery","New York, NY, USA","Proceedings of the 46th ACM Technical Symposium on Computer Science Education","Kansas City, Missouri, USA","2015","9781450329668","","https://doi.org/10.1145/2676723.2677213;http://dx.doi.org/10.1145/2676723.2677213","10.1145/2676723.2677213","Computer science is a discipline which has swiftly evolved since its inception in the 1950s. This has invariably meant that new courses introduced into a curriculum are often of the bleeding-edge sort: genetic algorithms, parallel processing, mobile-based applications. Programming languages too have changed as progress ensues. Yet little is discussed of the darker side of computer science - the huge repositories of what some term legacy software running our financial, scientific and engineering systems. This paper looks at the introduction of a course which teaches legacy languages and the process of re-engineering with the aim to instill a reverence for the role legacy software plays in the modern world.","fortran, cobol, ada, reengineering, legacy software","","SIGCSE '15"
"Conference Paper","Hooshangi S,Weiss R,Cappos J","Can the Security Mindset Make Students Better Testers?","","2015","","","404–409","Association for Computing Machinery","New York, NY, USA","Proceedings of the 46th ACM Technical Symposium on Computer Science Education","Kansas City, Missouri, USA","2015","9781450329668","","https://doi.org/10.1145/2676723.2677268;http://dx.doi.org/10.1145/2676723.2677268","10.1145/2676723.2677268","Writing secure code requires a programmer to think both as a defender and an attacker. One can draw a parallel between this model of thinking and techniques used in test-driven development, where students learn by thinking about how to effectively test their code and anticipate possible bugs. In this study, we analyzed the quality of both attack and defense code that students wrote for an assignment given in an introductory security class of 75 (both graduate and senior undergraduate levels) at NYU. We made several observations regarding students' behaviors and the quality of both their defensive and offensive code. We saw that student defensive programs (i.e., assignments) are highly unique and that their attack programs (i.e., test cases) are also relatively unique. In addition, we examined how student behaviors in writing defense programs correlated with their attack program's effectiveness. We found evidence that students who learn to write good defensive programs can write effective attack programs, but the converse is not true. While further exploration of causality is needed, our results indicate that a greater pedagogical emphasis on defensive security may benefit students more than one that emphasizes offense.","security, python, testing, access control","","SIGCSE '15"
"Conference Paper","Atre R,Jannesari A,Wolf F","The Basic Building Blocks of Parallel Tasks","","2015","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 International Workshop on Code Optimisation for Multi and Many Cores","San Francisco Bay Area, CA, USA","2015","9781450333160","","https://doi.org/10.1145/2723772.2723778;http://dx.doi.org/10.1145/2723772.2723778","10.1145/2723772.2723778","Discovery of parallelization opportunities in sequential programs can greatly reduce the time and effort required to parallelize any application. Identification and analysis of code that contains little to no internal parallelism can also help expose potential parallelism. This paper provides a technique to identify a block of code called Computational Unit (CU) that performs a unit of work in a program. A CU can assist in discovering the potential parallelism in a sequential program by acting as a basic building block for tasks. CUs are used along with dynamic analysis information to identify the tasks that contain tightly coupled code within them. This process in turn reveals the tasks that are weakly dependent or independent. The independent tasks can be run in parallel and the dependent tasks can be analyzed to check if the dependences can be resolved. To evaluate our technique, different benchmark applications are parallelized using our identified tasks and the speedups are reported. In addition, existing parallel implementations of the applications are compared with the identified tasks for the respective applications.","","","COSMIC '15"
"Journal Article","Makedonski P,Sudau F,Grabowski J","Towards a Model-Based Software Mining Infrastructure","SIGSOFT Softw. Eng. Notes","2015","40","1","1–8","Association for Computing Machinery","New York, NY, USA","","","2015-02","","0163-5948","https://doi.org/10.1145/2693208.2693224;http://dx.doi.org/10.1145/2693208.2693224","10.1145/2693208.2693224","Software mining is concerned with two primary goals: the extraction of basic facts from software repositories and the derivation of knowledge resulting from the assessment of the basic facts. Facts extraction approaches rely on custom and task-specific infrastructures and tools. The resulting facts assets are usually represented in heterogeneous formats at a low level of abstraction. Due to this, facts extracted from different sources are also not well integrated, even if they are related. To manage this, existing infrastructures often aim at supporting an all-in-one information meta-structures which try to integrate all facts in one connected whole. We propose a generic infrastructure that translates extracted facts to homogeneous high-level representations conforming to domain-specific metamodels, and then transforms these high-level model instances to instances of domain-specific models related to a particular assessment task, which can be incrementally enriched with additional facts as these become available or necessary. This allows researchers and practitioners to focus on the assessment task at hand, without being concerned with low-level representation details or complex data models containing large amounts of often irrelevant data. We present an example scenario with a concrete instantiation of the proposed infrastructure targeting the assessment of developer behaviour.","Modeling, data integration, domain modeling, data mining, facts extraction, Infrastructure, Mining","",""
"Journal Article","Bokil P,Krishnan P,Venkatesh R","Achieving Effective Test Suites for Reactive Systems Using Specification Mining and Test Suite Reduction Techniques","SIGSOFT Softw. Eng. Notes","2015","40","1","1–8","Association for Computing Machinery","New York, NY, USA","","","2015-02","","0163-5948","https://doi.org/10.1145/2693208.2693226;http://dx.doi.org/10.1145/2693208.2693226","10.1145/2693208.2693226","Failures in reactive embedded systems are often unacceptable. Moreover, effective testing of such systems to detect potential critical failures is a difficult task.We present an automated black box test suite generation technique for reactive systems. The technique is based on dynamic mining of specifications, in form of a finite state machine (FSM), from initial runs. The set of test cases thus produced contain several redundant test cases, many of which are eliminated by a simple greedy test suite reduction algorithm to give the final test suite. The effectiveness of tests generated by our technique was evaluated using five case studies from the reactive embedded domain. Results indicate that a test suite generated by our technique is promising in terms of effectiveness and scalability. While the test suite reduction algorithm removes redundant test cases, the change in effectiveness of test suites due to this reduction is examined in the experimentation.We present our specification mining based test suite generation technique, the test suite reduction technique and results on industrial case studies.","test suite reduction, specification mining, black box testing","",""
"Conference Paper","Chen N,Hoi SC,Li S,Xiao X","SimApp: A Framework for Detecting Similar Mobile Applications by Online Kernel Learning","","2015","","","305–314","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighth ACM International Conference on Web Search and Data Mining","Shanghai, China","2015","9781450333177","","https://doi.org/10.1145/2684822.2685305;http://dx.doi.org/10.1145/2684822.2685305","10.1145/2684822.2685305","With the popularity of smart phones and mobile devices, the number of mobile applications (a.k.a. ""apps"") has been growing rapidly. Detecting semantically similar apps from a large pool of apps is a basic and important problem, as it is beneficial for various applications, such as app recommendation, app search, etc. However, there is no systematic and comprehensive work so far that focuses on addressing this problem. In order to fill this gap, in this paper, we explore multi-modal heterogeneous data in app markets (e.g., description text, images, user reviews, etc.), and present ""SimApp"" -- a novel framework for detecting similar apps using machine learning. Specifically, it consists of two stages: (i) a variety of kernel functions are constructed to measure app similarity for each modality of data; and (ii) an online kernel learning algorithm is proposed to learn the optimal combination of similarity functions of multiple modalities. We conduct an extensive set of experiments on a real-world dataset crawled from Google Play to evaluate SimApp, from which the encouraging results demonstrate that SimApp is effective and promising.","similarity function, online kernel learning, multiple kernels, multi-modal data, mobile applications","","WSDM '15"
"Conference Paper","Benson AR,Ballard G","A Framework for Practical Parallel Fast Matrix Multiplication","","2015","","","42–53","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming","San Francisco, CA, USA","2015","9781450332057","","https://doi.org/10.1145/2688500.2688513;http://dx.doi.org/10.1145/2688500.2688513","10.1145/2688500.2688513","Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.","shared memory, parallel linear algebra, fast matrix multiplication, dense linear algebra","","PPoPP 2015"
"Journal Article","Benson AR,Ballard G","A Framework for Practical Parallel Fast Matrix Multiplication","SIGPLAN Not.","2015","50","8","42–53","Association for Computing Machinery","New York, NY, USA","","","2015-01","","0362-1340","https://doi.org/10.1145/2858788.2688513;http://dx.doi.org/10.1145/2858788.2688513","10.1145/2858788.2688513","Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.","dense linear algebra, parallel linear algebra, fast matrix multiplication, shared memory","",""
"Journal Article","Huang Y,Zhao M,Xue CJ","Joint WCET and Update Activity Minimization for Cyber-Physical Systems","ACM Trans. Embed. Comput. Syst.","2015","14","1","","Association for Computing Machinery","New York, NY, USA","","","2015-01","","1539-9087","https://doi.org/10.1145/2680539;http://dx.doi.org/10.1145/2680539","10.1145/2680539","A cyber-physical system (CPS) is a desirable computing platform for many industrial and scientific applications, such as industrial process monitoring, environmental monitoring, chemical processes, and battlefield surveillance. The application of CPSs has two challenges: First, CPSs often include a number of sensor nodes. Update of preloaded code on remote sensor nodes powered by batteries is extremely energy consuming. The code update issue in the energy-sensitive CPS must be carefully considered. Second, CPSs are often real-time embedded systems with real-time properties. Worst-case execution time (WCET) is one of the most important metrics in real-time system design. Whereas existing works only consider one of these two challenges at a time, in this article, a compiler optimization—joint WCET and update-conscious compilation, or WUCC—is proposed to jointly consider WCET and code update for CPSs. The novelty of the proposed approach is that the WCET problem and code update problem are considered concurrently such that a balanced solution with minimal WCET and minimal code difference can be achieved. The experimental results show that the proposed technique can minimize WCET and code difference effectively.","cyber-physical system, compilation, WCET, code similarity, register allocation","",""
"Conference Paper","Fenske W,Schulze S","Code Smells Revisited: A Variability Perspective","","2015","","","3–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems","Hildesheim, Germany","2015","9781450332736","","https://doi.org/10.1145/2701319.2701321;http://dx.doi.org/10.1145/2701319.2701321","10.1145/2701319.2701321","Highly-configurable software systems (also called software product lines) gain momentum in both, academia and industry. For instance, the Linux kernel comes with over 12 000 configuration options and thus, can be customized to run on nearly every kind of system. To a large degree, this configurability is achieved through variable code structures, for instance, using conditional compilation. Such source code variability adds a new dimension of complexity, thus giving rise to new possibilities for design flaws. Code smells are an established concept to describe design flaws or decay in source code. However, existing smells have no notion of variability and thus do not support flaws regarding variable code structures. In this paper, we propose an initial catalog of four variability-aware code smells. We discuss the appearance and negative effects of these smells and present code examples from real-world systems. To evaluate our catalog, we have conducted a survey amongst 15 researchers from the field of software product lines. The results confirm that our proposed smells (a) have been observed in existing product lines and (b) are considered to be problematic for common software development activities, such as program comprehension, maintenance, and evolution.","Code Smells, Design Defects, Software Product Lines, Variability","","VaMoS '15"
"Conference Paper","Weber JH,Katahoire A,Price M","Uncovering Variability Models for Software Ecosystems from Multi-Repository Structures","","2015","","","103–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems","Hildesheim, Germany","2015","9781450332736","","https://doi.org/10.1145/2701319.2701333;http://dx.doi.org/10.1145/2701319.2701333","10.1145/2701319.2701333","Variability is a significant source of complexity in many large-scale software systems. Software variability must be managed in order to effectively tame the arising complexity. Consequently, variability management processes are at the heart of current software product line engineering practices. However, legacy software systems exist that have not been developed with such practices. Moreover, an increasing amount of software is developed in large, fragmented communities, also referred to as software ecosystems. Variability in such systems is often not explicitly managed and causes significant difficulties during software maintenance and evolution. Methods and tools for uncovering and explicitly managing this variability have been subject to ongoing research. This paper presents our research in progress of empirically studying the application and combination of such methods in the context of real-world industrial case study in the health care domain.","multi-repository software, software product lines, variability management, software ecosystems","","VaMoS '15"
"Conference Paper","Dalla Preda M,Giacobazzi R,Lakhotia A,Mastroeni I","Abstract Symbolic Automata: Mixed Syntactic/Semantic Similarity Analysis of Executables","","2015","","","329–341","Association for Computing Machinery","New York, NY, USA","Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","Mumbai, India","2015","9781450333009","","https://doi.org/10.1145/2676726.2676986;http://dx.doi.org/10.1145/2676726.2676986","10.1145/2676726.2676986","We introduce a model for mixed syntactic/semantic approximation of programs based on symbolic finite automata (SFA). The edges of SFA are labeled by predicates whose semantics specifies the denotations that are allowed by the edge. We introduce the notion of abstract symbolic finite automaton (ASFA) where approximation is made by abstract interpretation of symbolic finite automata, acting both at syntactic (predicate) and semantic (denotation) level. We investigate in the details how the syntactic and semantic abstractions of SFA relate to each other and contribute to the determination of the recognized language. Then we introduce a family of transformations for simplifying ASFA. We apply this model to prove properties of commonly used tools for similarity analysis of binary executables. Following the structure of their control flow graphs, disassembled binary executables are represented as (concrete) SFA, where states are program points and predicates represent the (possibly infinite) I/O semantics of each basic block in a constraint form. Known tools for binary code analysis are viewed as specific choices of symbolic and semantic abstractions in our framework, making symbolic finite automata and their abstract interpretations a unifying model for comparing and reasoning about soundness and completeness of analyses of low-level code.","symbolic automata, abstract interpretation","","POPL '15"
"Journal Article","Dalla Preda M,Giacobazzi R,Lakhotia A,Mastroeni I","Abstract Symbolic Automata: Mixed Syntactic/Semantic Similarity Analysis of Executables","SIGPLAN Not.","2015","50","1","329–341","Association for Computing Machinery","New York, NY, USA","","","2015-01","","0362-1340","https://doi.org/10.1145/2775051.2676986;http://dx.doi.org/10.1145/2775051.2676986","10.1145/2775051.2676986","We introduce a model for mixed syntactic/semantic approximation of programs based on symbolic finite automata (SFA). The edges of SFA are labeled by predicates whose semantics specifies the denotations that are allowed by the edge. We introduce the notion of abstract symbolic finite automaton (ASFA) where approximation is made by abstract interpretation of symbolic finite automata, acting both at syntactic (predicate) and semantic (denotation) level. We investigate in the details how the syntactic and semantic abstractions of SFA relate to each other and contribute to the determination of the recognized language. Then we introduce a family of transformations for simplifying ASFA. We apply this model to prove properties of commonly used tools for similarity analysis of binary executables. Following the structure of their control flow graphs, disassembled binary executables are represented as (concrete) SFA, where states are program points and predicates represent the (possibly infinite) I/O semantics of each basic block in a constraint form. Known tools for binary code analysis are viewed as specific choices of symbolic and semantic abstractions in our framework, making symbolic finite automata and their abstract interpretations a unifying model for comparing and reasoning about soundness and completeness of analyses of low-level code.","abstract interpretation, symbolic automata","",""
"Conference Paper","Li J,Wang C,Xiong Y,Hu Z","SWIN: Towards Type-Safe Java Program Adaptation between APIs","","2015","","","91–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2015 Workshop on Partial Evaluation and Program Manipulation","Mumbai, India","2015","9781450332972","","https://doi.org/10.1145/2678015.2682534;http://dx.doi.org/10.1145/2678015.2682534","10.1145/2678015.2682534","Java program adaptation between different APIs is a common task in software development. When an old API is upgraded to an incompatible new version, or when we want to migrate an application from one platform to another platform, we need to adapt programs between different APIs. Although different program transformation tools have been developed to automate the program adaptation task, no tool ensures type safety in transforming Java programs: given a transformation program and any well-typed Java program, the transformed result is still well-typed. As a matter of fact, it is often observed that a dedicated adaptation tool turns a working application into a set of incompatible programs. We address this problem by providing a type-safe transformation language, SWIN, for Java program adaptation between different APIs. SWIN is based on Twinning, a modern transformation language for Java programs. SWIN enhances Twinning with more flexible transformation rules, formal semantics, and, most importantly, full type-safe guarantee. We formally prove the type safety of SWIN on Featherweight Java, a known minimal formal core of Java. Our experience with three case studies shows that SWIN is as expressive as Twinning in specifying useful program transformations in the case studies while guaranteeing the type safety of the transformations.","type safety, transformation language, program transformation, api adaptation","","PEPM '15"
"Journal Article","Hao D,Zhang L,Zhang L,Rothermel G,Mei H","A Unified Test Case Prioritization Approach","ACM Trans. Softw. Eng. Methodol.","2014","24","2","","Association for Computing Machinery","New York, NY, USA","","","2014-12","","1049-331X","https://doi.org/10.1145/2685614;http://dx.doi.org/10.1145/2685614","10.1145/2685614","Test case prioritization techniques attempt to reorder test cases in a manner that increases the rate at which faults are detected during regression testing. Coverage-based test case prioritization techniques typically use one of two overall strategies: a total strategy or an additional strategy. These strategies prioritize test cases based on the total number of code (or code-related) elements covered per test case and the number of additional (not yet covered) code (or code-related) elements covered per test case, respectively. In this article, we present a unified test case prioritization approach that encompasses both the total and additional strategies. Our unified test case prioritization approach includes two models (basic and extended) by which a spectrum of test case prioritization techniques ranging from a purely total to a purely additional technique can be defined by specifying the value of a parameter referred to as the fp value. To evaluate our approach, we performed an empirical study on 28 Java objects and 40 C objects, considering the impact of three internal factors (model type, choice offp value, and coverage type) and three external factors (coverage granularity, test case granularity, and programming/testing paradigm), all of which can be manipulated by our approach. Our results demonstrate that a wide range of techniques derived from our basic and extended models with uniform fp values can outperform purely total techniques and are competitive with purely additional techniques. Considering the influence of each internal and external factor studied, the results demonstrate that various values of each factor have nontrivial influence on test case prioritization techniques.","Software testing, additional strategy, total strategy, test case prioritization","",""
"Journal Article","Cheah YW,Plale B","Provenance Quality Assessment Methodology and Framework","J. Data and Information Quality","2014","5","3","","Association for Computing Machinery","New York, NY, USA","","","2014-12","","1936-1955","https://doi.org/10.1145/2665069;http://dx.doi.org/10.1145/2665069","10.1145/2665069","Data provenance, a form of metadata describing the life cycle of a data product, is crucial in the sharing of research data. Research data, when shared over decades, requires recipients to make a determination of both use and trust. That is, can they use the data? More importantly, can they trust it? Knowing the data are of high quality is one factor to establishing fitness for use and trust. Provenance can be used to assert the quality of the data, but the quality of the provenance must be known as well. We propose a framework for assessing the quality of data provenance. We identify quality issues in data provenance, establish key quality dimensions, and define a framework of analysis. We apply the analysis framework to synthetic and real-world provenance.","provenance quality, scientific workflow, Data provenance, graph analysis","",""
"Conference Paper","Pewny J,Schuster F,Bernhard L,Holz T,Rossow C","Leveraging Semantic Signatures for Bug Search in Binary Programs","","2014","","","406–415","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Annual Computer Security Applications Conference","New Orleans, Louisiana, USA","2014","9781450330053","","https://doi.org/10.1145/2664243.2664269;http://dx.doi.org/10.1145/2664243.2664269","10.1145/2664243.2664269","Software vulnerabilities still constitute a high security risk and there is an ongoing race to patch known bugs. However, especially in closed-source software, there is no straightforward way (in contrast to source code analysis) to find buggy code parts, even if the bug was publicly disclosed.To tackle this problem, we propose a method called Tree Edit Distance Based Equational Matching (TEDEM) to automatically identify binary code regions that are ""similar"" to code regions containing a reference bug. We aim to find bugs both in the same binary as the reference bug and in completely unrelated binaries (even compiled for different operating systems). Our method even works on proprietary software systems, which lack source code and symbols.The analysis task is split into two phases. In a preprocessing phase, we condense the semantics of a given binary executable by symbolic simplification to make our approach robust against syntactic changes across different binaries. Second, we use tree edit distances as a basic block-centric metric for code similarity. This allows us to find instances of the same bug in different binaries and even spotting its variants (a concept called vulnerability extrapolation). To demonstrate the practical feasibility of the proposed method, we implemented a prototype of TEDEM that can find real-world security bugs across binaries and even across OS boundaries, such as in MS Word and the popular messengers Pidgin (Linux) and Adium (Mac OS).","","","ACSAC '14"
"Conference Paper","Shao Y,Luo X,Qian C,Zhu P,Zhang L","Towards a Scalable Resource-Driven Approach for Detecting Repackaged Android Applications","","2014","","","56–65","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th Annual Computer Security Applications Conference","New Orleans, Louisiana, USA","2014","9781450330053","","https://doi.org/10.1145/2664243.2664275;http://dx.doi.org/10.1145/2664243.2664275","10.1145/2664243.2664275","Repackaged Android applications (or simply apps) are one of the major sources of mobile malware and also an important cause of severe revenue loss to app developers. Although a number of solutions have been proposed to detect repackaged apps, the majority of them heavily rely on code analysis, thus suffering from two limitations: (1) poor scalability due to the billion opcode problem; (2) unreliability to code obfuscation/app hardening techniques. In this paper, we explore an alternative approach that exploits core resources, which have close relationships with codes, to detect repackaged apps. More precisely, we define new features for characterizing apps, investigate two kinds of algorithms for searching similar apps, and propose a two-stage methodology to speed up the detection. We realize our approach in a system named ResDroid and conduct large scale evaluation on it. The results show that ResDroid can identify repackaged apps efficiently and effectively even if they are protected by obfuscation or hardening systems.","","","ACSAC '14"
"Conference Paper","Udagawa Y","An Empirical Study on Retrieving Structural Clones Using Sequence Pattern Mining Algorithms","","2014","","","270–276","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Information Integration and Web-Based Applications & Services","Hanoi, Viet Nam","2014","9781450330015","","https://doi.org/10.1145/2684200.2684290;http://dx.doi.org/10.1145/2684200.2684290","10.1145/2684200.2684290","Many clone detection techniques focus on fragments of duplicated code, i.e., simple clones. Structural clones are simple clones within a syntactic boundary that are good candidates for refactoring. In this paper, a new approach for detection of structural clones in source code is presented. The proposed approach is parse-tree-based and is enhanced by frequent subsequence mining. It comprises three stages: preprocessing, mining frequent statement sequences, and fine-matching for structural clones using a modified longest common subsequence (LCS) algorithm. The lengths of control statements in a programming language and method identifiers differ; thus, a conventional LCS algorithm does not return the expected length of matched identifiers. We propose an encoding algorithm for control statements and method identifiers. Retrieval experiments were conducted using the Java SWING source code. The results show that the proposed data mining algorithm detects clones comprising 51 extracted statements. Our modified LCS algorithm retrieves a number of structural clones with arbitrary statement gaps.","Control statement, Java source code, Frequent subsequence mining, Method identifier, Structural clone","","iiWAS '14"
"Conference Paper","Davar Z,Handoko","Refactoring Object-Relational Database Applications by Applying Transformation Rules to Develop Better Performance","","2014","","","283–288","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Information Integration and Web-Based Applications & Services","Hanoi, Viet Nam","2014","9781450330015","","https://doi.org/10.1145/2684200.2684304;http://dx.doi.org/10.1145/2684200.2684304","10.1145/2684200.2684304","Object-relational database applications implemented in conventional procedural programming languages such as C, C++, and Java along with the embedded statements expressed in the non-procedural programming languages such as OQL, SQL and XQuery. Therefore, using transformation rules to optimise these applications by balancing the data processing load between the client and the server sides is required. Refactoring object-oriented applications, is one way to preserve output of the application but apply changes on design level. Implementation of object-relational applications with a large amount of procedural code, remains the majority of the data-processing to the client side. This often has catastrophic consequences for the performance of the application. Transformation rules need to be applied in an efficient way to come up with optimised applications.This research evaluates whether using transformation rules can be consider as a refactoring technology which can transfer the non-optimise object-relational application to the optimise ones. A systematic experimental study was conducted by incorporating transformation rules to monitor the number of Blocks-Read operations before and after applying the rules. It was concluded that as rules applied in an efficient way, the performance of applications increased. Also the efficient way of applying the rules is proposed.","Object-Relational Application, Software Patterns, Transformation Rule, Performance","","iiWAS '14"
"Conference Paper","Stegeman M,Barendsen E,Smetsers S","Towards an Empirically Validated Model for Assessment of Code Quality","","2014","","","99–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Koli Calling International Conference on Computing Education Research","Koli, Finland","2014","9781450330657","","https://doi.org/10.1145/2674683.2674702;http://dx.doi.org/10.1145/2674683.2674702","10.1145/2674683.2674702","We present a pilot study into developing a model of feedback on code quality in introductory programming courses. To devise such a model, we analyzed professional standards of code quality embedded in three popular software engineering handbooks and found 401 suggestions that we categorized into twenty topics. We recorded three instructors who performed a think-aloud judgment of student-submitted programs, and we interviewed them on the topics from the books, leading to 178 statements about code quality. The statements from the instructor interviews allowed us to generate a set of topics relevant to their practice of giving feedback, which we used to create criteria for the model. We used the instructor statements as well as the book suggestions to distinguish three levels of achievement for each criterion. This resulted in a total of 9 criteria for code quality. The interviews with the instructors generated a view of code quality that is very comparable to what was found in the handbooks, while the handbooks provide detailed suggestions that make our results richer than previously published grading schemes. As such, this process leads to an overview of code quality criteria and levels that can be very useful for constructing a standards-based rubric for introductory programming courses.","programming education, empirical validation, code quality, feedback, rubrics, assessment","","Koli Calling '14"
"Conference Paper","Simon,Snowdon S","Multiple-Choice vs Free-Text Code-Explaining Examination Questions","","2014","","","91–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Koli Calling International Conference on Computing Education Research","Koli, Finland","2014","9781450330657","","https://doi.org/10.1145/2674683.2674701;http://dx.doi.org/10.1145/2674683.2674701","10.1145/2674683.2674701","The BRACElet project has developed a number of code-explaining questions and thoroughly researched the novice programmer's difficulty in answering them correctly. In a prior study we explored whether students might perform better on multiple-choice code-explaining questions than on free-text code-explaining questions, and concluded that the multiple-choice form led to a perceptible but generally insignificant improvement in students' performance. However, that study compared different cohorts of students, leaving some doubt over its validity. In this study we seek to find a more definitive answer by including multiple-choice and free-text code-explaining questions in the same exam, and comparing the performances of individual students on the question pairs. We find that students perform substantially and significantly better on the multiple-choice questions. In the light of this finding we reconsider the question: when students cannot correctly describe the purpose of a small piece of code, is it because they do not understand the code; because they understand its detail but are unable to abstract that detail to determine the purpose; or because they understand the purpose but are unable to express it?","code-explaining questions, multiple choice, introductory programming, computing education","","Koli Calling '14"
"Conference Paper","Higo Y,Kusumoto S","How Should We Measure Functional Sameness from Program Source Code? An Exploratory Study on Java Methods","","2014","","","294–305","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635886;http://dx.doi.org/10.1145/2635868.2635886","10.1145/2635868.2635886","Program source code is one of the main targets of software engineering research. A wide variety of research has been conducted on source code, and many studies have leveraged structural, vocabulary, and method signature similarities to measure the functional sameness of source code. In this research, we conducted an empirical study to ascertain how we should use three similarities to measure functional sameness. We used two large datasets and measured the three similarities between all the method pairs in the datasets, each of which included approximately 15 million Java method pairs. The relationships between the three similarities were analyzed to determine how we should use each to detect functionally similar code. The results of our study revealed the following. (1) Method names are not always useful for detecting functionally similar code. Only if there are a small number of methods having a given name, the methods are likely to include functionally similar code. (2) Existing file-level, method-level, and block-level clone detection techniques often miss functionally similar code generated by copy-and-paste operations between different projects. (3) In the cases we use structural similarity for detecting functionally similar code, we obtained many false positives. However, we can avoid detecting most false positives by using a vocabulary similarity in addition to a structural one. (4) Using a vocabulary similarity to detect functionally similar code is not suitable for method pairs in the same file because such method pairs use many of the same program elements such as private methods or private fields.","Method name similarity, Functionally similar code, Vocabulary similarity, Structural similarity, Clone Detection","","FSE 2014"
"Conference Paper","Allamanis M,Sutton C","Mining Idioms from Source Code","","2014","","","472–483","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635901;http://dx.doi.org/10.1145/2635868.2635901","10.1145/2635868.2635901","We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.","naturalness of source code, syntactic code patterns, code idioms","","FSE 2014"
"Conference Paper","Luo L,Ming J,Wu D,Liu P,Zhu S","Semantics-Based Obfuscation-Resilient Binary Code Similarity Comparison with Applications to Software Plagiarism Detection","","2014","","","389–400","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635900;http://dx.doi.org/10.1145/2635868.2635900","10.1145/2635868.2635900","Existing code similarity comparison methods, whether source or binary code based, are mostly not resilient to obfuscations. In the case of software plagiarism, emerging obfuscation techniques have made automated detection increasingly difficult. In this paper, we propose a binary-oriented, obfuscation-resilient method based on a new concept, longest common subsequence of semantically equivalent basic blocks, which combines rigorous program semantics with longest common subsequence based fuzzy matching. We model the semantics of a basic block by a set of symbolic formulas representing the input-output relations of the block. This way, the semantics equivalence (and similarity) of two blocks can be checked by a theorem prover. We then model the semantics similarity of two paths using the longest common subsequence with basic blocks as elements. This novel combination has resulted in strong resiliency to code obfuscation. We have developed a prototype and our experimental results show that our method is effective and practical when applied to real-world software.","obfuscation, theorem proving, symbolic execution, Software plagiarism detection, binary code similarity comparison","","FSE 2014"
"Conference Paper","Milea NA,Jiang L,Khoo SC","Vector Abstraction and Concretization for Scalable Detection of Refactorings","","2014","","","86–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635926;http://dx.doi.org/10.1145/2635868.2635926","10.1145/2635868.2635926","Automated techniques have been proposed to either identify refactoring opportunities (i.e., code fragments that can be but have not yet been restructured in a program), or reconstruct historical refactorings (i.e., code restructuring operations that have happened between different versions of a program). In this paper, we propose a new technique that can detect both refactoring opportunities and historical refactorings in large code bases. The key of our technique is the design of vector abstraction and concretization operations that can encode code changes induced by certain refactorings as characteristic vectors. Thus, the problem of identifying refactorings can be reduced to the problem of identifying matching vectors, which can be solved efficiently. We have implemented our technique for Java. The prototype is applied to 200 bundle projects from the Eclipse ecosystem containing 4.5 million lines of code, and reports in total more than 32K instances of 17 types of refactoring opportunities, taking 25 minutes on average for each type. The prototype is also applied to 14 versions of 3 smaller programs (JMeter, Ant, XML-Security), and detects (1) more than 2.8K refactoring opportunities within individual versions with a precision of about 87%, and (2) more than 190 historical refactorings across consecutive versions of the programs with a precision of about 92%.","Vector-based Code Representation, Software Evolution, Refactoring Detection","","FSE 2014"
"Conference Paper","Behringer B","Integrating Approaches for Feature Implementation","","2014","","","775–778","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2666605;http://dx.doi.org/10.1145/2635868.2666605","10.1145/2635868.2666605","Compositional and annotative approaches are two competing yet complementary candidates for implementing feature-oriented software product lines. While the former provides real modularity, the latter excels concerning expressiveness. To combine the respective advantages of compositional and annotative approaches, we aim at unifying their underlying representations by leveraging the snippet system instead of directories and files. In addition, to exploit this unification, we propose different editable views.","virtual separation of concerns, Software product lines, snippets, features, modularity","","FSE 2014"
"Conference Paper","Zhang T,Song M,Kim M","Critics: An Interactive Code Review Tool for Searching and Inspecting Systematic Changes","","2014","","","755–758","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2661675;http://dx.doi.org/10.1145/2635868.2661675","10.1145/2635868.2661675","During peer code reviews, developers often examine program differences. When using existing program differencing tools, it is difficult for developers to inspect systematic changes—similar, related changes that are scattered across multiple files. Developers cannot easily answer questions such as ""what other code locations changed similar to this change?"" and ""are there any other locations that are similar to this code but are not updated?"" In this paper, we demonstrate Critics, an Eclipse plug-in that assists developers in inspecting systematic changes. It (1) allows developers to customize a context-aware change template, (2) searches for systematic changes using the template, and (3) detects missing or inconsistent edits. Developers can interactively refine the customized change template to see corresponding search results. Critics has potential to improve developer productivity in inspecting large, scattered edits during code reviews. The tool's demonstration video is available at https://www.youtube.com/watch?v=F2D7t_Z5rhk","program differencing, code reviews, Software Evolution","","FSE 2014"
"Conference Paper","Mazinanian D,Tsantalis N,Mesbah A","Discovering Refactoring Opportunities in Cascading Style Sheets","","2014","","","496–506","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635879;http://dx.doi.org/10.1145/2635868.2635879","10.1145/2635868.2635879","Cascading Style Sheets (CSS) is a language used for describing the look and formatting of HTML documents. CSS has been widely adopted in web and mobile development practice, since it enables a clean separation of content from presentation. The language exhibits complex features, such as inheritance, cascading and specificity, which make CSS code hard to maintain. Therefore, it is important to find ways to improve the maintainability of CSS code. In this paper, we propose an automated approach to remove duplication in CSS code. More specifically, we have developed a technique that detects three types of CSS declaration duplication and recommends refactoring opportunities to eliminate those duplications. Our approach uses preconditions that ensure the application of a refactoring will preserve the original document styling. We evaluate our technique on 38 real-world web systems and 91 CSS files, in total. Our findings show that duplication in CSS code is widely prevalent. Additionally, there is a significant number of presentation-preserving refactoring opportunities that can reduce the size of the CSS files and increase the maintainability of the code.","Cascading style sheets, duplication, refactoring","","FSE 2014"
"Conference Paper","Ying AT,Robillard MP","Selection and Presentation Practices for Code Example Summarization","","2014","","","460–471","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635877;http://dx.doi.org/10.1145/2635868.2635877","10.1145/2635868.2635877","Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology.","","","FSE 2014"
"Conference Paper","Wille D","Managing Lots of Models: The FaMine Approach","","2014","","","817–819","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2661681;http://dx.doi.org/10.1145/2635868.2661681","10.1145/2635868.2661681","In this paper we present recent developments in reverse engineering variability for block-based data-flow models.","SPL, Family Model Mining, Variability, Analysis","","FSE 2014"
"Conference Paper","Nguyen HA,Dyer R,Nguyen TN,Rajan H","Mining Preconditions of APIs in Large-Scale Code Corpus","","2014","","","166–177","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635924;http://dx.doi.org/10.1145/2635868.2635924","10.1145/2635868.2635924","Modern software relies on existing application programming interfaces (APIs) from libraries. Formal specifications for the APIs enable many software engineering tasks as well as help developers correctly use them. In this work, we mine large-scale repositories of existing open-source software to derive potential preconditions for API methods. Our key idea is that APIs’ preconditions would appear frequently in an ultra-large code corpus with a large number of API usages, while project-specific conditions will occur less frequently. First, we find all client methods invoking APIs. We then compute a control dependence relation from each call site and mine the potential conditions used to reach those call sites. We use these guard conditions as a starting point to automatically infer the preconditions for each API. We analyzed almost 120 million lines of code from SourceForge and Apache projects to infer preconditions for the standard Java Development Kit (JDK) library. The results show that our technique can achieve high accuracy with recall from 75–80% and precision from 82–84%. We also found 5 preconditions missing from human written specifications. They were all confirmed by a specification expert. In a user study, participants found 82% of the mined preconditions as a good starting point for writing specifications. Using our mining result, we also built a benchmark of more than 4,000 precondition-related bugs.","Big Code Mining, JML, Specification Mining, Preconditions","","FSE 2014"
"Conference Paper","Luo Q,Hariri F,Eloussi L,Marinov D","An Empirical Analysis of Flaky Tests","","2014","","","643–653","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering","Hong Kong, China","2014","9781450330565","","https://doi.org/10.1145/2635868.2635920;http://dx.doi.org/10.1145/2635868.2635920","10.1145/2635868.2635920","Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.","non-determinism, flaky tests, Empirical study","","FSE 2014"
"Conference Paper","Passier H,Stuurman S,Pootjes H","Beautiful JavaScript: How to Guide Students to Create Good and Elegant Code","","2014","","","65–76","Association for Computing Machinery","New York, NY, USA","Proceedings of the Computer Science Education Research Conference","Berlin, Germany","2014","9781450333474","","https://doi.org/10.1145/2691352.2691358;http://dx.doi.org/10.1145/2691352.2691358","10.1145/2691352.2691358","Programming is a complex task, which should be taught using authentic exercises, with supportive information and procedural information. Within the field of Computer Science, there are few examples of procedural information that guide students in how to proceed while solving a problem. We developed such guidelines for programming tasks in JavaScript, for students who have already learned to program using an object oriented language.Teaching JavaScript in an academic setting has advantages and disadvantages. The disadvantages are that the language is interpreted so there is no compiler to check for type errors, and that the language allows many 'awful' constructs. The advantage is that, because of those disadvantages, programmers should consciously apply rules for 'good' programs, instead of being able to rely on the errors and warnings that a compiler will raise.In this article, we show how we guide students to develop elegant code in JavaScript, by giving them a set of guidelines, and by advising a process of repeated refactoring until a program fulfills all requirements. To show that these guidelines work, we describe the development of a generic module for client-side form validation. The process followed and the resulting module both are valuable in an educational setting. As an example, it shows and explains precisely to students how such a module can be developed by following our guidelines, step by step.","modularity, javascript, procedural guidelines, form validation, client-side, web application, development of professional competencies in computer science, programming guidelines, module pattern, design principles","","CSERC '14"
"Conference Paper","Kintab GA,Roy CK,McCalla GI","Recommending Software Experts Using Code Similarity and Social Heuristics","","2014","","","4–18","IBM Corp.","USA","Proceedings of 24th Annual International Conference on Computer Science and Software Engineering","Markham, Ontario, Canada","2014","","","","","Successful collaboration among developers is crucial to the completion of software projects in a Distributed Software System Development (DSSD) environment. We have developed an Expert Recommender System Framework (ERSF) that assists a developer (called the ""Active Developer"") to find other developers who can help them to fix code with which they are having difficulty. The ERSF first looks for other developers with similar technical expertise, as measured by their prior work on code fragments that are similar to (clones of) the code that the Active Developer is working on (the ""code at hand""). As well, it analyzes the other developers' social relationships with the Active Developer (available from the DSSD environment) and their social activities within the ERSF (information which helps to maintain developer profiles used in this analysis). This information is then combined to provide a ranked list of potential helpers based on both technical and social measures. A proof of concept experiment shows that the ERSF can recommend experts with good to excellent accuracy, when compared with human rankings of appropriate experts in the same scenarios","","","CASCON '14"
"Conference Paper","Cordy JR,Dean TR,Synytskyy N","Practical Language-Independent Detection of near-Miss Clones","","2014","","","2","IBM Corp.","USA","Proceedings of 24th Annual International Conference on Computer Science and Software Engineering","Markham, Ontario, Canada","2014","","","","","Previous research shows that most software systems contain significant amounts of duplicated, or cloned, code. Some clones are exact duplicates of each other, while others differ in small details only. We designate these almost-perfect clones as ""near-miss"" clones. While technically difficult, detection of near-miss clones has many benefits, both academic and practical. Finding these clones can give us better insight into the way developers maintain and reuse code, and we can also parameterize and remove near-miss clones to reduce overall source code size and decrease system complexity. This paper presents a simple, general and practical way to detect near-miss clones, and summarizes the results ofits application to two production websites. We use standard lexical comparison tools coupled with language-specific extractors to locate potential clones. Our approach separates code comparisons from code understanding, and makes the comparisons language independent. This makes it easy to adapt to different programming languages.","","","CASCON '14"
"Conference Paper","Carter KM,Riordan JF,Okhravi H","A Game Theoretic Approach to Strategy Determination for Dynamic Platform Defenses","","2014","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the First ACM Workshop on Moving Target Defense","Scottsdale, Arizona, USA","2014","9781450331500","","https://doi.org/10.1145/2663474.2663478;http://dx.doi.org/10.1145/2663474.2663478","10.1145/2663474.2663478","Moving target defenses based on dynamic platforms have been proposed as a way to make systems more resistant to attacks by changing the properties of the deployed platforms. Unfortunately, little work has been done on discerning effective strategies for the utilization of these systems, instead relying on two generally false premises: simple randomization leads to diversity and platforms are independent. In this paper, we study the strategic considerations of deploying a dynamic platform system by specifying a relevant threat model and applying game theory and statistical analysis to discover optimal usage strategies. We show that preferential selection of platforms based on optimizing platform diversity approaches the statistically optimal solution and significantly outperforms simple randomization strategies. Counter to popular belief, this deterministic strategy leverages fewer platforms than may be generally available, which increases system security.","game theory, moving target, system diversity","","MTD '14"
"Conference Paper","Jin X,Hu X,Ying K,Du W,Yin H,Peri GN","Code Injection Attacks on HTML5-Based Mobile Apps: Characterization, Detection and Mitigation","","2014","","","66–77","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security","Scottsdale, Arizona, USA","2014","9781450329576","","https://doi.org/10.1145/2660267.2660275;http://dx.doi.org/10.1145/2660267.2660275","10.1145/2660267.2660275","Due to the portability advantage, HTML5-based mobile apps are getting more and more popular.Unfortunately, the web technology used by HTML5-based mobile apps has a dangerous feature, which allows data and code to be mixed together, making code injection attacks possible. In this paper, we have conducted a systematic study on this risk in HTML5-based mobile apps. We found a new form of code injection attack, which inherits the fundamental cause of Cross-Site Scripting attack (XSS), but it uses many more channels to inject code than XSS. These channels, unique to mobile devices, include Contact, SMS, Barcode, MP3, etc. To assess the prevalence of the code injection vulnerability in HTML5-based mobile apps, we have developed a vulnerability detection tool to analyze 15,510 PhoneGap apps collected from Google Play. 478 apps are flagged as vulnerable, with only 2.30% false-positive rate. We have also implemented a prototype called NoInjection as a Patch to PhoneGap in Android to defend against the attack.","HTML5-based mobile application, static analysis, code injection","","CCS '14"
"Journal Article","Hall BR","A Synthesized Definition of Computer Ethics","SIGCAS Comput. Soc.","2014","44","3","21–35","Association for Computing Machinery","New York, NY, USA","","","2014-10","","0095-2737","https://doi.org/10.1145/2684097.2684102;http://dx.doi.org/10.1145/2684097.2684102","10.1145/2684097.2684102","Computing ethics is a complex area of study that is of significant importance to the computing community and global society. However, research and education in computing ethics are difficult due to the diverse meanings of ethics. This paper presents details of a content analysis study that analyzed definitions of computer ethics. The purpose of this study was to educe and present the meaning of computing ethics, resulting in a thematic definition of computing ethics for use in education and research. This paper presents definition themes that emerged: interdisciplinary, collaboration, scholars and professionals, methodically study, practically affect, contributions and costs, computing artifacts, and global society. The results of this study can assist computing ethicists with research, aid computing educators with curriculum development, and provide a theoretical frame for relating ethics to computing. This exploration demonstrates that groups within the computing community can find common ground, even on such a difficult and complex matter as ethics.","education, definition, computer ethics, computing, community","",""
"Conference Paper","Renaux T,Hoste L,Scholliers C,De Meuter W","Software Engineering Principles in the Midas Gesture Specification Language","","2014","","","9–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Workshop on Programming for Mobile & Touch","Portland, Oregon, USA","2014","9781450322959","","https://doi.org/10.1145/2688471.2688478;http://dx.doi.org/10.1145/2688471.2688478","10.1145/2688471.2688478","While many technologies for gesture-based interaction have been proposed and implemented, few focus on core software engineering principles that are commonplace in traditional programming languages. The lack of such principles restricts the applicability of those technologies when developing large scale gesture enabled systems. This paper describes the software engineering challenges associated with developing multi-touch gesture-based interaction, and proposes a solution in the form of the Midas declarative gesture specification language. Midas embeds concepts of logical programming languages and complex event processing to ease the development of gesture based applications. We show how it can be applied to multi-touch gesture recognition, and evaluated our solution in real-world applications.","gesture recognition, declarative specification, multi-touch","","PROMOTO '14"
"Conference Paper","Hsiao CH,Cafarella M,Narayanasamy S","Using Web Corpus Statistics for Program Analysis","","2014","","","49–65","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages & Applications","Portland, Oregon, USA","2014","9781450325851","","https://doi.org/10.1145/2660193.2660226;http://dx.doi.org/10.1145/2660193.2660226","10.1145/2660193.2660226","Several program analysis tools - such as plagiarism detection and bug finding - rely on knowing a piece of code's relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common a line of code is. Natural language processing researchers have encountered a similar problem and addressed it using an n-gram model of text frequency, derived from statistics computed over text corpora.We propose and compute an n-gram model for programming languages, computed over a corpus of 2.8 million JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code n-gram as a subgraph of the program dependence graph that contains all nodes and edges reachable in n steps from the statement. We can count n-grams in a program and count the frequency of n-grams in the corpus, enabling us to compute tf-idf-style measures that capture the differing importance of different lines of code. We demonstrate the power of this approach by implementing a plagiarism detector with accuracy that beats previous techniques, and a bug-finding tool that discovered over a dozen previously unknown bugs in a collection of real deployed programs.","plagiarism detection, corpus-driven, programmatic n-gram, javascript, copy-paste bug","","OOPSLA '14"
"Journal Article","Hsiao CH,Cafarella M,Narayanasamy S","Using Web Corpus Statistics for Program Analysis","SIGPLAN Not.","2014","49","10","49–65","Association for Computing Machinery","New York, NY, USA","","","2014-10","","0362-1340","https://doi.org/10.1145/2714064.2660226;http://dx.doi.org/10.1145/2714064.2660226","10.1145/2714064.2660226","Several program analysis tools - such as plagiarism detection and bug finding - rely on knowing a piece of code's relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common a line of code is. Natural language processing researchers have encountered a similar problem and addressed it using an n-gram model of text frequency, derived from statistics computed over text corpora.We propose and compute an n-gram model for programming languages, computed over a corpus of 2.8 million JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code n-gram as a subgraph of the program dependence graph that contains all nodes and edges reachable in n steps from the statement. We can count n-grams in a program and count the frequency of n-grams in the corpus, enabling us to compute tf-idf-style measures that capture the differing importance of different lines of code. We demonstrate the power of this approach by implementing a plagiarism detector with accuracy that beats previous techniques, and a bug-finding tool that discovered over a dozen previously unknown bugs in a collection of real deployed programs.","programmatic n-gram, corpus-driven, javascript, plagiarism detection, copy-paste bug","",""
"Conference Paper","Gligoric M,Schulte W,Prasad C,van Velzen D,Narasamdya I,Livshits B","Automated Migration of Build Scripts Using Dynamic Analysis and Search-Based Refactoring","","2014","","","599–616","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages & Applications","Portland, Oregon, USA","2014","9781450325851","","https://doi.org/10.1145/2660193.2660239;http://dx.doi.org/10.1145/2660193.2660239","10.1145/2660193.2660239","The efficiency of a build system is an important factor for developer productivity. As a result, developer teams have been increasingly adopting new build systems that allow higher build parallelization. However, migrating the existing legacy build scripts to new build systems is a tedious and error-prone process. Unfortunately, there is insufficient support for automated migration of build scripts, making the migration more problematic.We propose the first dynamic approach for automated migration of build scripts to new build systems. Our approach works in two phases. First, from a set of execution traces, we synthesize build scripts that accurately capture the intent of the original build. The synthesized build scripts are typically long and hard to maintain. Second, we apply refactorings that raise the abstraction level of the synthesized scripts (e.g., introduce functions for similar fragments). As different refactoring sequences may lead to different build scripts, we use a search-based approach that explores various sequences to identify the best (e.g., shortest) build script. We optimize search-based refactoring with partial-order reduction to faster explore refactoring sequences. We implemented the proposed two phase migration approach in a tool called METAMORPHOSIS that has been recently used at Microsoft.","search-based refactoring, build system, migration","","OOPSLA '14"
"Journal Article","Gligoric M,Schulte W,Prasad C,van Velzen D,Narasamdya I,Livshits B","Automated Migration of Build Scripts Using Dynamic Analysis and Search-Based Refactoring","SIGPLAN Not.","2014","49","10","599–616","Association for Computing Machinery","New York, NY, USA","","","2014-10","","0362-1340","https://doi.org/10.1145/2714064.2660239;http://dx.doi.org/10.1145/2714064.2660239","10.1145/2714064.2660239","The efficiency of a build system is an important factor for developer productivity. As a result, developer teams have been increasingly adopting new build systems that allow higher build parallelization. However, migrating the existing legacy build scripts to new build systems is a tedious and error-prone process. Unfortunately, there is insufficient support for automated migration of build scripts, making the migration more problematic.We propose the first dynamic approach for automated migration of build scripts to new build systems. Our approach works in two phases. First, from a set of execution traces, we synthesize build scripts that accurately capture the intent of the original build. The synthesized build scripts are typically long and hard to maintain. Second, we apply refactorings that raise the abstraction level of the synthesized scripts (e.g., introduce functions for similar fragments). As different refactoring sequences may lead to different build scripts, we use a search-based approach that explores various sequences to identify the best (e.g., shortest) build script. We optimize search-based refactoring with partial-order reduction to faster explore refactoring sequences. We implemented the proposed two phase migration approach in a tool called METAMORPHOSIS that has been recently used at Microsoft.","migration, search-based refactoring, build system","",""
"Conference Paper","Song L,Lu S","Statistical Debugging for Real-World Performance Problems","","2014","","","561–578","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages & Applications","Portland, Oregon, USA","2014","9781450325851","","https://doi.org/10.1145/2660193.2660234;http://dx.doi.org/10.1145/2660193.2660234","10.1145/2660193.2660234","Design and implementation defects that lead to inefficient computation widely exist in software. These defects are difficult to avoid and discover. They lead to severe performance degradation and energy waste during production runs, and are becoming increasingly critical with the meager increase of single-core hardware performance and the increasing concerns about energy constraints. Effective tools that diagnose performance problems and point out the inefficiency root cause are sorely needed.The state of the art of performance diagnosis is preliminary. Profiling can identify the functions that consume the most computation resources, but can neither identify the ones that waste the most resources nor explain why. Performance-bug detectors can identify specific type of inefficient computation, but are not suited for diagnosing general performance problems. Effective failure diagnosis techniques, such as statistical debugging, have been proposed for functional bugs. However, whether they work for performance problems is still an open question.In this paper, we first conduct an empirical study to understand how performance problems are observed and reported by real-world users. Our study shows that statistical debugging is a natural fit for diagnosing performance problems, which are often observed through comparison-based approaches and reported together with both good and bad inputs. We then thoroughly investigate different design points in statistical debugging, including three different predicates and two different types of statistical models, to understand which design point works the best for performance diagnosis. Finally, we study how some unique nature of performance bugs allows sampling techniques to lower the overhead of run-time performance diagnosis without extending the diagnosis latency.","performance bugs, performance diagnosis, statistical debugging, empirical study","","OOPSLA '14"
"Journal Article","Song L,Lu S","Statistical Debugging for Real-World Performance Problems","SIGPLAN Not.","2014","49","10","561–578","Association for Computing Machinery","New York, NY, USA","","","2014-10","","0362-1340","https://doi.org/10.1145/2714064.2660234;http://dx.doi.org/10.1145/2714064.2660234","10.1145/2714064.2660234","Design and implementation defects that lead to inefficient computation widely exist in software. These defects are difficult to avoid and discover. They lead to severe performance degradation and energy waste during production runs, and are becoming increasingly critical with the meager increase of single-core hardware performance and the increasing concerns about energy constraints. Effective tools that diagnose performance problems and point out the inefficiency root cause are sorely needed.The state of the art of performance diagnosis is preliminary. Profiling can identify the functions that consume the most computation resources, but can neither identify the ones that waste the most resources nor explain why. Performance-bug detectors can identify specific type of inefficient computation, but are not suited for diagnosing general performance problems. Effective failure diagnosis techniques, such as statistical debugging, have been proposed for functional bugs. However, whether they work for performance problems is still an open question.In this paper, we first conduct an empirical study to understand how performance problems are observed and reported by real-world users. Our study shows that statistical debugging is a natural fit for diagnosing performance problems, which are often observed through comparison-based approaches and reported together with both good and bad inputs. We then thoroughly investigate different design points in statistical debugging, including three different predicates and two different types of statistical models, to understand which design point works the best for performance diagnosis. Finally, we study how some unique nature of performance bugs allows sampling techniques to lower the overhead of run-time performance diagnosis without extending the diagnosis latency.","statistical debugging, empirical study, performance bugs, performance diagnosis","",""
"Conference Paper","Philips L,De Roover C,Van Cutsem T,De Meuter W","Towards Tierless Web Development without Tierless Languages","","2014","","","69–81","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software","Portland, Oregon, USA","2014","9781450332101","","https://doi.org/10.1145/2661136.2661146;http://dx.doi.org/10.1145/2661136.2661146","10.1145/2661136.2661146","Tierless programming languages enable developing the typical server, client and database tiers of a web application as a single mono-linguistic program. This development style is in stark contrast to the current practice which requires combining multiple technologies and programming languages. A myriad of tierless programming languages has already been proposed, often featuring a JavaScript-like syntax. Instead of introducing yet another, we advocate that it should be possible to develop tierless web applications in existing general-purpose languages. This not only reduces the complexity that developers are exposed to, but also precludes the need for new development tools. We concretize this novel approach to tierless programming by discussing requirements on its future instantiations. We explore the design space of the program analysis for determining and the program transformation for realizing the tier split respectively. The former corresponds to new adaptations of an old familiar, program slicing, for tier splitting. The latter includes several strategies for handling cross-tier function calls and data accesses. Using a prototype instantiation for JavaScript, we demonstrate the feasibility of our approach on an example web application. We conclude with a discussion of open questions and challenges for future research.","javascript, program slicing, tier splitting, tierless programming","","Onward! 2014"
"Conference Paper","Samimi H,Deaton C,Ohshima Y,Warth A,Millstein T","Call by Meaning","","2014","","","11–28","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software","Portland, Oregon, USA","2014","9781450332101","","https://doi.org/10.1145/2661136.2661152;http://dx.doi.org/10.1145/2661136.2661152","10.1145/2661136.2661152","Software development involves stitching existing components together. These data/service components are usually not well understood, as they are made by others and often obtained from somewhere on the Internet. This makes software development a daunting challenge, requiring programmers to manually discover the resources they need, understand their capabilities, adapt these resources to their needs, and update the system as external components change.Software researchers have long realized the problem why automation seems impossible: the lack of semantic ""understanding"" on the part of the machine about those components. A multitude of solutions have been proposed under the umbrella term Semantic Web (SW), in which semantic markup of the components with concepts from semantic ontologies and the ability to invoke queries over those concepts enables a form of automated discovery and mediation among software services.On another front, programming languages rarely provide mechanisms for anchoring objects/data to real-world concepts. Inspired by the aspirations of SW, in this paper we reformulate its visions from the perspective of a programming model, i.e., that components themselves should be able to interact using semantic ontologies, rather than having a separate markup language and composition platform. In the vision, a rich specification language and common sense knowledge base over real-world concepts serves as a lingua franca to describe software components. Components can query the system to automatically (1) discover other components that provide needed functionality/data (2) discover the appropriate API within that component in order to obtain what is intended, and even (3) implicitly interpret the provided data in the desired form independent of the form originally presented by the provider component.By demonstrating a successful case of realization of this vision on a microexample, we hope to show how a programming languages (PL) approach to SW can be superior to existing engineered solutions, since the generality and expressiveness in the language can be harnessed, and encourage PL researchers to jump on the SW bandwagon.","automated discovery, language design, program composition, specifications, meanings, semantic web","","Onward! 2014"
"Conference Paper","Reynders B,Devriese D,Piessens F","Multi-Tier Functional Reactive Programming for the Web","","2014","","","55–68","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software","Portland, Oregon, USA","2014","9781450332101","","https://doi.org/10.1145/2661136.2661140;http://dx.doi.org/10.1145/2661136.2661140","10.1145/2661136.2661140","The development of robust and efficient interactive web applications is challenging, because developers have to deal with multiple programming languages, asynchronous events, propagating data and events between clients and servers, data consistency and much more. Several approaches for (partly) addressing these challenges have been proposed. Two relevant ones are (1) multi-tier languages and (2) functional reactive programming (FRP). Multi-tier programming languages support the development of client and server in a single language, and hide much of the complexity related to distribution. FRP offers the right abstractions to make event-driven programming convenient, safe and composable. However, existing web frameworks and programming languages exploit the benefits of both approaches separately, for example by restricting the use of FRP to the client side.We propose multi-tier FRP for the Web, a novel approach to writing web applications that deeply integrates FRP and multi-tier languages, and where the whole is greater than the sum of its parts. In multi-tier FRP, the developer programs server and client together as an FRP application composed of behaviors (signals) and events. He/she chooses explicitly where the boundary between server and client is crossed. To make our approach more concrete and provide evidence of its potential, this paper presents a concrete design and implementation of a multi-tier FRP API for the web in the programming language Scala, using an embedded JavaScript DSL that makes Scala usable as a multi-tier language. This allows us to present initial evidence of the benefits of the multi-tier FRP approach on example applications, and to experiment with possible answers to the remaining questions. Concretely, we show possible solutions for problems like exposing client identity on the server and efficiently pre-loading clients with the latest application state. Our results show that multi-tier FRP is a promising, declarative, yet practical way of writing web applications.","functional reactive programming, frp, multi-tier web framework","","Onward! 2014"
"Conference Paper","Stefik A,Hanenberg S","The Programming Language Wars: Questions and Responsibilities for the Programming Language Community","","2014","","","283–299","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software","Portland, Oregon, USA","2014","9781450332101","","https://doi.org/10.1145/2661136.2661156;http://dx.doi.org/10.1145/2661136.2661156","10.1145/2661136.2661156","The discipline of computer science has a long and complicated history with computer programming languages. Historically, inventors have created language products for a wide variety of reasons, from attempts at making domain specific tasks easier or technical achievements, to economic, social, or political reasons. As a consequence, the modern programming language industry now has a large variety of incompatible programming languages, each of which with unique syntax, semantics, toolsets, and often their own standard libraries, lifetimes, and costs. In this paper, we suggest that the programming language wars, a term which describes the broad divergence and impact of language designs, including often pseudo-scientific claims made that they are good or bad, may be negatively impacting the world. This broad problem, which is almost completely ignored in computer science, needs to be acted upon by the community.","evidence standards, stability of the academic literature, the programming language wars","","Onward! 2014"
"Conference Paper","Park S,Kim H,Kim J,Han H","Detecting Binary Theft via Static Major-Path Birthmarks","","2014","","","224–229","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems","Towson, Maryland","2014","9781450330602","","https://doi.org/10.1145/2663761.2664191;http://dx.doi.org/10.1145/2663761.2664191","10.1145/2663761.2664191","Software birthmarks are used for detecting software plagiarism. For binaries, not many reliable birthmarks are developed. API sequences are known to be successful birthmarks, but dynamically extracted sequences are often too large and unnecessarily repetitive. In this paper, we propose a static approach to generate API sequences along major paths, which are analyzed from control flow graphs of binaries. Since our API sequences are extracted along the most plausible paths of the binary codes, they can represent actual API sequences from executing binaries, but in a more concise form. In addition, as it is a static analysis, we can apply to partial binary objects, which cannot be executed on their own. Our similarity measures use the Smith-Waterman algorithm that is one of the popular sequence alignment algorithms for DNA sequence analysis. We evaluate our static path-based API sequence with multiple versions of five applications. In our experiment, our method reports a quite reliable similarity result for binary codes.","binary level similarity, static major-path, birthmark","","RACS '14"
"Conference Paper","Duboscq G,Würthinger T,Mössenböck H","Speculation without Regret: Reducing Deoptimization Meta-Data in the Graal Compiler","","2014","","","187–193","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools","Cracow, Poland","2014","9781450329262","","https://doi.org/10.1145/2647508.2647521;http://dx.doi.org/10.1145/2647508.2647521","10.1145/2647508.2647521","Speculative optimizations are used in most Just In Time (JIT) compilers in order to take advantage of dynamic runtime feedback. These speculative optimizations usually require the compiler to produce meta-data that the Virtual Machine (VM) can use as fallback when a speculation fails. This meta-data can be large and incurs a significant memory overhead since it needs to be stored alongside the machine code for as long as the machine code lives. The design of the Graal compiler leads to many speculations falling back to a similar state and location. In this paper we present deoptimization grouping, an optimization using this property of the Graal compiler to reduce the amount of meta-data that must be stored by the VM without having to modify the VM. We compare our technique with existing meta-data compression techniques from the HotSpot Virtual Machine and study how well they combine. In order to make informed decisions about speculation meta-data, we present an empirical analysis of the origin, impact and usages of this meta-data.","Java virtual machine, just-in-time compilation, metadata, speculative optimization","","PPPJ '14"
"Conference Paper","Bettini L,Damiani F","Generic Traits for the Java Platform","","2014","","","5–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools","Cracow, Poland","2014","9781450329262","","https://doi.org/10.1145/2647508.2647518;http://dx.doi.org/10.1145/2647508.2647518","10.1145/2647508.2647518","A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. In this paper we present the extended version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj provides a full Eclipse IDE that aims to support an incremental adoption of traits in existing Java projects. This new version fully supports Java generics: traits can have type parameters just like in Java, so that they can completely interoperate with any existing Java library. Furthermore, Xtraitj now supports Java annotations, so that it can integrate with frameworks like JUnit 4.","IDE, Java, implementation, trait, eclipse","","PPPJ '14"
"Conference Paper","Jiang Y,Adams B,Khomh F,German DM","Tracing Back the History of Commits in Low-Tech Reviewing Environments: A Case Study of the Linux Kernel","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","Torino, Italy","2014","9781450327749","","https://doi.org/10.1145/2652524.2652542;http://dx.doi.org/10.1145/2652524.2652542","10.1145/2652524.2652542","Context: During software maintenance, people typically go back to the original reviews of a patch to understand the actual design rationale and potential risks of the code. Whereas modern web-based reviewing environments like gerrit make this process relatively easy, the low-tech, mailing-list based reviewing environments of many open source systems make linking a commit back to its reviews and earlier versions far from trivial, since (1) a commit has no physical link with any reviewing email, (2) the discussed patches are not always fully identical to the accepted commits and (3) some discussions last across multiple email threads, each of which containing potentially multiple versions of the same patch.Goal: To support maintainers in reconstructing the reviewing history of kernel patches, and studying (for the first time) the characteristics of the recovered reviewing histories.Method: This paper performs a comparative empirical study on the Linux kernel mailing lists of 3 email-to-email and email-to-commit linking techniques based on checksums, common patch lines and clone detection.Results: Around 25% of the patches had an (until now) hidden reviewing history of more than four weeks, and patches with multiple versions typically are larger and have a higher acceptance rate than patches with just one version.Conclusion: The plus-minus-line-based technique is the best approach for linking patch emails to commits, while it needs to be combined with the checksum-based technique for linking different patch versions.","clone detection, review, software engineering, mailing list, open source, linux kernel, low-tech reviewing environment, traceability","","ESEM '14"
"Conference Paper","Caneill M,Zacchiroli S","Debsources: Live and Historical Views on Macro-Level Software Evolution","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","Torino, Italy","2014","9781450327749","","https://doi.org/10.1145/2652524.2652528;http://dx.doi.org/10.1145/2652524.2652528","10.1145/2652524.2652528","Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution---i.e., on the evolution of large software collections over many years---are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users.Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects.Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources.debian.net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all current and historical Debian releases into it.Results. The obtained dataset and Web portal provide both long term-views over the past 20 years of FOSS evolution and live insights on what is happening at sub-day granularity. By writing simple plugins ( 100 lines of Python each) and adding them to our Debsources instance we have been able to easily replicate and extend past empirical analyses on metrics as diverse as lines of code, number of packages, and rate of change---and make them perennial. We have obtained slightly different results than our reference study, but confirmed the general trends and updated them in light of 7 extra years of evolution history.Conclusions. Debsources is a flexible platform to monitor large FOSS collections over long periods of time. Its main instance and dataset are valuable resources for scholars interested in macro-level software evolution.","open source, debian, software evolution, free software, source code","","ESEM '14"
"Conference Paper","Duc AN,Mockus A,Hackbarth R,Palframan J","Forking and Coordination in Multi-Platform Development: A Case Study","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","Torino, Italy","2014","9781450327749","","https://doi.org/10.1145/2652524.2652546;http://dx.doi.org/10.1145/2652524.2652546","10.1145/2652524.2652546","[Context] With the proliferation of desktop and mobile platforms the development and maintenance of identical or similar applications on multiple platforms is urgently needed. [Goal] We study a software product deployed to more than 25 software/hardware combinations over 10 years to understand multi-platform development practices. [Method] We use semi structured interviews, project wikis, VCSs and issue tracking systems to understand and quantify these practices. [Results] We find the projects using MR cloning, MR review meeting, cross platform coordinator's role as three primary means of coordination. We find that forking code temporarily relieves the coordination needs and is driven by divergent schedule, market needs, and organizational policy. Based on our qualitative findings we propose quantitative measures of coordination, redundant work, and parallel development. [Conclusions] A model of coordination intensity suggests that it is related to the amount of paralel and redundant work. We hope that this work will provide a basis for quantitative understanding of issues faced in multi-platform software development.","multiple platform development, fork, coordination, empirical study","","ESEM '14"
"Conference Paper","Bavota G,Panichella S,Tsantalis N,Di Penta M,Oliveto R,Canfora G","Recommending Refactorings Based on Team Co-Maintenance Patterns","","2014","","","337–342","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering","Vasteras, Sweden","2014","9781450330138","","https://doi.org/10.1145/2642937.2642948;http://dx.doi.org/10.1145/2642937.2642948","10.1145/2642937.2642948","Refactoring aims at restructuring existing source code when undisciplined development activities have deteriorated its comprehensibility and maintainability. There exist various approaches for suggesting refactoring opportunities, based on different sources of information, e.g., structural, semantic, and historical. In this paper we claim that an additional source of information for identifying refactoring opportunities, sometimes orthogonal to the ones mentioned above, is team development activity. When the activity of a team working on common modules is not aligned with the current design structure of a system, it would be possible to recommend appropriate refactoring operations---e.g., extract class/method/package---to adjust the design according to the teams' activity patterns. Results of a preliminary study---conducted in the context of extract class refactoring---show the feasibility of the approach, and also suggest that this new refactoring dimension can be complemented with others to build better refactoring recommendation tools.","developers, teams, refactoring","","ASE '14"
"Conference Paper","Lutz R,Diehl S","Using Visual Dataflow Programming for Interactive Model Comparison","","2014","","","653–664","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering","Vasteras, Sweden","2014","9781450330138","","https://doi.org/10.1145/2642937.2642984;http://dx.doi.org/10.1145/2642937.2642984","10.1145/2642937.2642984","In software engineering the comparison of graph-based models is a well-known problem. Although different comparison metrics have been proposed, there are situations in which automatic or pre-configured approaches do not provide reasonable results. Especially when models contain semantic similarities or differences, additional human knowledge is often required. However, only few approaches tackle the problem of how to support humans when comparing models.In this paper, we propose a tool for interactive model comparison. Its design was informed by a set of guidelines that we identified in previous work. In particular, our prototype leverages visual dataflow programming to allow users to implement custom comparison strategies. To this end, they can combine metrics and graph operations to compute similarities and differences, and use color coding to visualize the gained results. Additionally, a qualitative user study allowed to assess whether and how our tool facilitates iterative exploration of similarities and differences between models.","comparison, graphs, human-centered, models, tools, visual dataflow programming","","ASE '14"
"Conference Paper","Assunção WK,Vergilio SR","Feature Location for Software Product Line Migration: A Mapping Study","","2014","","","52–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2","Florence, Italy","2014","9781450327398","","https://doi.org/10.1145/2647908.2655967;http://dx.doi.org/10.1145/2647908.2655967","10.1145/2647908.2655967","Developing software from scratch is a high cost and error-prone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches.","evolution, reengineering, software product line, reuse","","SPLC '14"
"Conference Paper","Holthusen S,Wille D,Legat C,Beddig S,Schaefer I,Vogel-Heuser B","Family Model Mining for Function Block Diagrams in Automation Software","","2014","","","36–43","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2","Florence, Italy","2014","9781450327398","","https://doi.org/10.1145/2647908.2655965;http://dx.doi.org/10.1145/2647908.2655965","10.1145/2647908.2655965","Automation systems are mostly individual highly customized system variants, consisting both of hardware and software. In order to reduce development effort, it is a common practice to use a clone-and-own approach by modifying an existing variant to fit the changed requirements of a new variant. The information about the commonalities and differences between those variants is usually not well documented and leads to problems in maintenance, testing and evolution. To alleviate these problems, in this paper, we present an improved version of a family mining approach for automatically discovering commonality and variability between related system variants. We apply this approach to function block diagrams used to develop automation software and show its feasibility by a manufacturing case study.","family mining, automation software, re-engineering, software engineering","","SPLC '14"
"Conference Paper","Makhdoom S,Khan MA,Siddiqui JH","Incremental Symbolic Execution for Automated Test Suite Maintenance","","2014","","","271–276","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering","Vasteras, Sweden","2014","9781450330138","","https://doi.org/10.1145/2642937.2642961;http://dx.doi.org/10.1145/2642937.2642961","10.1145/2642937.2642961","Scaling software analysis techniques based on source-code, such as symbolic execution and data flow analyses, remains a challenging problem for systematically checking software systems. In this work, we aim to efficiently apply symbolic execution in increments based on versions of code. Our technique is based entirely on dynamic analysis and patches completely automated test suites based on the code changes. Our key insight is that we can eliminate constraint solving for unchanged code by checking constraints using the test suite of a previous version. Checking constraints is orders of magnitude faster than solving them. This is in contrast to previous techniques that rely on inexact static analysis or cache of previously solved constraints. Our technique identifies ranges of paths, each bounded by two concrete tests from the previous test suite. Exploring these path ranges covers all paths affected by code changes up to a given depth bound. Our experiments show that incremental symbolic execution based on dynamic analysis is an order of magnitude faster than running complete standard symbolic execution on the new version of code.","klee, symbolic execution, incremental analysis","","ASE '14"
"Conference Paper","Park J,Kim M,Bae DH","An Empirical Study on Reducing Omission Errors in Practice","","2014","","","121–126","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering","Vasteras, Sweden","2014","9781450330138","","https://doi.org/10.1145/2642937.2642956;http://dx.doi.org/10.1145/2642937.2642956","10.1145/2642937.2642956","Since studies based on mining software repositories sparked interests in the field of guiding software changes, many change recommendation techniques have been proposed to reduce omission errors. While these techniques only used existing software commit data sets to evaluate their effectiveness, we use the data set of supplementary patches which correct initial incomplete patches to investigate how much actual omission errors could be prevented in practice. We find that while a single trait is inadequate, combining multiple traits is limited as well for predicting supplementary change locations. Neither does a boosting approach improve accuracy significantly, nor filtering based on developer or package specific information necessarily improves the accuracy. Developers rarely repeat the same mistakes, making the potential value of history-based change prediction less promising. We share our skepticism that omission errors are hard to prevent in practice based on a systematic evaluation of a supplementary patch data set.","mining version history, supplementary patch, omission error","","ASE '14"
"Conference Paper","Domis D,Sehestedt S,Gamer T,Aleksy M,Koziolek H","Customizing Domain Analysis for Assessing the Reuse Potential of Industrial Software Systems: Experience Report","","2014","","","310–319","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Software Product Line Conference - Volume 1","Florence, Italy","2014","9781450327404","","https://doi.org/10.1145/2648511.2648547;http://dx.doi.org/10.1145/2648511.2648547","10.1145/2648511.2648547","In companies with a large portfolio of software or software-intensive products, functional overlaps are often perceived between independent products. In such situations it is advisable to systematically analyze the potential of systematic reuse and Software Product Lines. To this end, several domain analysis approaches, e.g., SEI Technical Probe, have been proposed to decide whether a set of products with a perceived functional overlap should be integrated into a single product line. Based on the principles of those approaches we devised our own approach. One important property is the inherent flexibility of the method to be able to apply it to four different application cases in industrial software products at ABB. In this paper we present our refined approach for domain analysis. The results and lessons learned are meant to support industrial researchers and practitioners alike. Moreover, the lessons learned highlight real-world findings concerning software reuse.","domain analysis, software reuse, software product lines","","SPLC '14"
"Conference Paper","Lillack M,Bucholdt C,Schilling D","Detection of Code Clones in Software Generators","","2014","","","37–44","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Feature-Oriented Software Development","Väster\ras, Sweden","2014","9781450329804","","https://doi.org/10.1145/2660190.2662116;http://dx.doi.org/10.1145/2660190.2662116","10.1145/2660190.2662116","Macro-based generators are in use for more than 40 years to generate Cobol source code and implement variability. Over the course of time, the systems were extended with many similar functionalities by copying and adapting existing pieces of code. The resulting generators are hard to understand and difficult to maintain. Clone detection can identify similar pieces of code which is a prerequisite to extract common features thus enabling a move to a feature-oriented product line.This paper presents Hanni, a tool that combines clone detection of the input and output of generators to improve detection quality. Hanni uses standard textual clone detection tools on macro-based generators to detect clones in the macros and the generated Cobol. A mapping of the clones from the two sources is used to verify the detected clones and even suggest possible semantic clones. We are using generator examples from different industries based on the ADS generator framework to evaluate our tool. The results show that code clones are very common in these generators and possible problems in the detection can be identified using the generated files.","macros, software generators, code clones, feature-oriented refactoring","","FOSD '14"
"Conference Paper","Toll M,Minto WR","Principles of Pattern Enabled Java Development","","2014","","","","The Hillside Group","USA","Proceedings of the 21st Conference on Pattern Languages of Programs","Monticello, Illinois","2014","","","","","Pattern Enabled Development® (PED) seeks to advance Java application construction by inspiring Pattern First Thinking among software engineers, enabling project teams with a Pattern Palette, and harmonizing enterprise communication with a Pattern Language.","","","PLoP '14"
"Conference Paper","Honarvar AR,Sami A","CBR Clone Based Software Flaw Detection Issues","","2014","","","487–491","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Conference on Security of Information and Networks","Glasgow, Scotland, UK","2014","9781450330336","","https://doi.org/10.1145/2659651.2659745;http://dx.doi.org/10.1145/2659651.2659745","10.1145/2659651.2659745","The biggest problem in computer security is that most systems aren't constructed with security in mind. Being aware of common security weaknesses in programming might sound like a good way to avoid them, but awareness by itself often proves to be insufficient. Understanding security is one thing and applying your understanding in a complete and consistent fashion to meet your security goals is quite another. For this reason, static analysis is advocated as a technique for finding common security errors in source code. Manual security static analysis is a tedious work, so automatic tools which can guide programmers to detect security concerns is suggested. Good static analysis tools provide a fast way to get a detailed security related evaluation of program code. In this paper a new architecture (CBRFD) for software flaw detector, based on the concept of clone detection and case base reasoning, is proposed and various issues which concern detection of security weakness of codes through code clone detector is investigated.","static security analysis, flaw clone detector, static analysis tools, Software flaw detection","","SIN '14"
"Journal Article","Maalej W,Tiarks R,Roehm T,Koschke R","On the Comprehension of Program Comprehension","ACM Trans. Softw. Eng. Methodol.","2014","23","4","","Association for Computing Machinery","New York, NY, USA","","","2014-09","","1049-331X","https://doi.org/10.1145/2622669;http://dx.doi.org/10.1145/2622669","10.1145/2622669","Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge.We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments.Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.","context-aware software engineering, Empirical software engineering, program comprehension, information needs, knowledge sharing","",""
"Journal Article","Hall T,Zhang M,Bowes D,Sun Y","Some Code Smells Have a Significant but Small Effect on Faults","ACM Trans. Softw. Eng. Methodol.","2014","23","4","","Association for Computing Machinery","New York, NY, USA","","","2014-09","","1049-331X","https://doi.org/10.1145/2629648;http://dx.doi.org/10.1145/2629648","10.1145/2629648","We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.","Software code smells, defects","",""
"Conference Paper","Slesarenko A,Filippov A,Romanov A","First-Class Isomorphic Specialization by Staged Evaluation","","2014","","","35–46","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th ACM SIGPLAN Workshop on Generic Programming","Gothenburg, Sweden","2014","9781450330428","","https://doi.org/10.1145/2633628.2633632;http://dx.doi.org/10.1145/2633628.2633632","10.1145/2633628.2633632","The state of the art approach for reducing complexity in software development is to use abstraction mechanisms of programming languages such as modules, types, higher-order functions etc. and develop high-level frameworks and domain-specific abstractions. Abstraction mechanisms, however, along with simplicity, introduce also execution overhead and often lead to significant performance degradation. Avoiding abstractions in favor of performance, on the other hand, increases code complexity and cost of maintenance.We develop a systematic approach and formalized framework for implementing software components with a first-class specialization capability. We show how to extend a higher-order functional language with abstraction mechanisms carefully designed to provide automatic and guaranteed elimination of abstraction overhead.We propose staged evaluation as a new method of program staging and show how it can be implemented as zipper-based traversal of program terms where one-hole contexts are generically constructed from the abstract syntax of the language.We show how generic programming techniques together with staged evaluation lead to a very simple yet powerful method of isomorphic specialization which utilizes first-class definitions of isomorphisms between data types to provide guarantee of abstraction elimination.We give a formalized description of the isomorphic specialization algorithm and show how it can be implemented as a set of term rewriting rules using active patterns and staged evaluation.We implemented our approach as a generic programming framework with first-class staging, term rewriting and isomorphic specialization and show in our evaluation that the proposed capabilities give rise to a new paradigm to develop domain-specific software components without abstraction penalty.","staging, isomorphisms, generic programming, multi-stage programming, polytypic programming, dsl, specialization, domain-specific languages","","WGP '14"
"Conference Paper","Haenni N,Lungu M,Schwarz N,Nierstrasz O","A Quantitative Analysis of Developer Information Needs in Software Ecosystems","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 European Conference on Software Architecture Workshops","Vienna, Austria","2014","9781450327787","","https://doi.org/10.1145/2642803.2642815;http://dx.doi.org/10.1145/2642803.2642815","10.1145/2642803.2642815","We present the results of an investigation into the nature of information needs of software developers who work in projects that are part of larger ecosystems. This work is based on a quantitative survey of 75 professional software developers. We corroborate the results identified in the survey with needs and motivations proposed in a previous survey and discover that tool support for developers working in an ecosystem context is even more meager than we thought: mailing lists and internet search are the most popular tools developers use to satisfy their ecosystem-related information needs.","open source software, frameworks and libraries, program comprehension, programmer needs, Software ecosystems","","ECSAW '14"
"Conference Paper","Bhattacharyya A,Hoefler T","PEMOGEN: Automatic Adaptive Performance Modeling during Program Runtime","","2014","","","393–404","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd International Conference on Parallel Architectures and Compilation","Edmonton, AB, Canada","2014","9781450328098","","https://doi.org/10.1145/2628071.2628100;http://dx.doi.org/10.1145/2628071.2628100","10.1145/2628071.2628100","Traditional means of gathering performance data are tracing, which is limited by the available storage, and profiling, which has limited accuracy. Performance modeling is often used to interpret the tracing data and generate performance predictions. We aim to complement the traditional data collection mechanisms with online performance modeling, a method that generates performance models while the application is running. This allows us to greatly reduce the storage overhead while still producing accurate predictions. We present PEMOGEN, our compilation and modeling framework that automatically instruments applications to generate performance models during program execution. We demonstrate the ability of PEMOGEN to both reduce storage cost and improve the prediction accuracy compared to traditional techniques such as least squares fitting. With our tool, we automatically detect 3,370 kernels from fifteen NAS and Mantevo applications and model their execution time with a median coefficient of variation R2 of 0.81. These automatically generated performance models can be used to quickly assess the scaling and potential bottlenecks with regards to any input parameter and the number of processes of a parallel application.","performance modeling, performance analysis, lasso","","PACT '14"
"Conference Paper","Jamshidi DA,Samadi M,Mahlke S","D2MA: Accelerating Coarse-Grained Data Transfer for GPUs","","2014","","","431–442","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd International Conference on Parallel Architectures and Compilation","Edmonton, AB, Canada","2014","9781450328098","","https://doi.org/10.1145/2628071.2628072;http://dx.doi.org/10.1145/2628071.2628072","10.1145/2628071.2628072","To achieve high performance on many-core architectures like GPUs, it is crucial to efficiently utilize the available memory bandwidth. Currently, it is common to use fast, on-chip scratchpad memories, like the shared memory available on GPUs' shader cores, to buffer data for computation. This buffering, however, has some sources of inefficiency that hinder it from most efficiently utilizing the available memory resources. These issues stem from shader resources being used for repeated, regular address calculations, a need to shuffle data multiple times between a physically unified on-chip memory, and forcing all threads to synchronize to ensure RAW consistency based on the speed of the slowest threads. To address these inefficiencies, we propose Data-Parallel DMA, or D2MA. D2MA is a reimagination of traditional DMA that addresses the challenges of extending DMA to thousands of concurrently executing threads. D2MA de-couples address generation from the shader's computational resources, provides a more direct and efficient path for data in global memory to travel into the shared memory, and introduces a novel dynamic synchronization scheme that is transparent to the programmer. These advancements allow D2MA to achieve speedups as high as 2.29x, and reduces the average time to buffer data by 81% on average.","dma, software-managed caches, throughput processing, shared memory, gpus, dynamic management","","PACT '14"
"Conference Paper","Ramsey N","On Teaching *how to Design Programs*: Observations from a Newcomer","","2014","","","153–166","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming","Gothenburg, Sweden","2014","9781450328739","","https://doi.org/10.1145/2628136.2628137;http://dx.doi.org/10.1145/2628136.2628137","10.1145/2628136.2628137","This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new ""review and refactor"" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. It connects the process to interactive ""world programs."" It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.","racket, program by design, reflective practice, introductory programming course, how to design programs","","ICFP '14"
"Journal Article","Ramsey N","On Teaching *how to Design Programs*: Observations from a Newcomer","SIGPLAN Not.","2014","49","9","153–166","Association for Computing Machinery","New York, NY, USA","","","2014-08","","0362-1340","https://doi.org/10.1145/2692915.2628137;http://dx.doi.org/10.1145/2692915.2628137","10.1145/2692915.2628137","This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new ""review and refactor"" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. It connects the process to interactive ""world programs."" It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.","how to design programs, program by design, introductory programming course, reflective practice, racket","",""
"Conference Paper","Strandh R","Resolving Metastability Issues During Bootstrapping","","2014","","","103–106","Association for Computing Machinery","New York, NY, USA","Proceedings of ILC 2014 on 8th International Lisp Conference","Montreal, QC, Canada","2014","9781450329316","","https://doi.org/10.1145/2635648.2635656;http://dx.doi.org/10.1145/2635648.2635656","10.1145/2635648.2635656","The fact that CLOS is defined as a CLOS program introduces two categories of issues that must be addressed, namely bootstrapping issues and metastability issues [2]. Of the two, the latter is the more difficult one, and also the one that has the most negative impact on the elegance of the code in that it requires base cases to be handled specially.We describe satiation, a technique by which metastability issues can be turned into bootstrapping issues, thereby simplifying them and keeping the code elegant. Satiation consists of pre-loading the call history of a generic function with respect to a set of argument classes so that the base cases are handled without invoking the full protocol for computing the effective methods at runtime.","CLOS, Bootstrapping, Metastability, Common Lisp","","ILC '14"
"Journal Article","Dong W,Chen C,Bu J,Liu W","Optimizing Relocatable Code for Efficient Software Update in Networked Embedded Systems","ACM Trans. Sen. Netw.","2014","11","2","","Association for Computing Machinery","New York, NY, USA","","","2014-07","","1550-4859","https://doi.org/10.1145/2629479;http://dx.doi.org/10.1145/2629479","10.1145/2629479","Recent advances in Microelectronic Mechanical Systems (MEMS) and wireless communication technologies have fostered the rapid development of networked embedded systems like wireless sensor networks. System software for these self-organizing systems often needs to be updated for a variety of reasons. We present a holistic software update (i.e., reprogramming) system called R3 for networked embedded systems. R3 has two salient features. First, the binary differencing algorithm within R3 (R3diff) ensures an optimal result in terms of the delta size under a configurable cost measure. Second, the similarity preserving method within R3 (R3sim) optimizes the binary code format for achieving a large similarity with a small metadata overhead. Overall, R3 achieves the smallest delta size compared with other software update approaches such as Stream, Rsync, RMTD, Zephyr, Hermes, and R2 (e.g., 50%--99% reduction compared to Stream and about 20%--40% reduction compared to R2). R3’s implementation on TelosB/TinyOS is lightweight and efficient. We release our code at http://code.google.com/p/r3-dongw.","software update, Wireless sensor network, reprogramming, differencing algorithm","",""
"Conference Paper","Kartsaklis C,Hernandez OR","HERCULES/PL: The Pattern Language of HERCULES","","2014","","","5–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st Workshop on Programming Language Evolution","Uppsala, Sweden","2014","9781450328876","","https://doi.org/10.1145/2717124.2717127;http://dx.doi.org/10.1145/2717124.2717127","10.1145/2717124.2717127","Interrogating the structure of a program for patterns of interest is attractive to the broader spectrum of software engineering. The very approach by which a pattern is constructed remains a concern for the source code mining community. This paper presents a pattern programming model, for the C and Fortran programming languages, using a compiler directives approach. We discuss our specification, called HERCULES/PL, throughout a number of examples and show how different patterns can be constructed, plus some preliminary results.","compiler directives, pattern construction, source code mining","","PLE '14"
"Conference Paper","Bogdanov D,Laud P,Randmets J","Domain-Polymorphic Programming of Privacy-Preserving Applications","","2014","","","53–65","Association for Computing Machinery","New York, NY, USA","Proceedings of the Ninth Workshop on Programming Languages and Analysis for Security","Uppsala, Sweden","2014","9781450328623","","https://doi.org/10.1145/2637113.2637119;http://dx.doi.org/10.1145/2637113.2637119","10.1145/2637113.2637119","Secure Multi-party Computation (SMC) is seen as one of the main enablers for secure outsourcing of computation. Currently, there are many different SMC techniques (garbled circuits, secret sharing, homomorphic encryption, etc.) and none of them is clearly superior to others in terms of efficiency, security guarantees, ease of implementation, etc. For maximum efficiency, and for obeying the trust policies, a privacy-preserving application may wish to use several different SMC techniques for different operations it performs. A straightforward implementation of this application may result in a program that (i) contains a lot of duplicated code, differing only in the used SMC technique; (ii) is difficult to maintain, if policies or SMC implementations change; and (iii) is difficult to reuse in similar applications using different SMC techniques.In this paper, we propose a programming language called SecreC with associated compilation techniques for simple orchestration of multiple SMC techniques and multiple protection domains. It is a simple imperative language with function calls where the types of data items are annotated with protection domains and where the function declarations may be domain-polymorphic. This allows most of the program code working with private data to be written in a SMC-technique-agnostic manner. It also allows rapid deployment of new SMC techniques and implementations in existing applications. We have implemented the compiler for the language, integrated it with Sharemind SMC framework, and are currently using it for new privacy-preserving applications.","Secure Multi-Party Computation, Software Engineering","","PLAS'14"
"Conference Paper","Zhang F,Huang H,Zhu S,Wu D,Liu P","ViewDroid: Towards Obfuscation-Resilient Mobile Application Repackaging Detection","","2014","","","25–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM Conference on Security and Privacy in Wireless & Mobile Networks","Oxford, United Kingdom","2014","9781450329729","","https://doi.org/10.1145/2627393.2627395;http://dx.doi.org/10.1145/2627393.2627395","10.1145/2627393.2627395","In recent years, as mobile smart device sales grow quickly, the development of mobile applications (apps) keeps accelerating, so does mobile app repackaging. Attackers can easily repackage an app under their own names or embed advertisements to earn pecuniary profits. They can also modify a popular app by inserting malicious payloads into the original app and leverage its popularity to accelerate malware propagation. In this paper, we propose ViewDroid, a user interface based approach to mobile app repackaging detection. Android apps are user interaction intensive and event dominated, and the interactions between users and apps are performed through user interface, or views. This observation inspires the design of our new birthmark for Android apps, namely, feature view graph, which captures users' navigation behavior across app views. Our experimental results demonstrate that this birthmark can characterize Android apps from a higher level abstraction, making it resilient to code obfuscation. ViewDroid can detect repackaged apps at a large scale, both effectively and efficiently. Our experiments also show that the false positive and false negative rates of ViewDroid are both very low.","repackaging, mobile application, obfuscation resilient, user interface","","WiSec '14"
"Conference Paper","Milea NA,Jiang L,Khoo SC","Scalable Detection of Missed Cross-Function Refactorings","","2014","","","138–148","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 International Symposium on Software Testing and Analysis","San Jose, CA, USA","2014","9781450326452","","https://doi.org/10.1145/2610384.2610394;http://dx.doi.org/10.1145/2610384.2610394","10.1145/2610384.2610394","Refactoring is an important way to improve the design of existing code. Identifying refactoring opportunities (i.e., code fragments that can be refactored) in large code bases is a challenging task. In this paper, we propose a novel, automated and scalable technique for identifying cross-function refactoring opportunities that span more than one function (e.g., Extract Method and Inline Method). The key of our technique is the design of efficient vector inlining operations that emulate the effect of method inlining among code fragments, so that the problem of identifying cross-function refactoring can be reduced to the problem of finding similar vectors before and after inlining. We have implemented our technique in a prototype tool named ReDex which encodes Java programs to particular vectors. We have applied the tool to a large code base, 4.5 million lines of code, comprising of 200 bundle projects in the Eclipse ecosystem (e.g., Eclipse JDT, Eclipse PDE, Apache Commons, Hamcrest, etc.). Also, different from many other studies on detecting refactoring, ReDex only searches for code fragments that can be, but have not yet been, refactored in a way similar to some refactoring that happened in the code base. Our results show that ReDex can find 277 cross-function refactoring opportunities in 2 minutes, and 223 cases were labelled as true opportunities by users, and cover many categories of cross-function refactoring operations in classical refactoring books, such as Self Encapsulate Field, Decompose Conditional Expression, Hide Delegate, Preserve Whole Object, etc.","Vector-based representation, Refactoring, Software Evolution","","ISSTA 2014"
"Conference Paper","Marinescu P,Hosek P,Cadar C","Covrig: A Framework for the Analysis of Code, Test, and Coverage Evolution in Real Software","","2014","","","93–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 International Symposium on Software Testing and Analysis","San Jose, CA, USA","2014","9781450326452","","https://doi.org/10.1145/2610384.2610419;http://dx.doi.org/10.1145/2610384.2610419","10.1145/2610384.2610419","Software repositories provide rich information about the construction and evolution of software systems. While static data that can be mined directly from version control systems has been extensively studied, dynamic metrics concerning the execution of the software have received much less attention, due to the inherent difficulty of running and monitoring a large number of software versions. In this paper, we present Covrig, a flexible infrastructure that can be used to run each version of a system in isolation and collect static and dynamic software metrics, using a lightweight virtual machine environment that can be deployed on a cluster of local or cloud machines. We use Covrig to conduct an empirical study examining how code and tests co-evolve in six popular open-source systems. We report the main characteristics of software patches, analyse the evolution of program and patch coverage, assess the impact of nondeterminism on the execution of test suites, and investigate whether the coverage of code containing bugs and bug fixes is higher than average.","Patch characteristics, latent patch cover- age, nondeterministic coverage, coverage evolution, bugs and fixes","","ISSTA 2014"
"Conference Paper","Xing F,You H","Workload Aware Utilization Optimization for a Petaflop Supercomputer: Evidence Based Assessment Using Statistical Methods","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment","Atlanta, GA, USA","2014","9781450328937","","https://doi.org/10.1145/2616498.2616536;http://dx.doi.org/10.1145/2616498.2616536","10.1145/2616498.2616536","Nowadays, computing resources like supercomputers are shared by many users. Most systems are equipped with batch systems as their resource managers. From a user's perspective, the overall turnaround of each submitted job is measured by time-to-solution which consists of the sum of batch queuing time and execution time. On a busy machine, most jobs spend more time waiting in the batch queue than their real job executions. And rarely this is a topic of performance tuning and optimization of parallel computing. we propose a workload aware method systematically to predict jobs' batch queue waiting time patterns. Consequently, it will help user to optimize utilization and improve productivity. With workload data gathered from a supercomputer, we apply Bayesian framework to predict the temporal trend of long-time batch queue waiting probability. Thus, the workload of the machine not only can be predicted, we are able to provide users with a monthly updated reference chart to suggest job submission assembled with better chosen number of CPU and running time requests, which will avoid long-time waiting in batch queue. Our experiment shows that the model could make over 89% correct predictions for all cases we have tested.","workload, queuing time, near repeat, Kraken","","XSEDE '14"
"Conference Paper","Guo C,Herranz L,Wu L,Jiang S","Region Annotations in Hashing Based Image Retrieval","","2014","","","355–358","Association for Computing Machinery","New York, NY, USA","Proceedings of International Conference on Internet Multimedia Computing and Service","Xiamen, China","2014","9781450328104","","https://doi.org/10.1145/2632856.2632948;http://dx.doi.org/10.1145/2632856.2632948","10.1145/2632856.2632948","During recent years, large-scale data has become more and more present in our daily life, requiring better, faster, and more effective ways to discriminate between unrelated content and what we are truly interested in. In particular, retrieving similar images is a fundamental problem in both image processing and computer vision. Since finding similar images high-dimensional image features is not feasible in practice, approximate hashing techniques have been proved very effective due to their great efficiency and reasonably accuracy. Current hashing methods focus on image as a whole. However, sometimes the relevant content might be just bounded to a region of interest while the background part seems to be needless for users. In this paper, we propose to exploit region-level annotations, whenever they are available, for both training supervised hashing methods and also in the query image. Our objective is to evaluate whether region-level features can be helpful in a supervised hashing scenario. Experimental results confirm that region-level annotations in both the query and training images increase the accuracy.","Region-based hashing, supervised hashing, image retrieval","","ICIMCS '14"
"Journal Article","Bland M","Finding More than One Worm in the Apple","Commun. ACM","2014","57","7","58–64","Association for Computing Machinery","New York, NY, USA","","","2014-07","","0001-0782","https://doi.org/10.1145/2622630;http://dx.doi.org/10.1145/2622630","10.1145/2622630","If you see something, say something.","","",""
"Conference Paper","Frajták K,Bureš M,Jelínek I","Reducing User Input Validation Code in Web Applications Using Pex Extension","","2014","","","302–308","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Computer Systems and Technologies","Ruse, Bulgaria","2014","9781450327534","","https://doi.org/10.1145/2659532.2659633;http://dx.doi.org/10.1145/2659532.2659633","10.1145/2659532.2659633","Validation of user input data is very important in web application. Not only it protects the system from various exploits, but it also improves the user experience. User immediately sees what values are missing or are not valid and should be fixed. It is important to validate code on client side in the browser, but that does not mean that the validation on server side can be omitted. The golden rule of the web applications is not to trust user input and validate code on server side as well. The user input validation is therefore duplicated -- it validates the input values first on client side using JavaScript before the data is sent to server and then the received data is validated again on the server side. Changes made to the validation code must be synchronized in code on both sides. All implementations must be also unit tested, multiple sets of unit tests must be created and maintained. We will describe how we extended white-box testing tool Pex to generate user input validation code for web applications created on .NET platform. The JavaScript client side validation code is generated from the controller code written in C#. The code then validates input values on the client side. Most of the testing can be automated executing generated test. Testing resources -- i.e time spent on testing and number of testers involved people -- are saved.","user input validation, web application testing, code generation","","CompSysTech '14"
"Conference Paper","Clarke D,Clear T,Fisler K,Hauswirth M,Krishnamurthi S,Politz JG,Tirronen V,Wrigstad T","In-Flow Peer Review","","2014","","","59–79","Association for Computing Machinery","New York, NY, USA","Proceedings of the Working Group Reports of the 2014 on Innovation & Technology in Computer Science Education Conference","Uppsala, Sweden","2014","9781450334068","","https://doi.org/10.1145/2713609.2713612;http://dx.doi.org/10.1145/2713609.2713612","10.1145/2713609.2713612","Peer-review is a valuable tool that helps both the reviewee, who receives feedback about his work, and the reviewer, who sees different potential solutions and improves her ability to critique work. In-flow peer-review (IFPR) is peer-review done while an assignment is in progress. Peer-review done during this time is likely to result in greater motivation for both reviewer and reviewee. This workinggroup report summarizes IFPR and discusses numerous dimensions of the process, each of which alleviates some problems while raising associated concerns.","","","ITiCSE-WGR '14"
"Conference Paper","Politz JG,Patterson D,Krishnamurthi S,Fisler K","CaptainTeach: Multi-Stage, in-Flow Peer Review for Programming Assignments","","2014","","","267–272","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 Conference on Innovation & Technology in Computer Science Education","Uppsala, Sweden","2014","9781450328333","","https://doi.org/10.1145/2591708.2591738;http://dx.doi.org/10.1145/2591708.2591738","10.1145/2591708.2591738","Computing educators have used peer review in various ways in courses at many levels. Few of these efforts have applied peer review to multiple deliverables (such as specifications, tests, and code) within the same programming problem, or to assignments that are still in progress (as opposed to completed). This paper describes CaptainTeach, a programming environment enhanced with peer-review capabilities at multiple stages within assignments in progress. Multi-stage, in-flow peer review raises many logistical and pedagogical issues. This paper describes CaptainTeach and our experience using it in two undergraduate courses (one first-year and one upper-level); our analysis emphasizes issues that arise from the conjunction of multiple stages and in-flow reviewing, rather than peer review in general.","peer review, testing, learning environments","","ITiCSE '14"
"Conference Paper","Viennot N,Garcia E,Nieh J","A Measurement Study of Google Play","","2014","","","221–233","Association for Computing Machinery","New York, NY, USA","The 2014 ACM International Conference on Measurement and Modeling of Computer Systems","Austin, Texas, USA","2014","9781450327893","","https://doi.org/10.1145/2591971.2592003;http://dx.doi.org/10.1145/2591971.2592003","10.1145/2591971.2592003","Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.","authentication, oauth, android, google play, security, mobile computing, clone detection, decompilation","","SIGMETRICS '14"
"Journal Article","Viennot N,Garcia E,Nieh J","A Measurement Study of Google Play","SIGMETRICS Perform. Eval. Rev.","2014","42","1","221–233","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0163-5999","https://doi.org/10.1145/2637364.2592003;http://dx.doi.org/10.1145/2637364.2592003","10.1145/2637364.2592003","Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook.","android, authentication, mobile computing, clone detection, oauth, google play, decompilation, security","",""
"Conference Paper","Kim CH,Rhee J,Zhang H,Arora N,Jiang G,Zhang X,Xu D","IntroPerf: Transparent Context-Sensitive Multi-Layer Performance Inference Using System Stack Traces","","2014","","","235–247","Association for Computing Machinery","New York, NY, USA","The 2014 ACM International Conference on Measurement and Modeling of Computer Systems","Austin, Texas, USA","2014","9781450327893","","https://doi.org/10.1145/2591971.2592008;http://dx.doi.org/10.1145/2591971.2592008","10.1145/2591971.2592008","Performance bugs are frequently observed in commodity software. While profilers or source code-based tools can be used at development stage where a program is diagnosed in a well-defined environment, many performance bugs survive such a stage and affect production runs. OS kernel-level tracers are commonly used in post-development diagnosis due to their independence from programs and libraries; however, they lack detailed program-specific metrics to reason about performance problems such as function latencies and program contexts. In this paper, we propose a novel performance inference system, called IntroPerf, that generates fine-grained performance information -- like that from application profiling tools -- transparently by leveraging OS tracers that are widely available in most commodity operating systems. With system stack traces as input, IntroPerf enables transparent context-sensitive performance inference, and diagnoses application performance in a multi-layered scope ranging from user functions to the kernel. Evaluated with various performance bugs in multiple open source software projects, IntroPerf automatically ranks potential internal and external root causes of performance bugs with high accuracy without any prior knowledge about or instrumentation on the subject software. Our results show IntroPerf's effectiveness as a lightweight performance introspection tool for post-development diagnosis.","stack trace analysis, context-sensitive performance analysis, performance inference","","SIGMETRICS '14"
"Journal Article","Kim CH,Rhee J,Zhang H,Arora N,Jiang G,Zhang X,Xu D","IntroPerf: Transparent Context-Sensitive Multi-Layer Performance Inference Using System Stack Traces","SIGMETRICS Perform. Eval. Rev.","2014","42","1","235–247","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0163-5999","https://doi.org/10.1145/2637364.2592008;http://dx.doi.org/10.1145/2637364.2592008","10.1145/2637364.2592008","Performance bugs are frequently observed in commodity software. While profilers or source code-based tools can be used at development stage where a program is diagnosed in a well-defined environment, many performance bugs survive such a stage and affect production runs. OS kernel-level tracers are commonly used in post-development diagnosis due to their independence from programs and libraries; however, they lack detailed program-specific metrics to reason about performance problems such as function latencies and program contexts. In this paper, we propose a novel performance inference system, called IntroPerf, that generates fine-grained performance information -- like that from application profiling tools -- transparently by leveraging OS tracers that are widely available in most commodity operating systems. With system stack traces as input, IntroPerf enables transparent context-sensitive performance inference, and diagnoses application performance in a multi-layered scope ranging from user functions to the kernel. Evaluated with various performance bugs in multiple open source software projects, IntroPerf automatically ranks potential internal and external root causes of performance bugs with high accuracy without any prior knowledge about or instrumentation on the subject software. Our results show IntroPerf's effectiveness as a lightweight performance introspection tool for post-development diagnosis.","stack trace analysis, context-sensitive performance analysis, performance inference","",""
"Conference Paper","Edler von Koch TJ,Franke B,Bhandarkar P,Dasgupta A","Exploiting Function Similarity for Code Size Reduction","","2014","","","85–94","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems","Edinburgh, United Kingdom","2014","9781450328777","","https://doi.org/10.1145/2597809.2597811;http://dx.doi.org/10.1145/2597809.2597811","10.1145/2597809.2597811","For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4%, and up to 11.5% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.","function similarity, function merging, code size","","LCTES '14"
"Journal Article","Edler von Koch TJ,Franke B,Bhandarkar P,Dasgupta A","Exploiting Function Similarity for Code Size Reduction","SIGPLAN Not.","2014","49","5","85–94","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0362-1340","https://doi.org/10.1145/2666357.2597811;http://dx.doi.org/10.1145/2666357.2597811","10.1145/2666357.2597811","For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4%, and up to 11.5% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.","function similarity, code size, function merging","",""
"Conference Paper","Ramirez A,Falcon AJ,Santana OJ,Valero M","Author Retrospective for Software Trace Cache","","2014","","","45–47","Association for Computing Machinery","New York, NY, USA","ACM International Conference on Supercomputing 25th Anniversary Volume","Munich, Germany","2014","9781450328401","","https://doi.org/10.1145/2591635.2594508;http://dx.doi.org/10.1145/2591635.2594508","10.1145/2591635.2594508","In superscalar processors, capable of issuing and executing multiple instructions per cycle, fetch performance represents an upper bound to the overall processor performance. Unless there is some form of instruction re-use mechanism, you cannot execute instructions faster than you can fetch them. Instruction Level Parallelism, represented by wide issue out oforder superscalar processors, was the trending topic during the end of the 90's and early 2000's. It is indeed the most promising way to continue improving processor performance in a way that does not impact application development, unlike current multicore architectures which require parallelizing the applications (a process that is still far from being automated in the general case). Widening superscalar processor issue was the promise of neverending improvements to single thread performance, as identified by Yale N. Patt et al. in the 1997 special issue of IEEE Computer about ""Billion transistor processors"" [1].However, instruction fetch performance is limited by the control flow of the program. The basic fetch stage implementation can read instructions from a single cache line, starting from the current fetch address and up to the next control flow instruction. That is one basic block per cycle at most.Given that the typical basic block size in SPEC integer benchmarks is 4-6 instructions, fetch performance was limited to those same 4-6 instructions per cycle, making 8-wide and 16-wide superscalar processors impractical. It became imperative to find mechanisms to fetch more than 8 instructions per cycle, and that meant fetching more than one basic block per cycle.","superscalar processors, ilp, instruction fetch, binary translation","",""
"Conference Paper","David Y,Yahav E","Tracelet-Based Code Search in Executables","","2014","","","349–360","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation","Edinburgh, United Kingdom","2014","9781450327848","","https://doi.org/10.1145/2594291.2594343;http://dx.doi.org/10.1145/2594291.2594343","10.1145/2594291.2594343","We address the problem of code search in executables. Given a function in binary form and a large code base, our goal is to statically find similar functions in the code base. Towards this end, we present a novel technique for computing similarity between functions. Our notion of similarity is based on decomposition of functions into tracelets: continuous, short, partial traces of an execution. To establish tracelet similarity in the face of low-level compiler transformations, we employ a simple rewriting engine. This engine uses constraint solving over alignment constraints and data dependencies to match registers and memory addresses between tracelets, bridging the gap between tracelets that are otherwise similar. We have implemented our approach and applied it to find matches in over a million binary functions. We compare tracelet matching to approaches based on n-grams and graphlets and show that tracelet matching obtains dramatically better precision and recall.","static binary analysis, x86-64, x86","","PLDI '14"
"Journal Article","David Y,Yahav E","Tracelet-Based Code Search in Executables","SIGPLAN Not.","2014","49","6","349–360","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0362-1340","https://doi.org/10.1145/2666356.2594343;http://dx.doi.org/10.1145/2666356.2594343","10.1145/2666356.2594343","We address the problem of code search in executables. Given a function in binary form and a large code base, our goal is to statically find similar functions in the code base. Towards this end, we present a novel technique for computing similarity between functions. Our notion of similarity is based on decomposition of functions into tracelets: continuous, short, partial traces of an execution. To establish tracelet similarity in the face of low-level compiler transformations, we employ a simple rewriting engine. This engine uses constraint solving over alignment constraints and data dependencies to match registers and memory addresses between tracelets, bridging the gap between tracelets that are otherwise similar. We have implemented our approach and applied it to find matches in over a million binary functions. We compare tracelet matching to approaches based on n-grams and graphlets and show that tracelet matching obtains dramatically better precision and recall.","x86-64, static binary analysis, x86","",""
"Journal Article","Kaur R,Singh S","Clone Detection in Software Source Code Using Operational Similarity of Statements","SIGSOFT Softw. Eng. Notes","2014","39","3","1–5","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0163-5948","https://doi.org/10.1145/2597716.2597723;http://dx.doi.org/10.1145/2597716.2597723","10.1145/2597716.2597723","This paper presents a technique to detect clones in source code by comparing the operations performed in the statements comprising a function. The key concept used is that two functions are considered clones if the statements in the functions perform the same operation up to a certain extent. This could be ascertained by categorizing the available statement types based on the operations performed (for instance, addition, multiplication, function invocation, etc). Then, a category is assigned to each statement present in every function in the source code. Comparisons are then made between functions by comparing the categories of the statements to each other. If one function contains exactly the same categories of statement as another (same operations performed in both the functions), or contains a subset of statement categories (operations performed in one function are subset of another), then these functions are judged to be clones.","code quality, software clone detection","",""
"Journal Article","Singh S,Kaur R","Clone Detection in UML Class Models Using Class Metrics","SIGSOFT Softw. Eng. Notes","2014","39","3","1–3","Association for Computing Machinery","New York, NY, USA","","","2014-06","","0163-5948","https://doi.org/10.1145/2597716.2597726;http://dx.doi.org/10.1145/2597716.2597726","10.1145/2597716.2597726","This paper presents a technique to detect clones in UML class models. Class metrics (number of attributes, number of operations) of a class, class attribute names, root nodes, child nodes and class method names are compared with corresponding metrics, attribute names, root node, child nodes and method names of another class. Based on the number of matched attributes, operations and metrics, a percentage of cloning is calculated. We declare two classes as clones of each other if the matching percentage of the class metrics, class attribute names and class method names is greater than a specific threshold.","clone detection","",""
"Conference Paper","Bossert G,Guihéry F,Hiet G","Towards Automated Protocol Reverse Engineering Using Semantic Information","","2014","","","51–62","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security","Kyoto, Japan","2014","9781450328005","","https://doi.org/10.1145/2590296.2590346;http://dx.doi.org/10.1145/2590296.2590346","10.1145/2590296.2590346","Network security products, such as NIDS or application firewalls, tend to focus on application level communication flows. However, adding support for new proprietary and often undocumented protocols, implies the reverse engineering of these protocols. Currently, this task is performed manually. Considering the difficulty and time needed for manual reverse engineering of protocols, one can easily understand the importance of automating this task. This is even given more significance in today's cybersecurity context where reaction time and automated adaptation become a priority. Several studies were carried out to infer protocol's specifications from traces. As shown in this article, they do not provide accurate results on complex protocols and are often not applicable in an operational context to provide parsers or traffic generators, some key indicators of the quality of obtained specifications. In addition, too few previous works have resulted in the publication of tools that would allow the scientific community to experimentally validate and compare the different approaches.In this paper, we infer the specifications out of complex protocols by means of an automated approach and novel techniques. Based on communication traces, we reverse the vocabulary of a protocol by considering embedded contextual information. We also use this information to improve message clustering and to enhance the identification of fields boundaries. We then show the viability of our approach through a comparative study including our reimplementation of three other state-of-the-art approaches (ASAP, Discoverer and ScriptGen).","contextual clustering, protocol reverse engineering, semantic sequence alignment","","ASIA CCS '14"
"Conference Paper","Omar R,El-Mahdy A,Rohou E","Arbitrary Control-Flow Embedding into Multiple Threads for Obfuscation: A Preliminary Complexity and Performance Analysis","","2014","","","51–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Workshop on Security in Cloud Computing","Kyoto, Japan","2014","9781450328050","","https://doi.org/10.1145/2600075.2600080;http://dx.doi.org/10.1145/2600075.2600080","10.1145/2600075.2600080","With the proliferation of cloud computing, security becomes a key issue. Code obfuscation is a promising approach in that domain. It has been proposed to make an adversary confused about understanding the program, thereby making attacks more difficult. In this paper, we propose a novel method which exploits multi-core processing to substantially increase the complexity of programs, complicating the original ones. At basic block level, this method automatically partitions any serial thread into an arbitrary number of parallel threads. Moreover, the program semantics are preserved through using guards, guaranteeing that one basic-block be active at a time. Our method allows for $m^n$ possible combinations for a given program, for $m$ threads and $n$ basic-blocks, which significantly complicates the execution state, as well as changes the layout of basic blocks in memory. We provide a proof of correctness of the algorithm, as well as an implementation within the LLVM framework. For a naive implementation, a slow down of 10-130 is measured depending on the communication distance among threads, while code similarity (as a measure of complexity) is less than 2% in most cases. We further explore the effect of a thread affined loop assignment on performance; promising results indicate a maximum of three times slowdown, with negligible change in complexity.","cloud computing, performance, obfuscation, compilers","","SCC '14"
"Journal Article","Stolee KT,Elbaum S,Dobos D","Solving the Search for Source Code","ACM Trans. Softw. Eng. Methodol.","2014","23","3","","Association for Computing Machinery","New York, NY, USA","","","2014-06","","1049-331X","https://doi.org/10.1145/2581377;http://dx.doi.org/10.1145/2581377","10.1145/2581377","Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries.We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification.We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find approximate matches and then guide modifications to match the user specifications when exact matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified.","Semantic code search, symbolic analysis, SMT solvers, lightweight specification","",""
"Journal Article","Francesco N,Lettieri G,Santone A,Vaglini G","GreASE: A Tool for Efficient “Nonequivalence” Checking","ACM Trans. Softw. Eng. Methodol.","2014","23","3","","Association for Computing Machinery","New York, NY, USA","","","2014-06","","1049-331X","https://doi.org/10.1145/2560563;http://dx.doi.org/10.1145/2560563","10.1145/2560563","Equivalence checking plays a crucial role in formal verification to ensure the correctness of concurrent systems. However, this method cannot be scaled as easily with the increasing complexity of systems due to the state explosion problem. This article presents an efficient procedure, based on heuristic search, for checking Milner's strong and weak equivalence; to achieve higher efficiency, we actually search for a difference between two processes to be discovered as soon as possible, thus the heuristics aims to find a counterexample, even if not the minimum one, to prove nonequivalence. The presented algorithm builds the system state graph on-the-fly, during the checking, and the heuristics promotes the construction of the more promising subgraph. The heuristic function is syntax based, but the approach can be applied to different specification languages such as CCS, LOTOS, and CSP, provided that the language semantics is based on the concept of transition. The algorithm to explore the search space of the problem is based on a greedy technique; GreASE (Greedy Algorithm for System Equivalence), the tool supporting the approach, is used to evaluate the achieved reduction of both state-space size and time with respect to other verification environments.","heuristic searches, Formal methods, equivalence checking","",""
"Conference Paper","Hao S,Liu B,Nath S,Halfond WG,Govindan R","PUMA: Programmable UI-Automation for Large-Scale Dynamic Analysis of Mobile Apps","","2014","","","204–217","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services","Bretton Woods, New Hampshire, USA","2014","9781450327930","","https://doi.org/10.1145/2594368.2594390;http://dx.doi.org/10.1145/2594368.2594390","10.1145/2594368.2594390","Mobile app ecosystems have experienced tremendous growth in the last six years. This has triggered research on dynamic analysis of performance, security, and correctness properties of the mobile apps in the ecosystem. Exploration of app execution using automated UI actions has emerged as an important tool for this research. However, existing research has largely developed analysis-specific UI automation techniques, wherein the logic for exploring app execution is intertwined with the logic for analyzing app properties. PUMA is a programmable framework that separates these two concerns. It contains a generic UI automation capability (often called a Monkey) that exposes high-level events for which users can define handlers. These handlers can flexibly direct the Monkey's exploration, and also specify app instrumentation for collecting dynamic state information or for triggering changes in the environment during app execution. Targeted towards operators of app marketplaces, PUMA incorporates mechanisms for scaling dynamic analysis to thousands of apps. We demonstrate the capabilities of PUMA by analyzing seven distinct performance, security, and correctness properties for 3,600 apps downloaded from the Google Play store.","large scale, mobile apps, programming framework, separation of concerns, ui-automation, dynamic analysis","","MobiSys '14"
"Conference Paper","Muscar A","Programming Safe Agents in Blueprint","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)","Thessaloniki, Greece","2014","9781450325387","","https://doi.org/10.1145/2611040.2611098;http://dx.doi.org/10.1145/2611040.2611098","10.1145/2611040.2611098","While there are plenty of agent languages around, those who are targeted towards practical aspects are few and far between. Most alternatives serve mainly as experimentation vehicles for Distributed Artificial Intelligence (i.e. Jason, GOAL). It is our opinion that if agents are to get any mainstream attention they have to come with solutions to current problems like concurrency and distribution such as those found in real world scenarios, e.g. cloud and mobile systems. While some of the existing solutions partially address some of these issues, they do so in an unconvincing way.In this paper we propose Blueprint, an agent oriented programming language with built-in support for concurrency. Other key features are static typing and checked communication protocols which prevent a part of the errors that can appear in agents developed with existing languages. We walk through Blueprint's main features, shortcomings, and future extensions, motivating our design decisions.","Agent oriented programming, concurrency, protocol verification","","WIMS '14"
"Conference Paper","Sampath P,Rajeev AC,Ramesh S","Translation Validation for Stateflow to C","","2014","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st Annual Design Automation Conference","San Francisco, CA, USA","2014","9781450327305","","https://doi.org/10.1145/2593069.2593237;http://dx.doi.org/10.1145/2593069.2593237","10.1145/2593069.2593237","Code generators play a critical role in the Model Based Development of complex software systems. This is particularly true in the automotive domain, where the code auto-generated from Simulink/Stateflow models is directly flashed onto embedded controllers. Testing based approaches are popular for validating the translation of models to code. However, these approaches cannot guarantee the absence of bugs introduced during translation.We use translation validation as an alternative to testing. By checking the equivalence between a Stateflow model and C code, it can provide formal guarantees about the absence of bugs introduced during translation. To the best of our knowledge, this is the first application of translation validation to code generation from hierarchical state-machine modeling languages. Our approach builds upon the significant progress over the last few years in software model-checking: we use a state-of-the-art off-the-shelf software model-checker, CBMC, to make translation validation practical for a complex modeling language like Stateflow. We have applied this approach to Stateflow models from the automotive domain.","C, Stateflow, Translation validation","","DAC '14"
"Conference Paper","Cong J,Ghodrat MA,Gill M,Grigorian B,Gururaj K,Reinman G","Accelerator-Rich Architectures: Opportunities and Progresses","","2014","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the 51st Annual Design Automation Conference","San Francisco, CA, USA","2014","9781450327305","","https://doi.org/10.1145/2593069.2596667;http://dx.doi.org/10.1145/2593069.2596667","10.1145/2593069.2596667","To drastically improve energy efficiency, we believe future processors need to go beyond parallelization and provide architecture support for customization, enabling systems to adapt to different application domains. In particular, we believe future architectures will make extensive use of accelerators to significantly reduce energy consumption. Such architectures present many new challenges and opportunities, such as accelerator synthesis, scheduling, sharing, virtualization, memory hierarchy optimization, and efficient compilation and runtime support. With respect to these areas, we review the progress of our research in the Center for Domain-Specific Computing (supported by the NSF Expeditions-in-Computing Award), and discuss ongoing work and additional challenges.","Chip multiprocessor, Hardware Accelerators, Accelerator Sharing, Accelerator Virtualization","","DAC '14"
"Conference Paper","Lin Y,Xing Z,Xue Y,Liu Y,Peng X,Sun J,Zhao W","Detecting Differences across Multiple Instances of Code Clones","","2014","","","164–174","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568298;http://dx.doi.org/10.1145/2568225.2568298","10.1145/2568225.2568298","Clone detectors find similar code fragments (i.e., instances of code clones) and report large numbers of them for industrial systems. To maintain or manage code clones, developers often have to investigate differences of multiple cloned code fragments. However,existing program differencing techniques compare only two code fragments at a time. Developers then have to manually combine several pairwise differencing results. In this paper, we present an approach to automatically detecting differences across multiple clone instances. We have implemented our approach as an Eclipse plugin and evaluated its accuracy with three Java software systems. Our evaluation shows that our algorithm has precision over 97.66% and recall over 95.63% in three open source Java projects. We also conducted a user study of 18 developers to evaluate the usefulness of our approach for eight clone-related refactoring tasks. Our study shows that our approach can significantly improve developers’performance in refactoring decisions, refactoring details, and task completion time on clone-related refactoring tasks. Automatically detecting differences across multiple clone instances also opens opportunities for building practical applications of code clones in software maintenance, such as auto-generation of application skeleton, intelligent simultaneous code editing.","Code clone, Program differencing, Human study","","ICSE 2014"
"Conference Paper","Chen K,Liu P,Zhang Y","Achieving Accuracy and Scalability Simultaneously in Detecting Application Clones on Android Markets","","2014","","","175–186","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568286;http://dx.doi.org/10.1145/2568225.2568286","10.1145/2568225.2568286","Besides traditional problems such as potential bugs, (smartphone) application clones on Android markets bring new threats. That is, attackers clone the code from legitimate Android applications, assemble it with malicious code or advertisements, and publish these ``purpose-added"" app clones on the same or other markets for benefits. Three inherent and unique characteristics make app clones difficult to detect by existing techniques: a billion opcode problem caused by cross-market publishing, gap between code clones and app clones, and prevalent Type 2 and Type 3 clones. Existing techniques achieve either accuracy or scalability, but not both. To achieve both goals, we use a geometry characteristic, called centroid, of dependency graphs to measure the similarity between methods (code fragments) in two apps. Then we synthesize the method-level similarities and draw a Y/N conclusion on app (core functionality) cloning. The observed ``centroid effect"" and the inherent ``monotonicity"" property enable our approach to achieve both high accuracy and scalability. We implemented the app clone detection system and evaluated it on five whole Android markets (including 150,145 apps, 203 million methods and 26 billion opcodes). It takes less than one hour to perform cross-market app clone detection on the five markets after generating centroids only once.","clone detection, Software analysis, centroid, Android","","ICSE 2014"
"Conference Paper","Keivanloo I,Rilling J,Zou Y","Spotting Working Code Examples","","2014","","","664–675","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568292;http://dx.doi.org/10.1145/2568225.2568292","10.1145/2568225.2568292","Working code examples are useful resources for pragmatic reuse in software development. A working code example provides a solution to a specific programming problem. Earlier studies have shown that existing code search engines are not successful in finding working code examples. They fail in ranking high quality code examples at the top of the result set. To address this shortcoming, a variety of pattern-based solutions are proposed in the literature. However, these solutions cannot be integrated seamlessly in Internet-scale source code engines due to their high time complexity or query language restrictions. In this paper, we propose an approach for spotting working code examples which can be adopted by Internet-scale source code search engines. The time complexity of our approach is as low as the complexity of existing code search engines on the Internet and considerably lower than the pattern-based approaches supporting free-form queries. We study the performance of our approach using a representative corpus of 25,000 open source Java projects. Our findings support the feasibility of our approach for Internet-scale code search. We also found that our approach outperforms Ohloh Code search engine, previously known as Koders, in spotting working code examples.","working code example, clone detection, Source code search","","ICSE 2014"
"Conference Paper","Nguyen HV,Nguyen HA,Nguyen AT,Nguyen TN","Mining Interprocedural, Data-Oriented Usage Patterns in JavaScript Web Applications","","2014","","","791–802","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568302;http://dx.doi.org/10.1145/2568225.2568302","10.1145/2568225.2568302","A frequently occurring usage of program elements in a programming language and software libraries is called a usage pattern. In JavaScript (JS) Web applications, JS usage patterns in their source code have special characteristics that pose challenges in pattern mining. They involve nested data objects with no corresponding names or types. JS functions can be also used as data objects. JS usages are often cross-language, inter-procedural, and involve control and data flow dependencies among JS program entities and data objects whose data types are revealed only at run time due to dynamic typing in JS. This paper presents JSModel, a novel graph-based representation for JS usages, and JSMiner, a scalable approach to mine inter-procedural, data-oriented JS usage patterns. Our empirical evaluation on several Web programs shows that JSMiner efficiently detects more JS patterns with higher accuracy than a state-of-the-art approach. We conducted experiments to show JSModel's usefulness in two applications: detecting anti-patterns (buggy patterns) and documenting JS APIs via pattern skeletons. Our controlled experiment shows that the mined patterns are useful as JS documentation and code templates.","Mining, Usage Patterns, Web Applications, JavaScript","","ICSE 2014"
"Conference Paper","Negara S,Codoban M,Dig D,Johnson RE","Mining Fine-Grained Code Changes to Detect Unknown Change Patterns","","2014","","","803–813","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568317;http://dx.doi.org/10.1145/2568225.2568317","10.1145/2568225.2568317","Identifying repetitive code changes benefits developers, tool builders, and researchers. Tool builders can automate the popular code changes, thus improving the productivity of developers. Researchers can better understand the practice of code evolution, advancing existing code assistance tools and benefiting developers even further. Unfortunately, existing research either predominantly uses coarse-grained Version Control System (VCS) snapshots as the primary source of code evolution data or considers only a small subset of program transformations of a single kind - refactorings. We present the first approach that identifies previously unknown frequent code change patterns from a fine-grained sequence of code changes. Our novel algorithm effectively handles challenges that distinguish continuous code change pattern mining from the existing data mining techniques. We evaluated our algorithm on 1,520 hours of code development collected from 23 developers, and showed that it is effective, useful, and scales to large amounts of data. We analyzed some of the mined code change patterns and discovered ten popular kinds of high-level program transformations. More than half of our 420 survey participants acknowledged that eight out of ten transformations are relevant to their programming activities.","Code Changes, Data Mining, Program Transformation","","ICSE 2014"
"Conference Paper","Begel A,Zimmermann T","Analyze This! 145 Questions for Data Scientists in Software Engineering","","2014","","","12–23","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568233;http://dx.doi.org/10.1145/2568225.2568233","10.1145/2568225.2568233","In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.","Software Engineering, Analytics, Data Science","","ICSE 2014"
"Conference Paper","Ge X,Murphy-Hill E","Manual Refactoring Changes with Automated Refactoring Validation","","2014","","","1095–1105","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568280;http://dx.doi.org/10.1145/2568225.2568280","10.1145/2568225.2568280","Refactoring, the practice of applying behavior-preserving changes to existing code, can enhance the quality of software systems. Refactoring tools can automatically perform and check the correctness of refactorings. However, even when developers have these tools, they still perform about 90% of refactorings manually, which is error-prone. To address this problem, we propose a technique called GhostFactor separating transformation and correctness checking: we allow the developer to transform code manually, but check the correctness of her transformation automatically. We implemented our technique as a Visual Studio plugin, then evaluated it with a human study of eight software developers; GhostFactor improved the correctness of manual refactorings by 67%.","IDE, Restructuring, Refactoring, Tool","","ICSE 2014"
"Conference Paper","Vakilian M,Johnson RE","Alternate Refactoring Paths Reveal Usability Problems","","2014","","","1106–1116","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th International Conference on Software Engineering","Hyderabad, India","2014","9781450327565","","https://doi.org/10.1145/2568225.2568282;http://dx.doi.org/10.1145/2568225.2568282","10.1145/2568225.2568282","Modern Integrated Development Environments (IDEs) support many refactorings. Yet, programmers greatly underuse automated refactorings. Recent studies have applied traditional usability testing methodologies such as surveys, lab studies, and interviews to find the usability problems of refactoring tools. However, these methodologies can identify only certain kinds of usability problems. The critical incident technique (CIT) is a general methodology that uncovers usability problems by analyzing troubling user interactions. We adapt CIT to refactoring tools and show that alternate refactoring paths are indicators of the usability problems of refactoring tools. We define an alternate refactoring path as a sequence of user interactions that contains cancellations, reported messages, or repeated invocations of the refactoring tool. We evaluated our method on a large corpus of refactoring usage data, which we collected during a field study on 36 programmers over three months. This method revealed 15 usability problems, 13 of which were previously unknown. We reported these problems and proposed design improvements to Eclipse developers. The developers acknowledged all of the problems and have already fixed four of them. This result suggests that analyzing alternate paths is effective at discovering the usability problems of interactive program transformation (IPT) tools.","evaluation, usability, critical incident, Refactoring, empirical","","ICSE 2014"
"Conference Paper","Gajinov V,Stipić S,Erić I,Unsal OS,Ayguadé E,Cristal A","DaSH: A Benchmark Suite for Hybrid Dataflow and Shared Memory Programming Models: With Comparative Evaluation of Three Hybrid Dataflow Models","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM Conference on Computing Frontiers","Cagliari, Italy","2014","9781450328708","","https://doi.org/10.1145/2597917.2597942;http://dx.doi.org/10.1145/2597917.2597942","10.1145/2597917.2597942","The current trend in development of parallel programming models is to combine different well established models into a single programming model in order to support efficient implementation of a wide range of real world applications. The dataflow model has particularly managed to recapture the interest of the research community due to its ability to express parallelism efficiently. Thus, a number of recently proposed hybrid parallel programming models combine dataflow and traditional shared memory. Their findings have influenced the introduction of task dependency in the recently published OpenMP 4.0 standard.In this paper, we present DaSH - the first comprehensive benchmark suite for hybrid dataflow and shared memory programming models. DaSH features 11 benchmarks, each representing one of the Berkeley dwarfs that capture patterns of communication and computation common to a wide range of emerging applications. We also include sequential and shared-memory implementations based on OpenMP and TBB to facilitate easy comparison between hybrid dataflow implementations and traditional shared memory implementations based on work-sharing and/or tasks. Finally, we use DaSH to evaluate three different hybrid dataflow models, identify their advantages and shortcomings, and motivate further research on their characteristics.","transactional memory, shared memory, dataflow","","CF '14"
"Conference Paper","Javed MA,Zdun U","A Systematic Literature Review of Traceability Approaches between Software Architecture and Source Code","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering","London, England, United Kingdom","2014","9781450324762","","https://doi.org/10.1145/2601248.2601278;http://dx.doi.org/10.1145/2601248.2601278","10.1145/2601248.2601278","The links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system. There are several primary studies on traceability between software architecture and source code but so far no systematic literature review (SLR) has been undertaken. This study presents an SLR which has been carried out to discover the existing traceability approaches and tools between software architecture and source code, as well as the empirical evidence for these approaches, their benefits and liabilities, their relations to software architecture understanding, and issues, barriers, and challenges of the approaches. In our SLR the ACM Guide to Computing Literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing. The search strategy identified 742 citations, out of which 11 have been included in our study, dated from 1999 to July, 2013, after applying our inclusion and exclusion criteria. Our SLR resulted in the identification of the current state-of-the-art of traceability approaches and tools between software architecture and source code, as well as gaps and pointers for further research. Moreover, the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them.","source code, software architecture, systematic literature review, traceability","","EASE '14"
"Journal Article","Bland M","Finding More Than One Worm in the Apple: If You See Something, Say Something","Queue","2014","12","5","10–21","Association for Computing Machinery","New York, NY, USA","","","2014-05","","1542-7730","https://doi.org/10.1145/2620660.2620662;http://dx.doi.org/10.1145/2620660.2620662","10.1145/2620660.2620662","In February Apple revealed and fixed an SSL (Secure Sockets Layer) vulnerability that had gone undiscovered since the release of iOS 6.0 in September 2012. It left users vulnerable to man-in-the-middle attacks thanks to a short circuit in the SSL/TLS (Transport Layer Security) handshake algorithm introduced by the duplication of a goto statement. Since the discovery of this very serious bug, many people have written about potential causes. A close inspection of the code, however, reveals not only how a unit test could have been written to catch the bug, but also how to refactor the existing code to make the algorithm testable - as well as more clues to the nature of the error and the environment that produced it.","","",""
"Conference Paper","Fast E,Steffee D,Wang L,Brandt JR,Bernstein MS","Emergent, Crowd-Scale Programming Practice in the IDE","","2014","","","2491–2500","Association for Computing Machinery","New York, NY, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","Toronto, Ontario, Canada","2014","9781450324731","","https://doi.org/10.1145/2556288.2556998;http://dx.doi.org/10.1145/2556288.2556998","10.1145/2556288.2556998","While emergent behaviors are uncodified across many domains such as programming and writing, interfaces need explicit rules to support users. We hypothesize that by codifying emergent programming behavior, software engineering interfaces can support a far broader set of developer needs. To explore this idea, we built Codex, a knowledge base that records common practice for the Ruby programming language by indexing over three million lines of popular code. Codex enables new data-driven interfaces for programming systems: statistical linting, identifying code that is unlikely to occur in practice and may constitute a bug; pattern annotation, automatically discovering common programming idioms and annotating them with metadata using expert crowdsourcing; and library generation, constructing a utility package that encapsulates and reflects emergent software practice. We evaluate these applications to find Codex captures a broad swatch of programming practice, statistical linting detects problematic code snippets, and pattern annotation discovers nontrivial idioms such as basic HTTP authentication and database migration templates. Our work suggests that operationalizing practice-driven knowledge in structured domains such as programming can enable a new class of user interfaces.","programming tools, data mining","","CHI '14"
"Conference Paper","Gurgel A,Macia I,Garcia A,von Staa A,Mezini M,Eichberg M,Mitschke R","Blending and Reusing Rules for Architectural Degradation Prevention","","2014","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Modularity","Lugano, Switzerland","2014","9781450327725","","https://doi.org/10.1145/2577080.2577087;http://dx.doi.org/10.1145/2577080.2577087","10.1145/2577080.2577087","As software systems are maintained, their architecture often de-grades through the processes of architectural drift and erosion. These processes are often intertwined and the same modules in the code become the locus of both drift and erosion symptoms. Thus, architects should elaborate architecture rules for detecting occur-rences of both degradation symptoms. While the specification of such rules is time-consuming, they are similar across software projects adhering to similar architecture decompositions. Unfortu-nately, existing anti-degradation techniques are limited as they focus only on detecting either drift or erosion symptoms. They also do not support the reuse of recurring anti-degradation rules. In this context, the contribution of this paper is twofold. First, it presents TamDera, a domain-specific language for: (i) specifying rule-based strategies to detect both erosion and drift symptoms, and (ii) promoting the hierarchical and compositional reuse of design rules across multiple projects. The language was designed with usual concepts from programming languages in mind such as, inheritance and modularization. Second, we evaluated to what extent developers would benefit from the definition and reuse of hybrid rules. Our study involved 21 versions pertaining to 5 software projects, and more than 600 rules. On average 45% of classes that had drift symptoms in first versions presented inter-related erosion problems in latter versions or vice-versa. Also, up to 72% of all the TamDera rules in a project are from a pre-defined library of reusable rules. They were responsible for detecting on average of 73% of the inter-related degradation symptoms across the projects.","design rules, architectural degradation, reuse","","MODULARITY '14"
"Conference Paper","Cho J,Ryu S","JavaScript Module System: Exploring the Design Space","","2014","","","229–240","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Modularity","Lugano, Switzerland","2014","9781450327725","","https://doi.org/10.1145/2577080.2577088;http://dx.doi.org/10.1145/2577080.2577088","10.1145/2577080.2577088","While JavaScript is one of the most widely used programming languages not only for web applications but also for large projects, it does not provide a language-level module system. JavaScript developers have used the module pattern to avoid name conflicts by themselves, but the prevalent uses of multiple libraries and even multiple versions of a single library in one application complicate maintenance of namespace. The next release of the JavaScript language specification will support a module system, but the module proposal in prose does not clearly describe its semantics. Several tools attempt to support the new features in the next release of JavaScript by translating them into the current JavaScript, but their module semantics do not faithfully implement the proposal.In this paper, we identify some of the design issues in the JavaScript module system. We describe ambiguous or undefined semantics of the module system with concrete examples, show how the existing tools support them in a crude way, and discuss reasonable choices for the design issues. We specify the formal semantics of the module system, which provides unambiguous description of the design choices, and we provide its implementation as a source-to-source transformation from JavaScript with modules to the plain JavaScript that the current JavaScript engines can evaluate.","module system, source-to-source transformation, javascript","","MODULARITY '14"
"Conference Paper","Denil J,Mosterman PJ,Vangheluwe H","Rule-Based Model Transformation for, and in Simulink","","2014","","","","Society for Computer Simulation International","San Diego, CA, USA","Proceedings of the Symposium on Theory of Modeling & Simulation - DEVS Integrative","Tampa, Florida","2014","","","","","Over the past decade, the design of embedded systems has come to rely on models as electronic artifacts that are both analysable and executable. Such executable models are at the core of Model-Based Design. Simulink® is a popular Model-Based Design tool that supports simulation of models in various stages of design. While Simulink supports relating the various different models used in design, the technology to do so relies on the underlying Simulink code base. Instead, this paper employs explicit models of the relations between the various different design models. In particular, a rule-based approach is presented for model-to-model transformations. The abstraction from the code base provides benefits such as a more intuitive representation and the ability to more effectively reason about the transformations. The transformation rules and schedules are designed by augmenting standard Simulink model elements (e.g., blocks) for use in model transformation based on the structured RAMification approach. The approach is illustrated by the transformation of a continuous-time model, part of an adaptive controller, to a disrete-time counterpart, which is consecutively optimized for simulation.","model-driven engineering, model-based design, model-transformation, simulink","","DEVS '14"
"Journal Article","Ganesan D,Lindvall M","ADAM: External Dependency-Driven Architecture Discovery and Analysis of Quality Attributes","ACM Trans. Softw. Eng. Methodol.","2014","23","2","","Association for Computing Machinery","New York, NY, USA","","","2014-04","","1049-331X","https://doi.org/10.1145/2529998;http://dx.doi.org/10.1145/2529998","10.1145/2529998","This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed.","testability, quality, knowledge base, software architecture, module and runtime views, external entities, Concerns, maintainability, reverse engineering","",""
"Journal Article","Pan K,Wu X,Xie T","Guided Test Generation for Database Applications via Synthesized Database Interactions","ACM Trans. Softw. Eng. Methodol.","2014","23","2","","Association for Computing Machinery","New York, NY, USA","","","2014-04","","1049-331X","https://doi.org/10.1145/2491529;http://dx.doi.org/10.1145/2491529","10.1145/2491529","Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called SynDB that synthesizes new database interactions to replace the original ones from the database application under test. In this way, we bridge various constraints within a database application: query-construction constraints, query constraints, database schema constraints, and query-result-manipulation constraints. We then apply a state-of-the-art DSE engine called Pex for .NET from Microsoft Research to generate both program inputs and database states. The evaluation results show that tests generated by our approach can achieve higher code coverage than existing test generation approaches for database applications.","dynamic symbolic execution, synthesized database interactions, Automatic test generation, database application testing","",""
"Conference Paper","Weeks M","Creating a Web-Based, 2-D Action Game in JavaScript with HTML5","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2014 ACM Southeast Regional Conference","Kennesaw, Georgia","2014","9781450329231","","https://doi.org/10.1145/2638404.2638466;http://dx.doi.org/10.1145/2638404.2638466","10.1145/2638404.2638466","This paper details the author's findings and experience in developing a 2-D action game as a web-based application. The player controls an on-screen character in a 2-D world, collecting items along the way toward the goal of returning a chalice to the white castle, similar to the Atari 2600 game Adventure. The game uses JavaScript and HTML5, and does not need any additional software or plugins. Game content can be changed using a related ""world"" editor, also available as a web-page. This paper explores the game engine creation, with standard game concepts like tiles and sprites, along with HTML5 specifics like working with a canvas, accelerometer, sound, and keyboard.While other HTML5 based games do exist, there is little in terms of academic literature describing how this is done. This paper shows that a 2-D action game can be made with HTML5, and details how other developers could do it, too.","JavaScript, HTML, games, game engine","","ACM SE '14"
"Conference Paper","Ferreira M,Barbosa E,Macia I,Arcoverde R,Garcia A","Detecting Architecturally-Relevant Code Anomalies: A Case Study of Effectiveness and Effort","","2014","","","1158–1163","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th Annual ACM Symposium on Applied Computing","Gyeongju, Republic of Korea","2014","9781450324694","","https://doi.org/10.1145/2554850.2555036;http://dx.doi.org/10.1145/2554850.2555036","10.1145/2554850.2555036","Code anomalies are structural problems in the program. Even though they might represent symptoms of architecture degradation, several code anomalies do not contribute to this process. Source code inspection by developers might not support time-effective detection of architecturally-relevant anomalies in a program. Hence, they usually rely on multiple software metrics known to effectively detect code anomalies. However, there is still no empirical knowledge about the time effectiveness of metric-based strategies to detect architecturally-relevant anomalies. Given the longitudinal nature of this activity, we performed a first exploratory case study to address this gap. We compare metrics-based strategies with manual inspections made by the actual software developers. The study was conducted in the context of a legacy software system with 30K lines of code, 415 architectural elements, 210 versions, and embracing reengineering effort for almost 3 years. Effectiveness was assessed in terms of several quantitative and qualitative indicators. To measure the effort, we computed the amount of time used in several activities required to identify architecturally-relevant code anomalies. The results of our study shed light on potential effort reduction and effectiveness improvements of metrics-based strategies.","effectiveness, effort, code anomaly, architectural degeneration","","SAC '14"
"Conference Paper","Singh B,Shankar A,Wolff F,Papachristou C,Weyer D,Clay S","Cross-Correlation of Specification and RTL for Soft IP Analysis","","2014","","","","European Design and Automation Association","Leuven, BEL","Proceedings of the Conference on Design, Automation & Test in Europe","Dresden, Germany","2014","9783981537024","","","","Semiconductor companies often use 3rd party IPs in order to improve their design productivity. In practice, there are risks involved in using a 3rd party IP as bugs may creep in due to versioning issues, poor documentation, and mismatches between specification and RTL. As a result of this, 3rd party IP specification and RTL must be carefully evaluated. Our methodology addresses this issue, which cross-correlates specification and RTL to discover these discrepancies. The key innovative ideas in our approach are to use prior and trusted experience about designs, which include their specs and RTL code. Also, we have captured this trusted experience into two knowledge bases (KB), Spec-KB and RTL-KB. Finally, knowledge base rules are used to cross-correlate the RTL blocks to the specs. We have tested our approach by analyzing several 3rd party IPs. We have defined metrics for specification coverage and RTL identification coverage to quantify our results.","","","DATE '14"
"Conference Paper","Guerre A,Acquaviva JT,Lhuillier Y","A Unified Methodology for a Fast Benchmarking of Parallel Architecture","","2014","","","","European Design and Automation Association","Leuven, BEL","Proceedings of the Conference on Design, Automation & Test in Europe","Dresden, Germany","2014","9783981537024","","","","Benchmarking of architectures is today jeopardized by the explosion of parallel architectures and the dispersion of parallel programming models. Parallel programming requires architecture dependent compilers and languages as well as high programming expertise. Thus, an objective comparison has become a harder task. This paper presents a novel methodology to evaluate and to compare parallel architectures in order to ease the programmer work. It is based on the usage of micro-benchmarks, code profiling and characterization tools. The main contribution of this methodology is a semi-automatic prediction of the performance for sequential applications on a set of parallel architectures. In addition the performance estimation is correlated with the cost of other criteria such as power or portability. Our methodology prediction was validated on an industrial application. Results are within a range of 20%.","","","DATE '14"
"Conference Paper","Syer MD,Jiang ZM,Nagappan M,Hassan AE,Nasser M,Flora P","Continuous Validation of Load Test Suites","","2014","","","259–270","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering","Dublin, Ireland","2014","9781450327336","","https://doi.org/10.1145/2568088.2568101;http://dx.doi.org/10.1145/2568088.2568101","10.1145/2568088.2568101","Ultra-Large-Scale (ULS) systems face continuously evolving field workloads in terms of activated/disabled feature sets, varying usage patterns and changing deployment configurations. These evolving workloads often have a large impact, on the performance of a ULS system. Hence, continuous load testing is critical to ensuring the error-free operation of such systems. A common challenge facing performance analysts is to validate if a load test closely resembles the current field workloads. Such validation may be performed by comparing execution logs from the load test and the field. However, the size and unstructured nature of execution logs makes such a comparison unfeasible without automated support. In this paper, we propose an automated approach to validate whether a load test resembles the field workload and, if not, determines how they differ by compare execution logs from a load test and the field. Performance analysts can then update their load test cases to eliminate such differences, hence creating more realistic load test cases. We perform three case studies on two large systems: one open-source system and one enterprise system. Our approach identifies differences between load tests and the field with a precision of >75% compared to only >16% for the state-of-the-practice.","execution logs, performance engineering, continuous testing","","ICPE '14"
"Conference Paper","Gaudencio M,Dantas A,Guerrero DD","Can Computers Compare Student Code Solutions as Well as Teachers?","","2014","","","21–26","Association for Computing Machinery","New York, NY, USA","Proceedings of the 45th ACM Technical Symposium on Computer Science Education","Atlanta, Georgia, USA","2014","9781450326056","","https://doi.org/10.1145/2538862.2538973;http://dx.doi.org/10.1145/2538862.2538973","10.1145/2538862.2538973","In introductory programming courses it is common to demand from students exercises based on the production of code. However, it is difficult for the teacher to give fast feedback to the students about the main solutions tried, the main errors and the drawbacks and advantages of certain solutions. If we could use automatic code comparison algorithms to build visualisation tools to support the teacher in analysing how each solution provided is similar or different from another, such information would be able to be rapidly obtained. However, can computers compare students code solutions as well as teachers? In this work we present an experiment in which we have requested teachers to compare different code solutions to the same problem. Then we have evaluated the level of agreement among each teacher comparison strategy and some algorithms generally used for plagiarism detection and automatic grading. We found out a maximum rate of 77% of agreement between one of the teachers and the algorithms, but a minimum agreement of 75%. However, for most of the teachers, the maximum agreement rate was over 90% for at least one of the automatic strategies to compare code. We have also detected that the level of agreement among teachers regarding their personal strategies to compare students solutions was between 62% and 95%, which shows that there may be more agreement between a teacher and an algorithm than between a teacher and one of her colleagues regarding their strategies to compare students' solutions. The results also seem to support that comparison of students' codes has significant potential to be automated to help teachers in their work.","CS1, programming, code comparison algorithms","","SIGCSE '14"
"Conference Paper","Mitchell M,Tian G,Wang Z","Systematic Audit of Third-Party Android Phones","","2014","","","175–186","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM Conference on Data and Application Security and Privacy","San Antonio, Texas, USA","2014","9781450322782","","https://doi.org/10.1145/2557547.2557557;http://dx.doi.org/10.1145/2557547.2557557","10.1145/2557547.2557557","Android has become the leading smartphone platform with hundreds of devices from various manufacturers available on the market today. All these phones closely resemble each other with similar hardware and software features. Manufacturers must therefore customize the official Android system to differentiate their devices. Unfortunately, such heavily customization by third-party manufacturers often leads to serious vulnerabilities that do not exist in the official Android system. In this paper, we propose a comparative approach to systematically audit software in third-party phones by comparing them side-by-side to the official system. Specifically, we first retrieve pre-loaded apps and libraries from the phone and build a matching base system from the Android open source project repository. We then compare corresponding apps and libraries for potential vulnerabilities. To facilitate this process, we have designed and implemented DexDiff, a system that can pinpoint fine structural differences between two Android binaries and also present the changes in their surrounding contexts. Our experiments show that DexDiff is efficient and scalable. For example, it spends less than two and half minutes to process two 16.5MB (in total) files. DexDiff is also able to reveal a new vulnerability and details of the invasive CIQ mobile intelligence software.","dexdiff, android, security audit, static analysis, bindiff","","CODASPY '14"
"Conference Paper","Wang J,Rubin N,Yalamanchili S","ParallelJS: An Execution Framework for JavaScript on Heterogeneous Systems","","2014","","","72–80","Association for Computing Machinery","New York, NY, USA","Proceedings of Workshop on General Purpose Processing Using GPUs","Salt Lake City, UT, USA","2014","9781450327664","","https://doi.org/10.1145/2588768.2576788;http://dx.doi.org/10.1145/2588768.2576788","10.1145/2588768.2576788","JavaScript has been recognized as one of the most widely used script languages. Optimizations of JavaScript engines on mainstream web browsers enable efficient execution of JavaScript programs on CPUs. However, running JavaScript applications on emerging heterogeneous architectures that feature massively parallel hardware such as GPUs has not been well studied.This paper proposes a framework for flexible mapping of JavaScript onto heterogeneous systems that have both CPUs and GPUs. The framework includes a frontend compiler, a construct library and a runtime system. JavaScript programs written with high-level constructs are compiled to GPU binary code and scheduled to GPUs by the runtime. Experiments show that the proposed framework achieves up to 26.8x speedup executing JavaScript applications on parallel GPUs over a mainstream web browser that runs on CPUs.","Parallel Construct, JavaScript, GPGPU","","GPGPU-7"
"Conference Paper","Machado IC,Santos AR,Cavalcanti YC,Trzan EG,de Souza MM,de Almeida ES","Low-Level Variability Support for Web-Based Software Product Lines","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems","Sophia Antipolis, France","2014","9781450325561","","https://doi.org/10.1145/2556624.2556637;http://dx.doi.org/10.1145/2556624.2556637","10.1145/2556624.2556637","The Web systems domain has faced an increasing number of devices, browsers, and platforms to cope with, driving software systems to be more flexible to accomodate them. Software product line (SPL) engineering can be used as a strategy to implement systems capable of handling such a diversity. To this end, automated tool support is almost indispensable. However, current tool support gives more emphasis to modeling variability in the problem domain, over the support of variability at the solution domain. There is a need for mapping the variability between both abstraction levels, so as to determine what implementation impact a certain variability has. In this paper, we propose the FeatureJS, a FeatureIDE extension aiming at Javascript and HTML support for SPL engineering. The tool combines feature-oriented programming and preprocessors, as a strategy to map variability at source code with the variability modeled at a higher level of abstraction. We carried out a preliminary evaluation with an industrial project, aiming to characterize the capability of the tool to handle SPL engineering in the Web systems domain.","Eclipse plugin, web systems domain, feature oriented software development, FeatureIDE, software product line engineering, feature composition","","VaMoS '14"
"Conference Paper","Fenske W,Thüm T,Saake G","A Taxonomy of Software Product Line Reengineering","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems","Sophia Antipolis, France","2014","9781450325561","","https://doi.org/10.1145/2556624.2556643;http://dx.doi.org/10.1145/2556624.2556643","10.1145/2556624.2556643","In the context of single software systems, refactoring is commonly accepted to be the process of restructuring an existing body of code in order to improve its internal structure without changing its external behavior. This process is vital to the maintenance and evolution of software systems.Software product line engineering is a paradigm for the construction and customization of large-scale software systems. As systems grow in complexity and size, maintaining a clean structure becomes arguably more important. However, product line literature uses the term ""refactoring"" for such a wide range of reengineering activities that it has become difficult to see how these activities pertain to maintenance and evolution and how they are related.We improve this situation in the following way: i) We identify the dimensions along which product line reengineering occurs. ii) We derive a taxonomy that distinguishes and relates these reengineering activities. iii) We propose definitions for the three main branches of this taxonomy. iv) We classify a corpus of existing work.","taxonomy, reengineering, software product lines, refactoring","","VaMoS '14"
"Conference Paper","Berger T,Guo J","Towards System Analysis with Variability Model Metrics","","2014","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems","Sophia Antipolis, France","2014","9781450325561","","https://doi.org/10.1145/2556624.2556641;http://dx.doi.org/10.1145/2556624.2556641","10.1145/2556624.2556641","Variability models are central artifacts in highly configurable systems. They aim at planning, developing, and configuring systems by describing configuration knowledge at different levels of formality. The existence of large models using a variety of modeling concepts in heterogeneous languages with intricate semantics calls for a unified measuring approach. In this position paper, we attempt to take a first step towards such a measurement. We discuss perspectives of metrics, define low-level measurement goals, and conceive and implement metrics based on variability modeling concepts found in real-world languages and models. An evaluation of these metrics with real-world models and codebases provides insight into the benefits of such metrics for the defined perspectives.","empirical software engineering, metrics, variability modeling, feature modeling, software product lines","","VaMoS '14"
"Conference Paper","Chen X,Wang AY,Tempero E","A Replication and Reproduction of Code Clone Detection Studies","","2014","","","105–114","Australian Computer Society, Inc.","AUS","Proceedings of the Thirty-Seventh Australasian Computer Science Conference - Volume 147","Auckland, New Zealand","2014","9781921770302","","","","Code clones, fragments of code that are similar in some way, are regarded as costly. In order to understand the level of threat and opportunity of clones, we need to be able to efficiently detect clones in existing code. Recently, a new clone detection technique, CMCD, has been proposed. Our goal is to evaluate it and, if possible, improve on the original. We replicated the original study to evaluate the effectiveness of basic CMCD technique, improved it based on our experience with the replication, and applied it to a 43 open-source Java code from the Qualitas Corpus. We confirmed the effectiveness of the original technique but found some weaknesses. We improved the technique, and applied our improved technique. We found that that 1 in 2 systems had at least 10% cloned code, not counting the original, indicating that cloned code is quite common.","","","ACSC '14"
"Conference Paper","Krebbers R","An Operational and Axiomatic Semantics for Non-Determinism and Sequence Points in C","","2014","","","101–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","San Diego, California, USA","2014","9781450325448","","https://doi.org/10.1145/2535838.2535878;http://dx.doi.org/10.1145/2535838.2535878","10.1145/2535838.2535878","The C11 standard of the C programming language does not specify the execution order of expressions. Besides, to make more effective optimizations possible (eg. delaying of side-effects and interleaving), it gives compilers in certain cases the freedom to use even more behaviors than just those of all execution orders.Widely used C compilers actually exploit this freedom given by the C standard for optimizations, so it should be taken seriously in formal verification. This paper presents an operational and axiomatic semantics (based on separation logic) for non-determinism and sequence points in C. We prove soundness of our axiomatic semantics with respect to our operational semantics. This proof has been fully formalized using the Coq proof assistant.","operational semantics, coq, separation logic, c verification, interactive theorem proving","","POPL '14"
"Journal Article","Krebbers R","An Operational and Axiomatic Semantics for Non-Determinism and Sequence Points in C","SIGPLAN Not.","2014","49","1","101–112","Association for Computing Machinery","New York, NY, USA","","","2014-01","","0362-1340","https://doi.org/10.1145/2578855.2535878;http://dx.doi.org/10.1145/2578855.2535878","10.1145/2578855.2535878","The C11 standard of the C programming language does not specify the execution order of expressions. Besides, to make more effective optimizations possible (eg. delaying of side-effects and interleaving), it gives compilers in certain cases the freedom to use even more behaviors than just those of all execution orders.Widely used C compilers actually exploit this freedom given by the C standard for optimizations, so it should be taken seriously in formal verification. This paper presents an operational and axiomatic semantics (based on separation logic) for non-determinism and sequence points in C. We prove soundness of our axiomatic semantics with respect to our operational semantics. This proof has been fully formalized using the Coq proof assistant.","interactive theorem proving, c verification, operational semantics, separation logic, coq","",""
"Journal Article","Plank JS,Blaum M","Sector-Disk (SD) Erasure Codes for Mixed Failure Modes in RAID Systems","ACM Trans. Storage","2014","10","1","","Association for Computing Machinery","New York, NY, USA","","","2014-01","","1553-3077","https://doi.org/10.1145/2560013;http://dx.doi.org/10.1145/2560013","10.1145/2560013","Traditionally, when storage systems employ erasure codes, they are designed to tolerate the failures of entire disks. However, the most common types of failures are latent sector failures, which only affect individual disk sectors, and block failures which arise through wear on SSD’s. This article introduces SD codes, which are designed to tolerate combinations of disk and sector failures. As such, they consume far less storage resources than traditional erasure codes. We specify the codes with enough detail for the storage practitioner to employ them, discuss their practical properties, and detail an open-source implementation.","fault tolerance, sector failures, Erasure codes, storage systems, RAID","",""
"Conference Paper","Gauthier F,Lavoie T,Merlo E","Uncovering Access Control Weaknesses and Flaws with Security-Discordant Software Clones","","2013","","","209–218","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th Annual Computer Security Applications Conference","New Orleans, Louisiana, USA","2013","9781450320153","","https://doi.org/10.1145/2523649.2523650;http://dx.doi.org/10.1145/2523649.2523650","10.1145/2523649.2523650","Software clone detection techniques identify fragments of code that share some level of syntactic similarity. In this study, we investigate security-sensitive clone clusters: clusters of syntactically similar fragments of code that are protected by some privileges. From a security perspective, security-sensitive clone clusters can help reason about the implemented security model: given syntactically similar fragments of code, it is expected that they are protected by similar privileges. We hypothesize that clones that violate this assumption, defined as security-discordant clones, are likely to reveal weaknesses and flaws in access control models.In order to characterize security-discordant clones, we investigated two of the largest and most popular open-source PHP applications: Joomla! and Moodle, with sizes ranging from hundred thousands to more than a million lines of code. Investigation of security-discordant clone clusters in these systems revealed several previously undocumented, recurring, and application-independent security weaknesses. Moreover, security-discordant clones also revealed four, previously unreported, security flaws. Results also show how these flaws were revealed through the investigation of as little as 2% of the code base. Distribution of weaknesses and flaws between the two systems is investigated and discussed. Potential extensions to this exploratory work are also presented.","PHP, clones, measurements, flaws, security, access control","","ACSAC '13"
"Conference Paper","Kirat D,Nataraj L,Vigna G,Manjunath BS","SigMal: A Static Signal Processing Based Malware Triage","","2013","","","89–98","Association for Computing Machinery","New York, NY, USA","Proceedings of the 29th Annual Computer Security Applications Conference","New Orleans, Louisiana, USA","2013","9781450320153","","https://doi.org/10.1145/2523649.2523682;http://dx.doi.org/10.1145/2523649.2523682","10.1145/2523649.2523682","In this work, we propose SigMal, a fast and precise malware detection framework based on signal processing techniques. SigMal is designed to operate with systems that process large amounts of binary samples. It has been observed that many samples received by such systems are variants of previously-seen malware, and they retain some similarity at the binary level. Previous systems used this notion of malware similarity to detect new variants of previously-seen malware. SigMal improves the state-of-the-art by leveraging techniques borrowed from signal processing to extract noise-resistant similarity signatures from the samples. SigMal uses an efficient nearest-neighbor search technique, which is scalable to millions of samples. We evaluate SigMal on 1.2 million recent samples, both packed and unpacked, observed over a duration of three months. In addition, we also used a constant dataset of known benign executables. Our results show that SigMal can classify 50% of the recent incoming samples with above 99% precision. We also show that SigMal could have detected, on average, 70 malware samples per day before any antivirus vendor detected them.","signal processing, malware similarity, detection","","ACSAC '13"
"Journal Article","Zibran MF,Saha RK,Roy CK,Schneider KA","Genealogical Insights into the Facts and Fictions of Clone Removal","SIGAPP Appl. Comput. Rev.","2013","13","4","30–42","Association for Computing Machinery","New York, NY, USA","","","2013-12","","1559-6915","https://doi.org/10.1145/2577554.2577559;http://dx.doi.org/10.1145/2577554.2577559","10.1145/2577554.2577559","Clone management has drawn immense interest from the research community in recent years. It is recognized that a deep understanding of how code clones change and are refactored is necessary for devising effective clone management tools and techniques. This paper presents an empirical study based on the clone genealogies from a significant number of releases of nine software systems, to characterize the patterns of clone change and removal in evolving software systems. With a blend of qualitative analysis, quantitative analysis and statistical tests of significance, we address a number of research questions. Our findings reveal insights into the removal of individual clone fragments and provide empirical evidence in support of conventional clone evolution wisdom. The results can be used to devise informed clone management tools and techniques.","reengineering, refactoring, clone removal, clone evolution","",""
"Conference Paper","Faruki P,Ganmoor V,Laxmi V,Gaur MS,Bharmal A","AndroSimilar: Robust Statistical Feature Signature for Android Malware Detection","","2013","","","152–159","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Conference on Security of Information and Networks","Aksaray, Turkey","2013","9781450324984","","https://doi.org/10.1145/2523514.2523539;http://dx.doi.org/10.1145/2523514.2523539","10.1145/2523514.2523539","Android Smartphone popularity has increased malware threats forcing security researchers and AntiVirus (AV) industry to carve out smart methods to defend Smartphone against malicious apps. Robust signature based solutions to mitigate threats become necessary to protect the Smartphone and confidential user data. In this paper we present AndroSimilar, a robust approach which generates signature by extracting statistically improbable features, to detect malicious Android apps. Proposed method is effective against code obfuscation and repackaging, widely used techniques to evade AV signature and to propagate unseen variants of known malware. AndroSimilar is a syntactic foot-printing mechanism that finds regions of statistical similarity with known malware to detect those unknown, zero day samples. Syntactic file similarity of whole file is considered instead of just opcodes for faster detection compared to known fuzzy hashing approaches. Results demonstrate robust detection of variants of known malware families. Proposed approach can be refined to deploy as Smartphone AV.","code obfuscation, improbable features, statistical features, similarity digest, Android malware","","SIN '13"
"Conference Paper","Marks L,Zou Y,Keivanloo I","An Empirical Study of the Factors Affecting Co-Change Frequency of Cloned Code","","2013","","","161–175","IBM Corp.","USA","Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research","Ontario, Canada","2013","","","","","Code clones are duplicated code fragments that are copied to re-use functionality and speed up development. However, due to the duplicate nature of code clones, inconsistent updates can lead to defects in software system. We extend the existing studies on the inconsistent co-change characteristics, by investigating further factors that affect clone evolution. We study the effect of development cycles, the number of developers, method names similarity and code complexity. Our empirical study includes six industrial software systems to determine if the observations are statistically significant. We discover that one way to improve maintenance of code clones is to decrease code complexity. We find that increased code complexity leads to a decrease in co-change, which can lead to software defects. Likewise, we find that method name similarity is an important factor on co-change frequency of cloned code. From development cycles point of view, we observe that co-change frequency of cloned code does not change significantly from early to later and from development to defect fixing cycles. As a result, we suggest assigning a higher priority for early refactoring (i.e., within the first six months) of all cloned methods with infrequent co-change focusing on clone classes with low similarity in method names and high code complexity.","","","CASCON '13"
"Conference Paper","Karademir S,Dean T,Leblanc S","Using Clone Detection to Find Malware in Acrobat Files","","2013","","","70–80","IBM Corp.","USA","Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research","Ontario, Canada","2013","","","","","One common vector of malware is JavaScript in Adobe Acrobat(PDF) files. In this paper, we investigate using near miss clone detectors to find the malware. We start by collecting a set of PDF files containing JavaScript malware and a set with clean JavaScript from the VirusTotal repository. We use the NiCad clone detector to find the classes of clones in a small subset of the malicious PDF files. We evaluate how clone classes can be used to find similar malicious files in the rest of the malicious collection while avoiding files in the benign collection. Our results show that a small training set produced 87% detection of previously known malware with 1% false positives.","","","CASCON '13"
"Conference Paper","Tsantalis N,Guana V,Stroulia E,Hindle A","A Multidimensional Empirical Study on Refactoring Activity","","2013","","","132–146","IBM Corp.","USA","Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research","Ontario, Canada","2013","","","","","In this paper we present an empirical study on the refactoring activity in three well-known projects. We have studied five research questions that explore the different types of refactorings applied to different types of sources, the individual contribution of team members on refactoring activities, the alignment of refactoring activity with release dates and testing periods, and the motivation behind the applied refactorings. The studied projects have a history of 12, 7, and 6 years, respectively. We have found that there is very little variation in the types of refactorings applied on test code, since the majority of the refactorings focus on the reorganization and renaming of classes. Additionally, we have identified that the refactoring decision making and application is often performed by individual refactoring ""managers"". We have found a strong alignment between refactoring activity and release dates. Moreover, we found that the development teams apply a considerable amount of refactorings during testing periods. Finally, we have also found that in addition to code smell resolution the main drivers for applying refactorings are the introduction of extension points, and the resolution of backward compatibility issues.","","","CASCON '13"
"Conference Paper","Ihantola P,Helminen J,Karavirta V","How to Study Programming on Mobile Touch Devices: Interactive Python Code Exercises","","2013","","","51–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Koli Calling International Conference on Computing Education Research","Koli, Finland","2013","9781450324823","","https://doi.org/10.1145/2526968.2526974;http://dx.doi.org/10.1145/2526968.2526974","10.1145/2526968.2526974","Scaffolded learning tasks where programs are constructed from predefined code fragments by dragging and dropping them (i.e. Parsons problems) are well suited to mobile touch devices, but quite limited in their applicability. They do not adequately cater for different approaches to constructing a program. After studying solutions to automatically assessed programming exercises, we found out that many different solutions are composed of a relatively small set of mutually similar code lines. Thus, they can be constructed by using the drag-and-drop approach if only it was possible to edit some small parts of the predefined fragments. Based on this, we have designed and implemented a new exercise type for mobile devices that builds on Parsons problems and falls somewhere between their strict scaffolding and full-blown coding exercises. In these exercises, we can gradually fade the scaffolding and allow programs to be constructed more freely so as not to restrict thinking and limit creativity too much while still making sure we are able to deploy them to small-screen mobile devices. In addition to the new concept and the related implementation, we discuss other possibilities of how programming could be practiced on mobile devices.","parsons puzzle, parsons problem, mobile touch devices, mobile learning, mLearning, learning, teaching, Python, programming","","Koli Calling '13"
"Conference Paper","Simon,Cook B,Sheard J,Carbone A,Johnson C","Academic Integrity: Differences between Computing Assessments and Essays","","2013","","","23–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Koli Calling International Conference on Computing Education Research","Koli, Finland","2013","9781450324823","","https://doi.org/10.1145/2526968.2526971;http://dx.doi.org/10.1145/2526968.2526971","10.1145/2526968.2526971","There appears to be a reasonably common understanding about plagiarism and collusion in essays and other assessment items written in prose text. However, most assessment items in computing are not based in prose. There are computer programs, databases, spreadsheets, and web designs, to name but a few. It is far from clear that the same sort of consensus about plagiarism and collusion applies when dealing with such assessment items; and indeed it is not clear that computing academics have the same core beliefs about originality of authorship as apply in the world of prose. We have conducted focus groups at three Australian universities to investigate what academics and students in computing think constitute breaches of academic integrity in non-text-based assessment items; how they regard such breaches; and how academics discourage such breaches, detect them, and deal with those that are found. We find a general belief that non-text-based computing assessments differ in this regard from text-based assessments, that the boundaries between acceptable and unacceptable practice are harder to define than they are for text assessments, and that there is a case for applying different standards to these two different types of assessment. We conclude by discussing what we can learn from these findings.","academic integrity, computing education, non-text-based assessment","","Koli Calling '13"
"Journal Article","Graziotin D,Jedlitschka A","Recent Developments in Product-Focused Software Process Improvement: PROFES 2013 Conference Report","SIGSOFT Softw. Eng. Notes","2013","38","6","29–34","Association for Computing Machinery","New York, NY, USA","","","2013-11","","0163-5948","https://doi.org/10.1145/2532780.2532789;http://dx.doi.org/10.1145/2532780.2532789","10.1145/2532780.2532789","This report summarizes the presentations and discussions that happened at PROFES 2013, the 14th International Conference on Product- Focused Software Process Improvement, which was held June 12-14, 2013 in Paphos, Cyprus. The main theme of PROFES is software process improvement (SPI) motivated by product, process, and service quality needs. PROFES 2013 addressed both quality engineering and management topics, divided into the areas of Decision Support in Software Engineering, Empirical Software Engineering, Managing Software Processes, Safety-Critical Software Engineering, Software Measurement, Software Process Improvement, and Software Maintenance.","experimentation, human factors, management, standardization, measurement, security, performance","",""
"Conference Paper","Maggi F,Valdi A,Zanero S","AndroTotal: A Flexible, Scalable Toolbox and Service for Testing Mobile Malware Detectors","","2013","","","49–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third ACM Workshop on Security and Privacy in Smartphones & Mobile Devices","Berlin, Germany","2013","9781450324915","","https://doi.org/10.1145/2516760.2516768;http://dx.doi.org/10.1145/2516760.2516768","10.1145/2516760.2516768","Although there are controversial opinions regarding how large the mobile malware phenomenon is in terms of absolute numbers, hype aside, the amount of new Android malware variants is increasing. This trend is mainly due to the fact that, as it happened with traditional malware, the authors are striving to repackage, obfuscate, or otherwise transform the executable code of their malicious apps in order to evade mobile security apps. There are about 85 of these apps only on the official marketplace. However, it is not clear how effective they are. Indeed, the sandboxing mechanism of Android does not allow (security) apps to audit other apps.We present AndroTotal, a publicly available tool, malware repository and research framework that aims at mitigating the above challenges, and allow researchers to automatically scan Android apps against an arbitrary set of malware detectors. We implemented AndroTotal and released it to the research community in April 2013. So far, we collected 18,758 distinct submitted samples and received the attention of several research groups (1,000 distinct accounts), who integrated their malware-analysis services with ours.","testing, malware detectors, android, malware","","SPSM '13"
"Conference Paper","Tschudin PS,Réveillère L,Jiang L,Lo D,Lawall J,Muller G","Understanding the Genetic Makeup of Linux Device Drivers","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventh Workshop on Programming Languages and Operating Systems","Farmington, Pennsylvania","2013","9781450324601","","https://doi.org/10.1145/2525528.2525536;http://dx.doi.org/10.1145/2525528.2525536","10.1145/2525528.2525536","Attempts have been made to understand driver development in terms of code clones. In this paper, we propose an alternate view, based on the metaphor of a gene. Guided by this metaphor, we study the structure of Linux 3.10 ethernet platform driver probe functions.","","","PLOS '13"
"Conference Paper","Terry DB,Prabhakaran V,Kotla R,Balakrishnan M,Aguilera MK,Abu-Libdeh H","Consistency-Based Service Level Agreements for Cloud Storage","","2013","","","309–324","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles","Farminton, Pennsylvania","2013","9781450323888","","https://doi.org/10.1145/2517349.2522731;http://dx.doi.org/10.1145/2517349.2522731","10.1145/2517349.2522731","Choosing a cloud storage system and specific operations for reading and writing data requires developers to make decisions that trade off consistency for availability and performance. Applications may be locked into a choice that is not ideal for all clients and changing conditions. Pileus is a replicated key-value store that allows applications to declare their consistency and latency priorities via consistency-based service level agreements (SLAs). It dynamically selects which servers to access in order to deliver the best service given the current configuration and system conditions. In application-specific SLAs, developers can request both strong and eventual consistency as well as intermediate guarantees such as read-my-writes. Evaluations running on a worldwide test bed with geo-replicated data show that the system adapts to varying client-server latencies to provide service that matches or exceeds the best static consistency choice and server selection scheme.","consistency, cloud computing, storage, service level agreement, replication","","SOSP '13"
"Conference Paper","Smaragdakis Y,Balatsouras G,Kastrinis G","Set-Based Pre-Processing for Points-to Analysis","","2013","","","253–270","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages & Applications","Indianapolis, Indiana, USA","2013","9781450323741","","https://doi.org/10.1145/2509136.2509524;http://dx.doi.org/10.1145/2509136.2509524","10.1145/2509136.2509524","We present set-based pre-analysis: a virtually universal optimization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how object values flow through a program's variables. Set-based pre-analysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent value-based points-to computation. Effectively, set-based pre-analysis puts the program in a normal form optimized for points-to analysis. Compared to other techniques for off-line optimization of points-to analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: set-based pre-analysis can be performed on the input just once, e.g., allowing the pre-optimization of libraries that are subsequently reused many times and for different analyses. In experiments with Java programs, set-based pre-analysis eliminates 30% of the program's local variables and 30% or more of computed context-sensitive points-to facts, over a wide set of benchmarks and analyses, resulting in a 20% average speedup (max: 110%, median: 18%).","off-line, optimization, points-to analysis","","OOPSLA '13"
"Journal Article","Smaragdakis Y,Balatsouras G,Kastrinis G","Set-Based Pre-Processing for Points-to Analysis","SIGPLAN Not.","2013","48","10","253–270","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2544173.2509524;http://dx.doi.org/10.1145/2544173.2509524","10.1145/2544173.2509524","We present set-based pre-analysis: a virtually universal optimization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how object values flow through a program's variables. Set-based pre-analysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent value-based points-to computation. Effectively, set-based pre-analysis puts the program in a normal form optimized for points-to analysis. Compared to other techniques for off-line optimization of points-to analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: set-based pre-analysis can be performed on the input just once, e.g., allowing the pre-optimization of libraries that are subsequently reused many times and for different analyses. In experiments with Java programs, set-based pre-analysis eliminates 30% of the program's local variables and 30% or more of computed context-sensitive points-to facts, over a wide set of benchmarks and analyses, resulting in a 20% average speedup (max: 110%, median: 18%).","optimization, points-to analysis, off-line","",""
"Conference Paper","Treichler S,Bauer M,Aiken A","Language Support for Dynamic, Hierarchical Data Partitioning","","2013","","","495–514","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages & Applications","Indianapolis, Indiana, USA","2013","9781450323741","","https://doi.org/10.1145/2509136.2509545;http://dx.doi.org/10.1145/2509136.2509545","10.1145/2509136.2509545","Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a per-region (or subregion) basis.We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion's type system, and show Legion type checking improves performance by up to 71% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three real-world applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.","regions, aliasing, independence, coherence, legion, hierarchical scheduling, data partitioning, type system","","OOPSLA '13"
"Journal Article","Treichler S,Bauer M,Aiken A","Language Support for Dynamic, Hierarchical Data Partitioning","SIGPLAN Not.","2013","48","10","495–514","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2544173.2509545;http://dx.doi.org/10.1145/2544173.2509545","10.1145/2544173.2509545","Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a per-region (or subregion) basis.We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion's type system, and show Legion type checking improves performance by up to 71% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three real-world applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.","aliasing, data partitioning, legion, coherence, type system, hierarchical scheduling, regions, independence","",""
"Conference Paper","Bergan T,Ceze L,Grossman D","Input-Covering Schedules for Multithreaded Programs","","2013","","","677–692","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages & Applications","Indianapolis, Indiana, USA","2013","9781450323741","","https://doi.org/10.1145/2509136.2509508;http://dx.doi.org/10.1145/2509136.2509508","10.1145/2509136.2509508","We propose constraining multithreaded execution to small sets of input-covering schedules, which we define as follows: given a program P, we say that a set of schedules ∑ covers all inputs of program P if, when given any input, P's execution can be constrained to some schedule in ∑ and still produce a semantically valid result.Our approach is to first compute a small ∑ for a given program P, and then, at runtime, constrain P's execution to always follow some schedule in ∑, and never deviate. We have designed an algorithm that uses symbolic execution to systematically enumerate a set of input-covering schedules, ∑. To deal with programs that run for an unbounded length of time, we partition execution into bounded epochs, find input-covering schedules for each epoch in isolation, and then piece the schedules together at runtime. We have implemented this algorithm along with a constrained execution runtime for pthreads programs, and we report resultsOur approach has the following advantage: because all possible runtime schedules are known a priori, we can seek to validate the program by thoroughly verifying each schedule in ∑, in isolation, without needing to reason about the huge space of thread interleavings that arises due to conventional nondeterministic execution.","determinism, constrained execution, static analysis, symbolic execution","","OOPSLA '13"
"Journal Article","Bergan T,Ceze L,Grossman D","Input-Covering Schedules for Multithreaded Programs","SIGPLAN Not.","2013","48","10","677–692","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2544173.2509508;http://dx.doi.org/10.1145/2544173.2509508","10.1145/2544173.2509508","We propose constraining multithreaded execution to small sets of input-covering schedules, which we define as follows: given a program P, we say that a set of schedules ∑ covers all inputs of program P if, when given any input, P's execution can be constrained to some schedule in ∑ and still produce a semantically valid result.Our approach is to first compute a small ∑ for a given program P, and then, at runtime, constrain P's execution to always follow some schedule in ∑, and never deviate. We have designed an algorithm that uses symbolic execution to systematically enumerate a set of input-covering schedules, ∑. To deal with programs that run for an unbounded length of time, we partition execution into bounded epochs, find input-covering schedules for each epoch in isolation, and then piece the schedules together at runtime. We have implemented this algorithm along with a constrained execution runtime for pthreads programs, and we report resultsOur approach has the following advantage: because all possible runtime schedules are known a priori, we can seek to validate the program by thoroughly verifying each schedule in ∑, in isolation, without needing to reason about the huge space of thread interleavings that arises due to conventional nondeterministic execution.","constrained execution, symbolic execution, determinism, static analysis","",""
"Conference Paper","McDirmid S","Usable Live Programming","","2013","","","53–62","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software","Indianapolis, Indiana, USA","2013","9781450324724","","https://doi.org/10.1145/2509578.2509585;http://dx.doi.org/10.1145/2509578.2509585","10.1145/2509578.2509585","Programming today involves code editing mixed with bouts of debugging to get feedback on code execution. For programming to be more fluid, editing and debugging should occur concurrently as live programming. This paper describes how live execution feedback can be woven into the editor by making places in program execution, not just code, navigable so that evaluation results can be probed directly within the code editor. A pane aside the editor also traces execution with entries that are similarly navigable, enabling quick problem diagnosis. Both probes and traces are refreshed continuously during editing, and are easily configured based on debugging needs. We demonstrate the usefulness of this live programming experience with a prototype.","debugging, live programming","","Onward! 2013"
"Conference Paper","Zheng Y,Bulej L,Zhang C,Kell S,Ansaloni D,Binder W","Dynamic Optimization of Bytecode Instrumentation","","2013","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th ACM Workshop on Virtual Machines and Intermediate Languages","Indianapolis, Indiana, USA","2013","9781450326018","","https://doi.org/10.1145/2542142.2542145;http://dx.doi.org/10.1145/2542142.2542145","10.1145/2542142.2542145","Accuracy, completeness, and performance are all major concerns in the context of dynamic program analysis. Emphasizing one of these factors may compromise the other factors. For example, improving completeness of an analysis may seriously impair performance. In this paper, we present an analysis model and a framework that enables reducing analysis overhead at runtime through adaptive instrumentation of the base program. Our approach targets analyses implemented with code instrumentation techniques on the Java platform. Overhead reduction is achieved by removing instrumentation from code locations that are considered unimportant for the analysis results, thereby avoiding execution of analysis code for those locations. For some analyses, our approach preserves result accuracy and completeness. For other analyses, accuracy and completeness may be traded for a major performance improvement. In this paper, we explore accuracy, completeness, and performance of our approach with two concrete analyses as case studies.","runtime adaptation, performance, jvm bytecode instrumentation","","VMIL '13"
"Conference Paper","Park C,Lee H,Ryu S","All about the with Statement in JavaScript: Removing with Statements in JavaScript Applications","","2013","","","73–84","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Symposium on Dynamic Languages","Indianapolis, Indiana, USA","2013","9781450324335","","https://doi.org/10.1145/2508168.2508173;http://dx.doi.org/10.1145/2508168.2508173","10.1145/2508168.2508173","The with statement in JavaScript makes static analysis of JavaScript applications difficult by introducing a new scope at run time and thus invalidating lexical scoping. Therefore, many static approaches to JavaScript program analysis and the strict mode of ECMAScript 5 simply disallow the with statement. To justify exclusion of the with statement, we should better understand the actual usage patterns of the with statement.In this paper, we present the usage patterns of the with statement in real-world JavaScript applications currently used in the 898 most popular web sites. We investigate whether we can rewrite the with statements in each pattern to other statements not using the with statement. We show that we can rewrite all the static occurrences of the with statement that do not have any dynamic code generating functions. Even though the rewriting process is not applicable to any dynamically generated with statements, our results are still promising. Because all the static approaches that disallow the with statement also disallow dynamic code generation, such static approaches can allow the with statement using our rewriting process. We formally present our rewriting strategy, provide its implementation, and show its faithfulness using extensive testing. We believe that removing with statements will simplify JavaScript program analysis designs without considering dynamic scope introduction while imposing fewer syntactic restrictions.","with statements, javascript, rewritability, static analysis","","DLS '13"
"Journal Article","Park C,Lee H,Ryu S","All about the with Statement in JavaScript: Removing with Statements in JavaScript Applications","SIGPLAN Not.","2013","49","2","73–84","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2578856.2508173;http://dx.doi.org/10.1145/2578856.2508173","10.1145/2578856.2508173","The with statement in JavaScript makes static analysis of JavaScript applications difficult by introducing a new scope at run time and thus invalidating lexical scoping. Therefore, many static approaches to JavaScript program analysis and the strict mode of ECMAScript 5 simply disallow the with statement. To justify exclusion of the with statement, we should better understand the actual usage patterns of the with statement.In this paper, we present the usage patterns of the with statement in real-world JavaScript applications currently used in the 898 most popular web sites. We investigate whether we can rewrite the with statements in each pattern to other statements not using the with statement. We show that we can rewrite all the static occurrences of the with statement that do not have any dynamic code generating functions. Even though the rewriting process is not applicable to any dynamically generated with statements, our results are still promising. Because all the static approaches that disallow the with statement also disallow dynamic code generation, such static approaches can allow the with statement using our rewriting process. We formally present our rewriting strategy, provide its implementation, and show its faithfulness using extensive testing. We believe that removing with statements will simplify JavaScript program analysis designs without considering dynamic scope introduction while imposing fewer syntactic restrictions.","with statements, javascript, rewritability, static analysis","",""
"Conference Paper","Weiher M,Hirschfeld R","Polymorphic Identifiers: Uniform Resource Access in Objective-Smalltalk","","2013","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Symposium on Dynamic Languages","Indianapolis, Indiana, USA","2013","9781450324335","","https://doi.org/10.1145/2508168.2508169;http://dx.doi.org/10.1145/2508168.2508169","10.1145/2508168.2508169","In object-oriented programming, polymorphic dispatch of operations decouples clients from specific providers of services and allows implementations to be modified or substituted without affecting clients.The Uniform Access Principle (UAP) tries to extend these qualities to resource access by demanding that access to state be indistinguishable from access to operations. Despite language features supporting the UAP, the overall goal of substitutability has not been achieved for either alternative resources such as keyed storage, files or web pages, or for alternate access mechanisms: specific kinds of resources are bound to specific access mechanisms and vice versa. Changing storage or access patterns either requires changes to both clients and service providers and trying to maintain the UAP imposes significant penalties in terms of code-duplication and/or performance overhead.We propose introducing first class identifiers as polymorphic names for storage locations to solve these problems. With these Polymorphic Identifiers, we show that we can provide uniform access to a wide variety of resource types as well as storage and access mechanisms, whether parametrized or direct, without affecting client code, without causing code duplication or significant performance penalties.","extensibility, identifiers, rest, uniform access principle","","DLS '13"
"Journal Article","Weiher M,Hirschfeld R","Polymorphic Identifiers: Uniform Resource Access in Objective-Smalltalk","SIGPLAN Not.","2013","49","2","61–72","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2578856.2508169;http://dx.doi.org/10.1145/2578856.2508169","10.1145/2578856.2508169","In object-oriented programming, polymorphic dispatch of operations decouples clients from specific providers of services and allows implementations to be modified or substituted without affecting clients.The Uniform Access Principle (UAP) tries to extend these qualities to resource access by demanding that access to state be indistinguishable from access to operations. Despite language features supporting the UAP, the overall goal of substitutability has not been achieved for either alternative resources such as keyed storage, files or web pages, or for alternate access mechanisms: specific kinds of resources are bound to specific access mechanisms and vice versa. Changing storage or access patterns either requires changes to both clients and service providers and trying to maintain the UAP imposes significant penalties in terms of code-duplication and/or performance overhead.We propose introducing first class identifiers as polymorphic names for storage locations to solve these problems. With these Polymorphic Identifiers, we show that we can provide uniform access to a wide variety of resource types as well as storage and access mechanisms, whether parametrized or direct, without affecting client code, without causing code duplication or significant performance penalties.","identifiers, rest, extensibility, uniform access principle","",""
"Conference Paper","Schulze S,Liebig J,Siegmund J,Apel S","Does the Discipline of Preprocessor Annotations Matter? A Controlled Experiment","","2013","","","65–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th International Conference on Generative Programming: Concepts & Experiences","Indianapolis, Indiana, USA","2013","9781450323734","","https://doi.org/10.1145/2517208.2517215;http://dx.doi.org/10.1145/2517208.2517215","10.1145/2517208.2517215","The C preprocessor (CPP) is a simple and language-independent tool, widely used to implement variable software systems using conditional compilation (i.e., by including or excluding annotated code). Although CPP provides powerful means to express variability, it has been criticized for allowing arbitrary annotations that break the underlying structure of the source code. We distinguish between disciplined annotations, which align with the structure of the source code, and undisciplined annotations, which do not. Several studies suggest that especially the latter type of annotations makes it hard to (automatically) analyze the code. However, little is known about whether the type of annotations has an effect on program comprehension. We address this issue by means of a controlled experiment with human subjects. We designed similar tasks for both, disciplined and undisciplined annotations, to measure program comprehension. Then, we measured the performance of the subjects regarding correctness and response time for solving the tasks. Our results suggest that there are no differences between disciplined and undisciplined annotations from a program-comprehension perspective. Nevertheless, we observed that finding and correcting errors is a time-consuming and tedious task in the presence of preprocessor annotations.","controlled experiment, program comprehension, variability, disciplined annotations, c preprocessor","","GPCE '13"
"Journal Article","Schulze S,Liebig J,Siegmund J,Apel S","Does the Discipline of Preprocessor Annotations Matter? A Controlled Experiment","SIGPLAN Not.","2013","49","3","65–74","Association for Computing Machinery","New York, NY, USA","","","2013-10","","0362-1340","https://doi.org/10.1145/2637365.2517215;http://dx.doi.org/10.1145/2637365.2517215","10.1145/2637365.2517215","The C preprocessor (CPP) is a simple and language-independent tool, widely used to implement variable software systems using conditional compilation (i.e., by including or excluding annotated code). Although CPP provides powerful means to express variability, it has been criticized for allowing arbitrary annotations that break the underlying structure of the source code. We distinguish between disciplined annotations, which align with the structure of the source code, and undisciplined annotations, which do not. Several studies suggest that especially the latter type of annotations makes it hard to (automatically) analyze the code. However, little is known about whether the type of annotations has an effect on program comprehension. We address this issue by means of a controlled experiment with human subjects. We designed similar tasks for both, disciplined and undisciplined annotations, to measure program comprehension. Then, we measured the performance of the subjects regarding correctness and response time for solving the tasks. Our results suggest that there are no differences between disciplined and undisciplined annotations from a program-comprehension perspective. Nevertheless, we observed that finding and correcting errors is a time-consuming and tedious task in the presence of preprocessor annotations.","c preprocessor, controlled experiment, variability, program comprehension, disciplined annotations","",""
"Conference Paper","Schulze S,Lochau M,Brunswig S","Implementing Refactorings for FOP: Lessons Learned and Challenges Ahead","","2013","","","33–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Feature-Oriented Software Development","Indianapolis, Indiana, USA","2013","9781450321686","","https://doi.org/10.1145/2528265.2528271;http://dx.doi.org/10.1145/2528265.2528271","10.1145/2528265.2528271","Software product lines (SPL) gain momentum as a mean for developing and managing a set of related software systems under one umbrella. While intensive research on design and implementation of SPLs exist, the consequences of continuous evolution over time such as a decay of design or implementation have been neglected so far. In this context, refactoring has been shown to be an appropriate mean for improving the structure of source code. In this paper, we provide support for fine-grained program refactoring of feature-oriented SPLs. Particularly, we extend existing, object-oriented refactorings by taking the additional dimension of variability into account. To this end, we present the tool VAmPiRE as a basic framework for such refactorings and explain our considerations during implementation, which has been mainly guided by the idea of decomposing refactorings for ease and understandability. Additionally, we provide a detailed discussion about problems and limitations we faced during the implementation and come up with future challenges that have to be tackled for reliable and automated refactoring of software product lines.","refactoring, feature-oriented programming, software product lines, maintenance, software evolution","","FOSD '13"
"Conference Paper","Chen X,Zhang Y,Zhang X,Wu Y,Huang G,Mei H","Towards Runtime Model Based Integrated Management of Cloud Resources","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th Asia-Pacific Symposium on Internetware","Changsha, China","2013","9781450323697","","https://doi.org/10.1145/2532443.2532444;http://dx.doi.org/10.1145/2532443.2532444","10.1145/2532443.2532444","Although there are many management systems, Cloud management still faces with great challenges, due to the diversity of Cloud resources and ever-changing management requirements. Integration and adaptation become important for constructing a cloud management system, because a redevelopment solution based on existing systems is usually more practicable than developing the management system from scratch. However, the workload of redevelopment is also very high. As the runtime model is causally connected with the corresponding running system automatically, constructing an integrated Cloud management system based on runtime models can benefit from the model-specific natures to reduce the development workload. Therefore, in this paper, we present a runtime model based approach to constructing cloud management system. First, we construct the runtime model of each Cloud resource based on its own management interfaces. Second, we construct a composite model reflecting integration management requirements through merging the distributed runtime models. Third, we make Cloud management meet the adaptation requirements through model transformation from the composite model to the customized models specific to different administrators. Such architecture-level integrated management brings many advantages related to the interoperability, reusability and simplicity. The experiment on a real-world cloud demonstrates the feasibility, effectiveness and benefits of the new approach to integrated management of Cloud resources.","models at runtime, cloud management, software architecture","","Internetware '13"
"Conference Paper","Lochmann K,Ramadani J,Wagner S","Are Comprehensive Quality Models Necessary for Evaluating Software Quality?","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Conference on Predictive Models in Software Engineering","Baltimore, Maryland, USA","2013","9781450320160","","https://doi.org/10.1145/2499393.2499404;http://dx.doi.org/10.1145/2499393.2499404","10.1145/2499393.2499404","The concept of software quality is very complex and has many facets. Reflecting all these facets and at the same time measuring everything related to these facets results in comprehensive but large quality models and extensive measurements. In contrast, there are also many smaller, focused quality models claiming to evaluate quality with few measures.We investigate if and to what extent it is possible to build a focused quality model with similar evaluation results as a comprehensive quality model but with far less measures needed to be collected and, hence, reduced effort. We make quality evaluations with the comprehensive Quamoco base quality model and build focused quality models based on the same set of measures and data from over 2,000 open source systems. We analyse the ability of the focused model to predict the results of the Quamoco model by comparing them with a random predictor as a baseline. We calculate the standardised accuracy measure SA and effect sizes.We found that for the Quamoco model and its 378 automatically collected measures, we can build a focused model with only 10 measures but an accuracy of 61% and a medium to high effect size. We conclude that we can build focused quality models to get an impression of a system's quality similar to comprehensive models. However, when including manually collected measures, the accuracy of the models stayed below 50%. Hence, manual measures seem to have a high impact and should therefore not be ignored in a focused model.","quality evaluation, software quality, quality model","","PROMISE '13"
"Conference Paper","Ahmad S,Kamvar S","The Dog Programming Language","","2013","","","463–472","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology","St. Andrews, Scotland, United Kingdom","2013","9781450322683","","https://doi.org/10.1145/2501988.2502026;http://dx.doi.org/10.1145/2501988.2502026","10.1145/2501988.2502026","Today, most popular software applications are deployed in the cloud, interact with many users, and run on multiple platforms from Web browsers to mobile operating systems. While these applications confer a number of benefits to their users, building them brings many challenges: manually managing state between asynchronous user actions, creating and maintaining separate code bases for each desired client platform and gracefully scaling to handle a large number of concurrent users. Dog is a new programming language that provides a solution to these challenges and others through a unique runtime model that allows developers to model scalable cross-client applications as an imperative control-flow --- simplifying many development tasks. In this paper we describe the key features of Dog and show its utility through several applications that are difficult and time-consuming to write in existing languages, but are simple and easily written in Dog in a few lines of code.","programming languages, application development","","UIST '13"
"Conference Paper","Ginosar S,De Pombo LF,Agrawala M,Hartmann B","Authoring Multi-Stage Code Examples with Editable Code Histories","","2013","","","485–494","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology","St. Andrews, Scotland, United Kingdom","2013","9781450322683","","https://doi.org/10.1145/2501988.2502053;http://dx.doi.org/10.1145/2501988.2502053","10.1145/2501988.2502053","Multi-stage code examples present multiple versions of a program where each stage increases the overall complexity of the code. In order to acquire strategies of program construction using a new language or API, programmers consult multi-stage code examples in books, tutorials and online videos. Authoring multi-stage code examples is currently a tedious process, as it involves keeping several stages of code synchronized in the face of edits and error corrections. We document these difficulties with a formative study examining how programmers author multi-stage code examples. We then present an IDE extension that helps authors create multi-stage code examples by propagating changes (insertions, deletions and modifications) to multiple saved versions of their code. Our system adapts revision control algorithms to the specific task of evolving example code. An informal evaluation finds that taking snapshots of a program as it is being developed and editing these snapshots in hindsight help users in creating multi-stage code examples.","editable histories, tutorials, examples, programming","","UIST '13"
"Conference Paper","D'Agostini C,Cava R,Dorneles CF,Firmenich S,Freitas CM,Palanque P,Winckler M","Proposta de Um Framework Para Visualização de Dados Agregados Por Similaridade Para Auxiliar Consultas Durante a Navegação Na Web","","2013","","","148–157","Brazilian Computer Society","Porto Alegre, BRA","Proceedings of the 12th Brazilian Symposium on Human Factors in Computing Systems","Manaus, Brazil","2013","9788576692782","","","","In the last decade, several specialized tools have been created upon similarity functions that, given a keyword and a context, determine the degree of similarity (or probability) that information in a dataset corresponds to the user's query. Quite often such tools are meant for experts and require training and knowledge on the application domain to be used. However, given the huge amount of information available on the Web, resolving ambiguities becomes a daily task for most users. In this paper, we present a technique for embedding into a Web browser tools for solving ambiguities between keywords that users might found while navigating the Web. A prototype illustrating such techniques has been developed as a proof of concept. The tool presents the degree of similarity directly on Web pages as a contextual help menu. The overall approach includes different datasets and similarity functions and is flexible enough to support extensions for covering additional contexts of use.","similarity functions, contextual help, web navigation","","IHC '13"
"Conference Paper","Edwards C,Gruner S","A New Tool for URDAD to Java EE EJB Transformations","","2013","","","144–153","Association for Computing Machinery","New York, NY, USA","Proceedings of the South African Institute for Computer Scientists and Information Technologists Conference","East London, South Africa","2013","9781450321129","","https://doi.org/10.1145/2513456.2513459;http://dx.doi.org/10.1145/2513456.2513459","10.1145/2513456.2513459","Following the Object Management Group's (OMG) Model-Driven Architecture (MDA) approach, the semi-formal, service-orientated ""Use Case, Responsibility Driven Analysis and Design"" (URDAD) method is used by requirements engineers to specify a software system's functional properties in a Platform Independent Model (PIM). PIMs are represented using the URDAD Domain Specific Language (DSL), and thus conform to the URDAD MOF meta model. As a result, they can be transformed into Platform-Specific Models (PSM) for frameworks such as Java Platform Enterprise Edition (JEE) Enterprise Java Beans (EJB). This paper describes the semi-automatic transformation of a URDAD PIM into a EJB PSM, which is the basis for the further generation of EJB program code. For this purpose, a new prototype CASE tool was implemented to facilitate such transformations. The tool was evaluated using a non-trivial example project, with results indicating that it produces the PSM and template code that constitutes the static Java EE EJB structural representation of the example PIM.","QVT, JaMoPP, MOF, model transformation, URDAD, EJB, CASE tool, MDA","","SAICSIT '13"
"Conference Paper","Lee YR,Kang B,Im EG","Function Matching-Based Binary-Level Software Similarity Calculation","","2013","","","322–327","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 Research in Adaptive and Convergent Systems","Montreal, Quebec, Canada","2013","9781450323482","","https://doi.org/10.1145/2513228.2513300;http://dx.doi.org/10.1145/2513228.2513300","10.1145/2513228.2513300","This paper proposes a method to calculate similarities of software without any source code information. The proposed method can be used for various applications such as detecting the source code theft and copyright infringement, as well as locating updated parts of software including malware. To determine the similarities of software, we used an approach that matches similar functions included in software. Our function-based matching process is composed of two steps. In step 1, the structural information of call graph in binary file is used to match functions, and the matched functions are not processed in step 2 to reduce the number of detailed matching. In step 2, by using instruction mnemonics, N-gram similarity-based matching is performed. Using the structural matching proposed in this paper, about 30% improvement in the matching performance is achieved with the four-tuple matching which also reduces the false positive rate compared to previous studies. Our other experimental results showed that, in comparison to source code-based approaches, our proposed method has only about 3% difference in similarity calculation with real software samples. Therefore, we argue that our proposed method makes a contribution in the field of binary-based software similarity calculation.","N-gram, static analysis, software similarity, call graph, malware, binary analysis, function matching","","RACS '13"
"Conference Paper","Ko J,Shim H,Kim D,Jeong YS,Cho SJ,Park M,Han S,Kim SB","Measuring Similarity of Android Applications via Reversing and K-Gram Birthmarking","","2013","","","336–341","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 Research in Adaptive and Convergent Systems","Montreal, Quebec, Canada","2013","9781450323482","","https://doi.org/10.1145/2513228.2513308;http://dx.doi.org/10.1145/2513228.2513308","10.1145/2513228.2513308","By measuring similarity of programs, we can determine whether someone illegally copies a program from another program or not. If the similarity is significantly high, it means that a program is a copy of the other. This paper proposes three techniques to measure similarity of the Dalvik executable codes (DEXs) in the Android application Packages (APKs). Firstly, we decompile the DEXs of candidate applications into Java sources and compute the similarity between the decompiled sources. Secondly, candidate DEXs are disassembled and the similarities between disassembled codes are measured. Finally, we extract k-gram based software birthmark form the dissembled codes and calculate the similarity of sample DEXs by comparing the extracted birthmarks. We perform several experiments to identify effects of the three techniques. With the analysis of the experimental results, the advantages and disadvantages of each technique are discussed.","software theft, reversing, Android application, software similarity, K-gram","","RACS '13"
"Conference Paper","Bai K,Lu J,Shrivastava A,Holton B","CMSM: An Efficient and Effective Code Management for Software Managed Multicores","","2013","","","","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis","","2013","9781479914173","","","","As we scale the number of cores in a multicore processor, scaling the memory hierarchy is a major challenge. Software Managed Multicore (SMM) architectures are one of the promising solutions. In an SMM architecture, there are no caches, and each core has only a local scratchpad memory. If all the code and data of the task mapped to a core do not fit on its local scratchpad memory, then explicit code and data management is required. In this paper, we solve the problem of efficiently managing code on an SMM architecture. We extend the state of the art by: i) correctly calculating the code management overhead, ii) even in the presence of branches in the task, and iii) developing a heuristic CMSM (Code Mapping for Software Managed multicores) that results in efficient code management execution on the local scratchpad memory. Our experimental results collected after executing applications from MiBench suite [1] on the Cell SPEs (Cell is an SMM architecture) [2], demonstrate that correct management cost calculation and branch consideration can improve performance by 12%. Our heuristic CMSM can reduce runtime in more than 80% of the cases, and by up to 20% on our set of benchmarks.","code, multi-core processor, scratchpad memory, instruction, SPM, local memory, embedded systems","","CODES+ISSS '13"
"Conference Paper","Mazumder B,Hallstrom JO","An Efficient Code Update Solution for Wireless Sensor Network Reprogramming","","2013","","","","IEEE Press","Montreal, Quebec, Canada","Proceedings of the Eleventh ACM International Conference on Embedded Software","","2013","9781479914432","","","","We present an incremental code update strategy used to efficiently reprogram wireless sensor nodes. We adapt a linear space and quadratic time algorithm (Hirschberg's algorithm) for computing maximal common subsequences to build an edit map specifying an edit sequence, required to transform the code running in a sensor network to a new code image. We then present a heuristic-based optimization strategy for efficient edit script encoding to reduce the edit map size. Finally, we present experimental results to demonstrate the reduction in data size to reprogram a network using this mechanism. The approach achieves reductions of 99.987% for simple changes, and between 86.95% and 94.58% for more complex changes, compared to full image transmissions --- leading to significantly lower energy costs for wireless sensor network reprogramming. We compare the results with reductions achieved by other incremental update strategies described in prior work.","wireless sensor networks, code distribution, longest common subsequence, reprogramming, incremental code update","","EMSOFT '13"
"Conference Paper","Li H,Thompson S","Multicore Profiling for Erlang Programs Using Percept2","","2013","","","33–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twelfth ACM SIGPLAN Workshop on Erlang","Boston, Massachusetts, USA","2013","9781450323857","","https://doi.org/10.1145/2505305.2505311;http://dx.doi.org/10.1145/2505305.2505311","10.1145/2505305.2505311","Erlang is a functional programming language with built-in support for concurrency based on share-nothing processes and asynchronous message passing. The design of Erlang makes it suitable for writing concurrent and parallel applications, taking full advantage of the computing power of modern multicore machines. However many existing Erlang applications are sequential, in need of parallelisation.In this paper, we present the Erlang concurrency profiling tool Percept2,and demonstrate how the information provided by it can help the user to explore the potential parallelism in an Erlang application and how the system performs on the Erlang multicore system. Percept2 thus allows users improve the performance and scalability of their applications.","profiling, tracing, parallelisation, erlang, multicore","","Erlang '13"
"Conference Paper","Horpácsi D","Extending Erlang by Utilising RefactorErl","","2013","","","63–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twelfth ACM SIGPLAN Workshop on Erlang","Boston, Massachusetts, USA","2013","9781450323857","","https://doi.org/10.1145/2505305.2505314;http://dx.doi.org/10.1145/2505305.2505314","10.1145/2505305.2505314","In this paper, we present the idea of utilising a refactoring tool for implementing extensions to a programming language. We elaborate the correspondence between the main components of the compiler and the refactoring tool, and examine how analysis and transformation features of the tool can be exploited for turning its refactoring framework into a translation framework. The presented method allows one, for instance, to make the Erlang language supportive for embedding domain specific languages as well as to make its functions portable.","precompiler, domain specific language, erlang, static code analysis, transformation, preprocessor, translation, refactoring, language extension","","Erlang '13"
"Conference Paper","Francalanza A,Zerafa T","Towards an Abstraction for Remote Evaluation in Erlang","","2013","","","75–76","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twelfth ACM SIGPLAN Workshop on Erlang","Boston, Massachusetts, USA","2013","9781450323857","","https://doi.org/10.1145/2505305.2505316;http://dx.doi.org/10.1145/2505305.2505316","10.1145/2505305.2505316","Erlang is an industry-standard cross-platform functional programming language and runtime system (ERTS) intended for the development of scalable enterprise projects that are inherently concurrent and distributed systems [1]. In essence, an Erlang system consists of a number of actors [3] (processes) executing concurrently across a number of nodes. These actors interact with one another (mainly) through asynchronous messaging and are also capable of spawning further actors, either locally or at a remote node.","code migration, distributed erlang","","Erlang '13"
"Conference Paper","Francisco MA,López M,Ferreiro H,Castro LM","Turning Web Services Descriptions into Quickcheck Models for Automatic Testing","","2013","","","79–86","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twelfth ACM SIGPLAN Workshop on Erlang","Boston, Massachusetts, USA","2013","9781450323857","","https://doi.org/10.1145/2505305.2505306;http://dx.doi.org/10.1145/2505305.2505306","10.1145/2505305.2505306","In this work, we face the problem of generating good quality test suites and test cases for web services. We present a framework to test web services based on their formal description, following a black-box approach and using Property-Based Testing.Web services are a popular solution to integrate components when building a software system, or to allow communication between a system and third-party users, providing a flexible, reusable mechanism to access its functionalities. Testing of web services is a key activity: we need to verify their behaviour and ensure their quality as much as possible, as efficiently as possible.By automatically deriving QuickCheck models from its WSDL description and its OCL semantic constraints, we enable generation and execution of great amounts of automatically generated test cases. Thus, we avoid the usual compromise between effort and cost, which too often leads to smaller and less exhaustive test suites than desirable.To illustrate the advantages of our framework, we present an industrial case study: a distributed system which serves media contents customers' TV screens.","ocl, web services, model-based testing, quickcheck, wsdl, erlang","","Erlang '13"
"Conference Paper","Madsen FM,Filinski A","Towards a Streaming Model for Nested Data Parallelism","","2013","","","13–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd ACM SIGPLAN Workshop on Functional High-Performance Computing","Boston, Massachusetts, USA","2013","9781450323819","","https://doi.org/10.1145/2502323.2502330;http://dx.doi.org/10.1145/2502323.2502330","10.1145/2502323.2502330","The language-integrated cost semantics for nested data parallelism pioneered by NESL provides an intuitive, high-level model for predicting performance and scalability of parallel algorithms with reasonable accuracy. However, this predictability, obtained through a uniform, parallelism-flattening execution strategy, comes at the price of potentially prohibitive space usage in the common case of computations with an excess of available parallelism, such as dense-matrix multiplication.We present a simple nested data-parallel functional language and associated cost semantics that retains NESL's intuitive work--depth model for time complexity, but also allows highly parallel computations to be expressed in a space-efficient way, in the sense that memory usage on a single (or a few) processors is of the same order as for a sequential formulation of the algorithm, and in general scales smoothly with the actually realized degree of parallelism, not the potential parallelism.The refined semantics is based on distinguishing formally between fully materialized (i.e., explicitly allocated in memory all at once) ""vectors"" and potentially ephemeral ""sequences"" of values, with the latter being bulk-processable in a streaming fashion. This semantics is directly compatible with previously proposed piecewise execution models for nested data parallelism, but allows the expected space usage to be reasoned about directly at the source-language level.The language definition and implementation are still very much work in progress, but we do present some preliminary examples and timings, suggesting that the streaming model has practical potential.","dataflow networks, cost semantics, space efficiency","","FHPC '13"
"Conference Paper","Rakić G,Budimac Z,Savić M","Language Independent Framework for Static Code Analysis","","2013","","","236–243","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th Balkan Conference in Informatics","Thessaloniki, Greece","2013","9781450318518","","https://doi.org/10.1145/2490257.2490273;http://dx.doi.org/10.1145/2490257.2490273","10.1145/2490257.2490273","The aim of this paper is to describe a framework consisting of a set of static analyzers. The main characteristic of all incorporated tools is their independency of input programming language. This independency is based on a universal representation of the source code that is to be analyzed. The overall goal of this research is to provide a framework that is suitable for consistent analysis of the source code with the intention to ensure, check, and consequently increase the quality of the heterogeneous software products. The framework currently integrates three tools: software metrics tool -- SMIILE, extractor of software networks -- SNEIPL and structure change analyzer -- SSCA, with tendency to extend this set of components. Further application of these tools in collaboration with other tools on higher level provides even broader applicability of described framework.","software quality, software tools, static analysis","","BCI '13"
"Conference Paper","Öqvist J,Hedin G","Extending the JastAdd Extensible Java Compiler to Java 7","","2013","","","147–152","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools","Stuttgart, Germany","2013","9781450321112","","https://doi.org/10.1145/2500828.2500843;http://dx.doi.org/10.1145/2500828.2500843","10.1145/2500828.2500843","JastAddJ is an extensible Java compiler, implemented using reference attribute grammars. It has been shown previously how the language constructs of Java 5, like generics, could be modularly added to the original JastAddJ compiler that supported Java 1.4.In this paper we discuss our experiences from extending JastAddJ to support Java 7. In particular, we discuss how the Try-With-Resources statement and the Diamond operator could be implemented, and how efficient the resulting Java 7 compiler is regarding code size, compilation time, and memory usage.","Java, attribute grammars, compiler design","","PPPJ '13"
"Conference Paper","Kanda T,Ishio T,Inoue K","Extraction of Product Evolution Tree from Source Code of Product Variants","","2013","","","141–150","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2491637;http://dx.doi.org/10.1145/2491627.2491637","10.1145/2491627.2491637","A large number of software products may be derived from an original single product. Although software product line engineering is advocated as an effective approach to maintaining such a family of products, re-engineering existing products requires developers to understand the evolution history of the products. This can be challenging because developers typically only have access to product source code. In this research, we propose to extract a Product Evolution Tree that approximates the evolution history from source code of products. Our key idea is that two successive products are the most similar to one another in the evolution history. We construct a Product Evolution Tree as a minimum spanning tree whose cost function is defined by the number of similar files between products. As an experiment, we extracted Product Evolution Trees from 6 datasets of open-source projects. The result showed that 53% to 92% of edges in the extracted trees were consistent with the actual evolution history of the projects.","visualization, software evolution, software product line","","SPLC '13"
"Conference Paper","Tsuchiya R,Kato T,Washizaki H,Kawakami M,Fukazawa Y,Yoshimura K","Recovering Traceability Links between Requirements and Source Code in the Same Series of Software Products","","2013","","","121–130","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2491633;http://dx.doi.org/10.1145/2491627.2491633","10.1145/2491627.2491633","If traceability links between requirements and source code are not clarified when conducting maintenance and enhancements for the same series of software products, engineers cannot immediately find the correction location in the source code for requirement changes. However, manually recovering links in a large group of products requires significant costs and some links may be overlooked. Here, we propose a semi-automatic method to recover traceability links between requirements and source code in the same series of large software products. In order to support differences in representation between requirements and source code, we recover links by using the configuration management log as an intermediary. We refine the links by classifying requirements and code elements in terms of whether they are common or specific to the products. As a result of applying our method to real products that have 60KLOC, we have recovered valid traceability links within a reasonable amount of time. Automatic parts have taken 13 minutes 36 seconds, and non-automatic parts have taken about 3 hours, with a recall of 76.2% and a precision of 94.1%. Moreover, we recovered some links that were unknown to engineers. By recovering traceability links, software reusability will be improved, and software product line introduction will be facilitated.","traceability recovery, configuration management log, commonality and variability analysis","","SPLC '13"
"Conference Paper","Cordy JR","Submodel Pattern Extraction for Simulink Models","","2013","","","7–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2492153;http://dx.doi.org/10.1145/2491627.2492153","10.1145/2491627.2492153","Long before MDE became a popular method for software development, Simulink was firmly established as a tool for model--driven development of hybrid industrial systems. While practical and expressive for model creation and reuse, Simulink lacks for good abstraction mechanisms, and copy--paste--modify is a standard paradigm in Simulink development, leading to large numbers of variants of similar submodels. SIMONE is a framework and tool for automatically identifying, classifying and formalizing submodel patterns in Simulink models, using near-miss clone detection to find similarities and model merging to identify points of variance. The result is a set of submodel patterns which can be used as a reference for variance in the models, supporting consistency analysis, test optimization, fault identification and the disciplined generation of new subsystem instances using feature selection.","simulink, model-driven engineering, model patterns","","SPLC '13"
"Conference Paper","Linsbauer L,Lopez-Herrejon ER,Egyed A","Recovering Traceability between Features and Code in Product Variants","","2013","","","131–140","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2491630;http://dx.doi.org/10.1145/2491627.2491630","10.1145/2491627.2491630","Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.","traceability, features, product variants","","SPLC '13"
"Conference Paper","Huang C,Kamei Y,Yamashita K,Ubayashi N","Using Alloy to Support Feature-Based DSL Construction for Mining Software Repositories","","2013","","","86–89","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference Co-Located Workshops","Tokyo, Japan","2013","9781450323253","","https://doi.org/10.1145/2499777.2500714;http://dx.doi.org/10.1145/2499777.2500714","10.1145/2499777.2500714","The Mining Software Repositories (MSR) field reveals knowledge for software development by analyzing data stored in repositories such as source control and bug trace systems. In order to reveal the knowledge, MSR researchers need to perform complicated procedures iteratively. To help the complex work of MSR practitioners, we study the construction of domain specific languages (DSLs) for MSR. We have conducted feature-oriented domain analysis (FODA) on MSR and developed a DSL based on the feature model. In this paper, we expand our previous work and propose to construct not a single DSL but a DSL family. A DSL family consists of a series of DSLs with commonality in their domain but suitable to specific applications of MSR. To readily construct these DSLs, we use Alloy to encode the feature model. Our encoding includes not only the DSL features and their relations but also some composition rules that can be used to generate the syntax of DSLs. Based on this, we can automatically derive the language elements to construct DSLs suitable to specific purposes of MSR.","SPL, DSL, mining software repositories, FODA","","SPLC '13 Workshops"
"Conference Paper","Koziolek H,Goldschmidt T,de Gooijer T,Domis D,Sehestedt S","Experiences from Identifying Software Reuse Opportunities by Domain Analysis","","2013","","","208–217","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2491641;http://dx.doi.org/10.1145/2491627.2491641","10.1145/2491627.2491641","In a large corporate organization there are sometimes similar software products in certain subdomains with a perceived functional overlap. This promises to be an opportunity for systematic reuse to reduce software development and maintenance costs. In such situations companies have used different domain analysis approaches (e.g., SEI Technical Probe) that helped to assess technical and organizational potential for a software product line approach. We applied existing domain analysis approaches for software product line engineering and tailored them to include a feature analysis as well as architecture evaluation. In this paper, we report our experiences from applying the approach in two subdomains of industrial automation.","software product lines, business case, domain analysis","","SPLC '13"
"Conference Paper","Rubin J,Czarnecki K,Chechik M","Managing Cloned Variants: A Framework and Experience","","2013","","","101–110","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference","Tokyo, Japan","2013","9781450319683","","https://doi.org/10.1145/2491627.2491644;http://dx.doi.org/10.1145/2491627.2491644","10.1145/2491627.2491644","In our earlier work, we have proposed a generic framework for managing collections of related products realized via cloning -- both in the case when such products are refactored into a single-copy software product line representation and the case when they are maintained as distinct clones. In this paper, we ground the framework in empirical evidence and exemplify its usefulness. In particular, we systematically analyze three industrial case studies of organizations with cloned product lines and derive the set of basic operators comprising the framework. We discuss options for implementing the operators and benefits of the operator-based view.","legacy software product lines, industrial case studies, cloned product variants","","SPLC '13"
"Conference Paper","Wille D,Holthusen S,Schulze S,Schaefer I","Interface Variability in Family Model Mining","","2013","","","44–51","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Software Product Line Conference Co-Located Workshops","Tokyo, Japan","2013","9781450323253","","https://doi.org/10.1145/2499777.2500708;http://dx.doi.org/10.1145/2499777.2500708","10.1145/2499777.2500708","Model-driven development of software gains more and more importance, especially in domains with high complexity. In order to develop differing but still similar model-based systems, these models are often copied and modified according to the changed requirements. As the variability between these different models is not documented, issues arise during maintenance. For example, applying patches becomes a tedious task because errors have to be fixed in all of the created models and no information about modified and unchanged parts exists. In this paper, we present an approach to analyze related models and determine the variability between them. This analysis provides crucial information about the variability (i.e., changed parts, additional parts, and parts without any modification) between the models in order to create family models. The particular focus is the analysis of models containing components with differing interfaces.","analysis, family model mining, variability, SPL","","SPLC '13 Workshops"
"Conference Paper","Wang T,Harman M,Jia Y,Krinke J","Searching for Better Configurations: A Rigorous Approach to Clone Evaluation","","2013","","","455–465","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering","Saint Petersburg, Russia","2013","9781450322379","","https://doi.org/10.1145/2491411.2491420;http://dx.doi.org/10.1145/2491411.2491420","10.1145/2491411.2491420","Clone detection finds application in many software engineering activities such as comprehension and refactoring. However, the confounding configuration choice problem poses a widely-acknowledged threat to the validity of previous empirical analyses. We introduce desktop and parallelised cloud-deployed versions of a search based solution that finds suitable configurations for empirical studies. We evaluate our approach on 6 widely used clone detection tools applied to the Bellon suite of 8 subject systems. Our evaluation reports the results of 9.3 million total executions of a clone tool; the largest study yet reported. Our approach finds significantly better configurations (p < 0.05) than those currently used, providing evidence that our approach can ameliorate the confounding configuration choice problem.","Genetic Algorithms, Clone Detection, SBSE","","ESEC/FSE 2013"
"Conference Paper","Lohar S,Amornborvornwong S,Zisman A,Cleland-Huang J","Improving Trace Accuracy through Data-Driven Configuration and Composition of Tracing Features","","2013","","","378–388","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering","Saint Petersburg, Russia","2013","9781450322379","","https://doi.org/10.1145/2491411.2491432;http://dx.doi.org/10.1145/2491411.2491432","10.1145/2491411.2491432","Software traceability is a sought-after, yet often elusive quality in large software-intensive systems primarily because the cost and effort of tracing can be overwhelming. State-of-the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links. However, there is no simple one- size-fits all solution to trace retrieval. As this paper will show, finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links. We present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality. We utilize a machine-learning approach to search for the best configuration given an initial training set of validated trace links, a set of available tracing techniques specified in a feature model, and an architecture capable of instantiating all valid configurations of features. We evaluate our approach through a series of experiments using project data from the transportation, healthcare, and space exploration domains, and discuss its implementation in an industrial environment. Finally, we show how our approach can create a robust baseline against which new tracing techniques can be evaluated.","Trace retrieval, configuration, trace configuration","","ESEC/FSE 2013"
"Conference Paper","Linares-Vásquez M,Bavota G,Bernal-Cárdenas C,Di Penta M,Oliveto R,Poshyvanyk D","API Change and Fault Proneness: A Threat to the Success of Android Apps","","2013","","","477–487","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering","Saint Petersburg, Russia","2013","9781450322379","","https://doi.org/10.1145/2491411.2491428;http://dx.doi.org/10.1145/2491411.2491428","10.1145/2491411.2491428","During the recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. Many small and large software development companies invest considerable resources to target available opportunities. As of today, the markets for such devices feature over 850K+ apps for Android and 900K+ for iOS. Availability, cost, functionality, and usability are just some factors that determine the success or lack of success for a given app. Among the other factors, reliability is an important criteria: users easily get frustrated by repeated failures, crashes, and other bugs; hence, abandoning some apps in favor of others. This paper reports a study analyzing how the fault- and change-proneness of APIs used by 7,097 (free) Android apps relates to applications' lack of success, estimated from user ratings. Results of this study provide important insights into a crucial issue: making heavy use of fault- and change-prone APIs can negatively impact the success of these apps.","Mining Software Repositories, Android, Empirical Studies, API changes","","ESEC/FSE 2013"
"Conference Paper","Zulkernine F,Martin P,Powley W,Soltani S,Mankovskii S,Addleman M","CAPRI: A Tool for Mining Complex Line Patterns in Large Log Data","","2013","","","47–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications","Chicago, Illinois","2013","9781450323246","","https://doi.org/10.1145/2501221.2501228;http://dx.doi.org/10.1145/2501221.2501228","10.1145/2501221.2501228","Log files provide important information for troubleshooting complex systems. However, the structure and contents of the log data and messages vary widely. For automated processing, it is necessary to first understand the layout and the structure of the data, which becomes very challenging when a massive amount of data and messages are reported by different system components in the same log file. Existing approaches apply supervised mining techniques and return frequent patterns only for single line messages. We present CAPRI (type-CAsted Pattern and Rule mIner), which uses a novel pattern mining algorithm to efficiently mine structural line patterns from semi-structured multi-line log messages. It discovers line patterns in a type-casted format; categorizes all data lines; identifies frequent, rare and interesting line patterns, and uses unsupervised learning and incremental mining techniques. It also mines association rules to identify the contextual relationship between two successive line patterns. In addition, CAPRI lists the frequent term and value patterns given the minimum support thresholds. The line and term pattern information can be applied in the next stage to categorize and reformat multi-line data, extract variables from the messages, and discover further correlation among messages for troubleshooting complex systems. To evaluate our approach, we present a comparative study of our tool against some of the existing popular open-source research tools using three different layouts of log data including a complex multi-line log file from the z/OS mainframe system.","type-casting, association rule, line, value pattern mining, term","","BigMine '13"
"Conference Paper","Ouni A,Kessentini M,Sahraoui H,Hamdi MS","The Use of Development History in Software Refactoring Using a Multi-Objective Evolutionary Algorithm","","2013","","","1461–1468","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation","Amsterdam, The Netherlands","2013","9781450319638","","https://doi.org/10.1145/2463372.2463554;http://dx.doi.org/10.1145/2463372.2463554","10.1145/2463372.2463554","One of the widely used techniques for evolving software systems is refactoring, a maintenance activity that improves design structure while preserving the external behavior. Exploring past maintenance and development history can be an effective way of finding refactoring opportunities. Code elements which undergo changes in the past, at approximately the same time, bear a good probability for being semantically related. Moreover, these elements that experienced a huge number of refactoring in the past have a good chance for refactoring in the future. In addition, the development history can be used to propose new refactoring solutions in similar contexts. In this paper, we propose a multi-objective optimization-based approach to find the best sequence of refactorings that minimizes the number of bad-smells, and maximizes the use of development history and semantic coherence. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-off between these three objectives. We report the results of our experiments using different large open source projects.","semantics, code change, design defects, refactoring, search-based software engineering","","GECCO '13"
"Conference Paper","Stadler L,Duboscq G,Mössenböck H,Würthinger T,Simon D","An Experimental Study of the Influence of Dynamic Compiler Optimizations on Scala Performance","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th Workshop on Scala","Montpellier, France","2013","9781450320641","","https://doi.org/10.1145/2489837.2489846;http://dx.doi.org/10.1145/2489837.2489846","10.1145/2489837.2489846","Java Virtual Machines are optimized for performing well on traditional Java benchmarks, which consist almost exclusively of code generated by the Java source compiler (javac). Code generated by compilers for other languages has not received nearly as much attention, which results in performance problems for those languages.One important specimen of ""another language"" is Scala, whose syntax and features encourage a programming style that differs significantly from traditional Java code. It suffers from the same problem -- its code patterns are not optimized as well as the ones originating from Java code. JVM developers need to be aware of the differences between Java and Scala code, so that both types of code can be executed with optimal performance.This paper presents a detailed investigation of the performance impact of a large number of optimizations on the Scala DaCapo and the Java DaCapo benchmark suites. It describes the optimization techniques and analyzes the differences between traditional Java applications and Scala applications. The results help compiler engineers in understanding the characteristics of Scala.We performed these experiments on the work-in-progress Graal compiler. Graal is a new dynamic compiler for the HotSpot VM which aims to work well for a diverse set of workloads, including languages other than Java.","Scala, virtual machine, optimization, compiler, dynamic compiler","","SCALA '13"
"Conference Paper","Bjørling M,Axboe J,Nellans D,Bonnet P","Linux Block IO: Introducing Multi-Queue SSD Access on Multi-Core Systems","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Systems and Storage Conference","Haifa, Israel","2013","9781450321167","","https://doi.org/10.1145/2485732.2485740;http://dx.doi.org/10.1145/2485732.2485740","10.1145/2485732.2485740","The IO performance of storage devices has accelerated from hundreds of IOPS five years ago, to hundreds of thousands of IOPS today, and tens of millions of IOPS projected in five years. This sharp evolution is primarily due to the introduction of NAND-flash devices and their data parallel design. In this work, we demonstrate that the block layer within the operating system, originally designed to handle thousands of IOPS, has become a bottleneck to overall storage system performance, specially on the high NUMA-factor processors systems that are becoming commonplace. We describe the design of a next generation block layer that is capable of handling tens of millions of IOPS on a multi-core system equipped with a single storage device. Our experiments show that our design scales graciously with the number of cores, even on NUMA systems with multiple sockets.","latency, linux, throughput, solid state drives, non-volatile memory, block layer","","SYSTOR '13"
"Conference Paper","Gibler C,Stevens R,Crussell J,Chen H,Zang H,Choi H","AdRob: Examining the Landscape and Impact of Android Application Plagiarism","","2013","","","431–444","Association for Computing Machinery","New York, NY, USA","Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services","Taipei, Taiwan","2013","9781450316729","","https://doi.org/10.1145/2462456.2464461;http://dx.doi.org/10.1145/2462456.2464461","10.1145/2462456.2464461","Malicious activities involving Android applications are rising rapidly. As prior work on cyber-crimes suggests, we need to understand the economic incentives of the criminals to design the most effective defenses. In this paper, we investigate application plagiarism on Android markets at a large scale. We take the first step to characterize plagiarized applications and estimate their impact on the original application developers. We first crawled 265,359 free applications from 17 Android markets around the world and ran a tool to identify similar applications (""clones""). Based on the data, we examined properties of the cloned applications, including their distribution across different markets, application categories, and ad libraries. Next, we examined how cloned applications affect the original developers. We captured HTTP advertising traffic generated by mobile applications at a tier-1 US cellular carrier for 12 days. To associate each Android application with its advertising traffic, we extracted a unique advertising identifier (called the client ID) from both the applications and the network traces. We estimate a lower bound on the advertising revenue that cloned applications siphon from the original developers, and the user base that cloned applications divert from the original applications. To the best of our knowledge, this is the first large scale study on the characteristics of cloned mobile applications and their impact on the original developers.","measurement, advertising, android, underground economy, mobile, plagiarism","","MobiSys '13"
"Conference Paper","Atta I,Tözün P,Tong X,Ailamaki A,Moshovos A","STREX: Boosting Instruction Cache Reuse in OLTP Workloads through Stratified Transaction Execution","","2013","","","273–284","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th Annual International Symposium on Computer Architecture","Tel-Aviv, Israel","2013","9781450320795","","https://doi.org/10.1145/2485922.2485946;http://dx.doi.org/10.1145/2485922.2485946","10.1145/2485922.2485946","Online transaction processing (OLTP) workload performance suffers from instruction stalls; the instruction footprint of a typical transaction exceeds by far the capacity of an L1 cache, leading to ongoing cache thrashing. Several proposed techniques remove some instruction stalls in exchange for error-prone instrumentation to the code base, or a sharp increase in the L1-I cache unit area and power. Others reduce instruction miss latency by better utilizing a shared L2 cache. SLICC [2], a recently proposed thread migration technique that exploits transaction instruction locality, is promising for high core counts but performs sub-optimally or may hurt performance when running on few cores.This paper corroborates that OLTP transactions exhibit significant intra- and inter-thread overlap in their instruction footprint, and analyzes the instruction stall reduction benefits. This paper presents STREX, a hardware, programmer-transparent technique that exploits typical transaction behavior to improve instruction reuse in first level caches. STREX time-multiplexes the execution of similar transactions dynamically on a single core so that instructions fetched by one transaction are reused by all other transactions executing in the system as much as possible. STREX dynamically slices the execution of each transaction into cache-sized segments simply by observing when blocks are brought in the cache and when they are evicted. Experiments show that, when compared to baseline execution on 2--16 cores, STREX consistently improves performance while reducing the number of L1 instruction and data misses by 37% and 14% on average, respectively. Finally, this paper proposes a practical hybrid technique that combines STREX and SLICC, thereby guaranteeing performance benefits regardless of the number of available cores and the workload's footprint.","instruction cache, thread scheduling, OLTP, instruction locality","","ISCA '13"
"Journal Article","Atta I,Tözün P,Tong X,Ailamaki A,Moshovos A","STREX: Boosting Instruction Cache Reuse in OLTP Workloads through Stratified Transaction Execution","SIGARCH Comput. Archit. News","2013","41","3","273–284","Association for Computing Machinery","New York, NY, USA","","","2013-06","","0163-5964","https://doi.org/10.1145/2508148.2485946;http://dx.doi.org/10.1145/2508148.2485946","10.1145/2508148.2485946","Online transaction processing (OLTP) workload performance suffers from instruction stalls; the instruction footprint of a typical transaction exceeds by far the capacity of an L1 cache, leading to ongoing cache thrashing. Several proposed techniques remove some instruction stalls in exchange for error-prone instrumentation to the code base, or a sharp increase in the L1-I cache unit area and power. Others reduce instruction miss latency by better utilizing a shared L2 cache. SLICC [2], a recently proposed thread migration technique that exploits transaction instruction locality, is promising for high core counts but performs sub-optimally or may hurt performance when running on few cores.This paper corroborates that OLTP transactions exhibit significant intra- and inter-thread overlap in their instruction footprint, and analyzes the instruction stall reduction benefits. This paper presents STREX, a hardware, programmer-transparent technique that exploits typical transaction behavior to improve instruction reuse in first level caches. STREX time-multiplexes the execution of similar transactions dynamically on a single core so that instructions fetched by one transaction are reused by all other transactions executing in the system as much as possible. STREX dynamically slices the execution of each transaction into cache-sized segments simply by observing when blocks are brought in the cache and when they are evicted. Experiments show that, when compared to baseline execution on 2--16 cores, STREX consistently improves performance while reducing the number of L1 instruction and data misses by 37% and 14% on average, respectively. Finally, this paper proposes a practical hybrid technique that combines STREX and SLICC, thereby guaranteeing performance benefits regardless of the number of available cores and the workload's footprint.","OLTP, thread scheduling, instruction locality, instruction cache","",""
"Conference Paper","Sim J,Loh GH,Sridharan V,O'Connor M","Resilient Die-Stacked DRAM Caches","","2013","","","416–427","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th Annual International Symposium on Computer Architecture","Tel-Aviv, Israel","2013","9781450320795","","https://doi.org/10.1145/2485922.2485958;http://dx.doi.org/10.1145/2485922.2485958","10.1145/2485922.2485958","Die-stacked DRAM can provide large amounts of in-package, high-bandwidth cache storage. For server and high-performance computing markets, however, such DRAM caches must also provide sufficient support for reliability and fault tolerance. While conventional off-chip memory provides ECC support by adding one or more extra chips, this may not be practical in a 3D stack. In this paper, we present a DRAM cache organization that uses error-correcting codes (ECCs), strong checksums (CRCs), and dirty data duplication to detect and correct a wide range of stacked DRAM failures, from traditional bit errors to large-scale row, column, bank, and channel failures. With only a modest performance degradation compared to a DRAM cache with no ECC support, our proposal can correct all single-bit failures, and 99.9993% of all row, column, and bank failures, providing more than a 54,000x improvement in the FIT rate of silent-data corruptions compared to basic SECDED ECC protection.","error protection, reliability, die stacking, cache","","ISCA '13"
"Journal Article","Sim J,Loh GH,Sridharan V,O'Connor M","Resilient Die-Stacked DRAM Caches","SIGARCH Comput. Archit. News","2013","41","3","416–427","Association for Computing Machinery","New York, NY, USA","","","2013-06","","0163-5964","https://doi.org/10.1145/2508148.2485958;http://dx.doi.org/10.1145/2508148.2485958","10.1145/2508148.2485958","Die-stacked DRAM can provide large amounts of in-package, high-bandwidth cache storage. For server and high-performance computing markets, however, such DRAM caches must also provide sufficient support for reliability and fault tolerance. While conventional off-chip memory provides ECC support by adding one or more extra chips, this may not be practical in a 3D stack. In this paper, we present a DRAM cache organization that uses error-correcting codes (ECCs), strong checksums (CRCs), and dirty data duplication to detect and correct a wide range of stacked DRAM failures, from traditional bit errors to large-scale row, column, bank, and channel failures. With only a modest performance degradation compared to a DRAM cache with no ECC support, our proposal can correct all single-bit failures, and 99.9993% of all row, column, and bank failures, providing more than a 54,000x improvement in the FIT rate of silent-data corruptions compared to basic SECDED ECC protection.","error protection, cache, reliability, die stacking","",""
"Conference Paper","Wu MJ,Zhao M,Yeung D","Studying Multicore Processor Scaling via Reuse Distance Analysis","","2013","","","499–510","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th Annual International Symposium on Computer Architecture","Tel-Aviv, Israel","2013","9781450320795","","https://doi.org/10.1145/2485922.2485965;http://dx.doi.org/10.1145/2485922.2485965","10.1145/2485922.2485965","The trend for multicore processors is towards increasing numbers of cores, with 100s of cores--i.e. large-scale chip multiprocessors (LCMPs)--possible in the future. The key to realizing the potential of LCMPs is the cache hierarchy, so studying how memory performance will scale is crucial. Reuse distance (RD) analysis can help architects do this. In particular, recent work has developed concurrent reuse distance (CRD) and private reuse distance (PRD) profiles to enable analysis of shared and private caches. Also, techniques have been developed to predict profiles across problem size and core count, enabling the analysis of configurations that are too large to simulate.This paper applies RD analysis to study the scalability of multicore cache hierarchies. We present a framework based on CRD and PRD profiles for reasoning about the locality impact of core count and problem scaling. We find interference-based locality degradation is more significant than sharing-based locality degradation. For 256 cores running small problems, the former occurs at small cache sizes, allowing moderate capacity scaling of multicore caches to achieve the same cache performance (MPKI) as a single-core cache. At very large problems, interference-based locality degradation increases significantly in many of our benchmarks. For shared caches, this prevents most of our benchmarks from achieving constant-MPKI scaling within a 256 MB capacity budget; for private caches, all benchmarks cannot achieve constant-MPKI scaling within 256 MB.","","","ISCA '13"
"Journal Article","Wu MJ,Zhao M,Yeung D","Studying Multicore Processor Scaling via Reuse Distance Analysis","SIGARCH Comput. Archit. News","2013","41","3","499–510","Association for Computing Machinery","New York, NY, USA","","","2013-06","","0163-5964","https://doi.org/10.1145/2508148.2485965;http://dx.doi.org/10.1145/2508148.2485965","10.1145/2508148.2485965","The trend for multicore processors is towards increasing numbers of cores, with 100s of cores--i.e. large-scale chip multiprocessors (LCMPs)--possible in the future. The key to realizing the potential of LCMPs is the cache hierarchy, so studying how memory performance will scale is crucial. Reuse distance (RD) analysis can help architects do this. In particular, recent work has developed concurrent reuse distance (CRD) and private reuse distance (PRD) profiles to enable analysis of shared and private caches. Also, techniques have been developed to predict profiles across problem size and core count, enabling the analysis of configurations that are too large to simulate.This paper applies RD analysis to study the scalability of multicore cache hierarchies. We present a framework based on CRD and PRD profiles for reasoning about the locality impact of core count and problem scaling. We find interference-based locality degradation is more significant than sharing-based locality degradation. For 256 cores running small problems, the former occurs at small cache sizes, allowing moderate capacity scaling of multicore caches to achieve the same cache performance (MPKI) as a single-core cache. At very large problems, interference-based locality degradation increases significantly in many of our benchmarks. For shared caches, this prevents most of our benchmarks from achieving constant-MPKI scaling within a 256 MB capacity budget; for private caches, all benchmarks cannot achieve constant-MPKI scaling within 256 MB.","","",""
"Conference Paper","Karve S,Scaffidi C","Towards Mining Informal Online Data to Guide Component-Reuse Decisions","","2013","","","65–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International ACM Sigsoft Symposium on Component-Based Software Engineering","Vancouver, British Columbia, Canada","2013","9781450321228","","https://doi.org/10.1145/2465449.2465459;http://dx.doi.org/10.1145/2465449.2465459","10.1145/2465449.2465459","Online repositories provide components available for reuse, but this does not mean all such components are equally reusable. Components might be unreliable, overly specialized, or otherwise inappropriate for reuse. Repositories collect reviews, ratings, and other data intended to help software engineers choose components. But do these data actually provide any information related to reusability? If so, then how can such information be extracted from the data?To address this question, we analyzed online ratings, reviews and other data for nearly 1200 online components, computed statistics for each component based on these data, and used factor analysis to identify three groups of statistics (factors) that were each internally correlated. We then interviewed software engineers about the reusability of 36 other components and used linear regression to test how well the 3 factors actually corresponded to component reusability.We found that 2 of the 3 factors were indeed related to reusability. Specifically, the reusability of components could be predicted on the basis of component authors' prior work and the documentation provided about components. This result could be used in future work to develop enhanced search engines that highlight components which are potentially reusable and perhaps worthy of more time-consuming evaluation such as by applying formal methods. Additionally, our results reveal opportunities to improve online repositories through specific simplifications as well as enhancements.","component reuse, empirical software engineering, online ratings and reviews","","CBSE '13"
"Conference Paper","Klatt B,Küster M","Improving Product Copy Consolidation by Architecture-Aware Difference Analysis","","2013","","","117–122","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures","Vancouver, British Columbia, Canada","2013","9781450321266","","https://doi.org/10.1145/2465478.2465495;http://dx.doi.org/10.1145/2465478.2465495","10.1145/2465478.2465495","Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating customised product copies to a product line is still a labour-intensive challenge due to the required comprehension of differences among the implementations and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present our continued concept on using component architecture information to enhance a variability reverse engineering process. Including this information particularly improves the difference identification as well as the variation point analysis and -aggregation steps. We show how the concept can be applied by providing an illustrating example.","component architecture, reverse engineering, software product line","","QoSA '13"
"Conference Paper","Heger C","Systematic Guidance in Solving Performance and Scalability Problems","","2013","","","7–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Doctoral Symposium on Components and Architecture","Vancouver, British Columbia, Canada","2013","9781450321259","","https://doi.org/10.1145/2465498.2465500;http://dx.doi.org/10.1145/2465498.2465500","10.1145/2465498.2465500","The performance of enterprise software systems affects business critical metrics like conversion rate (proportion of visitors who become customers) and total cost of ownership for the software system. Keeping such systems responsive and scalable with a growing user base is challenging for software engineers. Solving performance problems is an error-prone and time consuming task that requires deep expert knowledge about the system and performance evaluation. Existing approaches to support the resolution of performance problems mainly focus on the architecture level neglecting influences of the implementation. In this proposal paper, we introduce a novel approach in order to support software engineers in solving performance and scalability problems that leverages known solutions to common problems. The known solutions are evaluated in the context of the particular software system. As a result, a detailed plan is derived that helps and guides software engineers in resolving the problem. We plan to conduct an industrial case study at SAP.","software engineer support, performance problem resolution, feedback provisioning, systematic guidance","","WCOP '13"
"Conference Paper","Chen Y,Groce A,Zhang C,Wong WK,Fern X,Eide E,Regehr J","Taming Compiler Fuzzers","","2013","","","197–208","Association for Computing Machinery","New York, NY, USA","Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation","Seattle, Washington, USA","2013","9781450320146","","https://doi.org/10.1145/2491956.2462173;http://dx.doi.org/10.1145/2491956.2462173","10.1145/2491956.2462173","Aggressive random testing tools (""fuzzers"") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.","fuzz testing, bug reporting, automated testing, compiler testing, test-case reduction, random testing, compiler defect","","PLDI '13"
"Journal Article","Chen Y,Groce A,Zhang C,Wong WK,Fern X,Eide E,Regehr J","Taming Compiler Fuzzers","SIGPLAN Not.","2013","48","6","197–208","Association for Computing Machinery","New York, NY, USA","","","2013-06","","0362-1340","https://doi.org/10.1145/2499370.2462173;http://dx.doi.org/10.1145/2499370.2462173","10.1145/2499370.2462173","Aggressive random testing tools (""fuzzers"") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.","compiler defect, random testing, compiler testing, test-case reduction, fuzz testing, automated testing, bug reporting","",""
"Conference Paper","Koliaï S,Bendifallah Z,Tribalat M,Valensi C,Acquaviva JT,Jalby W","Quantifying Performance Bottleneck Cost through Differential Analysis","","2013","","","263–272","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th International ACM Conference on International Conference on Supercomputing","Eugene, Oregon, USA","2013","9781450321303","","https://doi.org/10.1145/2464996.2465440;http://dx.doi.org/10.1145/2464996.2465440","10.1145/2464996.2465440","Accurate performance analysis is critical for understanding application efficiency and then driving software or hardware optimizations. Although most of static and dynamic performance analysis tools provide useful information, they are not completely satisfactory. Static performance analysis does not provide an accurate view due to the lack of runtime information (eg: cache behavior). On the other hand, profilers, generally mixed with hardware counters, provide a wide range of performance metrics but lack the ability to correlate performance informations with the appropriate code fragment, data structure or instruction. Finally, cycle accurate simulators are too complex and too costly to be used routinely for optimization of real life applications. This paper presents the Differential Analysis method, an approach designed for simple and automatic detection of performance bottlenecks. This approach relies on DECAN, a tool which generates different binary variants obtained by patching individual or groups of instructions. The different variants are then measured and compared, allowing to evaluate the cost of an instruction group and therefore its optimization potential benefit. Differential analysis is illustrated by the use of DECAN on a range of HPC applications to detect performance bottlenecks.","binary rewriting, performance evaluation, differential analysis, bottleneck detection","","ICS '13"
"Journal Article","Chaudhary R,Chatterjee R","Essence of Reusability in Aspect-Oriented Systems","SIGSOFT Softw. Eng. Notes","2013","38","3","1–5","Association for Computing Machinery","New York, NY, USA","","","2013-05","","0163-5948","https://doi.org/10.1145/2464526.2464532;http://dx.doi.org/10.1145/2464526.2464532","10.1145/2464526.2464532","Programmers practicing software development have long realized that reusability is an important area of concern- it is an attribute of quality, which depicts the extent to which a module can be used again in different applications with slight modification.Software professionals often adopt the Module-oriented Approach (MOA) and / or the Object-Oriented Approach (OOA) to develop reusable software. However, these approaches do not address crosscutting concerns (such as logging, security etc.), which are scattered throughout the code and which if poorly done adversely affect reusability and maintainability.Readers may note that ""reusability"" is a quality attribute whose assessment remains quite underexplored. This paper discusses the importance of reusability as a quality attribute and its essence in Aspect-Oriented Systems.This paper has been split into Introduction, Quality Parameters, Conclusion and Future Scope. In the Introduction, the focus is on how Aspect-Oriented Software development addresses the problem of crosscutting concerns. Quality parameters emphasize the concept of reusability within the domains of Object-Oriented Programming and Aspect-Oriented Programming and signify the importance of assessing reusability in Aspect-Oriented Systems. In the future, maintainability will be one of the factors to be considered for assessment of Aspect-Oriented Systems.","object-oriented approach, module-oriented approach, aspect-oriented software development, quality parameters","",""
"Conference Paper","Venkatasubramanyam RD,Gupta S,Singh HK","Prioritizing Code Clone Detection Results for Clone Management","","2013","","","30–36","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Clone detection through tools is a common practice in the software industry. Associated with clone detection is code clone management, which includes taking informed decisions for management of the large sets of clones as reported by the clone detection tools, a task that gets more challenging with larger code bases. In order to enable and ease the process of code clone management, we discuss various criteria that help in prioritizing the clone results. We consider the impact of clones with respect to factors of maintenance overhead, code quality, and refactoring cost. The criteria for prioritization are based on the need for industrial code to adhere to software quality standards. This paper attempts to provide a systematic approach for analyzing and prioritizing clones to determine the order of fixing. This methodology is currently being used in some of the Siemens Corporate Technology Development Center Asia Australia (CT DC AA) projects; a case study of one such project is presented in this paper.","code quality, code clone, clone prioritization, clone management, software clone quality","","IWSC '13"
"Conference Paper","Svajlenko J,Keivanloo I,Roy CK","Scaling Classical Clone Detection Tools for Ultra-Large Datasets: An Exploratory Study","","2013","","","16–22","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Detecting clones from large datasets is an interesting research topic for a number of reasons. However, building scalable clone detection tools is challenging and it is often impossible to use existing state of the art tools for such large datasets. In this research we have investigated the use of our Shuffling Framework for scaling classical clone detection tools to ultra large datasets. This framework achieves scalability on standard hardware by partitioning the dataset and shuffling the partitions over a number of detection rounds. This approach does not require modification to the subject tools, which allows their individual strengths and precisions to be captured at an acceptable loss of recall. In our study, we explored the performance and applicability of our framework for six clone detection tools. The clones found during our experiment were used to comment on the cloning habits of the global Java open-source development community.","clone detection, scalability, large dataset","","IWSC '13"
"Conference Paper","Schulze S,Meyer D","On the Robustness of Clone Detection to Code Obfuscation","","2013","","","62–68","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Code clones are a common reuse mechanism in software development. While there is an ongoing discussion about harmfulness and advantages of code cloning, this discussion is mainly centered around aspects of software quality. However, recent research has shown, that code cloning may have legal implications as well such as license violations. From this point of view, a developer may favor to hide his cloning activities. To this end, he could obfuscate the cloned code to deceive clone detectors. However, it is unknown how robust certain clone detection techniques are against code obfuscations. In this paper, we present a framework for semi-automated code obfuscations. Additionally, we present a case study to evaluate the robustness of selected clone detectors against such obfuscations.","","","IWSC '13"
"Conference Paper","Geesaman PL,Cordy JR,Zouaq A","Light-Weight Ontology Alignment Using Best-Match Clone Detection","","2013","","","1–7","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Ontologies are a key component of the Semantic Web, providing a common basis for representing and exchanging domain meaning in web documents and resources. Ontology alignment is the problem of relating the elements of two formal ontologies for a semantic domain, in order to identify common concepts and relationships represented using different terminology or language, and thus allow meaningful communication and exchange of documents and resources represented using different ontologies for the same domain. Many algorithms have been proposed for ontology alignment, each with their own strengths and weaknesses. The problem is in many ways similar to near-miss clone detection: while much of the description of concepts in two ontologies may be similar, there can be differences in structure or vocabulary that make similarity detection challenging. Based on our previous work extending clone detection to modelling languages such as WSDL using contextualization, in this work we apply near-miss clone detection to the problem of ontology alignment, and use the new notion of ""best-match"" clone detection to achieve results similar to many existing ontology alignment algorithms when applied to standard benchmarks.","OWL, clone detection techniques, ontology alignment, ontology matching","","IWSC '13"
"Conference Paper","Sajnani H,Lopes C","A Parallel and Efficient Approach to Large Scale Clone Detection","","2013","","","46–52","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Over the past few years, researchers have implemented various algorithms to improve the scalability of clone detection. Most of these algorithms focus on scaling vertically on a single machine, and require complex intermediate data structures (e.g., suffix tree, etc.). However, several new use-cases of clone detection have emerged, which are beyond the computational capacity of a single machine. Moreover, for some of these use-cases it may be expensive to invest upfront in the cost of building these data structures.In this paper, we propose a technique to horizontally scale clone detection across multiple machines using the popular MapReduce framework. The technique does not require building any complex intermediate data structures. Moreover, in order to increase the efficiency, the technique uses a filtering heuristic to prune the number of code block comparisons. The filtering heuristic is independent of our approach and it can be used in conjunction with other approaches to increase their efficiency. In our experiments, we found that: (i) the computation time to detect clones decreases by almost half every time we double the number of nodes; and (ii) the scaleup is linear, with a decline of not more than 70% compared to the ideal case, on a cluster of 2-32 nodes for 150-2800 projects.","","","IWSC '13"
"Conference Paper","Muddu B,Asadullah A,Bhat V","CPDP: A Robust Technique for Plagiarism Detection in Source Code","","2013","","","39–45","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","The advent of internet and growth of open source software repositories has made source code readily accessible to software developers. Although, reusing of source code has its own advantages, care must be taken to ensure that proprietary software does not infringe any licenses. In this context, plagiarism detection plays an important role. In this paper, we propose a robust technique to detect plagiarism in source code. Our approach uses a language aware token representation, which is resilient to code transformations and an improved querying and matching technique to detect plagiarism in software code. We evaluated our approach by comparing it with other plagiarism detection tools - Copy Paste Detector (CPD), Sherlock, CCFinder and Plaggie.","large systems maintenance, plagiarism detection, source indexing, clone detection, string matching, source code plagiarism","","IWSC '13"
"Conference Paper","Bauer V,Hauptmann B","Assessing Cross-Project Clones for Reuse Optimization","","2013","","","60–61","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Organizational structures (e. g., separate accounting, heterogeneous infrastructure, or different development processes) can restrict systematic reuse among projects within companies. As a consequence, code is often copied between projects which increases maintenance costs and can cause failures due to inconsistent bug fixing. Assessing cross-project clones helps to uncover organizational obstacles for code reuse and to leverage other ways of systematic reuse. Furthermore, knowing how strongly clones are entangled with the surrounding code helps to decide if and how to extract them to commonly used libraries. We propose to combine cross-project clone detection and dependency analyses to detect (1) what is cloned between projects, (2) how far the cloned code is entangled with the surrounding system and (3) what are candidates for extraction into common libraries.","cross-project, code reuse, clone detection","","IWSC '13"
"Conference Paper","Merlo E,Lavoie T,Potvin P,Busnel P","Large Scale Multi-Language Clone Analysis in a Telecommunication Industrial Setting","","2013","","","69–75","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","This paper presents results from an experience of transferring the CLAN clone detection technology into a telecommunication industrial setting. Eleven proprietary systems have been analyzed for a total of about 94 MLOC of C/C++ and Java source code. The characteristics of the analyzed systems together with a description of the Web portal that is used as an interface to the clone analysis environment is described. Reported results include figures and diagrams about clone frequencies, types, and similarity distributions. Processing times including parsing, clone clustering, and Dynamic Programming visualisation are presented. A discussion about lesson learned and future research work is also presented from an industrial point of view for real life practical applications of clone detection.","telecommunication software, clone detection, experience report","","IWSC '13"
"Conference Paper","Svajlenko J,Roy CK,Cordy JR","A Mutation Analysis Based Benchmarking Framework for Clone Detectors","","2013","","","8–9","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","In recent years, an abundant number of clone detectors have been proposed in literature. However, most of the tool papers have lacked a solid performance evaluation of the subject tools. This is due both to the lack of an available and reliable benchmark, and the manual efforts required to hand check a large number of candidate clones. In this tool demonstration paper we show how a mutation analysis based benchmarking framework can be used by developers and researchers to evaluate clone detection tools at a fine granularity with minimal effort.","evaluation, clone detection, mutation analysis, framework, benchmarking","","IWSC '13"
"Conference Paper","Chatterji D,Carver JC,Kraft NA","Cloning: The Need to Understand Developer Intent","","2013","","","14–15","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Many researchers have studied the positive and negative effects of code clones on software quality. However, little is known about the intent and rationale of the developers who clone code. Studies have shown that reusing code is a common practice for developers while programming, but there are many possible motivations for and approaches to code reuse. Although we have some ideas about the intentions of developers when cloning code, comprehensive research is needed to gather conclusive evidence about these intentions and categorize clones based on them. In this paper we argue that to provide developers with better clone management tools, we need to interview developers to better understand their intentions when managing cloned code.","developer behavior, software clones, code clones, clone management, software maintenance, empirical studies","","IWSC '13"
"Conference Paper","Steidl D,Göde N","Feature-Based Detection of Bugs in Clones","","2013","","","76–82","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Clones bear the risk of incomplete bugfixes when the bug is fixed in one code fragment but at least one of its copies is not changed and remains faulty. Although we find incompletely fixed clones in almost every system, it is usually time consuming to manually locate these clones inside the results of an ordinary clone detection tool. In this paper, we describe in how far certain features of clones can be used to automatically identify incomplete bugfixes. The results are relevant for developers to locate incomplete bugfixes---that is, defects still existing in the system---and for us as clone researchers to quickly find examples that motivate the use of clone management.","software quality, bug detection, code clones","","IWSC '13"
"Conference Paper","Bazrafshan S","No Clones, No Trouble?","","2013","","","37–38","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","In the last years research has focused on various fields of software clones, though there exists only a vague idea of costs and benefits. Without substantial research on the economic consequences of software clones, clone management will remain a questionable or even risky activity. Knowing how much effort is spent over the lifetime of a program to maintain source code that has been introduced in consequence of a deliberate clone removal might provide useful information in this respect. In this paper I present a framework to track code fragments that have been introduced by refactorings performed to remove existing code duplications. Tracking such code over time allows to investigate different aspects, for instance change frequency, that provide valuable insights regarding ongoing maintenance costs.","software maintenance, cost analysis, clone refactoring","","IWSC '13"
"Conference Paper","Lavoie T,Merlo E","How Much Really Changes? A Case Study of Firefox Version Evolution Using a Clone Detector","","2013","","","83–89","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions. We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle.","software evolution, firefox, clone detection","","IWSC '13"
"Conference Paper","Tempero E","Towards a Curated Collection of Code Clones","","2013","","","53–59","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","In order to do research on code clones, it is necessary to have information about code clones. For example, if the research is to improve clone detection, this information would be used to validate the detectors or provide a benchmark to compare different detectors. Or if the research is on techniques for managing clones, then the information would be used as input to such techniques. Typically, researchers have to develop clone information themselves, even if doing so is not the main focus of their research. If such information could be made available, they would be able to use their time more efficiently. If such information was usefully organised and its quality clearly identified, that is, the information is curated, then the quality of the research would be improved as well. In this paper, I describe the beginnings of a curated source of information about a collection of code clones from the Qualitas Corpus. I describe how this information is currently organised, discuss how it might be used, and proposed directions it might take in the future. The collection currently includes 1.3M method-level clone-pairs from 109 different open source Java Systems, applying to approximately 5.6M lines of code.","corpus, empirical studies, code analysis, code clones","","IWSC '13"
"Conference Paper","Harder J","The Limits of Clone Model Standardization","","2013","","","10–11","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","Many tools for clone detection exist. Each has its own model of clones and its own data format. This makes it difficult to share results, compare detectors, and replicate existing studies. Although there have been attempts to provide unified clone models in the past, no widely accepted unified clone model and data format has emerged. This paper discusses challenges that may be the reason why this is the case. In this paper we suggest that existing specialized models should be kept and supplemented with one unified model that serves for exchange only.","","","IWSC '13"
"Conference Paper","Tsantalis N,Krishnan GP","Refactoring Clones: A New Perspective","","2013","","","12–13","IEEE Press","San Francisco, California","Proceedings of the 7th International Workshop on Software Clones","","2013","9781467364454","","","","In this position paper we support that there is still great potential for advancements in the research area of software clone refactoring, and argue about some possible research objectives and directions through illustrative examples.","","","IWSC '13"
"Conference Paper","Hermans F,Sedee B,Pinzger M,van Deursen A","Data Clone Detection and Visualization in Spreadsheets","","2013","","","292–301","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Spreadsheets are widely used in industry: it is estimated that end-user programmers outnumber programmers by a factor 5. However, spreadsheets are error-prone, numerous companies have lost money because of spreadsheet errors. One of the causes for spreadsheet problems is the prevalence of copy-pasting. In this paper, we study this cloning in spreadsheets. Based on existing text-based clone detection algorithms, we have developed an algorithm to detect data clones in spreadsheets: formulas whose values are copied as plain text in a different location. To evaluate the usefulness of the proposed approach, we conducted two evaluations. A quantitative evaluation in which we analyzed the EUSES corpus and a qualitative evaluation consisting of two case studies. The results of the evaluation clearly indicate that 1) data clones are common, 2) data clones pose threats to spreadsheet quality and 3) our approach supports users in finding and resolving data clones.","","","ICSE '13"
"Conference Paper","Rahman MS,Aryani A,Roy CK,Perin F","On the Relationships between Domain-Based Coupling and Code Clones: An Exploratory Study","","2013","","","1265–1268","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Knowledge of similar code fragments, also known as code clones, is important to many software maintenance activities including bug fixing, refactoring, impact analysis and program comprehension. While a great deal of research has been conducted for finding techniques and implementing tools to identify code clones, little research has been done to analyze the relationships between code clones and other aspects of software. In this paper, we attempt to uncover the relationships between code clones and coupling among domain-level components. We report on a case study of a large-scale open source enterprise system, where we demonstrate that the probability of finding code clones among components with domain-based coupling is more than 90%. While such a probabilistic view does not replace a clone detection tool per se, it certainly has the potential to complement the existing tools by providing the probability of having code clones between software components. For example, it can both reduce the clone search space and provide a flexible and language independent way of focusing only on a specific part of the system. It can also provide a higher level of abstraction to look at the cloning relationships among software components.","","","ICSE '13"
"Conference Paper","Stephan M,Alalfi MH,Stevenson A,Cordy JR","Using Mutation Analysis for a Model-Clone Detector Comparison Framework","","2013","","","1261–1264","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Model-clone detection is a relatively new area and there are a number of different approaches in the literature. As the area continues to mature, it becomes necessary to evaluate and compare these approaches and validate new ones that are introduced. We present a mutation-analysis based model-clone detection framework that attempts to automate and standardize the process of comparing multiple Simulink model-clone detection tools or variations of the same tool. By having such a framework, new research directions in the area of model-clone detection can be facilitated as the framework can be used to validate new techniques as they arise. We begin by presenting challenges unique to model-clone tool comparison including recall calculation, the nature of the clones, and the clone report representation. We propose our framework, which we believe addresses these challenges. This is followed by a presentation of the mutation operators that we plan to inject into our Simulink models that will introduce variations of all the different model clone types that can then be searched for by each respective model-clone detector.","","","ICSE '13"
"Conference Paper","Koschke R,Juergens E,Rilling J","7th International Workshop on Software Clones (IWSC 2013)","","2013","","","1527–1528","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Software Clones are identical or similar pieces of code, models or designs. In this, the 7th International Workshop on Software Clones (IWSC, we will discuss issues in software clone detection, analysis and management, as well as applications to software engineering contexts that can benefit from knowledge of clones. These are important emerging topics in software engineering research and practice. Special emphasis will be given this time to clone management in practice, emphasizing use cases and experiences. We will also discuss broader topics on software clones, such as clone detection methods, clone classification, management, and evolution, the role of clones in software system architecture, quality and evolution, clones in plagiarism, licensing, and copyright, and other topics related to similarity in software systems. The format of this workshop will give enough time for intense discussions.","","","ICSE '13"
"Conference Paper","Panichella A,Dit B,Oliveto R,Di Penta M,Poshyvanyk D,De Lucia A","How to Effectively Use Topic Models for Software Engineering Tasks? An Approach Based on Genetic Algorithms","","2013","","","522–531","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is ableto identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.","","","ICSE '13"
"Conference Paper","Jacobellis J,Meng N,Kim M","LASE: An Example-Based Program Transformation Tool for Locating and Applying Systematic Edits","","2013","","","1319–1322","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Adding features and fixing bugs in software often require systematic edits which are similar, but not identical, changes to many code locations. Finding all edit locations and editing them correctly is tedious and error-prone. In this paper, we demonstrate an Eclipse plug-in called LASE that (1) creates context-aware edit scripts from two or more examples, and uses these scripts to (2) automatically identify edit locations and (3) transform the code. In LASE, users can view syntactic edit operations and corresponding context for each input example. They can also choose a different subset of the examples to adjust the abstraction level of inferred edits. When LASE locates target methods matching the inferred edit context and suggests customized edits, users can review and correct LASEs edit suggestion. These features can reduce developers burden in repetitively applying similar edits to different methods. The tools video demonstration is available at https://www.youtube.com/ watch?v=npDqMVP2e9Q.","","","ICSE '13"
"Conference Paper","Yamashita A,Moonen L","Exploring the Impact of Inter-Smell Relations on Software Maintainability: An Empirical Study","","2013","","","682–691","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Code smells are indicators of issues with source code quality that may hinder evolution. While previous studies mainly focused on the effects of individual code smells on maintainability, we conjecture that not only the individual code smells but also the interactions between code smells affect maintenance. We empirically investigate the interactions amongst 12 code smells and analyze how those interactions relate to maintenance problems. Professional developers were hired for a period of four weeks to implement change requests on four medium-sized Java systems with known smells. On a daily basis, we recorded what specific problems they faced and which artifacts were associated with them. Code smells were automatically detected in the pre-maintenance versions of the systems and analyzed using Principal Component Analysis (PCA) to identify patterns of co-located code smells. Analysis of these factors with the observed maintenance problems revealed how smells that were co-located in the same artifact interacted with each other, and affected maintainability. Moreover, we found that code smell interactions occurred across coupled artifacts, with comparable negative effects as same-artifact co-location. We argue that future studies into the effects of code smells on maintainability should integrate dependency analysis in their process so that they can obtain a more complete understanding by including such coupled interactions.","","","ICSE '13"
"Conference Paper","Meng N,Kim M,McKinley KS","LASE: Locating and Applying Systematic Edits by Learning from Examples","","2013","","","502–511","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Adding features and fixing bugs often require sys- tematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.","","","ICSE '13"
"Conference Paper","Janjic W,Atkinson C","Utilizing Software Reuse Experience for Automated Test Recommendation","","2013","","","100–106","IEEE Press","San Francisco, California","Proceedings of the 8th International Workshop on Automation of Software Test","","2013","9781467361613","","","","The development of defect tests is still a very labour intensive process that demands a high-level of domain knowledge, concentration and problem awareness from software engineers. Any technology that can reduce the manual effort involved in this process therefore has the potential to significantly reduce software development costs and time consumption. An idea for achieving this is to reuse the knowledge bound up in already existing test cases, either directly or indirectly, to assist in the development of tests for new software components and systems. Although general software reuse has received a lot of attention in the past -- both in academia and industry -- previous research has focussed on the reuse and recommendation of existing software artifacts in the creation of new product code rather than on the recommendation of tests. In this paper we focus on the latter, and present a novel automated test recommendation approach that leverages lessons learned from traditional software reuse to proactively make test case suggestions while an engineer is developing tests. In contrast, most existing testing-assistance tools provide ex post assistance to test developers in the form of coverage assessments and test quality evaluations. Our goal is to create an automated, non-intrusive recommendation system for efficient software test development. In this paper we set out the basic strategy by which this can be achieved and present a prototypical implementation of our test recommender system for Eclipse.","","","AST '13"
"Conference Paper","Balachandran V","Reducing Human Effort and Improving Quality in Peer Code Reviews Using Automatic Static Analysis and Reviewer Recommendation","","2013","","","931–940","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.","","","ICSE '13"
"Conference Paper","Dyer R,Nguyen HA,Rajan H,Nguyen TN","Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories","","2013","","","422–431","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.","","","ICSE '13"
"Conference Paper","Schwanke R,Xiao L,Cai Y","Measuring Architecture Quality by Structure plus History Analysis","","2013","","","891–900","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","This case study combines known software structure and revision history analysis techniques, in known and new ways, to predict bug-related change frequency, and uncover architecture-related risks in an agile industrial software development project. We applied a suite of structure and history measures and statistically analyzed the correlations between them. We detected architecture issues by identifying outliers in the distributions of measured values and investigating the architectural significance of the associated classes. We used a clustering method to identify sets of files that often change together without being structurally close together, investigating whether architecture issues were among the root causes. The development team confirmed that the identified clusters reflected significant architectural violations, unstable key interfaces, and important undocumented assumptions shared between modules. The combined structure diagrams and history data justified a refactoring proposal that was accepted by the project manager and implemented.","","","ICSE '13"
"Conference Paper","Sawadsky N,Murphy GC,Jiresal R","Reverb: Recommending Code-Related Web Pages","","2013","","","812–821","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","The web is an important source of development-related resources, such as code examples, tutorials, and API documentation. Yet existing development environments are largely disconnected from these resources. In this work, we explore how to provide useful web page recommendations to developers by focusing on the problem of refinding web pages that a developer has previously used. We present the results of a study about developer browsing activity in which we found that 13.7% of developers visits to code-related pages are revisits and that only a small fraction (7.4%) of these were initiated through a low-cost mechanism, such as a bookmark. To assist with code-related revisits, we introduce Reverb, a tool which recommends previously visited web pages that pertain to the code visible in the developer's editor. Through a field study, we found that, on average, Reverb can recommend a useful web page in 51% of revisitation cases.","","","ICSE '13"
"Conference Paper","Nistor A,Song L,Marinov D,Lu S","Toddler: Detecting Performance Problems via Similar Memory-Access Patterns","","2013","","","562–571","IEEE Press","San Francisco, CA, USA","Proceedings of the 2013 International Conference on Software Engineering","","2013","9781467330763","","","","Performance bugs are programming errors that create significant performance degradation. While developers often use automated oracles for detecting functional bugs, detecting performance bugs usually requires time-consuming, manual analysis of execution profiles. The human effort for performance analysis limits the number of performance tests analyzed and enables performance bugs to easily escape to production. Unfortunately, while profilers can successfully localize slow executing code, profilers cannot be effectively used as automated oracles. This paper presents TODDLER, a novel automated oracle for performance bugs, which enables testing for performance bugs to use the well established and automated process of testing for functional bugs. TODDLER reports code loops whose computation has repetitive and partially similar memory-access patterns across loop iterations. Such repetitive work is likely unnecessary and can be done faster. We implement TODDLER for Java and evaluate it on 9 popular Java codebases. Our experiments with 11 previously known, real-world performance bugs show that TODDLER finds these bugs with a higher accuracy than the standard Java profiler. Using TODDLER, we also found 42 new bugs in six Java projects: Ant, Google Core Libraries, JUnit, Apache Collections, JDK, and JFreeChart. Based on our bug reports, developers so far fixed 10 bugs and confirmed 6 more as real bugs.","","","ICSE '13"
"Conference Paper","Garousi G,Garousi V,Moussavi M,Ruhe G,Smith B","Evaluating Usage and Quality of Technical Software Documentation: An Empirical Study","","2013","","","24–35","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering","Porto de Galinhas, Brazil","2013","9781450318488","","https://doi.org/10.1145/2460999.2461003;http://dx.doi.org/10.1145/2460999.2461003","10.1145/2460999.2461003","Context: Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the lack of usage and quality of documentation in practice. Unfortunately, in many projects, practitioners find that software documentation artifacts are outdated, incomplete and sometimes not beneficial. Objective: Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company of GPS software systems, we propose in this paper an approach to analyze the usage and quality of software documentation in development and maintenance phases. Method: The approach incorporates inputs from automated analysis (e.g., mining of project's data) and also experts' opinion extracted from survey-based questionnaire. The approach has been designed based on the ""action-research"" approach and in close collaboration between industry and academia. Results: To evaluate the feasibility and usefulness of the proposed approach, we have applied it in an industrial setting and results are presented in this paper. One of the results is that, in the context of our case-study, usage of documentation for an implementation purpose is higher than the usage for maintenance purposes. Conclusion: It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the task to be completed (e.g. development and maintenance). In addition, we identify the most important and relevant quality attributes which are critical to improving documentation quality.","case study, software development, quality, maintenance, empirical software engineering, usage, technical software documentation, action research, software documentation","","EASE '13"
"Journal Article","Bathen LA,Ahn Y,Pasricha S,Dutt ND","MultiMaKe: Chip-Multiprocessor Driven Memory-Aware Kernel Pipelining","ACM Trans. Embed. Comput. Syst.","2013","12","1s","","Association for Computing Machinery","New York, NY, USA","","","2013-03","","1539-9087","https://doi.org/10.1145/2435227.2435255;http://dx.doi.org/10.1145/2435227.2435255","10.1145/2435227.2435255","The increasing demand for low-power and high-performance multimedia embedded systems has motivated the need for effective solutions to satisfy application bandwidth and latency requirements under a tight power budget. As technology scales, it is imperative that applications are optimized to take full advantage of the underlying resources and meet both power and performance requirements. We propose MultiMaKe, an application mapping design flow capable of discovering and enabling parallelism opportunities via code transformations, efficiently distributing the computational load across resources, and minimizing unnecessary data transfers. Our approach decomposes the application's tasks into smaller units of computations called kernels, which are distributed and pipelined across the different processing resources. We exploit the ideas of inter-kernel data reuse to minimize unnecessary data transfers between kernels, early execution edges to drive performance, and kernel pipelining to increase system throughput. Our experimental results on JPEG and JPEG2000 show up to 97% off-chip memory access reduction, and up to 80% execution time reduction over standard mapping and task-level pipelining approaches.","streaming applications, task mapping, pipelining, Data reuse, scratchpad memory, multiprocessors, scheduling, estimation","",""
"Conference Paper","Barbosa FS,Aguiar A","Using Roles to Model Crosscutting Concerns","","2013","","","97–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th Annual International Conference on Aspect-Oriented Software Development","Fukuoka, Japan","2013","9781450317665","","https://doi.org/10.1145/2451436.2451449;http://dx.doi.org/10.1145/2451436.2451449","10.1145/2451436.2451449","In object oriented languages the problem of crosscutting concerns, due to limitations in the composition mechanisms, is recurrent. In order to reduce this problem we propose to use roles as a way of composing classes that extends the Object Oriented approach and can be used to model crosscutting concerns. To support our approach we developed a role language that extends Java, while being compatible with existing virtual machines. As validation we conducted a case study using three open source systems. We identified crosscutting concerns in the systems and then modeled them using our role approach. Results show that roles are a viable option for modeling crosscutting concerns.","modularity, code reuse, roles, crosscutting concerns, composition","","AOSD '13"
"Conference Paper","Kim J,Moon BR","Disguised Malware Script Detection System Using Hybrid Genetic Algorithm","","2013","","","182–187","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480401;http://dx.doi.org/10.1145/2480362.2480401","10.1145/2480362.2480401","Malicious software, or malware for short, is one of the most serious threats to computer systems. There are various disguise techniques that hide malware from being detected, and these techniques are becoming more sophisticated. Traditional signature-based detection systems often can not cope with disguised malware timely. In this paper, we propose a new approach to detect disguised malware scripts. The proposed system consists of a metric-based detection algorithm and a hybrid genetic algorithm. We use the frequencies of token occurrences as a metric, and separate identifiers from other program tokens. The genetic algorithm tries further detection by extracting the main core of a program. Experimental tests showed that the proposed system successfully detected a number of newly generated malware scripts which existing anti-viruses missed more than half of. The system would be suitable for an offline malware detection which requires high quality.","hybrid genetic algorithm, metric-based method, malware disguise techniques, malware detection","","SAC '13"
"Conference Paper","Bhattacharjee A,Jamil HM","CodeBlast: A Two-Stage Algorithm for Improved Program Similarity Matching in Large Software Repositories","","2013","","","846–852","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480525;http://dx.doi.org/10.1145/2480362.2480525","10.1145/2480362.2480525","In this paper, we present an improved and novel directed graph matching algorithm, called CodeBlast, for searching functionally similar program segments in software repositories with greater effectiveness and accuracy. CodeBlast uses a novel canonical labeling concept to capture order independent data flow pattern in a program to encode the programŠs functional semantics and to aid matching. CodeBlast is capable of exact and approximate directed graph matching and is particularly suitable for matching Program Dependence Graphs. Introducing the notion of semantic equivalence in CodeBlast helps discover clone matches with high precision and recall that was not possible using systems such as JPlag, MOSS, and GPlag. We substantiate our claim through sufficient experimental evidence and comparative analysis with these leading systems.","","","SAC '13"
"Conference Paper","Zibran MF,Saha RK,Roy CK,Schneider KA","Evaluating the Conventional Wisdom in Clone Removal: A Genealogy-Based Empirical Study","","2013","","","1123–1130","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480573;http://dx.doi.org/10.1145/2480362.2480573","10.1145/2480362.2480573","Clone management has drawn immense interest from the research community in recent years. It is recognized that a deep understanding of how code clones change and are refactored is necessary for devising effective clone management tools and techniques. This paper presents an empirical study based on the clone genealogies from a significant number of releases of six software systems, to characterize the patterns of clone change and removal in evolving software systems. With a blend of qualitative analysis, quantitative analysis and statistical tests of significance, we address a number of research questions. Our findings reveal insights into the removal of individual clone fragments and provide empirical evidence in support of conventional clone evolution wisdom. The results can be used to devise informed clone management tools and techniques.","reengineering, clone evolution, refactoring, clone removal","","SAC '13"
"Conference Paper","Durelli RS,Santibáñez DS,Anquetil N,Delamaro ME,de Camargo VV","A Systematic Review on Mining Techniques for Crosscutting Concerns","","2013","","","1080–1087","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480567;http://dx.doi.org/10.1145/2480362.2480567","10.1145/2480362.2480567","Background: The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. Objectives: The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. Results: We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.","cross-cutting concerns, systematic review, aspect mining, concern mining","","SAC '13"
"Conference Paper","Kim D,Han Y,Cho SJ,Yoo H,Woo J,Nah Y,Park M,Chung L","Measuring Similarity of Windows Applications Using Static and Dynamic Birthmarks","","2013","","","1628–1633","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480666;http://dx.doi.org/10.1145/2480362.2480666","10.1145/2480362.2480666","A software birthmark is unique, as certain native characteristics of a program, hence can be used to measure the similarity between programs. In general, a static software birthmark does not need program execution, but is more vulnerable to attacks by semantic-preserving transformations. A dynamic software birthmark is applicable to packed executables, but cannot cover all the possible program paths. In this paper, we propose a novel effective technique to measure the similarity of Microsoft Windows applications using both static and dynamic birthmarks, which are based on the list of system APIs as well as the frequency of system API calls. Because system APIs are located in Windows system directories and act as a bridge between applications and the operating system, our birthmarks are resilient to obfuscations and compiler optimizations. A static birthmark consists of the system API call frequency of a target program, which can be extracted by scanning the executable file. A dynamic birthmark is the frequency of system API function calls, which can be extracted by a binary instrumentation tool during the execution of the program. To evaluate the effectiveness of the proposed technique, we compare various types of Windows applications using both the static and dynamic birthmarks. To demonstrate the robustness, we compare packed executables that were compressed by a binary packing tool. We carry out additional experiments for measuring the similarity between target Windows applications at the source code level and verify the evaluation results. The experimental results show that our birthmarks can effectively measure the similarity between Windows applications, as intended.","windows application, dynamic birthmark, system API, static birthmark, software similarity","","SAC '13"
"Conference Paper","Diwan P,Carey P,Franz E,Li Y,Bitterman T,Hudak DE,Ramnath R","Applying Software Product Line Engineering in Building Web Portals for Supercomputing Services","","2013","","","1765–1771","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual ACM Symposium on Applied Computing","Coimbra, Portugal","2013","9781450316569","","https://doi.org/10.1145/2480362.2480694;http://dx.doi.org/10.1145/2480362.2480694","10.1145/2480362.2480694","Supercomputing centers, typically non-profit, government or university-based organizations with scarce resources, are increasingly being requested to provide customized web portals for user-centered access to their services in order to support a demanding customer base. These portals often have very similar architectures and meet similar requirements, with the variations primarily being in the specialized analysis applications, and in the input and output of these applications. Given these characteristics, Software Production Line Engineering (SPLE) approaches will be valuable in enabling development teams to cost-effectively meet demands. In this paper, we demonstrate a suite of web portals developed at The Ohio Supercomputer Center (OSC) by applying SPLE methodologies. We show how we applied feature modeling on these applications to identify commonalities in their application level features despite differences in their problem domains. We describe a common framework (we term it Per User DrupaL, or PUDL), which serves as the common foundation for these portals. We demonstrate the effectiveness of SPLE in terms of reduced development time and effort, and discuss the technical challenges faced in this process. Finally we propose, as an extension to our work, an automation framework for portal generation, which users could build their own customized portals.","portals, feature modeling, software product line engineering, software-as-a-service, drupal, supercomputing, end-user computing, high performance computing","","SAC '13"
"Conference Paper","Li B,Tata S,Sismanis Y","Sparkler: Supporting Large-Scale Matrix Factorization","","2013","","","625–636","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Conference on Extending Database Technology","Genoa, Italy","2013","9781450315975","","https://doi.org/10.1145/2452376.2452449;http://dx.doi.org/10.1145/2452376.2452449","10.1145/2452376.2452449","Low-rank matrix factorization has recently been applied with great success on matrix completion problems for applications like recommendation systems, link predictions for social networks, and click prediction for web search. However, as this approach is applied to increasingly larger datasets, such as those encountered in web-scale recommender systems like Netflix and Pandora, the data management aspects quickly become challenging and form a road-block. In this paper, we introduce a system called Sparkler to solve such large instances of low rank matrix factorizations. Sparkler extends Spark, an existing platform for running parallel iterative algorithms on datasets that fit in the aggregate main memory of a cluster. Sparkler supports distributed stochastic gradient descent as an approach to solving the factorization problem -- an iterative technique that has been shown to perform very well in practice. We identify the shortfalls of Spark in solving large matrix factorization problems, especially when running on the cloud, and solve this by introducing a novel abstraction called ""Carousel Maps"" (CMs). CMs are well suited to storing large matrices in the aggregate memory of a cluster and can efficiently support the operations performed on them during distributed stochastic gradient descent. We describe the design, implementation, and the use of CMs in Sparkler programs. Through a variety of experiments, we demonstrate that Sparkler is faster than Spark by 4x to 21x, with bigger advantages for larger problems. Equally importantly, we show that this can be done without imposing any changes to the ease of programming. We argue that Sparkler provides a convenient and efficient extension to Spark for solving matrix factorization problems on very large datasets.","recommendation, matrix factorization, iterative data processing, spark, scalability","","EDBT '13"
"Conference Paper","Kadav A,Renzelmann MJ,Swift MM","Fine-Grained Fault Tolerance Using Device Checkpoints","","2013","","","473–484","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems","Houston, Texas, USA","2013","9781450318709","","https://doi.org/10.1145/2451116.2451168;http://dx.doi.org/10.1145/2451116.2451168","10.1145/2451116.2451168","Recovering faults in drivers is difficult compared to other code because their state is spread across both memory and a device. Existing driver fault-tolerance mechanisms either restart the driver and discard its state, which can break applications, or require an extensive logging mechanism to replay requests and recreate driver state. Even logging may be insufficient, though, if the semantics of requests are ambiguous. In addition, these systems either require large subsystems that must be kept up-to-date as the kernel changes, or require substantial rewriting of drivers.We present a new driver fault-tolerance mechanism that provides fine-grained control over the code protected. Fine-Grained Fault Tolerance (FGFT) isolates driver code at the granularity of a single entry point. It executes driver code as a transaction, allowing roll back if the driver fails. We develop a novel checkpointing mechanism to save and restore device state using existing power management code. Unlike past systems, FGFT can be incrementally deployed in a single driver without the need for a large kernel subsystem, but at the cost of small modifications to the driver.In the evaluation, we show that FGFT can have almost zero runtime cost in many cases, and that checkpoint-based recovery can reduce the duration of a failure by 79% compared to restarting the driver. Finally, we show that applying FGFT to a driver requires little effort, and the majority of drivers in common classes already contain the power-management code needed for checkpoint/restore.","checkpoints, device drivers","","ASPLOS '13"
"Journal Article","Kadav A,Renzelmann MJ,Swift MM","Fine-Grained Fault Tolerance Using Device Checkpoints","SIGPLAN Not.","2013","48","4","473–484","Association for Computing Machinery","New York, NY, USA","","","2013-03","","0362-1340","https://doi.org/10.1145/2499368.2451168;http://dx.doi.org/10.1145/2499368.2451168","10.1145/2499368.2451168","Recovering faults in drivers is difficult compared to other code because their state is spread across both memory and a device. Existing driver fault-tolerance mechanisms either restart the driver and discard its state, which can break applications, or require an extensive logging mechanism to replay requests and recreate driver state. Even logging may be insufficient, though, if the semantics of requests are ambiguous. In addition, these systems either require large subsystems that must be kept up-to-date as the kernel changes, or require substantial rewriting of drivers.We present a new driver fault-tolerance mechanism that provides fine-grained control over the code protected. Fine-Grained Fault Tolerance (FGFT) isolates driver code at the granularity of a single entry point. It executes driver code as a transaction, allowing roll back if the driver fails. We develop a novel checkpointing mechanism to save and restore device state using existing power management code. Unlike past systems, FGFT can be incrementally deployed in a single driver without the need for a large kernel subsystem, but at the cost of small modifications to the driver.In the evaluation, we show that FGFT can have almost zero runtime cost in many cases, and that checkpoint-based recovery can reduce the duration of a failure by 79% compared to restarting the driver. Finally, we show that applying FGFT to a driver requires little effort, and the majority of drivers in common classes already contain the power-management code needed for checkpoint/restore.","device drivers, checkpoints","",""
"Journal Article","Kadav A,Renzelmann MJ,Swift MM","Fine-Grained Fault Tolerance Using Device Checkpoints","SIGARCH Comput. Archit. News","2013","41","1","473–484","Association for Computing Machinery","New York, NY, USA","","","2013-03","","0163-5964","https://doi.org/10.1145/2490301.2451168;http://dx.doi.org/10.1145/2490301.2451168","10.1145/2490301.2451168","Recovering faults in drivers is difficult compared to other code because their state is spread across both memory and a device. Existing driver fault-tolerance mechanisms either restart the driver and discard its state, which can break applications, or require an extensive logging mechanism to replay requests and recreate driver state. Even logging may be insufficient, though, if the semantics of requests are ambiguous. In addition, these systems either require large subsystems that must be kept up-to-date as the kernel changes, or require substantial rewriting of drivers.We present a new driver fault-tolerance mechanism that provides fine-grained control over the code protected. Fine-Grained Fault Tolerance (FGFT) isolates driver code at the granularity of a single entry point. It executes driver code as a transaction, allowing roll back if the driver fails. We develop a novel checkpointing mechanism to save and restore device state using existing power management code. Unlike past systems, FGFT can be incrementally deployed in a single driver without the need for a large kernel subsystem, but at the cost of small modifications to the driver.In the evaluation, we show that FGFT can have almost zero runtime cost in many cases, and that checkpoint-based recovery can reduce the duration of a failure by 79% compared to restarting the driver. Finally, we show that applying FGFT to a driver requires little effort, and the majority of drivers in common classes already contain the power-management code needed for checkpoint/restore.","checkpoints, device drivers","",""
"Conference Paper","Guo PJ","Online Python Tutor: Embeddable Web-Based Program Visualization for Cs Education","","2013","","","579–584","Association for Computing Machinery","New York, NY, USA","Proceeding of the 44th ACM Technical Symposium on Computer Science Education","Denver, Colorado, USA","2013","9781450318686","","https://doi.org/10.1145/2445196.2445368;http://dx.doi.org/10.1145/2445196.2445368","10.1145/2445196.2445368","This paper presents Online Python Tutor, a web-based program visualization tool for Python, which is becoming a popular language for teaching introductory CS courses. Using this tool, teachers and students can write Python programs directly in the web browser (without installing any plugins), step forwards and backwards through execution to view the run-time state of data structures, and share their program visualizations on the web. In the past three years, over 200,000 people have used Online Python Tutor to visualize their programs. In addition, instructors in a dozen universities such as UC Berkeley, MIT, the University of Washington, and the University of Waterloo have used it in their CS1 courses. Finally, Online Python Tutor visualizations have been embedded within three web-based digital Python textbook projects, which collectively attract around 16,000 viewers per month and are being used in at least 25 universities. Online Python Tutor is free and open source software, available at pythontutor.com.","program visualization, CS1, python","","SIGCSE '13"
"Conference Paper","Henss J,Merkle P,Reussner RH","The OMPCM Simulator for Model-Based Software Performance Prediction: Poster Abstract","","2013","","","354–357","ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","Proceedings of the 6th International ICST Conference on Simulation Tools and Techniques","Cannes, France","2013","9781450324649","","","","Software performance models play an important role in early stage quality evaluations. Performance models in particular allow for comparing architectural alternatives before unfavourable design decisions are made that need to be revised in a costly procedure. The Palladio component model (PCM) is a modelling language for component-based software architectures. Instances of the PCM can already be analysed for their performance using analytical or simulative approaches. It is, however, difficult to obtain accurate performance predictions for network-intensive distributed systems. This is mainly due to the simplistic network model used so far. In this paper, we present the OMPCM simulator, which integrates OMNeT++ network simulation with architecture-level software performance prediction. OMPCM models can be automatically created from PCM models using a chain of model transformations.","palladio component model, simulation, component based software","","SimuTools '13"
"Journal Article","Monperrus M,Mezini M","Detecting Missing Method Calls as Violations of the Majority Rule","ACM Trans. Softw. Eng. Methodol.","2013","22","1","","Association for Computing Machinery","New York, NY, USA","","","2013-03","","1049-331X","https://doi.org/10.1145/2430536.2430541;http://dx.doi.org/10.1145/2430536.2430541","10.1145/2430536.2430541","When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices).","Bugdetection, static analysis, data mining","",""
"Journal Article","Fleming SD,Scaffidi C,Piorkowski D,Burnett M,Bellamy R,Lawrance J,Kwan I","An Information Foraging Theory Perspective on Tools for Debugging, Refactoring, and Reuse Tasks","ACM Trans. Softw. Eng. Methodol.","2013","22","2","","Association for Computing Machinery","New York, NY, USA","","","2013-03","","1049-331X","https://doi.org/10.1145/2430545.2430551;http://dx.doi.org/10.1145/2430545.2430551","10.1145/2430545.2430551","Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers’ needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective.","software maintenance, Information foraging","",""
"Journal Article","Wu MJ,Yeung D","Efficient Reuse Distance Analysis of Multicore Scaling for Loop-Based Parallel Programs","ACM Trans. Comput. Syst.","2013","31","1","","Association for Computing Machinery","New York, NY, USA","","","2013-02","","0734-2071","https://doi.org/10.1145/2427631.2427632;http://dx.doi.org/10.1145/2427631.2427632","10.1145/2427631.2427632","Reuse Distance (RD) analysis is a powerful memory analysis tool that can potentially help architects study multicore processor scaling. One key obstacle, however, is that multicore RD analysis requires measuring Concurrent Reuse Distance (CRD) and Private-LRU-stack Reuse Distance (PRD) profiles across thread-interleaved memory reference streams. Sensitivity to memory interleaving makes CRD and PRD profiles architecture dependent, preventing them from analyzing different processor configurations. For loop-based parallel programs, CRD and PRD profiles shift coherently across RD values with core count scaling because interleaving threads are symmetric. Simple techniques can predict such shifting, making the analysis of numerous multicore configurations from a small set of CRD and PRD profiles feasible. Given the ubiquity of parallel loops, such techniques will be extremely valuable for studying future large multicore designs.This article investigates using RD analysis to efficiently analyze multicore cache performance for loop-based parallel programs, making several contributions. First, we provide an in-depth analysis on how CRD and PRD profiles change with core count scaling. Second, we develop techniques to predict CRD and PRD profile scaling, in particular employing reference groups [Zhong et al. 2003] to predict coherent shift, demonstrating 90% or greater prediction accuracy. Third, our CRD and PRD profile analyses define two application parameters with architectural implications: Ccore is the minimum shared cache capacity that “contains” locality degradation due to core count scaling, and Cshare is the capacity at which shared caches begin to provide a cache-miss reduction compared to private caches. And fourth, we apply CRD and PRD profiles to analyze multicore cache performance. When combined with existing problem scaling prediction, our techniques can predict shared LLC MPKI (private L2 cache MPKI) to within 10.7% (13.9%) of simulation across 1,728 (1,440) configurations using only 36 measured CRD (PRD) profiles.","chip multiprocessors, reuse distance, Cache performance","",""
"Conference Paper","Le Nguyen TT,Carbone A,Sheard J,Schuhmacher M","Integrating Source Code Plagiarism into a Virtual Learning Environment: Benefits for Students and Staff","","2013","","","155–164","Australian Computer Society, Inc.","AUS","Proceedings of the Fifteenth Australasian Computing Education Conference - Volume 136","Adelaide, Australia","2013","9781921770210","","","","Source code plagiarism is a growing concern in computing related courses. There are a variety of tools to help academics detect suspicious similarity in computer programs. These are purpose-built and necessarily different from the more widely used text-matching tools for plagiarism detection in essays. However, not only is the adoption of these code plagiarism detection tools very modest, the lack of integration of these tools into learning environments means that they are, if used, just intended to identify offending students, rather than as an educational tool to raise their awareness of this sensitive problem. This paper describes the development of a plugin to integrate the two well-known code plagiarism detectors, JPlag and MOSS, into an open source virtual learning environment, Moodle, to address the needs of academics teaching computer programming at an Australian University. A study was carried out to evaluate the benefits offered by such integration for academics and students.","source code matching tools, academic integrity, plagiarism","","ACE '13"
"Conference Paper","Cesare S,Xiang Y","Simseer and Bugwise: Web Services for Binary-Level Software Similarity and Defect Detection","","2013","","","21–29","Australian Computer Society, Inc.","AUS","Proceedings of the Eleventh Australasian Symposium on Parallel and Distributed Computing - Volume 140","Adelaide, Australia","2013","9781921770258","","","","Simseer and Bugwise are online web services that perform binary program analysis: 1) Simseer identifies similarity between submitted executables based on similarity in the control flow of each binary. A software similarity service provides benefit in identifying malware variants and families, discovering software theft, and revealing plagiarism of software programs. Simseer additionally performs code packing detection and automated unpacking of hidden code using application-level emulation. Finally, Simseer uses the similarity information from a sample set to identify program relationships and families through visualization of an evolutionary tree. 2) Bugwise is a service that identifies software bugs and defects. To achieve this end, it performs decompilation and data flow analysis. Bugwise can identify a subset of use-after-free bugs and has already found defects in Debian Linux. Bugwise and Simseer are both built on Malwise, a platform of binary analysis.","computer security, bug detection, plagiarism detection, software theft detection, software similarity, cloud computing","","AusPDC '13"
"Conference Paper","Lakhotia A,Preda MD,Giacobazzi R","Fast Location of Similar Code Fragments Using Semantic 'Juice'","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd ACM SIGPLAN Program Protection and Reverse Engineering Workshop","Rome, Italy","2013","9781450318570","","https://doi.org/10.1145/2430553.2430558;http://dx.doi.org/10.1145/2430553.2430558","10.1145/2430553.2430558","Abstraction of semantics of blocks of a binary is termed as 'juice.' Whereas the denotational semantics summarizes the computation performed by a block, its juice presents a template of the relationships established by the block. BinJuice is a tool for extracting the 'juice' of a binary. It symbolically interprets individual blocks of a binary to extract their semantics: the effect of the block on the program state. The semantics is generalized to juice by replacing register names and literal constants by typed, logical variables. The juice also maintains algebraic constraints between the numeric variables. Thus, this juice forms a semantic template that is expected to be identical regardless of code variations due to register renaming, memory address allocation, and constant replacement. The terms in juice can be canonically ordered using a linear order presented. Thus semantically equivalent (rather, similar) code fragments can be identified by simple structural comparison of their juice, or by comparing their hashes. While BinJuice cannot find all equivalent constructs, for that would solve the Halting Problem, it does significantly improve the state-of-the-art in both the computational complexity as well as the set of equivalences it can establish. Preliminary results show that juice is effective in pairing code variants created by post-compile obfuscating transformations.","","","PPREW '13"
"Conference Paper","von Rhein A,Apel S,Kästner C,Thüm T,Schaefer I","The PLA Model: On the Combination of Product-Line Analyses","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems","Pisa, Italy","2013","9781450315418","","https://doi.org/10.1145/2430502.2430522;http://dx.doi.org/10.1145/2430502.2430522","10.1145/2430502.2430522","Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.","product-line analysis, PLA model, software product lines","","VaMoS '13"
"Conference Paper","Jensen JB,Benton N,Kennedy A","High-Level Separation Logic for Low-Level Code","","2013","","","301–314","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","Rome, Italy","2013","9781450318327","","https://doi.org/10.1145/2429069.2429105;http://dx.doi.org/10.1145/2429069.2429105","10.1145/2429069.2429105","Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs.The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures.We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection.","proof assistants, machine code, separation logic","","POPL '13"
"Journal Article","Jensen JB,Benton N,Kennedy A","High-Level Separation Logic for Low-Level Code","SIGPLAN Not.","2013","48","1","301–314","Association for Computing Machinery","New York, NY, USA","","","2013-01","","0362-1340","https://doi.org/10.1145/2480359.2429105;http://dx.doi.org/10.1145/2480359.2429105","10.1145/2480359.2429105","Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs.The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures.We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection.","separation logic, machine code, proof assistants","",""
"Conference Paper","Gordon AD,Aizatulin M,Borgstrom J,Claret G,Graepel T,Nori AV,Rajamani SK,Russo C","A Model-Learner Pattern for Bayesian Reasoning","","2013","","","403–416","Association for Computing Machinery","New York, NY, USA","Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","Rome, Italy","2013","9781450318327","","https://doi.org/10.1145/2429069.2429119;http://dx.doi.org/10.1145/2429069.2429119","10.1145/2429069.2429119","A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.","probabilistic programming, machine learning, model-learner pattern, bayesian reasoning","","POPL '13"
"Journal Article","Gordon AD,Aizatulin M,Borgstrom J,Claret G,Graepel T,Nori AV,Rajamani SK,Russo C","A Model-Learner Pattern for Bayesian Reasoning","SIGPLAN Not.","2013","48","1","403–416","Association for Computing Machinery","New York, NY, USA","","","2013-01","","0362-1340","https://doi.org/10.1145/2480359.2429119;http://dx.doi.org/10.1145/2480359.2429119","10.1145/2480359.2429119","A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.","machine learning, bayesian reasoning, probabilistic programming, model-learner pattern","",""
"Journal Article","Lee J,Ko Y,Lee K,Youn JM,Paek Y","Dynamic Code Duplication with Vulnerability Awareness for Soft Error Detection on VLIW Architectures","ACM Trans. Archit. Code Optim.","2013","9","4","","Association for Computing Machinery","New York, NY, USA","","","2013-01","","1544-3566","https://doi.org/10.1145/2400682.2400707;http://dx.doi.org/10.1145/2400682.2400707","10.1145/2400682.2400707","Soft errors are becoming a critical concern in embedded system designs. Code duplication techniques have been proposed to increase the reliability in multi-issue embedded systems such as VLIW by exploiting empty slots for duplicated instructions. However, they increase code size, another important concern, and ignore vulnerability differences in instructions, causing unnecessary or inefficient protection when selecting instructions to be duplicated under constraints. In this article, we propose a compiler-assisted dynamic code duplication method to minimize the code size overhead, and present vulnerability-aware duplication algorithms to maximize the effectiveness of instruction duplication with least overheads for VLIW architecture. Our experimental results with SoarGen and Synopsys simulation environments demonstrate that our proposals can reduce the code size by up to 40% and detect more soft errors by up to 82% via fault injection experiments over benchmarks from DSPstone and Livermore Loops as compared to the previously proposed instruction duplication technique.","VLIW architecture, Embedded systems, vulnerability-aware duplication algorithm, temporal vulnerability, soft errors, physical vulnerability, compiler-assisted dynamic code duplication scheme, code size","",""
"Conference Paper","Park S,Ko S,Choi J,Han H,Cho SJ,Choi J","Detecting Source Code Similarity Using Code Abstraction","","2013","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Conference on Ubiquitous Information Management and Communication","Kota Kinabalu, Malaysia","2013","9781450319584","","https://doi.org/10.1145/2448556.2448630;http://dx.doi.org/10.1145/2448556.2448630","10.1145/2448556.2448630","Various approaches have been proposed to develop effective methods to measure program similarity. Even commercial tools and freeware tools are available for measuring program similarity based on source code comparison. These tools are quite useful to handle small to middle scale software products, but limited for large scale software products. In addition, these tools may report similarity measures with less credentials for the source code either obfuscated by malicious users or generated by automatic program template generation tools. To handle large scale software, more drastic measures should be provided. In this paper, we propose an automatic abstraction method to summarize source code. We eliminate a large portion of source code which is less relevant to similarity comparison. With this abstraction, our similarity comparison method can provide more robust measures for obfuscation and automatic code generation. We evaluate our abstraction method by running through source comparison tool --- MOSS, a web-based similarity detection tool. According to our experiment with multiple versions of Apache HTTP server, Putty SSH client, and Lighttpd server, our abstraction method reports quite reliable results with abstracted source code, which are only 23--35% of original source code. As the execution time for pattern match is linearly proportional to the length of the source code, our method can reduce the execution time as much as the percentage of source code reduction.","similarity, large scale software, source code abstraction","","ICUIMC '13"
"Journal Article","Kim J,Lee S,Hwang SW,Kim S","Enriching Documents with Examples: A Corpus Mining Approach","ACM Trans. Inf. Syst.","2013","31","1","","Association for Computing Machinery","New York, NY, USA","","","2013-01","","1046-8188","https://doi.org/10.1145/2414782.2414783;http://dx.doi.org/10.1145/2414782.2414783","10.1145/2414782.2414783","Software developers increasingly rely on information from the Web, such as documents or code examples on application programming interfaces (APIs), to facilitate their development processes. However, API documents often do not include enough information for developers to fully understand how to use the APIs, and searching for good code examples requires considerable effort.To address this problem, we propose a novel code example recommendation system that combines the strength of browsing documents and searching for code examples and returns API documents embedded with high-quality code example summaries mined from the Web. Our evaluation results show that our approach provides code examples with high precision and boosts programmer productivity.","API document, ranking, Clustering, code search","",""
"Journal Article","Harman M,Mansouri SA,Zhang Y","Search-Based Software Engineering: Trends, Techniques and Applications","ACM Comput. Surv.","2012","45","1","","Association for Computing Machinery","New York, NY, USA","","","2012-12","","0360-0300","https://doi.org/10.1145/2379776.2379787;http://dx.doi.org/10.1145/2379776.2379787","10.1145/2379776.2379787","In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives.This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.","search-based techniques, survey, Software engineering","",""
"Conference Paper","Dang Y,Zhang D,Ge S,Chu C,Qiu Y,Xie T","XIAO: Tuning Code Clones at Hands of Engineers in Practice","","2012","","","369–378","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual Computer Security Applications Conference","Orlando, Florida, USA","2012","9781450313124","","https://doi.org/10.1145/2420950.2421004;http://dx.doi.org/10.1145/2420950.2421004","10.1145/2420950.2421004","During software development, engineers often reuse a code fragment via copy-and-paste with or without modifications or adaptations. Such practices lead to a number of the same or similar code fragments spreading within one or many large codebases. Detecting code clones has been shown to be useful towards security such as detection of similar security bugs and, more generally, quality improvement such as refactoring of code clones. A large number of academic research projects have been carried out on empirical studies or tool supports for detecting code clones. In this paper, we report our experiences of carrying out successful technology transfer of our new approach of code-clone detection, called XIAO. XIAO has been integrated into Microsoft Visual Studio 2012, to be benefiting a huge number of developers in industry. The main success factors of XIAO include its high tunability, scalability, compatibility, and explorability. Based on substantial industrial experiences, we present the XIAO approach with emphasis on these success factors of XIAO. We also present empirical results on applying XIAO on real scenarios within Microsoft for the tasks of security-bug detection and refactoring.","code duplication, duplicated security vulnerability, code-clone detection, code clone, code-clone search","","ACSAC '12"
"Conference Paper","Yamaguchi F,Lottmann M,Rieck K","Generalized Vulnerability Extrapolation Using Abstract Syntax Trees","","2012","","","359–368","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual Computer Security Applications Conference","Orlando, Florida, USA","2012","9781450313124","","https://doi.org/10.1145/2420950.2421003;http://dx.doi.org/10.1145/2420950.2421003","10.1145/2420950.2421003","The discovery of vulnerabilities in source code is a key for securing computer systems. While specific types of security flaws can be identified automatically, in the general case the process of finding vulnerabilities cannot be automated and vulnerabilities are mainly discovered by manual analysis. In this paper, we propose a method for assisting a security analyst during auditing of source code. Our method proceeds by extracting abstract syntax trees from the code and determining structural patterns in these trees, such that each function in the code can be described as a mixture of these patterns. This representation enables us to decompose a known vulnerability and extrapolate it to a code base, such that functions potentially suffering from the same flaw can be suggested to the analyst. We evaluate our method on the source code of four popular open-source projects: LibTIFF, FFmpeg, Pidgin and Asterisk. For three of these projects, we are able to identify zero-day vulnerabilities by inspecting only a small fraction of the code bases.","","","ACSAC '12"
"Conference Paper","Lindorfer M,Di Federico A,Maggi F,Comparetti PM,Zanero S","Lines of Malicious Code: Insights into the Malicious Software Industry","","2012","","","349–358","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual Computer Security Applications Conference","Orlando, Florida, USA","2012","9781450313124","","https://doi.org/10.1145/2420950.2421001;http://dx.doi.org/10.1145/2420950.2421001","10.1145/2420950.2421001","Malicious software installed on infected computers is a fundamental component of online crime. Malware development thus plays an essential role in the underground economy of cyber-crime. Malware authors regularly update their software to defeat defenses or to support new or improved criminal business models. A large body of research has focused on detecting malware, defending against it and identifying its functionality. In addition to these goals, however, the analysis of malware can provide a glimpse into the software development industry that develops malicious code.In this work, we present techniques to observe the evolution of a malware family over time. First, we develop techniques to compare versions of malicious code and quantify their differences. Furthermore, we use behavior observed from dynamic analysis to assign semantics to binary code and to identify functional components within a malware binary. By combining these techniques, we are able to monitor the evolution of a malware's functional components. We implement these techniques in a system we call Beagle, and apply it to the observation of 16 malware strains over several months. The results of these experiments provide insight into the effort involved in updating malware code, and show that Beagle can identify changes to individual malware components.","similarity, downloaders, malware, evolution","","ACSAC '12"
"Conference Paper","Srivastava A,Giffin J","Efficient Protection of Kernel Data Structures via Object Partitioning","","2012","","","429–438","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Annual Computer Security Applications Conference","Orlando, Florida, USA","2012","9781450313124","","https://doi.org/10.1145/2420950.2421012;http://dx.doi.org/10.1145/2420950.2421012","10.1145/2420950.2421012","Commodity operating system kernels isolate applications via separate memory address spaces provided by virtual memory management hardware. However, kernel memory is unified and mixes core kernel code with driver components of different provenance. Kernel-level malicious software exploits this lack of isolation between the kernel and its modules by illicitly modifying security-critical kernel data structures. In this paper, we design an access control policy and enforcement system that prevents kernel components with low trust from altering security-critical data used by the kernel to manage its own execution. Our policies are at the granularity of kernel variables and structure elements, and they can protect data structures dynamically allocated at runtime. Our hypervisor-based design uses memory page protection bits as part of its policy enforcement. The granularity difference between page-level protection and variable-level policies challenges the system's ability to remain performant. We develop kernel data-layout partitioning and reorganization techniques to maintain kernel performance in the presence of our protections. We show that our system can prevent malicious modifications to security-critical kernel data with small overhead. By offering protection for critical kernel data structures, we can detect unknown kernel-level malware and guarantee that security utilities relying on the integrity of kernel-level state remain accurate.","","","ACSAC '12"
"Journal Article","Taylor WE,Haddad HM","Flaws of the OO Paradigm: A Perspective from the Procedural Side","J. Comput. Sci. Coll.","2012","28","2","160–167","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2012-12","","1937-4771","","","Developers are familiar with the benefits of OO programming such as encapsulation, inheritance and polymorphism. The difficulties with OO programming are less well known, though there are many. Good developers should know both the strengths and weaknesses of the tools they use. This paper is a survey of common issues encountered when developers try to apply OO principles to large, complex real-world software projects.","OO programming, OO development issues, inheritance concerns, OO flaws","",""
"Conference Paper","Taherkhani A,Korhonen A,Malmi L","Automatic Recognition of Students' Sorting Algorithm Implementations in a Data Structures and Algorithms Course","","2012","","","83–92","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th Koli Calling International Conference on Computing Education Research","Koli, Finland","2012","9781450317955","","https://doi.org/10.1145/2401796.2401806;http://dx.doi.org/10.1145/2401796.2401806","10.1145/2401796.2401806","Computing educators often rely on black-box analysis to assess students' work automatically and give feedback. This approach does not allow analyzing the quality of programs and checking if they implement the required algorithm. We introduce an instrument for recognizing and classifying algorithms (Aari) in terms of white-box testing to identify authentic students' sorting algorithm implementations in a data structures and algorithms course. Aari uses machine learning techniques to classify new instances. The students were asked to submit a program to sort an array of integers in two rounds: at the beginning of the course before sorting algorithms were introduced, and after taking a lecture on sorting algorithms. We evaluated the performance of Aari with the implementations of each round separately. The results show that the sorting algorithms, which Aari has been trained to recognize, are recognized with an average accuracy of about 90%. When considering all the submitted sorting algorithm implementations (including the variations of the standard algorithms), Aari achieved an overall accuracy of 71% and 81% for the first and second round, respectively.In addition, we analyzed the students' implementations manually to gain a better understanding of the reasons of failure in the recognition process. This analysis revealed that students have many misconceptions related to sorting algorithms, which results in problematic implementations that are more inefficient compared with those of standard algorithms. We discuss these variations along with the application of the tool in an educational context, its limitations and some directions for future work.","algorithm recognition, roles of variables, program comprehension, decision tree classifiers","","Koli Calling '12"
"Conference Paper","Iwamoto K,Wasaki K","Malware Classification Based on Extracted API Sequences Using Static Analysis","","2012","","","31–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Asian Internet Engineering Conference","Bangkok, Thailand","2012","9781450318143","","https://doi.org/10.1145/2402599.2402604;http://dx.doi.org/10.1145/2402599.2402604","10.1145/2402599.2402604","In this paper, we propose a highly accurate, automatic malware-classification method, which extracts features by conducting static analysis of malware samples and the structure of malware source code. In the proposed extraction method, the presence and absence of particular pairs of consecutive Application Program Interface function calls (APIs) in the API-sequence graph are compared with those in the executable code for a sample within which malware features have been identified. To determine the degree of similarity between samples, Dice's coefficient is applied. To visualize the grouping of samples with similar features, we use hierarchical cluster analysis based on the extracted features. The results of the analysis are presented as a dendrogram with colored nodes for each family name. To evaluate the proposed method, we set up a malware-analysis system comprising a combination of disassembler, control-flow analyzer, API-sequence extractor, similarity calculator and hierarchical cluster analyzer. We acquired 4,684 malware samples, from 1,821 of which we successfully extracted API sequences to which we applied our proposed classification method. We found that the automatic hierarchical cluster analysis was processed rapidly, with significant clusters of variant groups obtained.","malware, control-flow analysis, classification, API sequence, static analysis","","AINTEC '12"
"Conference Paper","Nguyen AT,Nguyen TT,Nguyen HA,Nguyen TN","Multi-Layered Approach for Recovering Links between Bug Reports and Fixes","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393671;http://dx.doi.org/10.1145/2393596.2393671","10.1145/2393596.2393671","The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18%, 13--17%, and 8--17% in F-score, recall, and precision, respectively.","fixes, bug-to-fix links, mining software repository, bugs","","FSE '12"
"Conference Paper","Ray B,Wiley C,Kim M","REPERTOIRE: A Cross-System Porting Analysis Tool for Forked Software Projects","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393603;http://dx.doi.org/10.1145/2393596.2393603","10.1145/2393596.2393603","To create a new variant of an existing project, developers often copy an existing codebase and modify it. This process is called software forking. After forking software, developers often port new features or bug fixes from peer projects. Repertoire analyzes repeated work of cross-system porting among forked projects. It takes the version histories as input and identifies ported edits by comparing the content of individual patches. It also shows users the extent of ported edits, where and when the ported edits occurred, which developers ported code from peer projects, and how long it takes for patches to be ported.","software evolution, forking, porting, code clones, repetitive changes","","FSE '12"
"Conference Paper","Tao Y,Dang Y,Xie T,Zhang D,Kim S","How Do Software Engineers Understand Code Changes? An Exploratory Study in Industry","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393656;http://dx.doi.org/10.1145/2393596.2393656","10.1145/2393596.2393656","Software evolves with continuous source-code changes. These code changes usually need to be understood by software engineers when performing their daily development and maintenance tasks. However, despite its high importance, such change-understanding practice has not been systematically studied. Such lack of empirical knowledge hinders attempts to evaluate this fundamental practice and improve the corresponding tool support.To address this issue, in this paper, we present a large-scale quantitative and qualitative study at Microsoft. The study investigates the role of understanding code changes during software-development process, explores engineers' information needs for understanding changes and their requirements for the corresponding tool support. The study results reinforce our beliefs that understanding code changes is an indispensable task performed by engineers in software-development process. A number of insufficiencies in the current practice also emerge from the study results. For example, it is difficult to acquire important information needs such as a change's completeness, consistency, and especially the risk imposed by it on other software components. In addition, for understanding a composite change, it is valuable to decompose it into sub-changes that are aligned with individual development issues; however, currently such decomposition lacks tool support.","tool support, code change, information needs, code review","","FSE '12"
"Conference Paper","Ray B,Kim M","A Case Study of Cross-System Porting in Forked Projects","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393659;http://dx.doi.org/10.1145/2393596.2393659","10.1145/2393596.2393659","Software forking---creating a variant product by copying and modifying an existing product---is often considered an ad hoc, low cost alternative to principled product line development. To maintain such forked products, developers often need to port an existing feature or bug-fix from one product variant to another. As a first step towards assessing whether forking is a sustainable practice, we conduct an in-depth case study of 18 years of the BSD product family history. Our study finds that maintaining forked projects involves significant effort of porting patches from other projects. Cross-system porting happens periodically and the porting rate does not necessarily decrease over time. A significant portion of active developers participate in porting changes from peer projects. Surprisingly, ported changes are less defect-prone than non-ported changes. Our work is the first to comprehensively characterize the temporal, spatial, and developer dimensions of cross-system porting in the BSD family, and our tool Repertoire is the first automated tool for detecting ported edits with high accuracy of 94% precision and 84% recall. Our study finds that the upkeep work of porting changes from peer projects is significant and currently, porting practice seems to heavily depend on developers doing their porting job on time. This result calls for new techniques to automate cross-system porting to reduce the maintenance cost of forked projects.","forking, software evolution, code clones, porting, repetitive changes","","FSE '12"
"Conference Paper","Servant F,Jones JA","History Slicing: Assisting Code-Evolution Tasks","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393646;http://dx.doi.org/10.1145/2393596.2393646","10.1145/2393596.2393646","Many software-engineering tasks require developers to understand the history and evolution of source code. However, today's software-development techniques and tools are not well suited for the easy and efficient procurement of such information. In this paper, we present an approach called history slicing that can automatically identify a minimal number of code modifications, across any number of revisions, for any arbitrary segment of source code at fine granularity. We also present our implementation of history slicing, Chronos, that includes a novel visualization of the entire evolution for the code of interest. We provide two experiments: one experiment automatically computes 16,000 history slices to determine the benefit brought by various levels of automation, and another experiment that assesses the practical implications of history slicing for actual developers using the technique for actual software-maintenance tasks that involve code evolution. The experiments show that history slicing offered drastic improvements over the conventional techniques in three ways: (1) the amount of information needed to be examined and traced by developers was reduced by up to three orders of magnitude; (2) the correctness of developers attempting to solve software-maintenance tasks was more than doubled; and (3) the time to completion of these software-maintenance tasks was almost halved.","software evolution, software visualization, mining software repositories, program comprehension","","FSE '12"
"Conference Paper","Pinto LS,Sinha S,Orso A","Understanding Myths and Realities of Test-Suite Evolution","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393634;http://dx.doi.org/10.1145/2393596.2393634","10.1145/2393596.2393634","Test suites, once created, rarely remain static. Just like the application they are testing, they evolve throughout their lifetime. Test obsolescence is probably the most known reason for test-suite evolution---test cases cease to work because of changes in the code and must be suitably repaired. Repairing existing test cases manually, however, can be extremely time consuming, especially for large test suites, which has motivated the recent development of automated test-repair techniques. We believe that, for developing effective repair techniques that are applicable in real-world scenarios, a fundamental prerequisite is a thorough understanding of how test cases evolve in practice. Without such knowledge, we risk to develop techniques that may work well for only a small number of tests or, worse, that may not work at all in most realistic cases. Unfortunately, to date there are no studies in the literature that investigate how test suites evolve. To tackle this problem, in this paper we present a technique for studying test-suite evolution, a tool that implements the technique, and an extensive empirical study in which we used our technique to study many versions of six real-world programs and their unit test suites. This is the first study of this kind, and our results reveal several interesting aspects of test-suite evolution. In particular, our findings show that test repair is just one possible reason for test-suite evolution, whereas most changes involve refactorings, deletions, and additions of test cases. Our results also show that test modifications tend to involve complex, and hard-to-automate, changes to test cases, and that existing test-repair techniques that focus exclusively on assertions may have limited practical applicability. More generally, our findings provide initial insight on how test cases are added, removed, and modified in practice, and can guide future research efforts in the area of test-suite evolution.","test-suite maintenance, unit testing, test-suite evolution","","FSE '12"
"Conference Paper","Cossette BE,Walker RJ","Seeking the Ground Truth: A Retroactive Study on the Evolution and Migration of Software Libraries","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393661;http://dx.doi.org/10.1145/2393596.2393661","10.1145/2393596.2393661","Application programming interfaces (APIs) are a common and industrially-relevant means for third-party software developers to reuse external functionality. Several techniques have been proposed to help migrate client code between library versions with incompatible APIs, but it is not clear how well these perform in an absolute sense. We present a retroactive study into the presence and nature of API incompatibilities between several versions of a set of Java-based software libraries; for each, we perform a detailed, manual analysis to determine what the correct adaptations are to migrate from the older to the newer version. In addition, we investigate whether any of a set of adaptation recommender techniques is capable of identifying the correct adaptations for library migration. We find that a given API incompatibility can typically be addressed by only one or two recommender techniques, but sometimes none serve. Furthermore, those techniques give correct recommendations, on average, in only about 20% of cases.","recommendation systems, adaptive change, API","","FSE '12"
"Conference Paper","Robbes R,Lungu M,Röthlisberger D","How Do Developers React to API Deprecation? The Case of a Smalltalk Ecosystem","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393662;http://dx.doi.org/10.1145/2393596.2393662","10.1145/2393596.2393662","When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation---known as a ripple effect---is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes.Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on an empirical study of API deprecations that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating around the Squeak and Pharo software ecosystems: seven years of evolution, more than 3,000 contributors, and more than 2,600 distinct systems. We analyzed 577 methods and 186 classes that were deprecated, and answer research questions regarding the frequency, magnitude, duration, adaptation, and consistency of the ripple effects triggered by API changes.","empirical studies, ecosystems, mining software repositories","","FSE '12"
"Conference Paper","Kim M,Zimmermann T,Nagappan N","A Field Study of Refactoring Challenges and Benefits","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering","Cary, North Carolina","2012","9781450316149","","https://doi.org/10.1145/2393596.2393655;http://dx.doi.org/10.1145/2393596.2393655","10.1145/2393596.2393655","It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds that the binary modules refactored by this team experienced significant reduction in the number of inter-module dependencies and post-release defects, indicating a visible benefit of refactoring.","churn, software evolution, refactoring, empirical study, defects, component dependencies","","FSE '12"
"Conference Paper","Lee S,Vetter JS","Early Evaluation of Directive-Based GPU Programming Models for Productive Exascale Computing","","2012","","","","IEEE Computer Society Press","Washington, DC, USA","Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","Salt Lake City, Utah","2012","9781467308045","","","","Graphics Processing Unit (GPU)-based parallel computer architectures have shown increased popularity as a building block for high performance computing, and possibly for future Exascale computing. However, their programming complexity remains as a major hurdle for their widespread adoption. To provide better abstractions for programming GPU architectures, researchers and vendors have proposed several directive-based GPU programming models. These directive-based models provide different levels of abstraction, and required different levels of programming effort to port and optimize applications. Understanding these differences among these new models provides valuable insights on their applicability and performance potential. In this paper, we evaluate existing directive-based models by porting thirteen application kernels from various scientific domains to use CUDA GPUs, which, in turn, allows us to identify important issues in the functionality, scalability, tunability, and debuggability of the existing models. Our evaluation shows that directive-based models can achieve reasonable performance, compared to hand-written GPU codes.","","","SC '12"
"Conference Paper","Mondal M,Roy CK,Schneider KA","Connectivity of Co-Changed Method Groups: A Case Study on Open Source Systems","","2012","","","205–219","IBM Corp.","USA","Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research","Toronto, Ontario, Canada","2012","","","","","Software maintenance is an important and challenging phase of the software development life cycle because changes during this phase without proper awareness of dependencies among program modules can introduce faults in the software system. There is also a common intuition that cloned code introduces additional software maintenance challenges and difficulties. To support successful accomplishment of maintenance activities we consider two issues: (i) identifying coding characteristics that cause high source code modifications, and (ii) guidance for minimizing source code modifications.Focusing on these two issues we investigated the effects of method sharing (among different functionality) on method co-changeability and source code modifications. We proposed and empirically evaluated two metrics, (i) COMS (Co-changeability of Methods), and (ii) CCMS (Connectivity of Co-changed Method Groups). COMS measures the extent to which a method co-changes with other methods. CCMS quantifies the extent to which a particular functionality in a software system is connected with other functionality in that system. In other words CCMS measures the intensity of method sharing among different functionality or tasks (defined later). We investigated the impact of CCMS on COMS and source code modifications. Our comprehensive study on hundreds of revisions of six open source subject systems covering three programming languages (Java, C and C#) suggests that - (i) higher CCMS causes higher COMS as well as increased source code modifications, (ii) COMS in the cloned regions of a software system is negligible as compared to the COMS in the non-cloned regions, and (iii) in-spite of some issues (described later) cloning can be a possible way to reduce CCMS.","","","CASCON '12"
"Conference Paper","Smit M,Shtern M,Simmons B,Litoiu M","Partitioning Applications for Hybrid and Federated Clouds","","2012","","","27–41","IBM Corp.","USA","Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research","Toronto, Ontario, Canada","2012","","","","","On-demand access to computing resources as-a-service has the potential to allow enterprises to temporarily scale out of their private data center into the infrastructure of a public cloud provider during times of peak demand. However, concerns about privacy and security may limit the adoption of this technique. We describe an approach to partitioning a software application (particularly a client-facing web application) into components that can be run in the public cloud and components that should remain in the private data center. Static code analysis is used to automatically establish a partitioning based on low-effort input from the developer. Public and private versions of the application are created and deployed; at runtime, user navigation proceeds seamlessly with requests routed to the public or private data center as appropriate. We present implementations for both Java and PHP web applications, tested on sample applications.","","","CASCON '12"
"Conference Paper","Almeida LD,Baranauskas MC","Accessibility in Rich Internet Applications: People and Research","","2012","","","3–12","Brazilian Computer Society","Porto Alegre, BRA","Proceedings of the 11th Brazilian Symposium on Human Factors in Computing Systems","Cuiaba, Brazil","2012","9788576692621","","","","Accessibility in Rich Internet Applications (RIAs) is still far from reality for most of the Web applications currently available. Some factors that influence this scenario are the novelty of research and products for developing RIAs, and the challenging activity of identifying and involving representatives of RIAs target people. Aiming at clarifying the state-of-the-art of this research topic we conducted a Systematic Literature Review of studies addressing accessibility and awareness of others in RIAs. This paper presents our findings related to the overall contributions of the reviewed studies and analyzes the target people and the methods employed for involving them in the research lifecycle.","people, web 2.0, accessibility, RIA, systematic literature review","","IHC '12"
"Journal Article","Adams A,Jacobs DE,Dolson J,Tico M,Pulli K,Talvala EV,Ajdin B,Vaquero D,Lensch HP,Horowitz M,Park SH,Gelfand N,Baek J,Matusik W,Levoy M","The Frankencamera: An Experimental Platform for Computational Photography","Commun. ACM","2012","55","11","90–98","Association for Computing Machinery","New York, NY, USA","","","2012-11","","0001-0782","https://doi.org/10.1145/2366316.2366339;http://dx.doi.org/10.1145/2366316.2366339","10.1145/2366316.2366339","Although there has been much interest in computational photography within the research and photography communities, progress has been hampered by the lack of a portable, programmable camera with sufficient image quality and computing power. To address this problem, we have designed and implemented an open architecture and application programming interface (API) for such cameras: the Frankencamera. It consists of a base hardware specification, a software stack based on Linux, and an API for C++. Our architecture permits control and synchronization of the sensor and image processing pipeline at the microsecond timescale, as well as the ability to incorporate and synchronize external hardware like lenses and flashes. This paper specifies our architecture and API, and it describes two reference implementations we have built. Using these implementations, we demonstrate several computational photography applications: high dynamic range (HDR) viewfinding and capture, automated acquisition of extended dynamic range panoramas, foveal imaging, and inertial measurement unit (IMU)-based hand shake detection. Our goal is to standardize the architecture and distribute Frankencameras to researchers and students, as a step toward creating a community of photographer-programmers who develop algorithms, applications, and hardware for computational cameras.","","",""
"Conference Paper","Reddy NR,Bahadur S,Sateesh KR","Risk Chain Prediction Metrics for Predicting Fault Proneness in Object Oriented Systems","","2012","","","490–496","Association for Computing Machinery","New York, NY, USA","Proceedings of the Second International Conference on Computational Science, Engineering and Information Technology","Coimbatore UNK, India","2012","9781450313100","","https://doi.org/10.1145/2393216.2393298;http://dx.doi.org/10.1145/2393216.2393298","10.1145/2393216.2393298","The paper presents two atypical risk chain prediction metrics for barometer coupling and accord in software systems. Our aboriginal metric, Ideal Coupling between Object classes (ICBO), is based on the acclaimed CBO coupling metric, while the added metric, Ideal Lack of Cohesion on Methods (ILCOM5), is based on the LCOM5 accord metric. One advantage of the proposed risk chain prediction metrics is that they can be computed in a simpler way as compared to some of the structural metrics. We empirically advised ICBO and ILCOM5 for admiration fault proneness of classes in a ample accessible antecedent arrangement and compared these metrics with a host of absolute structural and risk chain prediction metrics for the aforementioned task.","","","CCSEIT '12"
"Conference Paper","Urma RG,Mycroft A","Programming Language Evolution via Source Code Query Languages","","2012","","","35–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM 4th Annual Workshop on Evaluation and Usability of Programming Languages and Tools","Tucson, Arizona, USA","2012","9781450316316","","https://doi.org/10.1145/2414721.2414728;http://dx.doi.org/10.1145/2414721.2414728","10.1145/2414721.2414728","Programming languages evolve just like programs. Language features are added and removed, for example when programs using them are shown to be error-prone. When language features are modified, deprecated, removed or even deemed unsuitable for the project at hand, it is necessary to analyse programs to identify occurrences to refactor.Source code query languages in principle provide a good way to perform this analysis by exploring codebases. Such languages are often used to identify code to refactor, bugs to fix or simply to understand a system better.This paper evaluates seven Java source code query languages: Java Tools Language, Browse-By-Query, SOUL, JQuery, .QL, Jackpot and PMD as to their power at expressing queries required by several use cases (such as code idioms to be refactored).","query languages, program analysis, source code","","PLATEAU '12"
"Conference Paper","Cain HW,Lipasti MH","Edge Chasing Delayed Consistency: Pushing the Limits of Weak Memory Models","","2012","","","15–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2012 ACM Workshop on Relaxing Synchronization for Multicore and Manycore Scalability","Tucson, Arizona, USA","2012","9781450316323","","https://doi.org/10.1145/2414729.2414733;http://dx.doi.org/10.1145/2414729.2414733","10.1145/2414729.2414733","In shared memory multiprocessors utilizing invalidation-based coherence protocols, cache misses caused by inter-processor communication are a dominant source of processor stall cycles for many applications. We explore a novel coherence protocol implementation called edge-chasing delayed consistency (ECDC) that mitigates some of the performance degradation caused by this class of misses. Edge-chasing delayed consistency allows a processor to non-speculatively continue reading a cache line after receiving an invalidation from another core, without changing the consistency model offered to programmers. While the idea of using stale data for as long as possible is enticing, our study shows that the benefits of such delay are small, and that the majority of these delayed invalidation benefits come from mitigating the false sharing problem, rather than any tolerance of races or an application's ability to consume stale data in a productive manner.","cache coherence, relaxed memory consistency","","RACES '12"
"Conference Paper","Mishne A,Shoham S,Yahav E","Typestate-Based Semantic Code Search over Partial Programs","","2012","","","997–1016","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Tucson, Arizona, USA","2012","9781450315616","","https://doi.org/10.1145/2384616.2384689;http://dx.doi.org/10.1145/2384616.2384689","10.1145/2384616.2384689","We present a novel code search approach for answering queries focused on API-usage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semantic-based techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (e.g. tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures a full(er) usage scenario.To answer a search query, we define a notion of relaxed inclusion matching a query against temporal specifications and their corresponding code snippets.We have implemented our approach in a tool called PRIME and applied it to search for API usage of several challenging APIs. PRIME was able to analyze and consolidate thousands of snippets per tested API, and our results indicate that the combination of a relatively precise analysis and consolidation allowed PRIME to answer challenging queries effectively.","specification mining, ranking code samples, typestate, code search engine, static analysis","","OOPSLA '12"
"Journal Article","Mishne A,Shoham S,Yahav E","Typestate-Based Semantic Code Search over Partial Programs","SIGPLAN Not.","2012","47","10","997–1016","Association for Computing Machinery","New York, NY, USA","","","2012-10","","0362-1340","https://doi.org/10.1145/2398857.2384689;http://dx.doi.org/10.1145/2398857.2384689","10.1145/2398857.2384689","We present a novel code search approach for answering queries focused on API-usage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semantic-based techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (e.g. tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures a full(er) usage scenario.To answer a search query, we define a notion of relaxed inclusion matching a query against temporal specifications and their corresponding code snippets.We have implemented our approach in a tool called PRIME and applied it to search for API usage of several challenging APIs. PRIME was able to analyze and consolidate thousands of snippets per tested API, and our results indicate that the combination of a relatively precise analysis and consolidation allowed PRIME to answer challenging queries effectively.","code search engine, ranking code samples, static analysis, specification mining, typestate","",""
"Conference Paper","Barišić A,Monteiro P,Amaral V,Goulão M,Monteiro M","Patterns for Evaluating Usability of Domain-Specific Languages","","2012","","","","The Hillside Group","USA","Proceedings of the 19th Conference on Pattern Languages of Programs","Tucson, Arizona","2012","9781450327862","","","","For years the development of software artifacts was the sole domain of developers and project managers. However, experience has taught us that the users play a very important role in software development and construction. The inclusion of the Domain Experts directly in the development cycle is a very important characteristic of Domain-Specific Languages, as they have often an important role in making and constraining the domain of the language.DSLs are credited with increased productivity and ease of use, but this fact is hardly ever proven. Moreover, Usability tests are frequently only performed at the final stages of the project when changes have a significant impact on the budget. To help prevent this, in this paper we present a pattern language for evaluating the usability of DSLs. These patterns can help show how to use an iterative usability validation development strategy to produce DSLs that can achieve a high degree of Usability.","usability evaluation of domain-specific language, pattern language, domain-specific language, usability evaluation","","PLoP '12"
"Conference Paper","Wirfs-Brock R,Yoder JW","Patterns for Sustaining Architectures","","2012","","","","The Hillside Group","USA","Proceedings of the 19th Conference on Pattern Languages of Programs","Tucson, Arizona","2012","9781450327862","","","","Unless ongoing attention is paid to architecture, as complex systems evolve to meet new requirements they can become unwieldy to maintain and devolve into poorly architected Big Balls of Mud. This paper presents two patterns for sustaining software architectures in the face of increasing complexity: Paving over the Wagon Trail and Wiping Your Feet at the Door. We will also recast Paving over the Wagon Trail into a third pattern that exacerbates the mud, Paving over the Cowpath, and explain why this is not an anti-pattern, but instead a darker form of well-intentioned tinkering. These patterns help with both mud prevention and sustaining complex, evolving architectures.","DSLs, architecture, sustainable architecture, big ball of mud, patterns","","PLoP '12"
"Conference Paper","Castanos J,Edelsohn D,Ishizaki K,Nagpurkar P,Nakatani T,Ogasawara T,Wu P","On the Benefits and Pitfalls of Extending a Statically Typed Language JIT Compiler for Dynamic Scripting Languages","","2012","","","195–212","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Tucson, Arizona, USA","2012","9781450315616","","https://doi.org/10.1145/2384616.2384631;http://dx.doi.org/10.1145/2384616.2384631","10.1145/2384616.2384631","Whenever the need to compile a new dynamically typed language arises, an appealing option is to repurpose an existing statically typed language Just-In-Time (JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compilers (RJIT compilers), however, have not yet delivered the hoped-for performance boosts. The performance of JVM languages, for instance, often lags behind standard interpreter implementations. Even more customized solutions that extend the internals of a JIT compiler for the target language compete poorly with those designed specifically for dynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a state-of-the-art, RJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow and Jython), but still shows a noticeable performance gap compared to PyPy, today's best performing Python JIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler as well as limitations of our current implementation. More importantly, this work offers the first in-depth look at benefits and limitations of the repurposed JIT compiler approach. We believe the most common pitfall of existing RJIT compilers is not focusing sufficiently on specialization, an abundant optimization opportunity unique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by applying traditional optimizations.","python, scripting languages","","OOPSLA '12"
"Journal Article","Castanos J,Edelsohn D,Ishizaki K,Nagpurkar P,Nakatani T,Ogasawara T,Wu P","On the Benefits and Pitfalls of Extending a Statically Typed Language JIT Compiler for Dynamic Scripting Languages","SIGPLAN Not.","2012","47","10","195–212","Association for Computing Machinery","New York, NY, USA","","","2012-10","","0362-1340","https://doi.org/10.1145/2398857.2384631;http://dx.doi.org/10.1145/2398857.2384631","10.1145/2398857.2384631","Whenever the need to compile a new dynamically typed language arises, an appealing option is to repurpose an existing statically typed language Just-In-Time (JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compilers (RJIT compilers), however, have not yet delivered the hoped-for performance boosts. The performance of JVM languages, for instance, often lags behind standard interpreter implementations. Even more customized solutions that extend the internals of a JIT compiler for the target language compete poorly with those designed specifically for dynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a state-of-the-art, RJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow and Jython), but still shows a noticeable performance gap compared to PyPy, today's best performing Python JIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler as well as limitations of our current implementation. More importantly, this work offers the first in-depth look at benefits and limitations of the repurposed JIT compiler approach. We believe the most common pitfall of existing RJIT compilers is not focusing sufficiently on specialization, an abundant optimization opportunity unique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by applying traditional optimizations.","scripting languages, python","",""
"Conference Paper","Tardieu O,Nystrom N,Peshansky I,Saraswat V","Constrained Kinds","","2012","","","811–830","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Tucson, Arizona, USA","2012","9781450315616","","https://doi.org/10.1145/2384616.2384675;http://dx.doi.org/10.1145/2384616.2384675","10.1145/2384616.2384675","Modern object-oriented languages such as X10 require a rich framework for types capable of expressing both value-dependency and genericity, and supporting pluggable, domain-specific extensions. In earlier work, we presented a framework for constrained types in object-oriented languages, parametrized by an underlying constraint system. Types are viewed as formulas Cc where C is the name of a class or an interface and c is a constraint on the immutable instance state (the properties) of C. Constraint systems are a very expressive framework for partial information. Many (value-)dependent type systems for object-oriented languages can be viewed as constrained types.This paper extends the constrained types approach to handle type-dependency (""genericity""). The key idea is to introduce constrained kinds: in the same way that constraints on values can be used to define constrained types, constraints on types can define constrained kinds.We develop a core programming language with constrained kinds. Generic types are supported by introducing type variables---literally, variables with ""type"" Type---and permitting programs to impose subtyping and equality constraints on such variables. We formalize the type-checking rules and establish soundness.While the language now intertwines constraints on types and values, its type system remains parametric in the choice of the value constraint system (language and solver). We demonstrate that constrained kinds are expressive and practical and sketch possible extensions with a discussion of the design and implementation of X10.","generics, X10, types, constraints","","OOPSLA '12"
"Journal Article","Tardieu O,Nystrom N,Peshansky I,Saraswat V","Constrained Kinds","SIGPLAN Not.","2012","47","10","811–830","Association for Computing Machinery","New York, NY, USA","","","2012-10","","0362-1340","https://doi.org/10.1145/2398857.2384675;http://dx.doi.org/10.1145/2398857.2384675","10.1145/2398857.2384675","Modern object-oriented languages such as X10 require a rich framework for types capable of expressing both value-dependency and genericity, and supporting pluggable, domain-specific extensions. In earlier work, we presented a framework for constrained types in object-oriented languages, parametrized by an underlying constraint system. Types are viewed as formulas Cc where C is the name of a class or an interface and c is a constraint on the immutable instance state (the properties) of C. Constraint systems are a very expressive framework for partial information. Many (value-)dependent type systems for object-oriented languages can be viewed as constrained types.This paper extends the constrained types approach to handle type-dependency (""genericity""). The key idea is to introduce constrained kinds: in the same way that constraints on values can be used to define constrained types, constraints on types can define constrained kinds.We develop a core programming language with constrained kinds. Generic types are supported by introducing type variables---literally, variables with ""type"" Type---and permitting programs to impose subtyping and equality constraints on such variables. We formalize the type-checking rules and establish soundness.While the language now intertwines constraints on types and values, its type system remains parametric in the choice of the value constraint system (language and solver). We demonstrate that constrained kinds are expressive and practical and sketch possible extensions with a discussion of the design and implementation of X10.","generics, constraints, types, X10","",""
"Conference Paper","Wartell R,Mohan V,Hamlen KW,Lin Z","Binary Stirring: Self-Randomizing Instruction Addresses of Legacy X86 Binary Code","","2012","","","157–168","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2012 ACM Conference on Computer and Communications Security","Raleigh, North Carolina, USA","2012","9781450316514","","https://doi.org/10.1145/2382196.2382216;http://dx.doi.org/10.1145/2382196.2382216","10.1145/2382196.2382216","Unlike library code, whose instruction addresses can be randomized by address space layout randomization (ASLR), application binary code often has static instruction addresses. Attackers can exploit this limitation to craft robust shell codes for such applications, as demonstrated by a recent attack that reuses instruction gadgets from the static binary code of victim applications.This paper introduces binary stirring, a new technique that imbues x86 native code with the ability to self-randomize its instruction addresses each time it is launched. The input to STIR is only the application binary code without any source code, debug symbols, or relocation information. The output is a new binary whose basic block addresses are dynamically determined at load-time. Therefore, even if an attacker can find code gadgets in one instance of the binary, the instruction addresses in other instances are unpredictable. An array of binary transformation techniques enable STIR to transparently protect large, realistic applications that cannot be perfectly disassembled due to computed jumps, code-data interleaving, OS callbacks, dynamic linking and a variety of other difficult binary features. Evaluation of STIR for both Windows and Linux platforms shows that stirring introduces about 1.6% overhead on average to application runtimes.","return-oriented programming, obfuscation, software security, randomization","","CCS '12"
"Conference Paper","Thober M,Pendergrass JA,Jurik AD","JMF: Java Measurement Framework: Language-Supported Runtime Integrity Measurement","","2012","","","21–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventh ACM Workshop on Scalable Trusted Computing","Raleigh, North Carolina, USA","2012","9781450316620","","https://doi.org/10.1145/2382536.2382542;http://dx.doi.org/10.1145/2382536.2382542","10.1145/2382536.2382542","Runtime integrity measurement systems provide the capability to observe the runtime state of a process and to determine whether or not it is acceptable. Existing software systems tend to forgo integrity checks altogether or to enlist static mechanisms (e.g., assertions) to detect unacceptable process states at runtime. A large and growing base of malicious software necessitates more sophisticated handling of threats to process integrity.In this paper, we describe an approach to runtime integrity measurement we call the Java Measurement Framework (JMF) that presents a new way to define and check runtime integrity policies. We define a policy language based on Java that provides an accessible way to write integrity policies and we describe a periodic, dynamic measurer that obtains snapshots of process state, which are evaluated with respect to a policy by an appraiser. With full process state available to the appraiser, policies can express rich relationships between multiple objects, thereby detecting abnormalities in an application's data structures. Our framework may be used to detect a powerful adversary who has the capability to modify both the runtime bytecode and data structures of Java applications. We show that our prototype implementation in Java has acceptable overhead and that it can be used to detect runtime integrity violations in several real Java programs.","runtime integrity measurement, java, integrity policy, attestation","","STC '12"
"Conference Paper","Rajanen M,Iivari N,Keskitalo E","Introducing Usability Activities into Open Source Software Development Projects: A Participative Approach","","2012","","","683–692","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Nordic Conference on Human-Computer Interaction: Making Sense Through Design","Copenhagen, Denmark","2012","9781450314824","","https://doi.org/10.1145/2399016.2399120;http://dx.doi.org/10.1145/2399016.2399120","10.1145/2399016.2399120","Usability is an important quality characteristic of software products and information systems. Different approaches for introducing usability activities into open source software (OSS) development have not yet been fully evaluated. This paper experiments with the introduction of usability activities into OSS development through a participative approach. An empirical case study was carried out in a game development OSS project. The results of this study suggest that it is beneficial to introduce usability activities into OSS development through the participative approach. In the participative approach the usability experts become recognized part of the development community through adapting their ways of work into the culture of the OSS project and submitting code patches. This participative approach had a clear impact in the case project as seen in changes in the user interface and in improved usability. The challenge of adapting usability and OSS development philosophies and practices should, however, be researched further.","participative approach, open source software, usability","","NordiCHI '12"
"Conference Paper","Taentzer G,Arendt T,Ermel C,Heckel R","Towards Refactoring of Rule-Based, in-Place Model Transformation Systems","","2012","","","41–46","Association for Computing Machinery","New York, NY, USA","Proceedings of the First Workshop on the Analysis of Model Transformations","Innsbruck, Austria","2012","9781450318037","","https://doi.org/10.1145/2432497.2432506;http://dx.doi.org/10.1145/2432497.2432506","10.1145/2432497.2432506","The more model transformations are applied in various application domains, the more questions about their quality arise. In this paper, we present a first approach towards improving the quality of endogenous in-place model transformation systems. This kind of model transformations is typically rule-based and well suited to perform model simulations and optimizations. After discussing suitable quality aims for this kind of model transformation systems and how they can be detected by smells, a first selection of refactorings is presented showing a variety of potential improvements of model transformation systems. Each refactoring is presented in a systematic way including an explanation how the quality is improved, a description of its pre- and post-conditions, a possible refactoring strategy, and an example. All discussed refactorings are implemented in Henshin, a model transformation engine based on graph transformation concepts, using Henshin in combination with the Eclipse plug-in EMF Refactor, a refactoring plug-in for defining and applying refactorings of EMF models.","Henshin, EMF model transformation, model transformation, higher order transformation, refactoring, EMF, mode-driven development","","AMT '12"
"Conference Paper","Selim GM,Cordy JR,Dingel J","Model Transformation Testing: The State of the Art","","2012","","","21–26","Association for Computing Machinery","New York, NY, USA","Proceedings of the First Workshop on the Analysis of Model Transformations","Innsbruck, Austria","2012","9781450318037","","https://doi.org/10.1145/2432497.2432502;http://dx.doi.org/10.1145/2432497.2432502","10.1145/2432497.2432502","Model Driven Development (MDD) is a software engineering approach in which models constitute the basic units of software development. A key part of MDD is the notion of automated model transformation, in which models are stepwise refined into more detailed models, and eventually into code. The correctness of transformations is essential to the success of MDD, and while much research has concentrated on formal verification, testing remains the most efficient method of validation. Transformation testing is however different from testing code, and presents new challenges. In this paper, we survey the model transformation testing phases and the approaches proposed in the literature for each phase.","test case generation, model transformation testing, contracts, mutation analysis, model driven development","","AMT '12"
"Conference Paper","Chénard G,Khriss I,Salah A","Towards the Automatic Discovery of Platform Transformation Templates of Legacy Object-Oriented Systems","","2012","","","51–56","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Models and Evolution","Innsbruck, Austria","2012","9781450317986","","https://doi.org/10.1145/2523599.2523609;http://dx.doi.org/10.1145/2523599.2523609","10.1145/2523599.2523609","Software modernization is needed to perform the evolution of a system when conventional practices can no longer achieve the desired evolution goal. In their initiative called architecture-driven modernization (ADM), the Object Management Group proposes to use MDA to perform this modernization. However, ADM needs new tools and techniques to migrate systems developed on a non-model-driven environment to a model-driven environment. One challenge to enable this migration is the discovery of a platform description model (PDM) from the implementation of a system. In this paper, we propose an approach to discover a view of the PDM from an object-oriented system source code. This view is given as a set of transformation templates parameterizing the source code of the system's implementation platform and expressed in the QVT language. The approach uses different analysis techniques and was validated on several systems written in Java and gives good results for a number of them.","code analysis, software comprehension, reverse engineering, software evolution, software modernization, model-driven engineering","","ME '12"
"Conference Paper","Störrle H","Making Sense of UML Class Model Changes by Textual Difference Presentation","","2012","","","3–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Models and Evolution","Innsbruck, Austria","2012","9781450317986","","https://doi.org/10.1145/2523599.2523601;http://dx.doi.org/10.1145/2523599.2523601","10.1145/2523599.2523601","Understanding the difference between two models, such as different versions of a design, can be difficult. It is a commonly held belief that the best way of presenting a model difference is by using graph or tree-based visualizations. We disagree and present an alternative approach where sets of low-level model differences are abstracted into high-level model differences that lend themselves to being presented textually. The results of preliminary user studies support our claim.","","","ME '12"
"Conference Paper","Bagheri H,Sullivan K","Pol: Specification-Driven Synthesis of Architectural Code Frameworks for Platform-Based Applications","","2012","","","93–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Conference on Generative Programming and Component Engineering","Dresden, Germany","2012","9781450311298","","https://doi.org/10.1145/2371401.2371416;http://dx.doi.org/10.1145/2371401.2371416","10.1145/2371401.2371416","Developing applications that use complex platforms for functionalities such as authentication and messaging is hard. Model-driven engineering promises to help, but transformation systems are themselves hard to produce. We contribute a new approach using constraint-based synthesis of partial code frameworks that developers complete by hand without the need for hand-coded transformation systems. Rather, synthesis is driven by formal, partial specifications of target platforms and application architectures, and by design (code) fragments encoding application-specific platform us-age patterns. We present results of an early evaluation using the case study method to test hypotheses of feasibility and potential industrial utility, using a laboratory model of a nationwide health information network as a subject system.","architecture, alloy, synthesis, platform, model-driven","","GPCE '12"
"Journal Article","Bagheri H,Sullivan K","Pol: Specification-Driven Synthesis of Architectural Code Frameworks for Platform-Based Applications","SIGPLAN Not.","2012","48","3","93–102","Association for Computing Machinery","New York, NY, USA","","","2012-09","","0362-1340","https://doi.org/10.1145/2480361.2371416;http://dx.doi.org/10.1145/2480361.2371416","10.1145/2480361.2371416","Developing applications that use complex platforms for functionalities such as authentication and messaging is hard. Model-driven engineering promises to help, but transformation systems are themselves hard to produce. We contribute a new approach using constraint-based synthesis of partial code frameworks that developers complete by hand without the need for hand-coded transformation systems. Rather, synthesis is driven by formal, partial specifications of target platforms and application architectures, and by design (code) fragments encoding application-specific platform us-age patterns. We present results of an early evaluation using the case study method to test hypotheses of feasibility and potential industrial utility, using a laboratory model of a nationwide health information network as a subject system.","architecture, model-driven, alloy, platform, synthesis","",""
"Conference Paper","Hulette GC,Sottile M,Malony AD","Composing Typemaps in Twig","","2012","","","41–49","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Conference on Generative Programming and Component Engineering","Dresden, Germany","2012","9781450311298","","https://doi.org/10.1145/2371401.2371408;http://dx.doi.org/10.1145/2371401.2371408","10.1145/2371401.2371408","Twig is a language for writing typemaps, programs which transform the type of a value while preserving its underlying meaning. Typemaps are typically used by tools that generate code, such as multi-language wrapper generators, to automatically convert types as needed. Twig builds on existing typemap tools in a few key ways. Twig's typemaps are composable so that complex transformations may be built from simpler ones. In addition, Twig incorporates an abstract, formal model of code generation, allowing it to output code for different target languages. We describe Twig's formal semantics and show how the language allows us to concisely express typemaps. Then, we demonstrate Twig's utility by building an example typemap.","foreign function interface, type mapping","","GPCE '12"
"Journal Article","Hulette GC,Sottile M,Malony AD","Composing Typemaps in Twig","SIGPLAN Not.","2012","48","3","41–49","Association for Computing Machinery","New York, NY, USA","","","2012-09","","0362-1340","https://doi.org/10.1145/2480361.2371408;http://dx.doi.org/10.1145/2480361.2371408","10.1145/2480361.2371408","Twig is a language for writing typemaps, programs which transform the type of a value while preserving its underlying meaning. Typemaps are typically used by tools that generate code, such as multi-language wrapper generators, to automatically convert types as needed. Twig builds on existing typemap tools in a few key ways. Twig's typemaps are composable so that complex transformations may be built from simpler ones. In addition, Twig incorporates an abstract, formal model of code generation, allowing it to output code for different target languages. We describe Twig's formal semantics and show how the language allows us to concisely express typemaps. Then, we demonstrate Twig's utility by building an example typemap.","type mapping, foreign function interface","",""
"Conference Paper","Passos L,Czarnecki K,Wąsowski A","Towards a Catalog of Variability Evolution Patterns: The Linux Kernel Case","","2012","","","62–69","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Feature-Oriented Software Development","Dresden, Germany","2012","9781450313094","","https://doi.org/10.1145/2377816.2377825;http://dx.doi.org/10.1145/2377816.2377825","10.1145/2377816.2377825","A complete understanding of evolution of variability requires analysis over all project spaces that contain it: source code, build system and the variability model. Aiming at better understanding of how complex variant-rich software evolve, we set to study one, the Linux kernel, in detail. We qualitatively analyze a number of evolution steps in the kernel history and present our findings as a preliminary sample of a catalog of evolution patterns. Our patterns focus on how the variability evolves when features are removed from the variability model, but are kept as part of the software. The identified patterns relate changes to the variability model, the build system, and implementation code. Despite preliminary, they already indicate evolution steps that have not been captured by prior studies, both empirical and theoretical.","Linux, software product lines, patterns, variability, evolution","","FOSD '12"
"Conference Paper","Guerra E,Kinoshita B","Patterns for Introducing a Superclass for Test Classes","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th Latin-American Conference on Pattern Languages of Programming","Natal, Rio Grande do Norte, Brazil","2012","9781450327879","","https://doi.org/10.1145/2591028.2600815;http://dx.doi.org/10.1145/2591028.2600815","10.1145/2591028.2600815","A test class is an isolated piece of software that is responsible for verifying the expected behavior of a class, a component or an application. Because test classes can be developed without consideration of good design, sometimes individual test classes might include redundant code or become large and unwieldy. This paper presents two patterns that propose the introduction of a common superclass for test classes in order to reduce code bulk, reuse more test code and achieve a better test organization.","test code design, patterns, test automation, software design","","SugarLoafPLoP '12"
"Conference Paper","Tosi D,Lavazza L,Morasca S,Taibi D","On the Definition of Dynamic Software Measures","","2012","","","39–48","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","Lund, Sweden","2012","9781450310567","","https://doi.org/10.1145/2372251.2372259;http://dx.doi.org/10.1145/2372251.2372259","10.1145/2372251.2372259","The quantification of several software attributes (e.g., size, complexity, cohesion, coupling) is usually carried out in a static fashion, and several hundreds of measures have been defined to this end. However, static measurement may only be an approximation for the measurement of these attributes during software use. The paper proposes a theoretical framework based on Axiomatic Approaches for the definition of sensible dynamic software measures that can dynamically capture these attributes. Dynamic measures based on this framework are defined for dynamically quantifying size and coupling. In this paper, we also compare dynamic measures of size and coupling against well-known static measures by correlating them with fault-pronenesses of four case studies.","dynamic measures, code coverage, dynamic size, dynamic coupling","","ESEM '12"
"Conference Paper","Yang Y,Xiang P,Mantor M,Rubin N,Zhou H","Shared Memory Multiplexing: A Novel Way to Improve GPGPU Throughput","","2012","","","283–292","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques","Minneapolis, Minnesota, USA","2012","9781450311823","","https://doi.org/10.1145/2370816.2370858;http://dx.doi.org/10.1145/2370816.2370858","10.1145/2370816.2370858","On-chip shared memory (a.k.a. local data share) is a critical resource to many GPGPU applications. In current GPUs, the shared memory is allocated when a thread block (also called a workgroup) is dispatched to a streaming multiprocessor (SM) and is released when the thread block is completed. As a result, the limited capacity of shared memory becomes a bottleneck for a GPU to host a high number of thread blocks, limiting the otherwise available thread-level parallelism (TLP). In this paper, we propose software and/or hardware approaches to multiplex the shared memory among multiple thread blocks.Our proposed approaches are based on our observation that the current shared memory management reserves shared memory too conservatively, for the entire lifetime of a thread block. If the shared memory is allocated only when it is actually used and freed immediately after, more thread blocks can be hosted in an SM without increasing the shared memory capacity. We propose three software approaches to enable shared memory multiplexing and implement them using a source-to-source compiler. The experimental results show that our proposed software approaches effectively improve the throughput of many GPGPU applications on both NVIDIA GTX285 and GTX480 GPUs (an average of 1.44X on GTX285, 1.70X on GTX480 with 16kB shared memory, and 1.26X on GTX480 with 48kB shared memory). We also propose hardware support for shared memory multiplexing, which incurs minor hardware changes to existing hardware and enables significant performance improvements (an average of 1.53X) to be achieved with very little change in GPGPU code.","shared memory, gpgpu, dynamic management","","PACT '12"
"Conference Paper","Oancea CE,Andreetta C,Berthold J,Frisch A,Henglein F","Financial Software on GPUs: Between Haskell and Fortran","","2012","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-Performance Computing","Copenhagen, Denmark","2012","9781450315777","","https://doi.org/10.1145/2364474.2364484;http://dx.doi.org/10.1145/2364474.2364484","10.1145/2364474.2364484","This paper presents a real-world pricing kernel for financial derivatives and evaluates the language and compiler tool chain that would allow expressive, hardware-neutral algorithm implementation and efficient execution on graphics-processing units (GPU). The language issues refer to preserving algorithmic invariants, e.g., inherent parallelism made explicit by map-reduce-scan functional combinators. Efficient execution is achieved by manually; applying a series of generally-applicable compiler transformations that allows the generated-OpenCL code to yield speedups as high as 70x and 540x on a commodity mobile and desktop GPU, respectively.Apart from the concrete speed-ups attained, our contributions are twofold: First, from a language perspective;, we illustrate that even state-of-the-art auto-parallelization techniques are incapable of discovering all the requisite data parallelism when rendering the functional code in Fortran-style imperative array processing form. Second, from a performance perspective;, we study which compiler transformations are necessary to map the high-level functional code to hand-optimized OpenCL code for GPU execution. We discover a rich optimization space with nontrivial trade-offs and cost models. Memory reuse in map-reduce patterns, strength reduction, branch divergence optimization, and memory access coalescing, exhibit significant impact individually. When combined, they enable essentially full utilization of all GPU cores.Functional programming has played a crucial double role in our case study: Capturing the naturally data-parallel structure of the pricing algorithm in a transparent, reusable and entirely hardware-independent fashion; and supporting the correctness of the subsequent compiler transformations to a hardware-oriented target language by a rich class of universally valid equational properties. Given the observed difficulty of automatically parallelizing imperative sequential code and the inherent labor of porting hardware-oriented and -optimized programs, our case study suggests that functional programming technology can facilitate high-level; expression of leading-edge performant portable; high-performance systems for massively parallel hardware architectures.","autoparallelization, memory coalescing, tiling, functional language, strength reduction","","FHPC '12"
"Conference Paper","Frisby N,Gill A,Alexander P","A Pattern for Almost Homomorphic Functions","","2012","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM SIGPLAN Workshop on Generic Programming","Copenhagen, Denmark","2012","9781450315760","","https://doi.org/10.1145/2364394.2364396;http://dx.doi.org/10.1145/2364394.2364396","10.1145/2364394.2364396","Modern type systems present the programmer with a trade-off between correctness and code complexity--more precise, or exact, types that allow only legal values prevent runtime errors while less precise types enable more reuse. Unfortunately, the software engineering benefits of reuse and avoiding duplicate code currently outweigh assurance gains of exact types. We factor out a pattern common in conversions that result from using exact types as a reusable function, extending existing generic programming techniques to avoid code duplication and enable reuse.","generic programming, type invariants, data types, type-level programming","","WGP '12"
"Conference Paper","Dagand PE,McBride C","Transporting Functions across Ornaments","","2012","","","103–114","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming","Copenhagen, Denmark","2012","9781450310543","","https://doi.org/10.1145/2364527.2364544;http://dx.doi.org/10.1145/2364527.2364544","10.1145/2364527.2364544","Programming with dependent types is a blessing and a curse. It is a blessing to be able to bake invariants into the definition of datatypes: we can finally write correct-by-construction software. However, this extreme accuracy is also a curse: a datatype is the combination of a structuring medium together with a special purpose logic. These domain-specific logics hamper any effort of code reuse among similarly structured data. In this paper, we exorcise our datatypes by adapting the notion of ornament to our universe of inductive families. We then show how code reuse can be achieved by ornamenting functions. Using these functional ornaments, we capture the relationship between functions such as the addition of natural numbers and the concatenation of lists. With this knowledge, we demonstrate how the implementation of the former informs the implementation of the latter: the user can ask the definition of addition to be lifted to lists and she will only be asked the details necessary to carry on adding lists rather than numbers. Our presentation is formalised in a type theory with a universe of datatypes and all our constructions have been implemented as generic programs, requiring no extension to the type theory.","ornament, dependent types, datatype","","ICFP '12"
"Journal Article","Dagand PE,McBride C","Transporting Functions across Ornaments","SIGPLAN Not.","2012","47","9","103–114","Association for Computing Machinery","New York, NY, USA","","","2012-09","","0362-1340","https://doi.org/10.1145/2398856.2364544;http://dx.doi.org/10.1145/2398856.2364544","10.1145/2398856.2364544","Programming with dependent types is a blessing and a curse. It is a blessing to be able to bake invariants into the definition of datatypes: we can finally write correct-by-construction software. However, this extreme accuracy is also a curse: a datatype is the combination of a structuring medium together with a special purpose logic. These domain-specific logics hamper any effort of code reuse among similarly structured data. In this paper, we exorcise our datatypes by adapting the notion of ornament to our universe of inductive families. We then show how code reuse can be achieved by ornamenting functions. Using these functional ornaments, we capture the relationship between functions such as the addition of natural numbers and the concatenation of lists. With this knowledge, we demonstrate how the implementation of the former informs the implementation of the latter: the user can ask the definition of addition to be lifted to lists and she will only be asked the details necessary to carry on adding lists rather than numbers. Our presentation is formalised in a type theory with a universe of datatypes and all our constructions have been implemented as generic programs, requiring no extension to the type theory.","datatype, dependent types, ornament","",""
"Conference Paper","Chakraborty S,Sarkar M,Mukherjee N","Implementation of Execution History in Non-Relational Databases for Feedback-Guided Job Modeling","","2012","","","476–482","Association for Computing Machinery","New York, NY, USA","Proceedings of the CUBE International Information Technology Conference","Pune, India","2012","9781450311854","","https://doi.org/10.1145/2381716.2381806;http://dx.doi.org/10.1145/2381716.2381806","10.1145/2381716.2381806","A feedback-guided Resource Requirement Prediction technique has been described in [2]. An Execution History is built and maintained for all the jobs which are executed in a large distributed system like Grid. When a new job arrives, its clones are sought for in the Execution History and if clones are found, relevant performance information are retrieved and used for estimating resource requirements. In this paper, we focus on the implementation details of Execution History. Instead of using relational databases, here we have used NoSQL database, MongoDB. The reasons for using MongoDB are discussed. Techniques for storing and retrieving data from the Execution History are also described. Overheads for such operations are measured and presented in the result part of this paper.","metrics, execution history, MongoDB, clone detection","","CUBE '12"
"Conference Paper","Martinez J,Thurimella AK","Collaboration and Source Code Driven Bottom-up Product Line Engineering","","2012","","","196–200","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Software Product Line Conference - Volume 2","Salvador, Brazil","2012","9781450310956","","https://doi.org/10.1145/2364412.2364445;http://dx.doi.org/10.1145/2364412.2364445","10.1145/2364412.2364445","Companies that develop similar software systems often transition from single-system development to software product line development. In this transition, reusable assets are identified and incrementally created over a period of time. Bottom-up Software Product Line Engineering approaches aid stakeholders to identify variability from the legacy artifacts. One of these artifacts is the legacy source code. In this paper, we contribute the Collaboration and Source Code Driven Bottom-up approach, with two main enhancements. We apply clone detection and architecture reengineering techniques for identifying variability from the legacy artifacts. These techniques which have been traditionally used for maintaining software are now used for identifying variability and analyze code coupling and cohesion from the legacy code. Our second enhancement is improving stakeholder collaboration by guiding the domain experts in order to decide on variability. In particular, we apply Questions, Options and Criteria technique for capturing rationale and supporting collaboration.","software product line engineering, knowledge management, rationale, architecture reengineering, clone detection, variability modeling","","SPLC '12"
"Conference Paper","Nunes C,Garcia A,Lucena C,Lee J","History-Sensitive Heuristics for Recovery of Features in Code of Evolving Program Families","","2012","","","136–145","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Software Product Line Conference - Volume 1","Salvador, Brazil","2012","9781450310949","","https://doi.org/10.1145/2362536.2362556;http://dx.doi.org/10.1145/2362536.2362556","10.1145/2362536.2362556","A program family might degenerate due to unplanned changes in its implementation, thus hindering the maintenance of family members. This degeneration is often induced by feature code that is changed individually in each member without considering other family members. Hence, as a program family evolves over time, it might no longer be possible to distinguish between common and variable features. One of the imminent activities to address this problem is the history-sensitive recovery of program family's features in the code. This recovery process encompasses the analysis of the evolution history of each family member in order to classify the implementation elements according to their variability nature. In this context, this paper proposes history-sensitive heuristics for the recovery of features in code of degenerate program families. Once the analysis of the family history is carried out, the feature elements are structured as Java project packages; they are intended to separate those elements in terms of their variability degree. The proposed heuristics are supported by a prototype tool called RecFeat. We evaluated the accuracy of the heuristics in the context of 33 versions of 2 industry program families. They presented encouraging results regarding recall measures that ranged from 85% to 100%; whereas the precision measures ranged from 71% to 99%.","software evolution, program families, heuristics, feature recovery","","SPLC '12"
"Conference Paper","de Oliveira TH,Becker M,Nakagawa EY","Supporting the Analysis of Bug Prevalence in Software Product Lines with Product Genealogy","","2012","","","181–185","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Software Product Line Conference - Volume 1","Salvador, Brazil","2012","9781450310949","","https://doi.org/10.1145/2362536.2362561;http://dx.doi.org/10.1145/2362536.2362561","10.1145/2362536.2362561","The term bug prevalence is derived from the medical world vocabulary and applied to Software Product Line (SPL), meaning all products that are affected by one particular bug. In single systems development, this concept is not relevant since a bug is either present or not. However, when it comes to SPL, analyzing the bug prevalence of a certain bug is still a challenge and a highly relevant topic, since the same bug may be present in several products. To support this analysis, the main contribution of this paper is the Product Genealogy approach. A core concept in our approach is the Product Genealogy Tree, in which the hierarchy of products in the SPL is represented, reflecting how each product evolved or was derived from another or from the core assets. In this context, the benefit of such a tree is the rapid visualization of the product's structure in the SPL, providing input on which products are to be examined initially. Besides that, in this paper we introduce a novel analogy between the medical genetics world and SPL in order to better explain the principles of our approach.","change impact, bug prevalence, software product line, product genealogy","","SPLC '12"
"Conference Paper","Rubin J,Kirshin A,Botterweck G,Chechik M","Managing Forked Product Variants","","2012","","","156–160","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th International Software Product Line Conference - Volume 1","Salvador, Brazil","2012","9781450310949","","https://doi.org/10.1145/2362536.2362558;http://dx.doi.org/10.1145/2362536.2362558","10.1145/2362536.2362558","We consider the problem of supporting effective code reuse as part of Software Product Line Engineering. Our approach is based on code forking -- a practice commonly used in industry where new products are created by cloning the existing ones. We propose to maintain meta-information allowing organization to reason about the developed product line in terms of features rather than incremental code changes made in different forks and to detect inconsistencies in implementations of these features. In addition, we propose to detect and maintain semantic, implementation-level require relationships between features, supporting the developers when they copy features from different branches or delete features in their own branch, thus facilitating reuse of features between products. Our approach aims at mitigating the disadvantages of the forking mechanism while leveraging its advantages. We illustrate the approach on an example, and discuss its possible implementation and integration with Software Configuration Management systems.","SCM, software configuration management, software product lines","","SPLC '12"
"Journal Article","Mondal M,Roy CK,Schneider KA","An Empirical Study on Clone Stability","SIGAPP Appl. Comput. Rev.","2012","12","3","20–36","Association for Computing Machinery","New York, NY, USA","","","2012-09","","1559-6915","https://doi.org/10.1145/2387358.2387360;http://dx.doi.org/10.1145/2387358.2387360","10.1145/2387358.2387360","Code cloning is a controversial software engineering practice due to contradictory claims regarding its effect on software maintenance. Code stability is a recently introduced measurement technique that has been used to determine the impact of code cloning by quantifying the changeability of a code region. Although most existing stability analysis studies agree that cloned code is more stable than non-cloned code, the studies have two major flaws: (i) each study only considered a single stability measurement (e.g., lines of code changed, frequency of change, age of change); and, (ii) only a small number of subject systems were analyzed and these were of limited variety.In this paper, we present a comprehensive empirical study on code stability using four different stability measuring methods. We use a recently introduced hybrid clone detection tool, NiCAD, to detect the clones and analyze their stability in different dimensions: by clone type, by measuring method, by programming language, and by system size and age. Our in-depth investigation on 12 diverse subject systems written in three programming languages considering three types of clones reveals that: (i) cloned code is generally less stable than non-cloned code, and more specifically both Type-1 and Type-2 clones show higher instability than Type-3 clones; (ii) clones in both Java and C systems exhibit higher instability compared to the clones in C# systems; (iii) a system's development strategy might play a key role in defining its comparative code stability scenario; and, (iv) cloned and non-cloned regions of a subject system do not follow any consistent change pattern.","changeability, types of clones, modification frequency, code stability, software clones, overall instability","",""
"Conference Paper","Van Ryseghem B,Ducasse S,Fabry J","Spec: A Framework for the Specification and Reuse of UIs and Their Models","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on Smalltalk Technologies","Ghent, Belgium","2012","9781450318976","","https://doi.org/10.1145/2448963.2448965;http://dx.doi.org/10.1145/2448963.2448965","10.1145/2448963.2448965","Implementing UIs is often a tedious task. To address this, UI Builders have been proposed to support the description of widgets, their location, and their logic. A missing aspect of UI Builders is however the ability to reuse and compose widget logic. In our experience, this leads to a significant amount of duplication in UI code. To address this issue, we built Spec: a UIBuilder for Pharo with a focus on reuse. With Spec, widget properties are defined declaratively and attached to specific classes known as composable classes. A composable class defines its own widget description as well as the model-widget bridge and widget interaction logic. This paper presents Spec, showing how it enables seamless reuse of widgets and how these can be customized. After presenting Spec and its implementation, we discuss how its use in Pharo 2.0 has cut in half the amount of lines of code of six of its tools, mostly through reuse. This shows that Spec meets its goals of allowing reuse and composition of widget logic.","","","IWST '12"
"Conference Paper","Naval S,Laxmi V,Gaur MS,Vinod P","SPADE: Signature Based PAcker DEtection","","2012","","","96–101","Association for Computing Machinery","New York, NY, USA","Proceedings of the First International Conference on Security of Internet of Things","Kollam, India","2012","9781450318228","","https://doi.org/10.1145/2490428.2490442;http://dx.doi.org/10.1145/2490428.2490442","10.1145/2490428.2490442","Malware is a powerful weapon to hamper various confidential and secure data of a personal computer. Code packing helps the malware authors to create new variants of existing malwares and thus signature based malware detection is defeated. Packing tools hinder the reverse engineering process and hence it is difficult for security researchers to perform analysis of new or unknown malware. Dynamic unpacker requires dedicated hardware and software for analyzing samples and it is computationally expensive. Hence a fast method is required for analysing packers used to create packed executable. Every packer uses its own unpacking algorithm to unpack the payload in memory, so if apriori information on packer used is available, the unpacking becomes easy. In this paper, we have proposed a novel technique for generating the signature of packed malware to identify the packer used for obfuscating the binary.","malware, unpacking, malware obfuscation, packing","","SecurIT '12"
"Conference Paper","Cong J,Ghodrat MA,Gill M,Grigorian B,Reinman G","CHARM: A Composable Heterogeneous Accelerator-Rich Microprocessor","","2012","","","379–384","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2012 ACM/IEEE International Symposium on Low Power Electronics and Design","Redondo Beach, California, USA","2012","9781450312493","","https://doi.org/10.1145/2333660.2333747;http://dx.doi.org/10.1145/2333660.2333747","10.1145/2333660.2333747","This work discusses CHARM, a Composable Heterogeneous Accelerator-Rich Microprocessor design that provides scalability, flexibility, and design reuse in the space of accelerator-rich CMPs. CHARM features a hardware structure called the accelerator block composer (ABC), which can dynamically compose a set of accelerator building blocks (ABBs) into a loosely coupled accelerator (LCA) to provide orders of magnitude improvement in performance and power efficiency. Our software infrastructure provides a data flow graph to describe the composition, and our hardware components dynamically map available resources to the data flow graph to compose the accelerator from components that may be physically distributed across the CMP. Our ABC is also capable of providing load balancing among available compute resources to increase accelerator utilization. Running medical imaging benchmarks, our experimental results show an average speedup of 2.1X (best case 3.7X) compared to approaches that use LCAs together with a hardware resource manager. We also gain in terms of energy consumption (average 2.4X; best case 4.7X).","chip multiprocessor, hardware accelerators, accelerator composition","","ISLPED '12"
"Journal Article","Dikaiakos MD,Katsifodimos A,Pallis G","Minersoft: Software Retrieval in Grid and Cloud Computing Infrastructures","ACM Trans. Internet Technol.","2012","12","1","","Association for Computing Machinery","New York, NY, USA","","","2012-07","","1533-5399","https://doi.org/10.1145/2220352.2220354;http://dx.doi.org/10.1145/2220352.2220354","10.1145/2220352.2220354","One of the main goals of Cloud and Grid infrastructures is to make their services easily accessible and attractive to end-users. In this article we investigate the problem of supporting keyword-based searching for the discovery of software files that are installed on the nodes of large-scale, federated Grid and Cloud computing infrastructures. We address a number of challenges that arise from the unstructured nature of software and the unavailability of software-related metadata on large-scale networked environments. We present Minersoft, a harvester that visits Grid/Cloud infrastructures, crawls their file systems, identifies and classifies software files, and discovers implicit associations between them. The results of Minersoft harvesting are encoded in a weighted, typed graph, called the Software Graph. A number of information retrieval (IR) algorithms are used to enrich this graph with structural and content associations, to annotate software files with keywords and build inverted indexes to support keyword-based searching for software. Using a real testbed, we present an evaluation study of our approach, using data extracted from production-quality Grid and Cloud computing infrastructures. Experimental results show that Minersoft is a powerful tool for software search and discovery.","Grid computing, resource management, Cloud computing, software search engine","",""
"Conference Paper","Poon JY,Sugiyama K,Tan YF,Kan MY","Instructor-Centric Source Code Plagiarism Detection and Plagiarism Corpus","","2012","","","122–127","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th ACM Annual Conference on Innovation and Technology in Computer Science Education","Haifa, Israel","2012","9781450312462","","https://doi.org/10.1145/2325296.2325328;http://dx.doi.org/10.1145/2325296.2325328","10.1145/2325296.2325328","Existing source code plagiarism systems focus on the problem of identifying plagiarism between pairs of submissions. The task of detection, while essential, is only a small part of managing plagiarism in an instructional setting. Holistic plagiarism detection and management requires coordination and sharing of assignment similarity -- elevating plagiarism detection from pairwise similarity to cluster-based similarity; from a single assignment to a sequence of assignments in the same course, and even among instructors of different courses.To address these shortcomings, we have developed Student Submissions Integrity Diagnosis (SSID), an open-source system that provides holistic plagiarism detection in an instructor-centric way. SSID's visuals show overviews of plagiarism clusters throughout all assignments in a course as well as highlighting most-similar submissions on any specific student. SSID supports plagiarism detection workflows; e.g., allowing student assistants to flag suspicious assignments for later review and confirmation by an instructor with proper authority. Evidence is automatically entered into SSID's logs and shared among instructors.We have additionally collected a source code plagiarism corpus, which we employ to identify and correct shortcomings of previous plagiarism detection engines and to optimize parameter tuning for SSID deployment. Since its deployment, SSID's workflow enhancements have made plagiarism detection in our faculty less tedious and more successful.","plagiarism assessment, user interface, plagiarism detection, corpus studies, programming, similarity","","ITiCSE '12"
"Conference Paper","Borchert C,Lohmann D,Spinczyk O","CiAO/IP: A Highly Configurable Aspect-Oriented IP Stack","","2012","","","435–448","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services","Low Wood Bay, Lake District, UK","2012","9781450313018","","https://doi.org/10.1145/2307636.2307676;http://dx.doi.org/10.1145/2307636.2307676","10.1145/2307636.2307676","Internet protocols are constantly gaining relevance for the domain of mobile and embedded systems. However, building complex network protocol stacks for small resource-constrained devices is more than just porting a reference implementation. Due to the cost pressure in this area especially the memory footprint has to be minimized. Therefore, embedded TCP/IP implementations tend to be statically configurable with respect to the concrete application scenario. This paper describes our software engineering approach for building CiAO/IP - a tailorable TCP/IP stack for small embedded systems, which pushes the limits of static configurability while retaining source code maintainability. Our evaluation results show that CiAO/IP thereby outperforms both lwIP and uIP in terms of code size (up to 90% less than uIP), throughput (up to 20% higher than lwIP), energy consumption (at least 40% lower than uIP) and, most importantly, tailorability.","aspectc++, aop, network protocol stacks, embedded systems, operating systems, aspect-oriented programming, internet protocol, tcp/ip","","MobiSys '12"
"Conference Paper","Lyberis S,Pratikakis P,Nikolopoulos DS,Schulz M,Gamblin T,de Supinski BR","The Myrmics Memory Allocator: Hierarchical,Message-Passing Allocation for Global Address Spaces","","2012","","","15–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2012 International Symposium on Memory Management","Beijing, China","2012","9781450313506","","https://doi.org/10.1145/2258996.2259001;http://dx.doi.org/10.1145/2258996.2259001","10.1145/2258996.2259001","Constantly increasing hardware parallelism poses more and more challenges to programmers and language designers. One approach to harness the massive parallelism is to move to task-based programming models that rely on runtime systems for dependency analysis and scheduling. Such models generally benefit from the existence of a global address space. This paper presents the parallel memory allocator of the Myrmics runtime system, in which multiple allocator instances organized in a tree hierarchy cooperate to implement a global address space with dynamic region support on distributed memory machines. The Myrmics hierarchical memory allocator is step towards improved productivity and performance in parallel programming. Productivity is improved through the use of dynamic regions in a global address space, which provide a convenient shared memory abstraction for dynamic and irregular data structures. Performance is improved through scaling on manycore systems without system-wide cache coherency. We evaluate the stand-alone allocator on an MPI-based x86 cluster and find that it scales well for up to 512 worker cores, while it can outperform Unified Parallel C by a factor of 3.7-10.7x.","parallel memory allocator, gas","","ISMM '12"
"Journal Article","Lyberis S,Pratikakis P,Nikolopoulos DS,Schulz M,Gamblin T,de Supinski BR","The Myrmics Memory Allocator: Hierarchical,Message-Passing Allocation for Global Address Spaces","SIGPLAN Not.","2012","47","11","15–24","Association for Computing Machinery","New York, NY, USA","","","2012-06","","0362-1340","https://doi.org/10.1145/2426642.2259001;http://dx.doi.org/10.1145/2426642.2259001","10.1145/2426642.2259001","Constantly increasing hardware parallelism poses more and more challenges to programmers and language designers. One approach to harness the massive parallelism is to move to task-based programming models that rely on runtime systems for dependency analysis and scheduling. Such models generally benefit from the existence of a global address space. This paper presents the parallel memory allocator of the Myrmics runtime system, in which multiple allocator instances organized in a tree hierarchy cooperate to implement a global address space with dynamic region support on distributed memory machines. The Myrmics hierarchical memory allocator is step towards improved productivity and performance in parallel programming. Productivity is improved through the use of dynamic regions in a global address space, which provide a convenient shared memory abstraction for dynamic and irregular data structures. Performance is improved through scaling on manycore systems without system-wide cache coherency. We evaluate the stand-alone allocator on an MPI-based x86 cluster and find that it scales well for up to 512 worker cores, while it can outperform Unified Parallel C by a factor of 3.7-10.7x.","gas, parallel memory allocator","",""
"Journal Article","Midtgaard J","Control-Flow Analysis of Functional Programs","ACM Comput. Surv.","2012","44","3","","Association for Computing Machinery","New York, NY, USA","","","2012-06","","0360-0300","https://doi.org/10.1145/2187671.2187672;http://dx.doi.org/10.1145/2187671.2187672","10.1145/2187671.2187672","We present a survey of control-flow analysis of functional programs, which has been the subject of extensive investigation throughout the past 30 years. Analyses of the control flow of functional programs have been formulated in multiple settings and have led to many different approximations, starting with the seminal works of Jones, Shivers, and Sestoft. In this article, we survey control-flow analysis of functional programs by structuring the multitude of formulations and approximations and comparing them.","Control-flow analysis, higher-order functions","",""
"Conference Paper","Kawachiya K,Takeuchi M,Zakirov S,Onodera T","Distributed Garbage Collection for Managed X10","","2012","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2012 ACM SIGPLAN X10 Workshop","Beijing, China","2012","9781450314916","","https://doi.org/10.1145/2246056.2246061;http://dx.doi.org/10.1145/2246056.2246061","10.1145/2246056.2246061","X10 is a programming language that incorporates distributed processing functions. The execution model of X10 is called ""APGAS"", where each object belongs to a specific place (an abstraction of a shared-memory computer), but can be remotely referenced from other places using a mechanism named GlobalRef. This means that a remotely-referenced object must not be collected as garbage even if there is no local reference to it.There is an implementation of X10 named ""Managed X10"", which uses multiple Java virtual machines (JVMs) to run an X10 application. In a Managed X10 environment, X10 objects are represented by Java objects, and unneeded objects are collected by the GC in the JVM. Each place is implemented as an individual JVM, so distributed processing is possible by using multiple JVMs on multiple computers. However, in the early implementation of Managed X10, objects that were ever remotely referenced through GlobalRef were registered in a management table and would never be collected. Therefore, if an application uses GlobalRefs periodically, its heap area is consumed by uncollectable objects and an OutOfMemoryError condition may eventually occur.To solve this problem, we developed a distributed GC for Managed X10. The core part of this GC is provided by an internal data structure called the ""Globalized Object Tracker"" (GOT), which is prepared for each remotely-referenced (globalized) object. GOT tracks the number of remote references to the object and handles the mapping between the object and its ID. When all remote references are removed, the distributed GC makes the object collectable by the local JVM GC.This paper first introduces the distributed execution model of X10 and the mechanism of GlobalRef, then explains how the distributed GC is implemented in Managed X10. The implementation does not modify the JVM, so it is highly portable. Through various evaluation experiments, we show that the remotely-referenced objects are correctly collected and there is almost no performance degradation in the proposed distributed GC.","Java, X10, distributed garbage collection","","X10 '12"
"Conference Paper","Khudia DS,Wright G,Mahlke S","Efficient Soft Error Protection for Commodity Embedded Microprocessors Using Profile Information","","2012","","","99–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, Tools and Theory for Embedded Systems","Beijing, China","2012","9781450312127","","https://doi.org/10.1145/2248418.2248433;http://dx.doi.org/10.1145/2248418.2248433","10.1145/2248418.2248433","Successive generations of processors use smaller transistors in the quest to make more powerful computing systems. It has been previously studied that smaller transistors make processors more susceptible to soft errors (transient faults caused by high energy particle strikes). Such errors can result in unexpected behavior and incorrect results. With smaller and cheaper transistors becoming pervasive in mainstream computing, it is necessary to protect these devices against soft errors; an increasing rate of faults necessitates the protection of applications running on commodity processors against soft errors. The existing methods of protecting against such faults generally have high area or performance overheads and thus are not directly applicable in the embedded design space. In order to protect against soft errors, the detection of these errors is a necessary first step so that a recovery can be triggered.To solve the problem of detecting soft errors cheaply, we propose a profiling-based software-only application analysis and transformation solution. The goal is to develop a low cost solution which can be deployed for off-the-shelf embedded processors. The solution works by intelligently duplicating instructions that are likely to affect the program output, and comparing results between original and duplicated instructions. The intelligence of our solution is garnered through the use of control flow, memory dependence, and value profiling to understand and exploit the common-case behavior of applications. Our solution is able to achieve 92% fault coverage with a 20% instruction overhead. This represents a 41% lower performance overhead than the best prior approaches with approximately the same fault coverage.","fault injection, profiling, soft errors, profile-based compiler analysis","","LCTES '12"
"Journal Article","Khudia DS,Wright G,Mahlke S","Efficient Soft Error Protection for Commodity Embedded Microprocessors Using Profile Information","SIGPLAN Not.","2012","47","5","99–108","Association for Computing Machinery","New York, NY, USA","","","2012-06","","0362-1340","https://doi.org/10.1145/2345141.2248433;http://dx.doi.org/10.1145/2345141.2248433","10.1145/2345141.2248433","Successive generations of processors use smaller transistors in the quest to make more powerful computing systems. It has been previously studied that smaller transistors make processors more susceptible to soft errors (transient faults caused by high energy particle strikes). Such errors can result in unexpected behavior and incorrect results. With smaller and cheaper transistors becoming pervasive in mainstream computing, it is necessary to protect these devices against soft errors; an increasing rate of faults necessitates the protection of applications running on commodity processors against soft errors. The existing methods of protecting against such faults generally have high area or performance overheads and thus are not directly applicable in the embedded design space. In order to protect against soft errors, the detection of these errors is a necessary first step so that a recovery can be triggered.To solve the problem of detecting soft errors cheaply, we propose a profiling-based software-only application analysis and transformation solution. The goal is to develop a low cost solution which can be deployed for off-the-shelf embedded processors. The solution works by intelligently duplicating instructions that are likely to affect the program output, and comparing results between original and duplicated instructions. The intelligence of our solution is garnered through the use of control flow, memory dependence, and value profiling to understand and exploit the common-case behavior of applications. Our solution is able to achieve 92% fault coverage with a 20% instruction overhead. This represents a 41% lower performance overhead than the best prior approaches with approximately the same fault coverage.","profiling, soft errors, profile-based compiler analysis, fault injection","",""
"Conference Paper","Chiw C,Kindlmann G,Reppy J,Samuels L,Seltzer N","Diderot: A Parallel DSL for Image Analysis and Visualization","","2012","","","111–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation","Beijing, China","2012","9781450312059","","https://doi.org/10.1145/2254064.2254079;http://dx.doi.org/10.1145/2254064.2254079","10.1145/2254064.2254079","Research scientists and medical professionals use imaging technology, such as computed tomography (CT) and magnetic resonance imaging (MRI) to measure a wide variety of biological and physical objects. The increasing sophistication of imaging technology creates demand for equally sophisticated computational techniques to analyze and visualize the image data. Analysis and visualization codes are often crafted for a specific experiment or set of images, thus imaging scientists need support for quickly developing codes that are reliable, robust, and efficient.In this paper, we present the design and implementation of Diderot, which is a parallel domain-specific language for biomedical image analysis and visualization. Diderot supports a high-level model of computation that is based on continuous tensor fields. These tensor fields are reconstructed from discrete image data using separable convolution kernels, but may also be defined by applying higher-order operations, such as differentiation (∇). Early experiments demonstrate that Diderot provides both a high-level concise notation for image analysis and visualization algorithms, as well as high sequential and parallel performance.","image analysis, scientific visualization, parallelism, domain specific languages","","PLDI '12"
"Journal Article","Chiw C,Kindlmann G,Reppy J,Samuels L,Seltzer N","Diderot: A Parallel DSL for Image Analysis and Visualization","SIGPLAN Not.","2012","47","6","111–120","Association for Computing Machinery","New York, NY, USA","","","2012-06","","0362-1340","https://doi.org/10.1145/2345156.2254079;http://dx.doi.org/10.1145/2345156.2254079","10.1145/2345156.2254079","Research scientists and medical professionals use imaging technology, such as computed tomography (CT) and magnetic resonance imaging (MRI) to measure a wide variety of biological and physical objects. The increasing sophistication of imaging technology creates demand for equally sophisticated computational techniques to analyze and visualize the image data. Analysis and visualization codes are often crafted for a specific experiment or set of images, thus imaging scientists need support for quickly developing codes that are reliable, robust, and efficient.In this paper, we present the design and implementation of Diderot, which is a parallel domain-specific language for biomedical image analysis and visualization. Diderot supports a high-level model of computation that is based on continuous tensor fields. These tensor fields are reconstructed from discrete image data using separable convolution kernels, but may also be defined by applying higher-order operations, such as differentiation (∇). Early experiments demonstrate that Diderot provides both a high-level concise notation for image analysis and visualization algorithms, as well as high sequential and parallel performance.","scientific visualization, image analysis, parallelism, domain specific languages","",""
"Conference Paper","Barbosa EA,Garcia A,Mezini M","A Recommendation System for Exception Handling Code","","2012","","","52–54","IEEE Press","Zurich, Switzerland","Proceedings of the 5th International Workshop on Exception Handling","","2012","9781467317665","","","","Even though exception handling mechanisms are part of most mainstream programming languages, software developers still struggle to implement proper exception handling code. In particular, they fail in implementing effective handler actions. This position paper discusses our ongoing work on implementing and assessing a recommendation system for recommending code fragments implementing exception handling code. These fragments are not meant to be reused as-is. Instead, they are meant to be used by the developers as examples of how to possibly handle their exceptions. The goal of the proposed recommendation system is to assist the learning process of software developers by providing concrete examples of exception handling code.","exception handling, recommendation system","","WEH '12"
"Conference Paper","Hasu T","Concrete Error Handling Mechanisms Should Be Configurable","","2012","","","46–48","IEEE Press","Zurich, Switzerland","Proceedings of the 5th International Workshop on Exception Handling","","2012","9781467317665","","","","We argue that programmers should not need to decide on a specific error handling mechanism when implementing a C or C++ library. Rather, it should be possible to make that decision at configuration time in order to achieve better portability and more convenient use of libraries.","programming languages, software engineering, exceptions, C++, portability, error handling, source-to-source translation, C","","WEH '12"
"Conference Paper","Keivanloo I,Rilling J","Clone Detection Meets Semantic Web-Based Transitive Closure Computation","","2012","","","12–16","IEEE Press","Zurich, Switzerland","Proceedings of the First International Workshop on Realizing AI Synergies in Software Engineering","","2012","9781467317535","","","","In this paper we discuss a new application of Semantic Web and Artificial Intelligence in software analysis research. We show on a concrete example - clone detection for object-oriented source code that transitivity closure computation can provide added value to the clone detection community. Our novel approach models the domain of discourse knowledge as a mixture of source code patterns and inheritance trees represented as Directed Acyclic Graphs. Our approach promotes the use of Semantic Web and inference engines in source code analysis. More specifically we take advantage of the Semantic Web and its support for knowledge modeling and transitive closure computation to detect semantic source code clones not detected by traditional detection tools.","clone detection, semantic web, object oriented","","RAISE '12"
"Conference Paper","Letouzey JL","The SQALE Method for Evaluating Technical Debt","","2012","","","31–36","IEEE Press","Zurich, Switzerland","Proceedings of the Third International Workshop on Managing Technical Debt","","2012","9781467317498","","","","This paper presents the SQALE (Software Quality Assessment Based on Lifecycle Expectations) method. We describe its Quality Model and Analysis Model which is used to estimate the Quality and the Technical Debt of an application source code. We provide recommendations and guidelines for using the SQALE indicators in order to analyse the structure and the impact of the Technical Debt.","SQALE, technical debt, source code, quality model, quality, analysis model","","MTD '12"
"Conference Paper","Fontana FA,Ferme V,Spinelli S","Investigating the Impact of Code Smells Debt on Quality Code Evaluation","","2012","","","15–22","IEEE Press","Zurich, Switzerland","Proceedings of the Third International Workshop on Managing Technical Debt","","2012","9781467317498","","","","Different forms of technical debt exist that have to be carefully managed. In this paper we focus our attention on design debt, represented by code smells. We consider three smells that we detect in open source systems of different domains. Our principal aim is to give advice on which design debt has to be paid first, according to the three smells we have analyzed. Moreover, we discuss if the detection of these smells could be tailored to the specific application domain of a system.","design debt, software quality metrics, code smell refactoring","","MTD '12"
"Conference Paper","Keivanloo I,Roy CK,Rilling J,Charland P","Shuffling and Randomization for Scalable Source Code Clone Detection","","2012","","","82–83","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","In this research, we present a novel approach that allows existing state of the art clone detection tools to scale to very large datasets. A key benefit of our approach is that the improved tools scalability is achieved using standard hardware and without modifying the original implementations of the subject tools. We use a hybrid approach comprising of shuffling, repetition, and random subset generation of the subject dataset. As part of the experimental evaluation, we applied our shuffling and randomization approach on two state of the art clone detection tools. Our experience shows that it is possible to scale the classical tools to a very large dataset using standard hardware, and without significantly affecting the overall recall while exploiting all the strengths of the original tools including the precision.","scalability, sampling, shuffling, clone detection","","IWSC '12"
"Conference Paper","Yoshimura K,Mibe R","Visualizing Code Clone Outbreak: An Industrial Case Study","","2012","","","96","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","This paper describes an industrial experience on code clone visualization. Cloning source code fragments is a common practice in software development process. However, uncontrolled proliferation of code clones causes a serious problem in terms of software maintenance. In this paper, we briefly share our experience on code clone visualization especially for stakeholders who are not software experts. We describe our prototype tool for code clone visualization, and the feedback we have received with analyzing an enterprise business system.","code clone, software visualization, legacy system","","IWSC '12"
"Conference Paper","Toomey W","Ctcompare: Code Clone Detection Using Hashed Token Sequences","","2012","","","92–93","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","There is much research on the use of tokenized source code to find code clones both within and between trees of source code. Some approaches have used suffix trees [1], [3]; others have used variations of longest common substring algorithms [4], [5].This paper outlines an algorithm, embodied in a new tool called ctcompare, that takes a different tokenization approach. Each code base to be compared is first lexically analysed to produce a sequence of tokens. These are then broken into overlapping tuples of N consecutive tokens. The tuples are then hashed and the hash values of token tuples are used to identify type-1 and type-2 clone pairs.Hashed token sequences combined with a database have already been used in earlier ctcompare versions and elsewhere [2], but with a significant performance penalty due to database insertions. The benefits of this approach over the existing research include the simultaneous comparison of multiple large code bases and fast absolute performance.","code redundancy, software, clone detection, code clone, hash function","","IWSC '12"
"Conference Paper","Stephan M,Alafi MH,Stevenson A,Cordy JR","Towards Qualitative Comparison of Simulink Model Clone Detection Approaches","","2012","","","84–85","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","In this position paper we briefly review the Simulink model clone detection approaches in literature, including a new one currently being developed, and outline our plan for an experimental comparison. We are using public and private Simulink models to compare approaches based on clone relevance, performance, types of clones detected, user interaction, adaptability, and the ability to identify recurring patterns using a combination of manual inspection and model visualization.","simulink, comparison, clone detection","","IWSC '12"
"Conference Paper","Elva R,Leavens GT","Semantic Clone Detection Using Method IOE-Behavior","","2012","","","80–81","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","This paper presents an algorithm for the detection of semantic clones in Java methods. Semantic clones are defined as functionally-identical code fragments. Our detection process operates on the premise that if two code fragments are semantic clones, then their input-output behavior would be identical. We adopt a wholistic approach to the definition of input-output behavior by including not only the parameters and return values of methods; but also their effects, as reflected in the pre- and post-states of the heap. We refer to this as a method's IOE-behavior (input, output and effects).","software clone detection, semantic clones, program understanding, method IOE-behavior","","IWSC '12"
"Conference Paper","Kapser CJ,Harder J,Baxter I","A Common Conceptual Model for Clone Detection Results","","2012","","","72–73","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","As the field of code clone research grows, the continuing problem of interoperability between code clone detection and analysis tools grows with it. As a step toward solving this problem, this paper presents a draft proposal for a generic model of code clone detection results. Using an online wiki, we hope to generate discussion and solidify a shared understanding of the core concepts of the problem domain, enabling us to ultimately develop a generic data exchange format.","","","IWSC '12"
"Conference Paper","Mondal M,Roy CK,Schneider KA","Dispersion of Changes in Cloned and Non-Cloned Code","","2012","","","29–35","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Currently, the impacts of clones in software maintenance activities are being investigated by different researchers in different ways. Comparative stability analysis of cloned and non-cloned regions of a subject system is a well-known way of measuring the impacts where the hypothesis is that, the more a region is stable the less it is harmful for maintenance. Each of the existing stability measurement methods lacks to address one important characteristic, dispersion, of the changes happening in the cloned and non-cloned regions of software systems. Change dispersion of a particular region quantifies the extent to which the changes are scattered over that region. The intuition is that, more dispersed changes require more efforts to be spent in the maintenance phase.Measurement of Dispersion requires the extraction of method genealogies. In this paper, we have measured the dispersions of changes in cloned and non-cloned regions of several subject systems using a concurrent and robust framework for method genealogy extraction. We implemented the framework on Actor Architecture platform which facilitates coarse grained parallellism with asynchronous message passing capabilities. Our experimental results on 12 open-source subject systems written in three different programming languages (Java, C and C#) using two clone detection tools suggest that, the changes in cloned regions are more dispersed than the changes in non-cloned regions. Also, Type-3 clones exhibit more dispersion as compared to the Type-1 and Type-2 clones. The subject systems written in Java and C show higher dispersions as well as increased maintenance efforts as compared to the subject systems written in C#.","average last change date, concurrent framework, dispersion, actor architecture, code stability, changeability, modification frequency","","IWSC '12"
"Conference Paper","Cuomo A,Santone A,Villano U","A Novel Approach Based on Formal Methods for Clone Detection","","2012","","","8–14","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","This paper presents an approach based on formal methods for detecting code clones. The methodology followed performs the analysis on Java bytecode, which is transformed into CCS (Calculus of Communicating Systems) processes which are successively checked for equivalence. A prototype tool targeted at the detection of Type 2 clones is presented. The experiments conducted on programs of different size assess the validity of the proposed approach, pointing out possible improvements for future research.","clone detection, formal methods, CCS","","IWSC '12"
"Conference Paper","Keivanloo I,Roy CK,Rilling J","Java Bytecode Clone Detection via Relaxation on Code Fingerprint and Semantic Web Reasoning","","2012","","","36–42","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","While finding clones in source code has drawn considerable attention, there has been only very little work in finding similar fragments in binary code and intermediate languages, such as Java bytecode. Some recent studies showed that it is possible to find distinct sets of clone pairs in bytecode representation of source code, which are not always detectable at source code-level. In this paper, we present a bytecode clone detection approach, called SeByte, which exploits the benefits of compilers (the bytecode representation) for detecting a specific type of semantic clones in Java bytecode. SeByte is a hybrid metric-based approach that takes advantage of both, Semantic Web technologies and Set theory. We use a two-step analysis process: (1) Pattern matching via Semantic Web querying and reasoning, and (2) Content matching, using Jaccard coefficient for set similarity measurement. Semantic Web-based pattern matching helps us to find method blocks which share similar patterns even in case of extreme dissimilarity (e.g., numerous repetitions or large gaps). Although it leads to high recall, it gives high false positive rate. We thus use the content matching (via Jaccard) to reduce false positive rate by focusing on content semantic resemblance. Our evaluation of four Java systems and five other tools shows that SeByte can detect a large number of semantic clones that are either not detected or supported by source code based clone detectors.","semantic web, clone detection, Java bytecode","","IWSC '12"
"Conference Paper","Alalfi MH,Cordy JR,Dean TR,Stephan M,Stevenson A","Near-Miss Model Clone Detection for Simulink Models","","2012","","","78–79","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","This paper describes our plan to adapt mature code-based clone detection techniques to the efficient identification of near-miss clones in models. Our goal is to leverage successful source text-based clone detection techniques by transforming graph-based models to normalized text form in order to capture semantically meaningful near-miss results that can help in further model analysis tasks. In this position paper we present a first example, adapting the NiCad code clone detector to identifying near-miss Simulink model clones at the ""system"" granularity. In current work we are extending this technique to the Simulink (entire) ""model"" and (more refined) ""block"" granularities as well.","","","IWSC '12"
"Conference Paper","Thomsen MJ,Henglein F","Clone Detection Using Rolling Hashing, Suffix Trees and Dagification: A Case Study","","2012","","","22–28","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Microsoft Dynamics NAV is a widely used enterprise resource planning system for small and medium-sized enterprises that, by design, encourages rapid customization by copy-paste programming. We report the results of analyzing clone detection for NAV using two previously published methods and one new algorithmic method: character-based sliding window sampling using Rabin-Karp hashing (MOSS), line-based sequence matching using suffix trees (CodeDup), and abstract-syntax-tree based graph sharing analysis (XMLClone). The latter is piggybacked on XMLStore, which stores XML trees as directed acyclic graphs (dags) where all isomorphic subtrees are identified and coalesced into single nodes, which can be done in linear time using multiset discrimination. This dagification discovers all well-formed Type-1 and, with suitable input normalization, Type-2 clones. We find that the subsequent dag analysis to discover Type-3 clones performs well on NAV source code, both in terms of computational complexity and precision. This suggests that efficient dagification and independently configurable dag interpretation may be valuable ingredients for modular clone detection.","multiset, ERP, similarity, suffix, MOSS, detection, tree, MS dynamics NAV, winnowing, CodeDup, XMLClone, discrimination, code, dagification, XMLStore, clone, hashing","","IWSC '12"
"Conference Paper","Duszynski S,Becker M","Recovering Variability Information from the Source Code of Similar Software Products","","2012","","","37–40","IEEE Press","Zurich, Switzerland","Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering","","2012","9781467317511","","","","We developed a reverse engineering technique, named Variant Analysis, aimed for recovering and visualizing information about commonalities and differences that exist in the source code of multiple similar software systems. The delivered information is available on any level of system hierarchy, from single lines of code up to whole software systems. The technique scales well for many compared system variants and for large software systems. We think Variant Analysis could be useful for practitioners who need to identify source-level similarities between many potentially unknown software systems -- either with the primary goal of understanding the variability in the systems, or with a further motivation such as preparation for an extractive introduction of the product line approach.","product lines, reverse engineering, software reuse, variant, visualization","","PLEASE '12"
"Conference Paper","Kamiya T","Conte*t Clones or Re-Thinking Clone on a Call Graph","","2012","","","74–75","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","To improve clone-detection methods and to enable new analysis methods with clone detection, e.g., to detect a wider range of bad smells and anti-patterns, this paper introduces a concept context clone, in a form comparable to the (traditional) content clone. A context clone is based on the similarity of the contexts in which code fragment is used, instead of the similarity of the code fragments themselves. This paper includes an explanation of context clones, research questions about the context clone, expected use of the context clone in a mixed way with the content clone, and an actual example of a context clone with a prototype tool.","code clone, reverse engineering, call graph","","IWSC '12"
"Conference Paper","Lavoie T,Merlo E","An Accurate Estimation of the Levenshtein Distance Using Metric Trees and Manhattan Distance","","2012","","","1–7","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","This paper presents an original clone detection technique which is an accurate approximation of the Levenshtein distance. It uses groups of tokens extracted from source code called windowed-tokens. From these, frequency vectors are then constructed and compared with the Manhattan distance in a metric tree. The goal of this new technique is to provide a very high precision clone detection technique while keeping a high recall. Precision and recall measurement is done with respect to the Levenshtein distance. The testbench is a large scale open source software. The collected results proved the technique to be fast, simple, and accurate. Finally, this article presents further research opportunities.","clone detection, Levenshtein distance, Manhattan distance, software clones","","IWSC '12"
"Conference Paper","Hauptmann B,Bauer V,Junker M","Using Edge Bundle Views for Clone Visualization","","2012","","","86–87","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Clone detection results are often voluminous and difficult to present. Most clone presentations focus on the quantitative clone results but do not relate them to the structure of the analyzed system. This makes it difficult to interpret the results and draw conclusions. We suggest using edge bundle views to interrelate the system's structure with the clone detection results. Using this technique, it is easier to interpret the clone results and direct further analysis effort.","clone presentation, clone detection, edge bundle views","","IWSC '12"
"Conference Paper","Yamanaka Y,Choi E,Yoshida N,Inoue K,Sano T","Industrial Application of Clone Change Management System","","2012","","","67–71","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Clone change management is one of crucial issues in open source software(OSS) development as well as in industrial software development (e.g., development of social infrastructure, financial system, and medical equipment). When an industrial developer fixes a defect, he/she has to find the code clones corresponding to the code fragment including it. So far, several studies performed on the analysis of clone evolution in OSS. However, to our knowledge, a few researches have been reported on an application of a clone change management system to industrial development process. In this paper, we propose a clone change management system based on the categorization of clone evolution, and then present case study of industrial application. In case study, we confirmed that the proposed system suggested two unintentionally developed clones in a half of the month.","code clone, software maintenance, clone evolution, change management","","IWSC '12"
"Conference Paper","Yang J,Hotta K,Higo Y,Igaki H,Kusumoto S","Filtering Clones for Individual User Based on Machine Learning Analysis","","2012","","","76–77","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Results from code clone detectors may contain plentiful useless code clones, and judging whether a code clone is useful varies from user to user based on different purposes of them. We are planing a system to study the judgment of each individual user by applying machine learning algorithms on code clones. We describe the reason why individual judgment should be respected and how in this paper.","classify, machine learning, code clone detector, token-based, judgment of user, filtering","","IWSC '12"
"Conference Paper","Venkatasubramanyam RD,Singh HK,Ravikanth K","A Method for Proactive Moderation of Code Clones in IDEs","","2012","","","62–66","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Duplicating code and modifying it is a useful convenience when editing within an IDE. This sequence of operations, termed copy-paste-modify, has the downside of proliferating ""nearly identical"" code segments or code clones and could lead to rapid degeneration of code. Although techniques for proactive identification of clones and differences between them have been studied, no clear method to control clone formation, based on ""acceptability criteria,"" is known. In this paper, we present a technique to moderate the genesis of clones through copy-paste-modify operations. Our approach is guided by associating constraints formulated from predefined guidelines, and checking for their satisfaction at the time of copy and upon modification. By encoding ""acceptability criteria"" as constraints, our approach provides the means necessary for controlled creation of clones.","software clones, product assessment, IDE, code clone, source code metrics, duplicates, moderation, software maintenance, software evolution","","IWSC '12"
"Conference Paper","Inoue K,Higo Y,Yoshida N,Choi E,Kusumoto S,Kim K,Park W,Lee E","Experience of Finding Inconsistently-Changed Bugs in Code Clones of Mobile Software","","2012","","","94–95","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","When we reuse a code fragment, some of the identifiers in the fragment might be systematically changed to others. Failing these changes would become a potential bug in the copied fragment. We have developed a tool CloneInspector to detect such inconsistent changes in the code clones, and applied it to two mobile software systems. Using this tool, we were effectively able to find latent bugs in those systems.","bug candidate, unchanged ratio, inconsistent change","","IWSC '12"
"Conference Paper","Chatterji D,Carver JC,Kraft NA","Claims and Beliefs about Code Clones: Do We Agree as a Community? A Survey","","2012","","","15–21","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Research on code clones and their impact on software development has been increasing in recent years. There are a number of potentially competing claims among members of the community. There is currently not enough empirical evidence to provide concrete information about these claims. This paper presents the results of a survey of members of the code clone community. The goal of the survey was to determine the level of agreement of community members regarding some key topics. While the results showed a good bit of agreement, there was no universal consensus on all topics. Survey respondents were not in complete agreement about the definitions of Type III and Type IV clones. The survey respondents were more uncertain about how developers behave when working with clones. From the survey it is clear that there are areas where more empirical research is needed to better understand how to effectively work with clones.","software maintenance, clone evolution, developer behavior, survey, code clones, clone management","","IWSC '12"
"Conference Paper","Wang W,Godfrey MW","We Have All of the Clones, Now What? Toward Integrating Clone Analysis into Software Quality Assessment","","2012","","","88–89","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Cloning might seems to be an unconventional way of designing and developing software, yet it is very widely practised in industrial development. The cloning research community has made substantial progress on modeling, detecting, and analyzing software clones. Although there is continuing discussion on the real role of clones on software quality, our community may agree on the need for advancing clone management techniques. Current clone management techniques concentrate on providing editing tools that allow developers to easily inspect clone instances, track their evolution, and check change consistency. In this position paper, we argue that better clone management can be achieved by responding to the fundamental needs of industry practitioners. And the possible research directions include a software problem-oriented taxonomy of clones, and a better structured clone detection report. We believe this line of research should inspire new techniques, and reach to a much wider range of professionals from both the research and industry community.","","","IWSC '12"
"Conference Paper","Tüzün E,Er E","A Case Study on Applying Clone Technology to an Industrial Application Framework","","2012","","","57–61","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Dealing with clones is a common problem in large-scale software projects. While most of the research in this area is focused on detecting, and investigating the reasons of clones, little research has been done on the use of clone technology in large-scale industrial software. In this paper we present an in-depth case study of cloning in Command and Control Software Framework (CCSF) developed by HAVELSAN, a large software company in the Turkish defense industry. We proposed a practical classification schema for clones based on their removal strategies. A detailed analysis on categorization of clones, and lessons learned in clone management of large-scale industrial software systems are presented.","application of clone analysis, industrial experiences with clone analysis, software clones, types of clones","","IWSC '12"
"Conference Paper","Dotzler G,Veldema R,Philippsen M","Annotation Support for Generic Patches","","2012","","","6–10","IEEE Press","Zurich, Switzerland","Proceedings of the Third International Workshop on Recommendation Systems for Software Engineering","","2012","9781467317597","","","","In large projects parallelization of existing programs or refactoring of source code is time consuming as well as error-prone and would benefit from tool support. However, existing automatic transformation systems are not extensively used because they either require tedious definitions of source code transformations or they lack general adaptability. In our approach, a programmer changes code inside a project, resulting in before and after source code versions. The difference (the generated transformation) is stored in a database. When presented with some arbitrary code, our tool mines the database to determine which of the generalized transformations possibly apply. Our system is different from a pure compiler based (semantics preserving) approach as we only suggest code modifications.Our contribution is a set of generalizing annotations that we have found by analyzing recurring patterns in open source projects. We show the usability of our system and the annotations by finding matches and applying generated transformations in real-world applications.","programming tools, optimizations, patches, code-refactoring","","RSSE '12"
"Conference Paper","Volanschi N","Safe Clone-Based Refactoring through Stereotype Identification and Iso-Generation","","2012","","","50–56","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","Most advanced existing tools for clone-based refactoring propose a limited number of pre-defined clone-removal transformations that can be applied automatically, typically under user control. This fixed set of refactorings usually guarantee that semantics is preserved, but is inherently limited to generally-applicable transformations (extract method, pull-up method, etc.). This tool design rules out many potential domain-specific or application-specific clone removals. Such cases are ordinarily recognized by humans as stereotypes derived from a higher-level concept and manually replaced with an appropriate abstraction. Thus, in current tools, generality is sacrificed for the safety of the transformation. This paper proposes an alternative approach, in which the spectrum of refactoring techniques is open, including manual interventions, while keeping strong safety guarantees based on the notion of iso-generation. Our method can operate on multiple languages and has been prototyped on a subset of a real-world legacy asset containing C and COBOL programs, with promising results.","refactoring, safety, clones, maintainability","","IWSC '12"
"Conference Paper","Tekin U,Erdemir U,Buzluca F","Mining Object-Oriented Design Models for Detecting Identical Design Structures","","2012","","","43–49","IEEE Press","Zurich, Switzerland","Proceedings of the 6th International Workshop on Software Clones","","2012","9781467317955","","","","The object-oriented design is the most popular design methodology of the last twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs which are particular for the specific application domain. Another common habit is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining based approach to detect identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and design clones, which may help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are the appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source projects show that we can successfully find many identical design structures in each project. We observe that usually most of the identical structures are an implementation of common design patterns; however we also detect various anti-patterns, domain-specific patterns, and design-level clones.","software motifs, software design models, clones, identical design structures, pattern extraction, graph mining","","IWSC '12"
"Conference Paper","Schiller TW,Lucia B","Playing Cupid: The IDE as a Matchmaker for Plug-Ins","","2012","","","1–6","IEEE Press","Zurich, Switzerland","Proceedings of the Second International Workshop on Developing Tools as Plug-Ins","","2012","9781467318204","","","","We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins.We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.","","","TOPI '12"
"Conference Paper","Ghezzi G,Würsch M,Giger E,Gall HC","An Architectural Blueprint for a Pluggable Version Control System for Software (Evolution) Analysis","","2012","","","13–18","IEEE Press","Zurich, Switzerland","Proceedings of the Second International Workshop on Developing Tools as Plug-Ins","","2012","9781467318204","","","","Current version control systems are not built to be systematically analyzed. They have greatly evolved since their first appearance, but their focus has always been towards supporting developers in forward engineering activities. Supporting the analysis of the development history has so far been neglected. A plethora of third party applications have been built to fill this gap. To extract the data needed, they use interfaces that were not built for that. Drawing from our experience in mining and analyzing version control repositories, we propose an architectural blueprint for a plug-in based version control system in which analyses can be directly plugged into it in a flexible and lightweight way, to support both developers and analysts. We show the potential of this approach in three usage scenarios and we also give some examples for these analysis plug-ins.","software evolution, version control systems, mining software repositories","","TOPI '12"
"Conference Paper","Vinco S,Chatterjee D,Bertacco V,Fummi F","SAGA: SystemC Acceleration on GPU Architectures","","2012","","","115–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 49th Annual Design Automation Conference","San Francisco, California","2012","9781450311991","","https://doi.org/10.1145/2228360.2228382;http://dx.doi.org/10.1145/2228360.2228382","10.1145/2228360.2228382","SystemC is a widespread language for HW/SW system simulation and design exploration, and thus a key development platform in embedded system design. However, the growing complexity of SoC designs is having an impact on simulation performance, leading to limited SoC exploration potential, which in turns affects development and verification schedules and time-to-market for new designs. Previous efforts have attempted to parallelize SystemC simulation, targeting both multiprocessors and GPUs. However, for practical designs, those approaches fall far short of satisfactory performance. This paper proposes SAGA, a novel simulation approach that fully exploits the intrinsic parallelism of RTL SystemC descriptions, targeting GPU platforms. By limiting synchronization events with ad-hoc static scheduling and separate independent dataflows, we shows that we can simulate complex SystemC descriptions up to 16 times faster than traditional simulators.","CUDA simulation acceleration, parallel SystemC","","DAC '12"
"Conference Paper","Breckel A","Error Mining: Bug Detection through Comparison with Large Code Databases","","2012","","","175–178","IEEE Press","Zurich, Switzerland","Proceedings of the 9th IEEE Working Conference on Mining Software Repositories","","2012","9781467317610","","","","Bugs are hard to find. Static analysis tools are capable of systematically detecting predefined sets of errors, but extending them to find new error types requires a deep understanding of the underlying programming language. Manual reviews on the other hand, while being able to reveal more individual errors, require much more time. We present a new approach to automatically detect bugs through comparison with a large code database. The source file is analyzed for similar but slightly different code fragments in the database. Frequent occurrences of common differences indicate a potential bug that can be fixed by applying the modification back to the original source file.In this paper, we give an overview of the resulting algorithm and some important implementation details. We further evaluate the circumstances under which good detection rates can be achieved. The results demonstrate that consistently high detection rates of up to 50% are possible for certain error types across different programming languages.","comparison-based bug detection, static analysis, code similarity, code databases","","MSR '12"
"Conference Paper","Keivanloo I,Forbes C,Hmood A,Erfani M,Neal C,Peristerakis G,Rilling J","A Linked Data Platform for Mining Software Repositories","","2012","","","32–35","IEEE Press","Zurich, Switzerland","Proceedings of the 9th IEEE Working Conference on Mining Software Repositories","","2012","9781467317610","","","","The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.","fact sharing, linked data, software mining","","MSR '12"
"Conference Paper","Park J,Kim M,Ray B,Bae DH","An Empirical Study of Supplementary Bug Fixes","","2012","","","40–49","IEEE Press","Zurich, Switzerland","Proceedings of the 9th IEEE Working Conference on Mining Software Repositories","","2012","9781467317610","","","","A recent study finds that errors of omission are harder for programmers to detect than errors of commission. While several change recommendation systems already exist to prevent or reduce omission errors during software development, there have been very few studies on why errors of omission occur in practice and how such errors could be prevented. In order to understand the characteristics of omission errors, this paper investigates a group of bugs that were fixed more than once in open source projects---those bugs whose initial patches were later considered incomplete and to which programmers applied supplementary patches.Our study on Eclipse JDT core, Eclipse SWT, and Mozilla shows that a significant portion of resolved bugs (22% to 33%) involves more than one fix attempt. Our manual inspection shows that the causes of omission errors are diverse, including missed porting changes, incorrect handling of conditional statements, or incomplete refactorings, etc. While many consider that missed updates to code clones often lead to omission errors, only a very small portion of supplementary patches (12% in JDT, 25% in SWT, and 9% in Mozilla) have a content similar to their initial patches. This implies that supplementary change locations cannot be predicted by code clone analysis alone. Furthermore, 14% to 15% of files in supplementary patches are beyond the scope of immediate neighbors of their initial patch locations---they did not overlap with the initial patch locations nor had direct structural dependencies on them (e.g. calls, accesses, subtyping relations, etc.). These results call for new types of omission error prevention approaches that complement existing change recommendation systems.","empirical study, patches, bug fixes, software evolution","","MSR '12"
"Conference Paper","Yoon Y,Myers BA","An Exploratory Study of Backtracking Strategies Used by Developers","","2012","","","138–144","IEEE Press","Zurich, Switzerland","Proceedings of the 5th International Workshop on Co-Operative and Human Aspects of Software Engineering","","2012","9781467318242","","","","Developers frequently backtrack while coding. They go back to an earlier state by removing inserted code or by restoring removed code for various reasons. However, little is known about when and how the developers backtrack, and modern IDEs do not provide much assistance for backtracking. As a first step towards gathering baseline knowledge about backtracking and designing more robust backtracking assistance tools in modern IDEs, we conducted an exploratory study with 12 professional developers and a follow-up online survey. Our study revealed several barriers they faced while backtracking. Subjects often manually commented and uncommented code, and often had difficulty finding relevant parts to backtrack. Backtracking was reported to be needed by 3/4 of the developers at least ""sometimes"".","undo, exploratory programming, component","","CHASE '12"
"Conference Paper","Zhao X,Osterweil LJ","An Approach to Modeling and Supporting the Rework Process in Refactoring","","2012","","","110–119","IEEE Press","Zurich, Switzerland","Proceedings of the International Conference on Software and System Process","","2012","9781467323529","","","","This paper presents the definition of a process for performing rework, and a tool that executes the process in order to support humans seeking help in being sure that they are carrying out rework completely and correctly. The process definition treats rework as the reinstantiation of previously-performed activities in new contexts, which requires the careful specification and management of the values of the artifacts that comprise key process execution history and contextual information. The rework tool exploits access to this information to provide human reworkers with guidance about the rework tasks to be done and with context and history information expected to be useful in guiding superior rework decisions. The paper presents a detailed example of the use of the process and tool in supporting a particular kind of rework, namely the refactoring of the design of an Object-Oriented program.","refactoring, rework, software process","","ICSSP '12"
"Conference Paper","Ubayashi N,Kamei Y","Verifiable Architectural Interface for Supporting Model-Driven Development with Adequate Abstraction Level","","2012","","","15–21","IEEE Press","Zurich, Switzerland","Proceedings of the 4th International Workshop on Modeling in Software Engineering","","2012","9781467317573","","","","It is not easy to design software architecture reflecting the intention of developers and implement the result of design as a program while preserving the architectural correctness and adequate abstraction level. Archface, an architectural interface mechanism, plays a role as an ADL at the design phase and as a programming interface at the implementation phase. Design and code can co-evolve with Archface at the center of the development process. This paper proposes a verifiable architectural interface that can check the traceability between design and code. For this checking, we use an SMT (Satisfiability Modulo Theories) solver, a tool for deciding the satisfiability of logical formulas. Adopting our approach, we can construct MDD tools supporting adequate abstraction level when they generate code, recover a design model from code, and check the traceability between a design model and its code.","architectural point, SMT solver, architecture, architectural interface, bidirectional traceability","","MiSE '12"
"Conference Paper","Choi E,Yoshida N,Inoue K","What Kind of and How Clones Are Refactored? A Case Study of Three OSS Projects","","2012","","","1–7","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifth Workshop on Refactoring Tools","Rapperswil, Switzerland","2012","9781450315005","","https://doi.org/10.1145/2328876.2328877;http://dx.doi.org/10.1145/2328876.2328877","10.1145/2328876.2328877","Although code clone (i.e. a code fragment that has similar or identical fragments) is regarded as one of the most typical bad smells, tools for identification of clone refactoring (i.e. merge code clones into a single method) are not commonly used. To promote the development of more widely-used tools for clone refactoring, we present an investigation of actual clone refactorings performed in the developments of three Open Source Software (OSS) projects. From the results, we confirmed that clone refactorings are mostly archived by two refactoring patterns, and token sequences of refactored code clones are suggested to have a difference of 50%.","Levenshtein distance, refactoring, code clone","","WRT '12"
"Conference Paper","Li H,Thompson S","Let's Make Refactoring Tools User-Extensible!","","2012","","","32–39","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifth Workshop on Refactoring Tools","Rapperswil, Switzerland","2012","9781450315005","","https://doi.org/10.1145/2328876.2328881;http://dx.doi.org/10.1145/2328876.2328881","10.1145/2328876.2328881","We present a framework for making a refactoring tool extensible, allowing users to define refactorings from scratch using the concrete syntax of the language, as well as to describe complex refactorings in a domain-specific language for scripting. We demonstrate the approach in practice through a series of examples.The extension framework is built into Wrangler, a tool for refactoring Erlang programs, but we argue that the approach is equally applicable to tools for other languages.","extensible, program transformation, analysis, API, concrete syntax, DSL, Erlang, Wrangler, refactoring","","WRT '12"
"Journal Article","Rompf T,Odersky M","Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs","Commun. ACM","2012","55","6","121–130","Association for Computing Machinery","New York, NY, USA","","","2012-06","","0001-0782","https://doi.org/10.1145/2184319.2184345;http://dx.doi.org/10.1145/2184319.2184345","10.1145/2184319.2184345","Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources.Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize.We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework.LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.","","",""
"Conference Paper","Stokes AB,Fernandes AA,Paton NW","Resilient Sensor Network Query Processing Using Logical Overlays","","2012","","","45–52","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eleventh ACM International Workshop on Data Engineering for Wireless and Mobile Access","Scottsdale, Arizona","2012","9781450314428","","https://doi.org/10.1145/2258056.2258066;http://dx.doi.org/10.1145/2258056.2258066","10.1145/2258056.2258066","The typical nodes used in mote-level wireless sensor networks (WSNs) are often brittle and severely resource-constrained. In particular, nodes are often battery-powered, thereby making energy depletion a significant risk. When changes to the connectivity graph occur as a result of node failure, the overall computation may collapse unless it is capable of adapting to the new WSN state. Sensor network query processors (SNQPs) construe a WSN as a distributed, continuous query platform where the streams of sensed values constitute the logical extents of interest. Crucially, in the context of this paper, they must make assumptions about the connectivity graph of the WSN at compile time that are likely not to hold for the lifetime of the compiled query evaluation plan (QEP) the SNQPs generate. This paper addresses the problem of extending the lifetime of an evaluating QEP in the event of node failures. The basic idea is to derive an equivalence class over the nodes in the WSN that are equipotent for a given QEP and then to assign each QEP fragment instance to a set of equipotent nodes (rather than a single one). In this respect, the scheduling of QEP fragment instances is onto an overlay network of logical nodes, each of which maps to many physical nodes in the connectivity graph. We contribute a description of how this approach has been implemented in an existing SNQP and present experimental results indicating that it significantly increases the overall lifetime of a query whilst incurring small runtime adaptation costs.","wireless sensor networks, sensor network query processors, resilience","","MobiDE '12"
"Journal Article","Counsell S,Swift S","Issues Arising from Refactoring Studies: An Experience Report","SIGSOFT Softw. Eng. Notes","2012","37","3","1–5","Association for Computing Machinery","New York, NY, USA","","","2012-05","","0163-5948","https://doi.org/10.1145/2180921.2180922;http://dx.doi.org/10.1145/2180921.2180922","10.1145/2180921.2180922","In theory, refactoring should reverse the trend in code decay and many studies have explored the different facets of refactoring (both its trends and characteristics). While much progress has been made in this area, a number of observations about refactoring studies have become evident to us over the past seven years in the time during which we have been undertaking empirical studies in this area. This paper outlines our experiences of the issues that arise with refactoring studies. We outline six of those issues, together forming the set of challenges that are still prevalent in this area. The purpose of the paper is thus to put under the spotlight the real potential benefits of refactoring, but more importantly the challenges that our experiences have raised","refactoring, empirical studies, OO","",""
"Conference Paper","Banabic R,Candea G","Fast Black-Box Testing of System Recovery Code","","2012","","","281–294","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th ACM European Conference on Computer Systems","Bern, Switzerland","2012","9781450312233","","https://doi.org/10.1145/2168836.2168865;http://dx.doi.org/10.1145/2168836.2168865","10.1145/2168836.2168865","Fault injection---a key technique for testing the robustness of software systems---ends up rarely being used in practice, because it is labor-intensive and one needs to choose between performing random injections (which leads to poor coverage and low representativeness) or systematic testing (which takes a long time to wade through large fault spaces). As a result, testers of systems with high reliability requirements, such as MySQL, perform fault injection in an ad-hoc manner, using explicitly-coded injection statements in the base source code and manual triggering of failures.This paper introduces AFEX, a technique and tool for automating the entire fault injection process, from choosing the faults to inject, to setting up the environment, performing the injections, and finally characterizing the results of the tests (e.g., in terms of impact, coverage, and redundancy). The AFEX approach uses a metric-driven search algorithm that aims to maximize the number of bugs discovered in a fixed amount of time. We applied AFEX to real-world systems---MySQL, Apache httpd, UNIX utilities, and MongoDB---and it uncovered new bugs automatically in considerably less time than other black-box approaches.","automated testing, fault injection, testing","","EuroSys '12"
"Journal Article","Singh S,Kahlon KS","Effectiveness of Refactoring Metrics Model to Identify Smelly and Error Prone Classes in Open Source Software","SIGSOFT Softw. Eng. Notes","2012","37","2","1–11","Association for Computing Machinery","New York, NY, USA","","","2012-04","","0163-5948","https://doi.org/10.1145/2108144.2108157;http://dx.doi.org/10.1145/2108144.2108157","10.1145/2108144.2108157","In order to improve software maintainability, possible improvement efforts must be made measurable. One such effort is refactoring the code which makes the code easier to read, understand and maintain. It is done by identifying the bad smell area in the code. This paper presents the results of an empirical study to develop a metrics model to identify the smelly classes. In addition, this metrics model is validated by identifying the smelly and error prone classes. The role of two new metrics (encapsulation and information hiding) is also investigated for identifying smelly and faulty classes in software code. This paper first presents a binary statistical analysis of the relationship between metrics and bad smells, the results of which show a significant relationship. Then, the metrics model (with significant metrics shortlisted from the binary analysis) for bad smell categorization (divided into five categories) is developed. To develop the model, three releases of the open source Mozila Firefox system are examined and the model is validated on one version of Mozila Sea Monkey, which has a strong industrial usage. The results show that metrics can predict smelly and faulty classes with high accuracy, but in the case of the categorized model, not all categories of bad smells can adequately be identified. Further, few categorised models can predict the faulty classes. Based on these results, we recommend more training for our model.","bad smells, refactoring, evolution, information hiding, empirical analysis, encapsulation","",""
"Journal Article","Eisenberg RJ","A Threshold Based Approach to Technical Debt","SIGSOFT Softw. Eng. Notes","2012","37","2","1–6","Association for Computing Machinery","New York, NY, USA","","","2012-04","","0163-5948","https://doi.org/10.1145/2108144.2108151;http://dx.doi.org/10.1145/2108144.2108151","10.1145/2108144.2108151","Nearly two decades ago, Ward Cunningham introduced us to the term ""technical debt"" as a means of describing the long term costs associated with a suboptimal software design and implementation. For most programs, especially those with a large legacy code baseline, achieving zero absolute debt is an unnecessary and unrealistic goal. It is important to recall that a primary reason for managing and eliminating debt is to drive down maintenance costs and to reduce defects. A sufficiently low, manageable level of debt can minimize the long-term impact, i.e., ""low debt interest payments"". In this article, we define an approach for establishing program specific thresholds to define manageable levels of technical debt.","technical debt, software quality, risk management, cost estimation","",""
"Conference Paper","Tribbey W,Mitropoulos F","Construction and Analysis of Vector Space Models for Use in Aspect Mining","","2012","","","220–225","Association for Computing Machinery","New York, NY, USA","Proceedings of the 50th Annual Southeast Regional Conference","Tuscaloosa, Alabama","2012","9781450312035","","https://doi.org/10.1145/2184512.2184564;http://dx.doi.org/10.1145/2184512.2184564","10.1145/2184512.2184564","A software system is designed so that its concerns are as independent as possible. Concerns upon which other concerns depend are called crosscutting concerns, examples of which are logging, authentication, and session management. Crosscutting concerns in a software system have the potential to increase the number of defects over time as the system is evolved. Aspect-oriented programming provides an additional layer of abstraction to the object-oriented programming paradigm for the purpose of separating concerns. The search for crosscutting concerns is referred to as aspect mining.Previous aspect mining algorithms used aggregated metric values as components in the vector space model. In this paper a new method for constructing vector space models is proposed that attempts to retain the detail present in the relationships between the elements of a software application. This is done through the use of pattern matrices derived from the non-aggregated metrics. The non-aggregated vector space models are then used in a clustering-based aspect mining algorithm and their performance is evaluated. The results show that this new approach to constructing vector space models is a viable one but needs further investigation. Issues with current measures for evaluating clustering-based aspect mining algorithms are highlighted and directions for further research are given.","aspect-oriented software development, refactoring, aspect mining, data mining, software evolution, clustering","","ACM-SE '12"
"Conference Paper","Mondal M,Roy CK,Rahman MS,Saha RK,Krinke J,Schneider KA","Comparative Stability of Cloned and Non-Cloned Code: An Empirical Study","","2012","","","1227–1234","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th Annual ACM Symposium on Applied Computing","Trento, Italy","2012","9781450308571","","https://doi.org/10.1145/2245276.2231969;http://dx.doi.org/10.1145/2245276.2231969","10.1145/2245276.2231969","Code cloning is a controversial software engineering practice due to contradictory claims regarding its effect on software maintenance. Code stability is a recently introduced measurement technique that has been used to determine the impact of code cloning by quantifying the changeability of a code region. Although most of the existing stability analysis studies agree that cloned code is more stable than non-cloned code, the studies have two major flaws: (i) each study only considered a single stability measurement (e.g., lines of code changed, frequency of change, age of change); and, (ii) only a small number of subject systems were analyzed and these were of limited variety.In this paper, we present a comprehensive empirical study on code stability using three different stability measuring methods. We use a recently introduced hybrid clone detection tool, NiCAD, to detect the clones and analyze their stability in four dimensions: by clone type, by measuring method, by programming language, and by system size and age. Our four-dimensional investigation on 12 diverse subject systems written in three programming languages considering three clone types reveals that: (i) Type-1 and Type-2 clones are unstable, but Type-3 clones are not; (ii) clones in Java and C systems are not as stable as clones in C# systems; (iii) a system's development strategy might play a key role in defining its comparative code stability scenario; and, (iv) cloned and non-cloned regions of a subject system do not follow a consistent change pattern.","average age, clone types, modification frequency, average last change date, code stability","","SAC '12"
"Conference Paper","Zibran MF,Roy CK","IDE-Based Real-Time Focused Search for near-Miss Clones","","2012","","","1235–1242","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th Annual ACM Symposium on Applied Computing","Trento, Italy","2012","9781450308571","","https://doi.org/10.1145/2245276.2231970;http://dx.doi.org/10.1145/2245276.2231970","10.1145/2245276.2231970","Code clone is a well-known code smell that needs to be detected and managed during the software development process. However, the existing clone detectors have one or more of the three shortcomings: (a) limitation in detecting Type-3 clones, (b) they come as stand-alone tools separate from IDE and thus cannot support clone-aware development, (c) they overwhelm the developer with all clones from the entire code-base, instead of a focused search for clones of a selected code segment of the developer's interest.This paper presents our IDE-integrated clone search tool, that addresses all the above issues. For clone detection, we adapt a suffix-tree-based hybrid algorithm. Through an asymptotic analysis, we show that our approach for clone detection is both time and memory efficient. Moreover, using three separate empirical studies, we demonstrate that our tool is flexibly usable for searching exact (Type-1) and near-miss (Type-2 and Type-3) clones with high precision and recall.","reengineering, clone detection, clone search, maintenance","","SAC '12"
"Conference Paper","Ubayashi N,Kamei Y","Architectural Point Mapping for Design Traceability","","2012","","","39–44","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eleventh Workshop on Foundations of Aspect-Oriented Languages","Potsdam, Germany","2012","9781450310994","","https://doi.org/10.1145/2162010.2162022;http://dx.doi.org/10.1145/2162010.2162022","10.1145/2162010.2162022","AOP can be applied to not only modularization of crosscutting concerns but also other kinds of software development processes. As one of the applications, this paper proposes a design traceability mechanism originating in join points and pointcuts. It is not easy to design software architecture reflecting the intention of developers and implement the result of design as a program while preserving the architectural correctness. To deal with this problem, we propose two novel ideas: Archpoint (Architectural point) and Archmapping (Archpoint Mapping). Archpoints are points for representing the essence of architectural design in terms of behavioral and structural aspects. By defining a set of archpoints, we can describe the inter-component structure and the message interaction among components. Archmapping is a mechanism for checking the bidirectional traceability between design and code. The traceability can be verified by checking whether archpoints in design are consistently mapped to program points in code. For this checking, we use an SMT (Satisfiability Modulo Theories) solver, a tool for deciding the satisfiability of logical formulas. The idea of archpoints, program points, and their selection originates in AOP.","design traceability, smt solver","","FOAL '12"
"Conference Paper","Makanju A,Zincir-Heywood AN,Milios EE,Latzel M","Spatio-Temporal Decomposition, Clustering and Identification for Alert Detection in System Logs","","2012","","","621–628","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th Annual ACM Symposium on Applied Computing","Trento, Italy","2012","9781450308571","","https://doi.org/10.1145/2245276.2245395;http://dx.doi.org/10.1145/2245276.2245395","10.1145/2245276.2245395","In this work, we propose an approach based on analyzing the spatio-temporal partitions of a system log, generated by supercomputers consisting of several nodes, for alert detection without employing semantic analysis. In this case, ""Spatial"" refers to the source of the log event and ""Temporal"" refers to the time the log event was reported. Our research shows that these spatio-temporal partitions can be clustered to separate normal activity from anomalous activity, with high accuracy. Therefore, our proposed method provides an effective alert detection mechanism.","fault management, systems administration, event log mining, network control and management","","SAC '12"
"Conference Paper","Giunta R,Pappalardo G,Tramontana E","AODP: Refactoring Code to Provide Advanced Aspect-Oriented Modularization of Design Patterns","","2012","","","1243–1250","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th Annual ACM Symposium on Applied Computing","Trento, Italy","2012","9781450308571","","https://doi.org/10.1145/2245276.2231971;http://dx.doi.org/10.1145/2245276.2231971","10.1145/2245276.2231971","Although solutions provided by design patterns are an invaluable resource for developers, some design patterns lead to placing code addressing design pattern concerns into the same class as application code. This weakens the modularity of an application because it makes classes more complex, more prone to changes and less reusable.In order to avoid the tangling of design pattern- and application-related code within classes, this paper proposes an approach for assisting the refactoring of an application that uses design patterns into an aspect-based version. This allows application classes, and aspects implementing design patterns, to stay independent of each other, thus greatly enhancing modularity. Developers intending to change the role of an application class need only update the code connecting it to the design pattern involved.","","","SAC '12"
"Conference Paper","Chiba S,Horie M,Kanazawa K,Takeyama F,Teramoto Y","Do We Really Need to Extend Syntax for Advanced Modularity?","","2012","","","95–106","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development","Potsdam, Germany","2012","9781450310925","","https://doi.org/10.1145/2162049.2162061;http://dx.doi.org/10.1145/2162049.2162061","10.1145/2162049.2162061","For every new language construct (or abstraction), we have been always developing new syntax. Is this a right approach? In this paper, we propose that, if we develop a new language construct for advanced modularity, we should consider the use of dynamic text for designing the construct. We mention that language constructs designed with only syntactic extensions (i.e. static text) are not satisfactory in aspect oriented programming. Then we present our two prototype systems to demonstrate language constructs designed with dynamic text. One is synchronous copy and paste and the other is a virtual-file editor named Kide. We show how they enable aspect-oriented programming in plain Java.","aspect-oriented programming, modularity, dynamic text","","AOSD '12"
"Conference Paper","Axelsen EW,Krogdahl S","Adaptable Generic Programming with Required Type Specifications and Package Templates","","2012","","","83–94","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th Annual International Conference on Aspect-Oriented Software Development","Potsdam, Germany","2012","9781450310925","","https://doi.org/10.1145/2162049.2162060;http://dx.doi.org/10.1145/2162049.2162060","10.1145/2162049.2162060","The aim of this work is to provide better support for adaption and refinement of generic code. This type of flexibility is desirable in order to fully reap the potential of generic programming. Our proposal for an improved mechanism is an extension to the previously published Package Templates (PT) mechanism, which is designed for development of reusable modules that can be adapted to their specific purpose when used in a program. The PT mechanism relies on compile-time specialization, and supports separate type checking and type-safe composition of modules. The extension to PT presented here is called required types, and can be seen as an enhanced form of type parameters, allowing them the same flexibility as other elements of the PT mechanism. We implement a subset of the Boost Graph Library in order to exemplify, validate, and compare our approach to other options.","reuse, templates, generic programming","","AOSD '12"
"Conference Paper","Kadav A,Swift MM","Understanding Modern Device Drivers","","2012","","","87–98","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems","London, England, UK","2012","9781450307598","","https://doi.org/10.1145/2150976.2150987;http://dx.doi.org/10.1145/2150976.2150987","10.1145/2150976.2150987","Device drivers are the single largest contributor to operating-system kernel code with over 5 million lines of code in the Linux kernel, and cause significant complexity, bugs and development costs. Recent years have seen a flurry of research aimed at improving the reliability and simplifying the development of drivers. However, little is known about what constitutes this huge body of code beyond the small set of drivers used for research.In this paper, we study the source code of Linux drivers to understand what drivers actually do, how current research applies to them and what opportunities exist for future research. We determine whether assumptions made by most driver research, such as that all drivers belong to a class, are indeed true. We also analyze driver code and abstractions to determine whether drivers can benefit from code re-organization or hardware trends. We develop a set of static-analysis tools to analyze driver code across various axes. Broadly, our study looks at three aspects of driver code (i) what are the characteristics of driver code functionality and how applicable is driver research to all drivers, (ii) how do drivers interact with the kernel, devices, and buses, and (iii) are there similarities that can be abstracted into libraries to reduce driver size and complexity?We find that many assumptions made by driver research do not apply to all drivers. At least 44% of drivers have code that is not captured by a class definition, 28% of drivers support more than one device per driver, and 15% of drivers do significant computation over data. From the driver interactions study, we find USB bus offers an efficient bus interface with significant standardized code and coarse-grained access, ideal for executing drivers in isolation. We also find that drivers for different buses and classes have widely varying levels of device interaction, which indicates that the cost of isolation will vary by class. Finally, from our driver similarity study, we find 8% of all driver code is substantially similar to code elsewhere and may be removed with new abstractions or libraries.","measurement, device drivers","","ASPLOS XVII"
"Journal Article","Kadav A,Swift MM","Understanding Modern Device Drivers","SIGARCH Comput. Archit. News","2012","40","1","87–98","Association for Computing Machinery","New York, NY, USA","","","2012-03","","0163-5964","https://doi.org/10.1145/2189750.2150987;http://dx.doi.org/10.1145/2189750.2150987","10.1145/2189750.2150987","Device drivers are the single largest contributor to operating-system kernel code with over 5 million lines of code in the Linux kernel, and cause significant complexity, bugs and development costs. Recent years have seen a flurry of research aimed at improving the reliability and simplifying the development of drivers. However, little is known about what constitutes this huge body of code beyond the small set of drivers used for research.In this paper, we study the source code of Linux drivers to understand what drivers actually do, how current research applies to them and what opportunities exist for future research. We determine whether assumptions made by most driver research, such as that all drivers belong to a class, are indeed true. We also analyze driver code and abstractions to determine whether drivers can benefit from code re-organization or hardware trends. We develop a set of static-analysis tools to analyze driver code across various axes. Broadly, our study looks at three aspects of driver code (i) what are the characteristics of driver code functionality and how applicable is driver research to all drivers, (ii) how do drivers interact with the kernel, devices, and buses, and (iii) are there similarities that can be abstracted into libraries to reduce driver size and complexity?We find that many assumptions made by driver research do not apply to all drivers. At least 44% of drivers have code that is not captured by a class definition, 28% of drivers support more than one device per driver, and 15% of drivers do significant computation over data. From the driver interactions study, we find USB bus offers an efficient bus interface with significant standardized code and coarse-grained access, ideal for executing drivers in isolation. We also find that drivers for different buses and classes have widely varying levels of device interaction, which indicates that the cost of isolation will vary by class. Finally, from our driver similarity study, we find 8% of all driver code is substantially similar to code elsewhere and may be removed with new abstractions or libraries.","measurement, device drivers","",""
"Journal Article","Kadav A,Swift MM","Understanding Modern Device Drivers","SIGPLAN Not.","2012","47","4","87–98","Association for Computing Machinery","New York, NY, USA","","","2012-03","","0362-1340","https://doi.org/10.1145/2248487.2150987;http://dx.doi.org/10.1145/2248487.2150987","10.1145/2248487.2150987","Device drivers are the single largest contributor to operating-system kernel code with over 5 million lines of code in the Linux kernel, and cause significant complexity, bugs and development costs. Recent years have seen a flurry of research aimed at improving the reliability and simplifying the development of drivers. However, little is known about what constitutes this huge body of code beyond the small set of drivers used for research.In this paper, we study the source code of Linux drivers to understand what drivers actually do, how current research applies to them and what opportunities exist for future research. We determine whether assumptions made by most driver research, such as that all drivers belong to a class, are indeed true. We also analyze driver code and abstractions to determine whether drivers can benefit from code re-organization or hardware trends. We develop a set of static-analysis tools to analyze driver code across various axes. Broadly, our study looks at three aspects of driver code (i) what are the characteristics of driver code functionality and how applicable is driver research to all drivers, (ii) how do drivers interact with the kernel, devices, and buses, and (iii) are there similarities that can be abstracted into libraries to reduce driver size and complexity?We find that many assumptions made by driver research do not apply to all drivers. At least 44% of drivers have code that is not captured by a class definition, 28% of drivers support more than one device per driver, and 15% of drivers do significant computation over data. From the driver interactions study, we find USB bus offers an efficient bus interface with significant standardized code and coarse-grained access, ideal for executing drivers in isolation. We also find that drivers for different buses and classes have widely varying levels of device interaction, which indicates that the cost of isolation will vary by class. Finally, from our driver similarity study, we find 8% of all driver code is substantially similar to code elsewhere and may be removed with new abstractions or libraries.","measurement, device drivers","",""
"Conference Paper","Hari SK,Adve SV,Naeimi H,Ramachandran P","Relyzer: Exploiting Application-Level Fault Equivalence to Analyze Application Resiliency to Transient Faults","","2012","","","123–134","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems","London, England, UK","2012","9781450307598","","https://doi.org/10.1145/2150976.2150990;http://dx.doi.org/10.1145/2150976.2150990","10.1145/2150976.2150990","Future microprocessors need low-cost solutions for reliable operation in the presence of failure-prone devices. A promising approach is to detect hardware faults by deploying low-cost monitors of software-level symptoms of such faults. Recently, researchers have shown these mechanisms work well, but there remains a non-negligible risk that several faults may escape the symptom detectors and result in silent data corruptions (SDCs). Most prior evaluations of symptom-based detectors perform fault injection campaigns on application benchmarks, where each run simulates the impact of a fault injected at a hardware site at a certain point in the application's execution (application fault site). Since the total number of application fault sites is very large (trillions for standard benchmark suites), it is not feasible to study all possible faults. Previous work therefore typically studies a randomly selected sample of faults. Such studies do not provide any feedback on the portions of the application where faults were not injected. Some of those instructions may be vulnerable to SDCs, and identifying them could allow protecting them through other means if needed.This paper presents Relyzer, an approach that systematically analyzes all application fault sites and carefully picks a small subset to perform selective fault injections for transient faults. Relyzer employs novel fault pruning techniques that prune faults that need detailed study by either predicting their outcomes or showing them equivalent to other faults. We find that Relyzer prunes about 99.78% of the total faults across twelve applications studied here, reducing the faults that require detailed simulation by 3 to 5 orders of magnitude for most of the applications. Fault injection simulations on the remaining faults can identify SDC causing faults in the entire application. Some of Relyzer's techniques rely on heuristics to determine fault equivalence. Our validation efforts show that Relyzer determines fault outcomes with 96% accuracy, averaged across all the applications studied here.","silent data corruption, transient faults, architecture, hardware reliability evaluation, low-cost hardware resiliency","","ASPLOS XVII"
"Journal Article","Hari SK,Adve SV,Naeimi H,Ramachandran P","Relyzer: Exploiting Application-Level Fault Equivalence to Analyze Application Resiliency to Transient Faults","SIGARCH Comput. Archit. News","2012","40","1","123–134","Association for Computing Machinery","New York, NY, USA","","","2012-03","","0163-5964","https://doi.org/10.1145/2189750.2150990;http://dx.doi.org/10.1145/2189750.2150990","10.1145/2189750.2150990","Future microprocessors need low-cost solutions for reliable operation in the presence of failure-prone devices. A promising approach is to detect hardware faults by deploying low-cost monitors of software-level symptoms of such faults. Recently, researchers have shown these mechanisms work well, but there remains a non-negligible risk that several faults may escape the symptom detectors and result in silent data corruptions (SDCs). Most prior evaluations of symptom-based detectors perform fault injection campaigns on application benchmarks, where each run simulates the impact of a fault injected at a hardware site at a certain point in the application's execution (application fault site). Since the total number of application fault sites is very large (trillions for standard benchmark suites), it is not feasible to study all possible faults. Previous work therefore typically studies a randomly selected sample of faults. Such studies do not provide any feedback on the portions of the application where faults were not injected. Some of those instructions may be vulnerable to SDCs, and identifying them could allow protecting them through other means if needed.This paper presents Relyzer, an approach that systematically analyzes all application fault sites and carefully picks a small subset to perform selective fault injections for transient faults. Relyzer employs novel fault pruning techniques that prune faults that need detailed study by either predicting their outcomes or showing them equivalent to other faults. We find that Relyzer prunes about 99.78% of the total faults across twelve applications studied here, reducing the faults that require detailed simulation by 3 to 5 orders of magnitude for most of the applications. Fault injection simulations on the remaining faults can identify SDC causing faults in the entire application. Some of Relyzer's techniques rely on heuristics to determine fault equivalence. Our validation efforts show that Relyzer determines fault outcomes with 96% accuracy, averaged across all the applications studied here.","low-cost hardware resiliency, hardware reliability evaluation, architecture, transient faults, silent data corruption","",""
"Journal Article","Hari SK,Adve SV,Naeimi H,Ramachandran P","Relyzer: Exploiting Application-Level Fault Equivalence to Analyze Application Resiliency to Transient Faults","SIGPLAN Not.","2012","47","4","123–134","Association for Computing Machinery","New York, NY, USA","","","2012-03","","0362-1340","https://doi.org/10.1145/2248487.2150990;http://dx.doi.org/10.1145/2248487.2150990","10.1145/2248487.2150990","Future microprocessors need low-cost solutions for reliable operation in the presence of failure-prone devices. A promising approach is to detect hardware faults by deploying low-cost monitors of software-level symptoms of such faults. Recently, researchers have shown these mechanisms work well, but there remains a non-negligible risk that several faults may escape the symptom detectors and result in silent data corruptions (SDCs). Most prior evaluations of symptom-based detectors perform fault injection campaigns on application benchmarks, where each run simulates the impact of a fault injected at a hardware site at a certain point in the application's execution (application fault site). Since the total number of application fault sites is very large (trillions for standard benchmark suites), it is not feasible to study all possible faults. Previous work therefore typically studies a randomly selected sample of faults. Such studies do not provide any feedback on the portions of the application where faults were not injected. Some of those instructions may be vulnerable to SDCs, and identifying them could allow protecting them through other means if needed.This paper presents Relyzer, an approach that systematically analyzes all application fault sites and carefully picks a small subset to perform selective fault injections for transient faults. Relyzer employs novel fault pruning techniques that prune faults that need detailed study by either predicting their outcomes or showing them equivalent to other faults. We find that Relyzer prunes about 99.78% of the total faults across twelve applications studied here, reducing the faults that require detailed simulation by 3 to 5 orders of magnitude for most of the applications. Fault injection simulations on the remaining faults can identify SDC causing faults in the entire application. Some of Relyzer's techniques rely on heuristics to determine fault equivalence. Our validation efforts show that Relyzer determines fault outcomes with 96% accuracy, averaged across all the applications studied here.","transient faults, silent data corruption, hardware reliability evaluation, low-cost hardware resiliency, architecture","",""
"Conference Paper","Zaremba W,Lin Y,Grover V","JaBEE: Framework for Object-Oriented Java Bytecode Compilation and Execution on Graphics Processor Units","","2012","","","74–83","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units","London, United Kingdom","2012","9781450312332","","https://doi.org/10.1145/2159430.2159439;http://dx.doi.org/10.1145/2159430.2159439","10.1145/2159430.2159439","There is an increasing interest from software developers in executing Java and .NET bytecode programs on General Purpose Graphics Processor Units (GPGPUs). Existing solutions have limited support for operations on objects and often require explicit handling of memory transfers between CPU and GPU. In this paper, we describe a Java Bytecode Execution Environment (JaBEE) which supports common object-oriented constructs such as dynamic dispatch, encapsulation and object creation on GPUs. This experimental environment facilitates GPU code compilation, execution and transparent memory management. We compare the performance of our approach with CPU-based and CUDA-C-based code executions of the same programs. We discuss challenges, limitations and opportunities of bytecode execution on GPGPUs.","bytecode, translation, virtual table, compilation, dynamic compilation, PTX, SIMD, embedded language, CUDA, VMKit, Java","","GPGPU-5"
"Journal Article","Mariani L,Micucci D","AuDeNTES: Automatic Detection of TeNtative Plagiarism According to a REference Solution","ACM Trans. Comput. Educ.","2012","12","1","","Association for Computing Machinery","New York, NY, USA","","","2012-03","","","https://doi.org/10.1145/2133797.2133799;http://dx.doi.org/10.1145/2133797.2133799","10.1145/2133797.2133799","In academic courses, students frequently take advantage of someone else’s work to improve their own evaluations or grades. This unethical behavior seriously threatens the integrity of the academic system, and teachers invest substantial effort in preventing and recognizing plagiarism.When students take examinations requiring the production of computer programs, plagiarism detection can be semiautomated using analysis techniques such as JPlag and Moss. These techniques are useful but lose effectiveness when the text of the exam suggests some of the elements that should be structurally part of the solution. A loss of effectiveness is caused by the many common parts that are shared between programs due to the suggestions in the text of the exam rather than plagiarism.In this article, we present the AuDeNTES anti-plagiarism technique. AuDeNTES detects plagiarism via the code fragments that better represent the individual students’ contributions by filtering from students’ submissions the parts that might be common to many students due to the suggestions in the text of the exam. The filtered parts are identified by comparing students’ submissions against a reference solution, which is a solution of the exam developed by the teachers. Specifically, AuDeNTES first produces tokenized versions of both the reference solution and the programs that must be analyzed. Then, AuDeNTES removes from the tokenized programs the tokens that are included in the tokenized reference solution. Finally, AuDeNTES computes the similarity among the filtered tokenized programs and produces a ranked list of program pairs suspected of plagiarism.An empirical comparison against multiple state-of-the-art plagiarism detection techniques using several sets of real students’ programs collected in early programming courses demonstrated that AuDeNTES identifies more plagiarism cases than the other techniques at the cost of a small additional inspection effort.","software plagiarism, token-based analysis, clone detection, Plagiarism detection","",""
"Conference Paper","Piech C,Sahami M,Koller D,Cooper S,Blikstein P","Modeling How Students Learn to Program","","2012","","","153–160","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd ACM Technical Symposium on Computer Science Education","Raleigh, North Carolina, USA","2012","9781450310987","","https://doi.org/10.1145/2157136.2157182;http://dx.doi.org/10.1145/2157136.2157182","10.1145/2157136.2157182","Despite the potential wealth of educational indicators expressed in a student's approach to homework assignments, how students arrive at their final solution is largely overlooked in university courses. In this paper we present a methodology which uses machine learning techniques to autonomously create a graphical model of how students in an introductory programming course progress through a homework assignment. We subsequently show that this model is predictive of which students will struggle with material presented later in the class.","intelligent tutor, program dissimilarity metric, hidden Markov model, student progress model, probabilistic graphical models","","SIGCSE '12"
"Conference Paper","Murphy L,McCauley R,Fitzgerald S","'Explain in Plain English' Questions: Implications for Teaching","","2012","","","385–390","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd ACM Technical Symposium on Computer Science Education","Raleigh, North Carolina, USA","2012","9781450310987","","https://doi.org/10.1145/2157136.2157249;http://dx.doi.org/10.1145/2157136.2157249","10.1145/2157136.2157249","This paper reports on the replication of a study of novice programmers, looking for relationships between ability to 'explain in plain English' the meaning of a code segment and success in writing code later in the semester. This study explores the question in a different learning environment and qualitatively evaluates 'explain in plain English' responses to identify implications for teaching. Statistical results from this study are similar to those of the earlier work. Results highlight students' fragile knowledge, particularly for students excluded from the primary analyses by a set of screening questions, and suggest the need for assessment and instruction of basic concepts later into the term than instructors are likely to expect.","mixed methods, qualitative research methods, computer science education research, explain in plain English","","SIGCSE '12"
"Conference Paper","Teague D,Corney M,Ahadi A,Lister R","Swapping as the ""Hello World"" of Relational Reasoning: Replications, Reflections and Extensions","","2012","","","87–94","Australian Computer Society, Inc.","AUS","Proceedings of the Fourteenth Australasian Computing Education Conference - Volume 123","Melbourne, Australia","2012","9781921770043","","","","At the previous conference in this series, Corney, Lister and Teague presented research results showing relationships between code writing, code tracing and code explaining, from as early as week 3 of semester. We concluded that the problems some students face in learning to program start very early in the semester. In this paper we report on our replication of that experiment, at two institutions, where one is the same as the original institution. In some cases, we did not find the same relationship between explaining code and writing code, but we believe this was because our teachers discussed the code in lectures between the two tests. Apart from that exception, our replication results at both institutions are consistent with our original study.","novice programmer, writing, explaining, tracing","","ACE '12"
"Journal Article","Cleemput JV,Coppens B,De Sutter B","Compiler Mitigations for Time Attacks on Modern X86 Processors","ACM Trans. Archit. Code Optim.","2012","8","4","","Association for Computing Machinery","New York, NY, USA","","","2012-01","","1544-3566","https://doi.org/10.1145/2086696.2086702;http://dx.doi.org/10.1145/2086696.2086702","10.1145/2086696.2086702","This paper studies and evaluates the extent to which automated compiler techniques can defend against timing-based side channel attacks on modern x86 processors. We study how modern x86 processors can leak timing information through side channels that relate to data flow. We study the efficiency, effectiveness, portability, predictability and sensitivity of several mitigating code transformations that eliminate or minimize key-dependent execution time variations. Furthermore, we discuss the extent to which compiler backends are a suitable tool to provide automated support for the proposed mitigations.","variable latency instructions, Time-based side channels, x86 architecture","",""
"Journal Article","Demme J,Sethumadhavan S","Approximate Graph Clustering for Program Characterization","ACM Trans. Archit. Code Optim.","2012","8","4","","Association for Computing Machinery","New York, NY, USA","","","2012-01","","1544-3566","https://doi.org/10.1145/2086696.2086700;http://dx.doi.org/10.1145/2086696.2086700","10.1145/2086696.2086700","An important aspect of system optimization research is the discovery of program traits or behaviors. In this paper, we present an automated method of program characterization which is able to examine and cluster program graphs, i.e., dynamic data graphs or control flow graphs. Our novel approximate graph clustering technology allows users to find groups of program fragments which contain similar code idioms or patterns in data reuse, control flow, and context. Patterns of this nature have several potential applications including development of new static or dynamic optimizations to be implemented in software or in hardware.For the SPEC CPU 2006 suite of benchmarks, our results show that approximate graph clustering is effective at grouping behaviorally similar functions. Graph based clustering also produces clusters that are more homogeneous than previously proposed non-graph based clustering methods. Further qualitative analysis of the clustered functions shows that our approach is also able to identify some frequent unexploited program behaviors. These results suggest that our approximate graph clustering methods could be very useful for program characterization.","","",""
"Journal Article","Stock K,Pouchet LN,Sadayappan P","Using Machine Learning to Improve Automatic Vectorization","ACM Trans. Archit. Code Optim.","2012","8","4","","Association for Computing Machinery","New York, NY, USA","","","2012-01","","1544-3566","https://doi.org/10.1145/2086696.2086729;http://dx.doi.org/10.1145/2086696.2086729","10.1145/2086696.2086729","Automatic vectorization is critical to enhancing performance of compute-intensive programs on modern processors. However, there is much room for improvement over the auto-vectorization capabilities of current production compilers through careful vector-code synthesis that utilizes a variety of loop transformations (e.g., unroll-and-jam, interchange, etc.).As the set of transformations considered is increased, the selection of the most effective combination of transformations becomes a significant challenge: Currently used cost models in vectorizing compilers are often unable to identify the best choices. In this paper, we address this problem using machine learning models to predict the performance of SIMD codes. In contrast to existing approaches that have used high-level features of the program, we develop machine learning models based on features extracted from the generated assembly code. The models are trained offline on a number of benchmarks and used at compile-time to discriminate between numerous possible vectorized variants generated from the input code.We demonstrate the effectiveness of the machine learning model by using it to guide automatic vectorization on a variety of tensor contraction kernels, with improvements ranging from 2× to 8× over Intel ICC's auto-vectorized code. We also evaluate the effectiveness of the model on a number of stencil computations and show good improvement over auto-vectorized code.","","",""
"Conference Paper","Schulze S,Thüm T,Kuhlemann M,Saake G","Variant-Preserving Refactoring in Feature-Oriented Software Product Lines","","2012","","","73–81","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems","Leipzig, Germany","2012","9781450310581","","https://doi.org/10.1145/2110147.2110156;http://dx.doi.org/10.1145/2110147.2110156","10.1145/2110147.2110156","A software product line (SPL) is an advanced concept to manage a family of programs under one umbrella. As with stand-alone programs, maintenance is an important challenge within SPL engineering. One pivotal activity during software maintenance is refactoring; that is, restructuring a program's source code while preserving its external behavior. However, for SPLs, this definition is not sufficient because it does not take into account the behavior of a set of programs. In this paper, we focus on the specific requirements for applying refactorings in feature-oriented SPLs. We propose variant-preserving refactoring for such SPLs to ensure the validity of all SPL variants after refactoring. Furthermore, we present a first approach how the traditional refactoring definition can be extended so that it can be applied to SPLs based on feature-oriented programming. Finally, we state our experiences of applying such refactorings for the removal of code clones in feature-oriented SPLs and discuss the generalizability for other SPL implementation techniques.","refactoring, software product lines, code clones, feature-oriented programming","","VaMoS '12"
"Conference Paper","Tamayo A,Granell C,Huerta J","Instance-Based XML Data Binding for Mobile Devices","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third International Workshop on Middleware for Pervasive Mobile and Embedded Computing","Lisbon, Portugal","2011","9781450310659","","https://doi.org/10.1145/2090316.2090318;http://dx.doi.org/10.1145/2090316.2090318","10.1145/2090316.2090318","XML and XML Schema are widely used in different domains for the definition of standards that enhance the interoperability between parts exchanging information through the Internet. The size and complexity of some standards, and their associated schemas, have been growing with time as new use case scenarios and data models are added to them. The common approach to deal with the complexity of producing XML processing code based on these schemas is the use of XML data binding generators. Unfortunately, these tools do not always produce code that fits the limitations of resource-constrained devices, such as mobile phones, in the presence of large schemas. In this paper we present Instance-based XML data binding, an approach to produce compact application-specific XML processing code for mobile devices. The approach utilises information extracted from a set of XML documents about how the application make use of the schemas.","XML schema, XML processing, XML data binding, mobile applications","","M-MPAC '11"
"Conference Paper","Venkatesh G,Sampson J,Goulding-Hotta N,Venkata SK,Taylor MB,Swanson S","QsCores: Trading Dark Silicon for Scalable Energy Efficiency with Quasi-Specific Cores","","2011","","","163–174","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture","Porto Alegre, Brazil","2011","9781450310536","","https://doi.org/10.1145/2155620.2155640;http://dx.doi.org/10.1145/2155620.2155640","10.1145/2155620.2155640","Transistor density continues to increase exponentially, but power dissipation per transistor is improving only slightly with each generation of Moore's law. Given the constant chip-level power budgets, this exponentially decreases the percentage of transistors that can switch at full frequency with each technology generation. Hence, while the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design. In this regime, utilizing transistors to design specialized cores that optimize energy-per-computation becomes an effective approach to improve system performance.To trade transistors for energy efficiency in a scalable manner, we propose Quasi-specific Cores, or QsCores, specialized processors capable of executing multiple general-purpose computations while providing an order of magnitude more energy efficiency than a general-purpose processor. The QsCores design flow is based on the insight that similar code patterns exist within and across applications. Our approach exploits these similar code patterns to ensure that a small set of specialized cores support a large number of commonly used computations.We evaluate QsCores's ability to target both a single application library (e.g., data structures) as well as a diverse workload consisting of applications selected from different domains (e.g., SPECINT, EEMBC, and Vision). Our results show that QsCores can provide 18.4 x better energy efficiency than general-purpose processors while reducing the amount of specialized logic required to support the workload by up to 66%.","conservation core, specialization, dark silicon, QsCores, merging, utilization wall, heterogeneous many-core","","MICRO-44"
"Journal Article","Gopalakrishnan G,Kirby RM,Siegel S,Thakur R,Gropp W,Lusk E,De Supinski BR,Schulz M,Bronevetsky G","Formal Analysis of MPI-Based Parallel Programs","Commun. ACM","2011","54","12","82–91","Association for Computing Machinery","New York, NY, USA","","","2011-12","","0001-0782","https://doi.org/10.1145/2043174.2043194;http://dx.doi.org/10.1145/2043174.2043194","10.1145/2043174.2043194","The goal is reliable parallel simulations, helping scientists understand nature, from how foams compress to how ribosomes construct proteins.","","",""
"Conference Paper","Zhang D,Dang Y,Lou JG,Han S,Zhang H,Xie T","Software Analytics as a Learning Case in Practice: Approaches and Experiences","","2011","","","55–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering","Lawrence, Kansas, USA","2011","9781450310222","","https://doi.org/10.1145/2070821.2070829;http://dx.doi.org/10.1145/2070821.2070829","10.1145/2070821.2070829","Software analytics is to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services. In this position paper, we advocate that when applying analytic technologies in practice of software analytics, one should (1) incorporate a broad spectrum of domain knowledge and expertise, e.g., management, machine learning, large-scale data processing and computing, and information visualization; and (2) investigate how practitioners take actions on the produced information, and provide effective support for such information-based action taking. Our position is based on our experiences of successful technology transfer on software analytics at Microsoft Research Asia.","technology transfer, software analytics, machine learning","","MALETS '11"
"Conference Paper","Griffith I,Wahl S,Izurieta C","Evolution of Legacy System Comprehensibility through Automated Refactoring","","2011","","","35–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering","Lawrence, Kansas, USA","2011","9781450310222","","https://doi.org/10.1145/2070821.2070826;http://dx.doi.org/10.1145/2070821.2070826","10.1145/2070821.2070826","Software engineering is a continually evolving discipline, wherein researchers and members of industry are working towards defining and refining what are known as ""best practices."" Best practices are the set of known correct engineering techniques that lead to quality software.When a software artifact is produced, it becomes temporally locked into a single instantiation of these best practices at a given point in time. If such software is not maintained in such a way as to keep it current with the evolution of practice, then there is a good chance that subsequent engineers may not understand the design choices made. There are known techniques, called refactorings, which allow for structural changes to software without altering the outward appearance and behavior, thus maintaining the intent of the original design. Unfortunately, refactoring requires an engineer to both understand the techniques to be applied and the code to which they are applied to. This is not always feasible.We have developed an automated system utilizing Evolutionary Algorithms to manipulate refactorings correctly without requiring an underlying understanding of the software. This then allows for sustained levels of quality of evolving software systems. The understandability, maintainability, and reusability of the software regenerate as best practices evolve.","code smell, software engineering, measurement, software evolution, refactoring, automation","","MALETS '11"
"Conference Paper","Yedlapalli P,Kultursay E,Kandemir MT","Cooperative Parallelization","","2011","","","134–141","IEEE Press","San Jose, California","Proceedings of the International Conference on Computer-Aided Design","","2011","9781457713989","","","","We propose a cooperation between the programmer, the compiler and the runtime system to identify, exploit and efficiently exercise the parallelism available in many pointer based applications. Our parallelization strategy, called Cooperative Parallelization, is driven by programmer directives as well as runtime information. We show that minimal information from the programmer can be combined with runtime information to extract latent parallelism in many pointer intensive applications that involve trees and linked lists. We implemented a compilation framework which automatically parallelizes programs annotated with parallelism directives. We evaluated our approach on a collection of linked list and tree based applications. Our results show that we can achieve speedups of up to 15x on a sixteen-core platform. We also compared our approach to OpenMP both qualitatively and quantitatively.","","","ICCAD '11"
"Conference Paper","Kang B,Kim HS,Kim T,Kwon H,Im EG","Fast Malware Family Detection Method Using Control Flow Graphs","","2011","","","287–292","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2011 ACM Symposium on Research in Applied Computation","Miami, Florida","2011","9781450310871","","https://doi.org/10.1145/2103380.2103439;http://dx.doi.org/10.1145/2103380.2103439","10.1145/2103380.2103439","As attackers make variants of existing malware, it is possible to detect unknown malware by comparing with already-known malware's information. Control flow graphs have been used in dynamic analysis of program source code. In this paper, we proposed a new method which can analyze and detect malware binaries using control flow graphs and Bloom filter by abstracting common characteristics of malware families. The experimental results showed that processing overhead of our proposed method is much lower than n-gram based methods.","control flow graph, network security, Bloom filter, malware analysis","","RACS '11"
"Conference Paper","Kleinschmager S,Hanenberg S","How to Rate Programming Skills in Programming Experiments? A Preliminary, Exploratory, Study Based on University Marks, Pretests, and Self-Estimation","","2011","","","15–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM SIGPLAN Workshop on Evaluation and Usability of Programming Languages and Tools","Portland, Oregon, USA","2011","9781450310246","","https://doi.org/10.1145/2089155.2089161;http://dx.doi.org/10.1145/2089155.2089161","10.1145/2089155.2089161","Rating of subjects is an important issue for empirical studies. First, it is desirable for studies that rely on comparisons between different groups to make sure that those groups are balanced, i.e. that subjects in different groups are comparable. Second, in order to understand to what extent the results of a study are generalizable it is necessary to understand whether the used subjects can be considered as representative. Third, for a deeper understanding of an experiment's results it is desirable to understand what different kinds of subjects achieved what results. This paper addresses this topic by a preliminary, exploratory study that analyzes three different possible criteria: university marks, self-estimation, and pretests. It turns out that neither university marks nor pretests yielded better results than self-estimation.","experimentation, programming, human subjects","","PLATEAU '11"
"Conference Paper","Artola L","Harnessing Collective Software Development","","2011","","","99–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion","Portland, Oregon, USA","2011","9781450309424","","https://doi.org/10.1145/2048147.2048183;http://dx.doi.org/10.1145/2048147.2048183","10.1145/2048147.2048183","Python has become established as the de facto scripting language in many industries including the post-production of visual effects. Its shallow learning curve enables a wider range of individuals to produce code much more quickly than before. This often leads to increased code duplication, competing tools and increased maintenance costs. This talk presents an attempt to harness all that coding power in a fast-paced production environment with the intention of increasing code reuse, reducing maintenance costs and improving the quality of the development process and the code itself. It describes philosophy, tools, techniques and challenges of harnessing collective software development in the pursuit of better software.","object-oriented programming, software development methodologies, procedural programming, software process, python","","OOPSLA '11"
"Conference Paper","Sunshine J,Naden K,Stork S,Aldrich J,Tanter É","First-Class State Change in Plaid","","2011","","","713–732","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications","Portland, Oregon, USA","2011","9781450309400","","https://doi.org/10.1145/2048066.2048122;http://dx.doi.org/10.1145/2048066.2048122","10.1145/2048066.2048122","Objects model the world, and state is fundamental to a faithful modeling. Engineers use state machines to understand and reason about state transitions, but programming languages provide little support for building software based on state abstractions. We propose Plaid, a language in which objects are modeled not just in terms of classes, but in terms of changing abstract states. Each state may have its own representation, as well as methods that may transition the object into a new state. A formal model precisely defines the semantics of core Plaid constructs such as state transition and trait-like state composition. We evaluate Plaid through a series of examples taken from the Plaid compiler and the standard libraries of Smalltalk and Java. These examples show how Plaid can more closely model state-based designs, enhancing understandability, enhancing dynamic error checking, and providing reuse benefits.","plaid, typestate, state-chart","","OOPSLA '11"
"Journal Article","Sunshine J,Naden K,Stork S,Aldrich J,Tanter É","First-Class State Change in Plaid","SIGPLAN Not.","2011","46","10","713–732","Association for Computing Machinery","New York, NY, USA","","","2011-10","","0362-1340","https://doi.org/10.1145/2076021.2048122;http://dx.doi.org/10.1145/2076021.2048122","10.1145/2076021.2048122","Objects model the world, and state is fundamental to a faithful modeling. Engineers use state machines to understand and reason about state transitions, but programming languages provide little support for building software based on state abstractions. We propose Plaid, a language in which objects are modeled not just in terms of classes, but in terms of changing abstract states. Each state may have its own representation, as well as methods that may transition the object into a new state. A formal model precisely defines the semantics of core Plaid constructs such as state transition and trait-like state composition. We evaluate Plaid through a series of examples taken from the Plaid compiler and the standard libraries of Smalltalk and Java. These examples show how Plaid can more closely model state-based designs, enhancing understandability, enhancing dynamic error checking, and providing reuse benefits.","typestate, plaid, state-chart","",""
"Conference Paper","Barbosa FS,Aguiar A","Reusable Roles, a Test with Patterns","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th Conference on Pattern Languages of Programs","Portland, Oregon, USA","2011","9781450312837","","https://doi.org/10.1145/2578903.2579149;http://dx.doi.org/10.1145/2578903.2579149","10.1145/2578903.2579149","Although roles have been around for a long time they have not yet reached mainstream programming languages. The variety of existing role models may be a limiting factor. We believe that for roles to be widely accepted they must enhance code reuse. An outcome would be a library of roles. We present and discuss what we feel are the characteristics that a role model must have to enable reusable and player-independent roles. In this paper we present our role model and JavaStage, a role language that extends Java, with examples of reusable roles. Finally, we present our steps towards the building of a role library, by presenting the roles developed from the analysis of the GoF Design Patterns. The results obtained, we developed roles for 10 of the 23 GoF patterns, are promising.","design patterns, modularity, libraries, roles","","PLoP '11"
"Conference Paper","Mustafa B","Visualizing the Modern Operating System: Simulation Experiments Supporting Enhanced Learning","","2011","","","209–214","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2011 Conference on Information Technology Education","West Point, New York, USA","2011","9781450310178","","https://doi.org/10.1145/2047594.2047650;http://dx.doi.org/10.1145/2047594.2047650","10.1145/2047594.2047650","An important area of modern computer organization and architecture is the operating system the internals of which is normally inaccessible for teaching and learning purposes. This paper describes an educational operating system simulator that is part of an integrated set of simulators designed to support students of computer architecture and operating systems. Examples of classroom assignments are presented demonstrating the simulator's support for a wide range of practical experiments. The pedagogical value of the simulator is assessed in terms of the educational impact of its visualization features and its functional capabilities for supporting students at different levels of learning. Finally, the preliminary results of the evaluation of the simulator that provide an indication of its value as a teaching and learning resource are presented.","simulation, pedagogy, operating system, visualization","","SIGITE '11"
"Journal Article","Kleanthous M,Sazeides Y","CATCH: A Mechanism for Dynamically Detecting Cache-Content-Duplication in Instruction Caches","ACM Trans. Archit. Code Optim.","2011","8","3","","Association for Computing Machinery","New York, NY, USA","","","2011-10","","1544-3566","https://doi.org/10.1145/2019608.2019610;http://dx.doi.org/10.1145/2019608.2019610","10.1145/2019608.2019610","Cache-content-duplication (CCD) occurs when there is a miss for a block in a cache and the entire content of the missed block is already in the cache in a block with a different tag. Caches aware of content-duplication can have lower miss penalty by fetching, on a miss to a duplicate block, directly from the cache instead of accessing lower in the memory hierarchy, and can have lower miss rates by allowing only blocks with unique content to enter a cache.This work examines the potential of CCD for instruction caches. We show that CCD is a frequent phenomenon and that an idealized duplication-detection mechanism for instruction caches has the potential to increase performance of an out-of-order processor, with a 16KB, 8-way, 8 instructions per block instruction cache, often by more than 10% and up to 36%.This work also proposes CATCH, a hardware mechanism for dynamically detecting CCD for instruction caches. Experimental results for an out-of-order processor show that a duplication-detection mechanism with a 1.38KB cost captures on average 58% of the CCD's idealized potential.","cache compression, Cache optimizations, cache content duplication","",""
"Conference Paper","Jang J,Brumley D,Venkataraman S","BitShred: Feature Hashing Malware for Scalable Triage and Semantic Analysis","","2011","","","309–320","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM Conference on Computer and Communications Security","Chicago, Illinois, USA","2011","9781450309486","","https://doi.org/10.1145/2046707.2046742;http://dx.doi.org/10.1145/2046707.2046742","10.1145/2046707.2046742","The sheer volume of new malware found each day is growing at an exponential pace. This growth has created a need for automatic malware triage techniques that determine what malware is similar, what malware is unique, and why. In this paper, we present BitShred, a system for large-scale malware similarity analysis and clustering, and for automatically uncovering semantic inter- and intra-family relationships within clusters. The key idea behind BitShred is using feature hashing to dramatically reduce the high-dimensional feature spaces that are common in malware analysis. Feature hashing also allows us to mine correlated features between malware families and samples using co-clustering techniques. Our evaluation shows that BitShred speeds up typical malware triage tasks by up to 2,365x and uses up to 82x less memory on a single CPU, all with comparable accuracy to previous approaches. We also develop a parallelized version of BitShred, and demonstrate scalability within the Hadoop framework.","co-clustering, hadoop, feature hashing, malware triage","","CCS '11"
"Conference Paper","Fournet C,Kohlweiss M,Strub PY","Modular Code-Based Cryptographic Verification","","2011","","","341–350","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM Conference on Computer and Communications Security","Chicago, Illinois, USA","2011","9781450309486","","https://doi.org/10.1145/2046707.2046746;http://dx.doi.org/10.1145/2046707.2046746","10.1145/2046707.2046746","Type systems are effective tools for verifying the security of cryptographic programs. They provide automation, modularity and scalability, and have been applied to large security protocols. However, they traditionally rely on abstract assumptions on the underlying cryptographic primitives, expressed in symbolic models. Cryptographers usually reason on security assumptions using lower level, computational models that precisely account for the complexity and success probability of attacks. These models are more realistic, but they are harder to formalize and automate. We present the first modular automated program verification method based on standard cryptographic assumptions. We show how to verify ideal functionalities and protocols written in ML by typing them against new cryptographic interfaces using F7, a refinement type checker coupled with an SMT-solver. We develop a probabilistic core calculus for F7 and formalize its type safety in Coq.We build typed module and interfaces for MACs, signatures, and encryptions, and establish their authenticity and secrecy properties. We relate their ideal functionalities and concrete implementations, using game-based program transformations behind typed interfaces. We illustrate our method on a series of protocol implementations.","cryptography, security protocols, refinement types","","CCS '11"
"Conference Paper","Tokunaga M,Yoshida N,Yoshioka K,Matsushita M,Inoue K","Towards a Collection of Refactoring Patterns Based on Code Clone Categorization","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Asian Conference on Pattern Languages of Programs","Tokyo, Japan","2011","9781450321099","","https://doi.org/10.1145/2524629.2524637;http://dx.doi.org/10.1145/2524629.2524637","10.1145/2524629.2524637","Code clone is a code fragment that has identical or similar fragments to it in the source code. Fowler and Kereivsky wrote several techniques to remove code clones in refactoring patterns that they documented. Those refactoring patterns include characteristics of code clone and corresponding steps to merge code clones. However, according to our experience in code clone research field, a lot of different types exist between code clones similar to each others, and most of those difference types are not mentioned in previous refactoring pattern. It is useful to develop a collection of patterns for clone refactoring based on precise code clone categorization. In this paper, we describe an approach to developing our intended collection of refactoring patterns and then propose an example of refactoring pattern that we have developed.","","","AsianPLoP '11"
"Conference Paper","Kahlout G","Implementing Patterns with Annotations","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Asian Conference on Pattern Languages of Programs","Tokyo, Japan","2011","9781450321099","","https://doi.org/10.1145/2524629.2524636;http://dx.doi.org/10.1145/2524629.2524636","10.1145/2524629.2524636","Annotation processors in Java can be used to inject design pattern implementations without the developer having to write the boilerplate code to implement the pattern. In this paper, dp4j is presented as an open-source Java tool to inject Singleton design pattern implementations into the Abstract Syntax Tree of Java classes annotated with the @Singleton annotation.","java compiler, refactoring, case, netbeans ide, java annotation processing, design patterns, singleton pattern, code analysis","","AsianPLoP '11"
"Journal Article","Dahotre A,Krishnamoorthy V,Corley M,Scaffidi C","Using Intelligent Tutors to Enhance Student Learning of Application Programming Interfaces","J. Comput. Sci. Coll.","2011","27","1","195–201","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2011-10","","1937-4771","","","An essential part of software engineering training is for students to learn how to effectively use application programming interfaces (APIs), but traditional instruction only provides direct support for helping students to learn the most commonly used APIs. This paper introduces a new approach whereby professors could delegate some of these training responsibilities to intelligent tutors, which are interactive instructional materials that tailor themselves to each student's progress. A prototype system has been developed that semi-automatically generates API tutors from open source code freely available on the web. As API tutors are published to a new website, students have an increasingly large menu of training materials available for them to choose from. A preliminary study indicates that the approach increases student learning on sample tasks.","","",""
"Journal Article","Orr G","Classroom Explorations in 3D Stereoscopy (S3D)","J. Comput. Sci. Coll.","2011","27","1","120–126","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2011-10","","1937-4771","","","3D Stereoscopy is a rich and fun interdisciplinary context for engaging students across campus. Students can practice programming and also experience how computing can be used as a tool for exploration and analysis of scientific (or other) questions.","","",""
"Journal Article","Singh S,Kahlon KS","Effectiveness of Encapsulation and Object-Oriented Metrics to Refactor Code and Identify Error Prone Classes Using Bad Smells","SIGSOFT Softw. Eng. Notes","2011","36","5","1–10","Association for Computing Machinery","New York, NY, USA","","","2011-09","","0163-5948","https://doi.org/10.1145/2020976.2020994;http://dx.doi.org/10.1145/2020976.2020994","10.1145/2020976.2020994","To assist maintenance and evolution teams, work needs to be done at the onset of software development. One such facilitation is refactoring the code, making it easier to read, understand and maintain. Refactoring is done by identifying bad smell areas in the code. In this paper, based on empirical analysis, we develop a metrics model to identify smelly classes. The role of two new metrics (encapsulation and information hiding) is also investigated for identifying smelly and faulty classes in software code. This paper first presents a binary statistical analysis of thev relationship between metrics and bad smells, the results of which show a significant relationship. Then, the metrics model (with significant metrics shortlisted from the binary analysis) for bad smell categorization (divided into five categories) is developed. To verify our model, we examine the open source Firefox system, which has a strong industrial usage. The results show that proposed metrics model for bad smell can predict faulty classes with high accuracy, but in the case of the categorized model not all categories of bad smells can adequately identified the faulty and smelly classes. Due to certain limitations of our study more experiments are required to generalize the results of bad smell and faulty class identification in software code.","information hiding, empirical analysis, refactoring, bad smells, encapsulation, evolution","",""
"Conference Paper","Beck F,Diehl S","On the Congruence of Modularity and Code Coupling","","2011","","","354–364","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering","Szeged, Hungary","2011","9781450304436","","https://doi.org/10.1145/2025113.2025162;http://dx.doi.org/10.1145/2025113.2025162","10.1145/2025113.2025162","Software systems are modularized to make their inherent complexity manageable. While there exists a set of well-known principles that may guide software engineers to design the modules of a software system, we do not know which principles are followed in practice. In a study based on 16 open source projects, we look at different kinds of coupling concepts between source code entities, including structural dependencies, fan-out similarity, evolutionary coupling, code ownership, code clones, and semantic similarity. The congruence between these coupling concepts and the modularization of the system hints at the modularity principles used in practice. Furthermore, the results provide insights on how to support developers to modularize software systems.","package design, code coupling, modularity","","ESEC/FSE '11"
"Conference Paper","Driscoll E,Burton A,Reps T","Checking Conformance of a Producer and a Consumer","","2011","","","113–123","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering","Szeged, Hungary","2011","9781450304436","","https://doi.org/10.1145/2025113.2025132;http://dx.doi.org/10.1145/2025113.2025132","10.1145/2025113.2025132","This paper addresses the problem of identifying incompatibilities between two programs that operate in a producer/consumer relationship. It describes the techniques that are incorporated in a tool called PCCA (Producer-Consumer Conformance Analyzer), which attempts to (i) determine whether the consumer is prepared to accept all messages that the producer can emit, or (ii) find a counter-example: a message that the producer can emit and the consumer considers ill-formed.","producer-consumer compatibility, language containment, visibly pushdown automata","","ESEC/FSE '11"
"Conference Paper","Lavallée M,Robillard PN","Causes of Premature Aging during Software Development: An Observational Study","","2011","","","61–70","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution","Szeged, Hungary","2011","9781450308489","","https://doi.org/10.1145/2024445.2024458;http://dx.doi.org/10.1145/2024445.2024458","10.1145/2024445.2024458","Much work has been done on the subject of what happens to software architecture during maintenance activities. There seems to be a consensus that it degrades during the evolution of the software. More recent work shows that this degradation occurs even during development activities: design decisions are either adjusted or forgotten. Some studies have looked into the causes of this degradation, but these have mostly done so at a very high level. This study examines three projects at code level. Three architectural pre-implementation designs are compared with their post-implementation design counterparts, with special attention paid to the causes of the changes. We found many negative changes causing anti-patterns, at the package, class, and method levels. After analysis of the code, we were able to find the specific reasons for the poor design decisions. Although the underlying causes are varied, they can be grouped into three basic categories: knowledge problems, artifact problems, and management problems. This categorization shows that anti-pattern causes are varied and are not all due to the developers. The main conclusion is that promoting awareness of anti-patterns to developers is insufficient to prevent them since some of the causes escape their grasp.","pre-implementation design, design erosion, software aging, post-implementation design","","IWPSE-EVOL '11"
"Journal Article","Dagenais B,Robillard MP","Recommending Adaptive Changes for Framework Evolution","ACM Trans. Softw. Eng. Methodol.","2011","20","4","","Association for Computing Machinery","New York, NY, USA","","","2011-09","","1049-331X","https://doi.org/10.1145/2000799.2000805;http://dx.doi.org/10.1145/2000799.2000805","10.1145/2000799.2000805","In the course of a framework’s evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework’s evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.","legacy study, mining software repositories, origin analysis, partial program analysis, Adaptive changes, recommendation system, software evolution, framework","",""
"Conference Paper","Ressia J,Gîrba T,Nierstrasz O,Perin F,Renggli L","Talents: Dynamically Composable Units of Reuse","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Workshop on Smalltalk Technologies","Edinburgh, United Kingdom","2011","9781450310505","","https://doi.org/10.1145/2166929.2166940;http://dx.doi.org/10.1145/2166929.2166940","10.1145/2166929.2166940","Reuse in object-oriented languages typically focuses on inheritance. Numerous techniques have been developed to provide finer-grained reuse of methods, such as flavors, mixins and traits. These techniques, however, only deal with reuse at the level of classes.Class-based reuse is inherently static. Increasing use of reflection and meta-programming techniques in real world applications underline the need for more dynamic approaches. New approaches have shifted to object-specific reuse. However, these techniques fail to provide a complete solution to the composition issues arising during reuse.We propose a new approach that deals with reuse at the object level and that supports behavioral composition. We introduce a new abstraction called a talent which models features that are shared between objects of different class hierarchies. Talents provide a composition mechanism that is as flexible as that of traits but which is dynamic.","Smalltalk, object-specific behavior, mixins, reflection, traits, object adaption","","IWST '11"
"Conference Paper","Duszynski S","A Scalable Goal-Oriented Approach to Software Variability Recovery","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Software Product Line Conference, Volume 2","Munich, Germany","2011","9781450307895","","https://doi.org/10.1145/2019136.2019185;http://dx.doi.org/10.1145/2019136.2019185","10.1145/2019136.2019185","Software reuse approaches, such as software product lines, can help to achieve considerable effort and cost savings in product development for sets of software systems with a significant overlap in functionality. However, in the practice many organizations at first develop a number of similar software products without explicitly planning for strategic reuse. In consequence, subsequent attempts to introduce reuse require a significant restructuring of the existing products. The restructuring is difficult because the precise information about the distribution of commonality and variability in the source code of the variants is often not available.The ongoing PhD thesis presented in this paper contributes to easing the task of extractive software reuse adoption: it proposes a reverse engineering approach for extracting the variability information from the source code of similar software products and outlines a method that guides the organization towards optimal use of this information in the process of planning and introducing software reuse.","reverse engineering, product lines, software reuse, source code mining, variability, variant, visualization","","SPLC '11"
"Conference Paper","Chaki S,Cohen C,Gurfinkel A","Supervised Learning for Provenance-Similarity of Binaries","","2011","","","15–23","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","San Diego, California, USA","2011","9781450308137","","https://doi.org/10.1145/2020408.2020419;http://dx.doi.org/10.1145/2020408.2020419","10.1145/2020408.2020419","Understanding, measuring, and leveraging the similarity of binaries (executable code) is a foundational challenge in software engineering. We present a notion of similarity based on provenance -- two binaries are similar if they are compiled from the same (or very similar) source code with the same (or similar) compilers. Empirical evidence suggests that provenance-similarity accounts for a significant portion of variation in existing binaries, particularly in malware. We propose and evaluate the applicability of classification to detect provenance-similarity. We evaluate a variety of classifiers, and different types of attributes and similarity labeling schemes, on two benchmarks derived from open-source software and malware respectively. We present encouraging results indicating that classification is a viable approach for automated provenance-similarity detection, and as an aid for malware analysts in particular.","software provenance, classification, binary similarity","","KDD '11"
"Conference Paper","Schuster C,Appeltauer M,Hirschfeld R","Context-Oriented Programming for Mobile Devices: JCop on Android","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM International Workshop on Context-Oriented Programming","Lancaster, United Kingdom","2011","9781450308915","","https://doi.org/10.1145/2068736.2068741;http://dx.doi.org/10.1145/2068736.2068741","10.1145/2068736.2068741","The behavior of mobile applications is particularly affected by their execution context, such as location and state a the mobile device. Among other approaches, context-oriented programming can help to achieve context-dependent behavior without sacrificing modularity or adhering to a certain framework or library by enabling fine-grained adaptation of default behavior per control-flow.However, context information relevant for mobile applications is mostly defined by external events and sensor data rather than by code and control flow. To accommodate this, the JCop language provides a more declarative approach by pointcut-like adaptation rules.In this paper, we explain how we applied JCop to the development of Android applications for which we extended the language semantics for static contexts and modified the compiler. Additionally, we outline the successful implementation of a simple, proof-of-concept mobile application using our approach and report on promising early evaluation results.","dynamic adaption, context-oriented programming, mobile applications","","COP '11"
"Journal Article","Hwang SW","Report on Data-Intensive Software Management and Mining","SIGMOD Rec.","2011","40","1","32–34","Association for Computing Machinery","New York, NY, USA","","","2011-07","","0163-5808","https://doi.org/10.1145/2007206.2007216;http://dx.doi.org/10.1145/2007206.2007216","10.1145/2007206.2007216","","","",""
"Conference Paper","Meyer C,Heeren C,Shaffer E,Tedesco J","CoMoTo: The Collaboration Modeling Toolkit","","2011","","","143–147","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education","Darmstadt, Germany","2011","9781450306973","","https://doi.org/10.1145/1999747.1999789;http://dx.doi.org/10.1145/1999747.1999789","10.1145/1999747.1999789","We are excited to introduce CoMoTo -- the Collaboration Modeling Toolkit -- a new, web-based application that expands and enhances well-known software similarity detection systems. CoMoTo is an end-to-end data management, analysis, and visualization system whose purpose is to assist instructors of courses requiring programming exercises to monitor and investigate the extent of student collaboration, both allowed and illicit. We describe CoMoTo's interface, which was designed to facilitate scrutiny of collaboration data projected along student, course, assignment, etc. attributes, and to allow for interactive visualization of pairwise similarity measures via a dynamic graph. We also elaborate on the details of CoMoTo's implementation. Finally, we briefly discuss two use cases that foreshadow CoMoTo's broad utility in student code analysis, not only for plagiarism detection, but also for investigating early student coding styles, and for evaluating software similarity detection systems, themselves.","pedagogy, program similarity","","ITiCSE '11"
"Conference Paper","Breuker DM,Derriks J,Brunekreef J","Measuring Static Quality of Student Code","","2011","","","13–17","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education","Darmstadt, Germany","2011","9781450306973","","https://doi.org/10.1145/1999747.1999754;http://dx.doi.org/10.1145/1999747.1999754","10.1145/1999747.1999754","In this paper we report about a large-scale measurement programme concerning the static quality of student-written Java code. The goal of the programme is two-fold: we investigate what metrics are useful for measuring static quality in an educational setting, and we investigate what conclusions can be drawn from the measurement results.","measurement programme, software quality, software metrics","","ITiCSE '11"
"Conference Paper","von Detten M,Becker S","Combining Clustering and Pattern Detection for the Reengineering of Component-Based Software Systems","","2011","","","23–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS","Boulder, Colorado, USA","2011","9781450307246","","https://doi.org/10.1145/2000259.2000265;http://dx.doi.org/10.1145/2000259.2000265","10.1145/2000259.2000265","During the software lifecycle, software systems have to be continuously maintained to counteract architectural deterioration and retain their software quality. In order to maintain a software it has to be understood first which can be supported by (semi-)automatic reverse engineering approaches. Reverse engineering is the analysis of software for the purpose of recovering its design documentation, e.g., in form of the conceptual architecture. Today, the most prevalent reverse engineering approaches are (1) the clustering based approach which groups the elements of a given software system based on metric values in order to provide an overview of the system and (2) the pattern-based approach which tries to detect pre-defined patterns in the software which can give insight about the original developers' intentions. In this paper, we present an approach towards combining these techniques: we show how the detection and removal of certain bad smells in a software system can improve the results of a clustering-based analysis. We propose to integrate this combination of reverse engineering approaches into a reengineering process for component-based software systems.","software architecture, reengineering, metrics, clustering, bad smell detection","","QoSA-ISARCS '11"
"Journal Article","Subhraveti D,Nieh J","Record and Transplay: Partial Checkpointing for Replay Debugging across Heterogeneous Systems","SIGMETRICS Perform. Eval. Rev.","2011","39","1","109–120","Association for Computing Machinery","New York, NY, USA","","","2011-06","","0163-5999","https://doi.org/10.1145/2007116.2007129;http://dx.doi.org/10.1145/2007116.2007129","10.1145/2007116.2007129","Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application environment and nondeterminism. To address this problem, we present Transplay, a system that captures production software bugs into small per-bug recordings which are used to reproduce the bugs on a completely different operating system without access to any of the original software used in the production environment. Transplay introduces partial checkpointing, a new mechanism that efficiently captures the partial state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries, libraries, support data, or the original execution environment. Transplay integrates with existing debuggers to provide standard debugging facilities to allow the user to examine the contents of variables and other program state at each source line of the application's replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that Transplay can reproduce real bugs and be used in production with modest recording overhead.","checkpoint-restart, record-replay, virtualization","",""
"Conference Paper","Subhraveti D,Nieh J","Record and Transplay: Partial Checkpointing for Replay Debugging across Heterogeneous Systems","","2011","","","109–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems","San Jose, California, USA","2011","9781450308144","","https://doi.org/10.1145/1993744.1993757;http://dx.doi.org/10.1145/1993744.1993757","10.1145/1993744.1993757","Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application environment and nondeterminism. To address this problem, we present Transplay, a system that captures production software bugs into small per-bug recordings which are used to reproduce the bugs on a completely different operating system without access to any of the original software used in the production environment. Transplay introduces partial checkpointing, a new mechanism that efficiently captures the partial state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries, libraries, support data, or the original execution environment. Transplay integrates with existing debuggers to provide standard debugging facilities to allow the user to examine the contents of variables and other program state at each source line of the application's replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that Transplay can reproduce real bugs and be used in production with modest recording overhead.","record-replay, checkpoint-restart, virtualization","","SIGMETRICS '11"
"Conference Paper","Hazelwood K","Process-Level Virtualization for Runtime Adaptation of Embedded Software","","2011","","","895–900","Association for Computing Machinery","New York, NY, USA","Proceedings of the 48th Design Automation Conference","San Diego, California","2011","9781450306362","","https://doi.org/10.1145/2024724.2024924;http://dx.doi.org/10.1145/2024724.2024924","10.1145/2024724.2024924","Modern processor architectures call for software that is highly tuned to an unpredictable operating environment. Process-level virtualization systems allow existing software to adapt to the operating environment, including resource contention and other dynamic events, by modifying the application instructions at runtime. While these systems are becoming widespread in the general-purpose computing communities, various challenges have prevented widespread adoption on resource-constrained devices, with memory and performance overheads being at the forefront. In this paper, we discuss the advantages and opportunities of runtime adaptation of embedded software. We also describe some of the existing dynamic binary modification tools that can be used to perform runtime adaptation, and discuss the challenges of balancing memory overheads and performance when developing these tools for embedded platforms.","virtualization software, runtime adaptation, dynamic binary optimization, embedded systems","","DAC '11"
"Conference Paper","Meng N,Kim M,McKinley KS","Systematic Editing: Generating Program Transformations from an Example","","2011","","","329–342","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation","San Jose, California, USA","2011","9781450306638","","https://doi.org/10.1145/1993498.1993537;http://dx.doi.org/10.1145/1993498.1993537","10.1145/1993498.1993537","Software modifications are often systematic ---they consist of similar, but not identical, program changes to multiple contexts. Existing tools for systematic program transformation are limited because they require programmers to manually prescribe edits or only suggest a location to edit with a related example. This paper presents the design and implementation of a program transformation tool called SYDIT. Given an example edit, SYDIT generates a context-aware, abstract edit script, and then applies the edit script to new program locations. To correctly encode a relative position of the edits in a new location, the derived edit script includes unchanged statements on which the edits are control and data dependent. Furthermore, to make the edit script applicable to a new context using different identifier names, the derived edit script abstracts variable, method, and type names. The evaluation uses 56 systematic edit pairs from five large software projects as an oracle. SYDIT has high coverage and accuracy. For 82% of the edits (46/56), SYDIT matches the context and applies an edit, producing code that is 96% similar to the oracle. Overall, SYDIT mimics human programmers correctly on 70% (39/56) of the edits. Generation of edit scripts seeks to improve programmer productivity by relieving developers from tedious, error-prone, manual code updates. It also has the potential to guide automated program repair by creating program transformations applicable to similar contexts.","empirical study, software evolution, program differencing, program transformation","","PLDI '11"
"Journal Article","Meng N,Kim M,McKinley KS","Systematic Editing: Generating Program Transformations from an Example","SIGPLAN Not.","2011","46","6","329–342","Association for Computing Machinery","New York, NY, USA","","","2011-06","","0362-1340","https://doi.org/10.1145/1993316.1993537;http://dx.doi.org/10.1145/1993316.1993537","10.1145/1993316.1993537","Software modifications are often systematic ---they consist of similar, but not identical, program changes to multiple contexts. Existing tools for systematic program transformation are limited because they require programmers to manually prescribe edits or only suggest a location to edit with a related example. This paper presents the design and implementation of a program transformation tool called SYDIT. Given an example edit, SYDIT generates a context-aware, abstract edit script, and then applies the edit script to new program locations. To correctly encode a relative position of the edits in a new location, the derived edit script includes unchanged statements on which the edits are control and data dependent. Furthermore, to make the edit script applicable to a new context using different identifier names, the derived edit script abstracts variable, method, and type names. The evaluation uses 56 systematic edit pairs from five large software projects as an oracle. SYDIT has high coverage and accuracy. For 82% of the edits (46/56), SYDIT matches the context and applies an edit, producing code that is 96% similar to the oracle. Overall, SYDIT mimics human programmers correctly on 70% (39/56) of the edits. Generation of edit scripts seeks to improve programmer productivity by relieving developers from tedious, error-prone, manual code updates. It also has the potential to guide automated program repair by creating program transformations applicable to similar contexts.","software evolution, program differencing, program transformation, empirical study","",""
"Conference Paper","Srivastava V,Bond MD,McKinley KS,Shmatikov V","A Security Policy Oracle: Detecting Security Holes Using Multiple API Implementations","","2011","","","343–354","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation","San Jose, California, USA","2011","9781450306638","","https://doi.org/10.1145/1993498.1993539;http://dx.doi.org/10.1145/1993498.1993539","10.1145/1993498.1993539","Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct.In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong!Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering ""deep"" bugs in them.","java class libraries, authorization, security, static analysis, access control","","PLDI '11"
"Journal Article","Srivastava V,Bond MD,McKinley KS,Shmatikov V","A Security Policy Oracle: Detecting Security Holes Using Multiple API Implementations","SIGPLAN Not.","2011","46","6","343–354","Association for Computing Machinery","New York, NY, USA","","","2011-06","","0362-1340","https://doi.org/10.1145/1993316.1993539;http://dx.doi.org/10.1145/1993316.1993539","10.1145/1993316.1993539","Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct.In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong!Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering ""deep"" bugs in them.","java class libraries, access control, security, static analysis, authorization","",""
"Conference Paper","Raman A,Kim H,Oh T,Lee JW,August DI","Parallelism Orchestration Using DoPE: The Degree of Parallelism Executive","","2011","","","26–37","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation","San Jose, California, USA","2011","9781450306638","","https://doi.org/10.1145/1993498.1993502;http://dx.doi.org/10.1145/1993498.1993502","10.1145/1993498.1993502","In writing parallel programs, programmers expose parallelism and optimize it to meet a particular performance goal on a single platform under an assumed set of workload characteristics. In the field, changing workload characteristics, new parallel platforms, and deployments with different performance goals make the programmer's development-time choices suboptimal. To address this problem, this paper presents the Degree of Parallelism Executive (DoPE), an API and run-time system that separates the concern of exposing parallelism from that of optimizing it. Using the DoPE API, the application developer expresses parallelism options. During program execution, DoPE's run-time system uses this information to dynamically optimize the parallelism options in response to the facts on the ground. We easily port several emerging parallel applications to DoPE's API and demonstrate the DoPE run-time system's effectiveness in dynamically optimizing the parallelism for a variety of performance goals.","parallelization, parallelism, run-time, pipeline, parametric, dynamic, loop nest, loop-level, optimization, nested, task, scheduling","","PLDI '11"
"Journal Article","Raman A,Kim H,Oh T,Lee JW,August DI","Parallelism Orchestration Using DoPE: The Degree of Parallelism Executive","SIGPLAN Not.","2011","46","6","26–37","Association for Computing Machinery","New York, NY, USA","","","2011-06","","0362-1340","https://doi.org/10.1145/1993316.1993502;http://dx.doi.org/10.1145/1993316.1993502","10.1145/1993316.1993502","In writing parallel programs, programmers expose parallelism and optimize it to meet a particular performance goal on a single platform under an assumed set of workload characteristics. In the field, changing workload characteristics, new parallel platforms, and deployments with different performance goals make the programmer's development-time choices suboptimal. To address this problem, this paper presents the Degree of Parallelism Executive (DoPE), an API and run-time system that separates the concern of exposing parallelism from that of optimizing it. Using the DoPE API, the application developer expresses parallelism options. During program execution, DoPE's run-time system uses this information to dynamically optimize the parallelism options in response to the facts on the ground. We easily port several emerging parallel applications to DoPE's API and demonstrate the DoPE run-time system's effectiveness in dynamically optimizing the parallelism for a variety of performance goals.","parametric, nested, loop nest, parallelism, pipeline, task, loop-level, parallelization, dynamic, optimization, scheduling, run-time","",""
"Conference Paper","Keivanloo I,Forbes C,Rilling J,Charland P","Towards Sharing Source Code Facts Using Linked Data","","2011","","","25–28","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation","Waikiki, Honolulu, HI, USA","2011","9781450305976","","https://doi.org/10.1145/1985429.1985436;http://dx.doi.org/10.1145/1985429.1985436","10.1145/1985429.1985436","Linked Data is designed to support interoperability and sharing of open datasets by allowing on the fly inter-linking of data using the basic layers of the Semantic Web and the HTTP protocol. In our research, we focus on providing a Uniform Resource Locator (URL) generation schema and a supporting ontological representation for the inter-linking of data extracted from source code ecosystems. As a result, we created the Source code ECOsystem Linked Data (SECOLD) framework that adheres to the Linked Data publication standard. The framework provides not only source code and facts that are usable by both humans and machines for browsing or querying, but it will also assist the research community at large in sharing and utilizing a standardized source code representation. The dataset has been submitted and registered to ckan.net, under the SECOLD project name, as the first source code Linked Data repository. In order to maintain its relevance to the research community, we plan to update the data set every four months.","ontology, semantic web, source code model, linked data","","SUITE '11"
"Conference Paper","Li Y","Reengineering a Scientific Software and Lessons Learned","","2011","","","41–45","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Engineering for Computational Science and Engineering","Waikiki, Honolulu, HI, USA","2011","9781450305983","","https://doi.org/10.1145/1985782.1985789;http://dx.doi.org/10.1145/1985782.1985789","10.1145/1985782.1985789","SeisSol is a scientific software for the numerical simulation of seismic wave phenomena. However, there are three main problems in the SeisSol project. First, the project documentation is incomplete. Second, the source code comprehensibility is low. Third, the dependencies between the modules in the system are complicated. To solve the problems and to enhance the software quality, we perform a reengineering process on SeisSol. The process contains four steps, reverse engineering, requirements reengineering, redesign and source code refactoring. In the requirements reengineering step, we employ a novel approach to elicit requirements efficiently for such a scientific computing project.Through the reengineering process, the documentation of source code, requirements and the improved design is generated, the system is more modularized and easier to be extended, as well as the source code are more comprehensible. e also discuss the lessons learned during the reengineering process.","requirements modeling, maintenance, scientific computing, domain specific, reengineering","","SECSE '11"
"Conference Paper","Krinke J","Is Cloned Code Older than Non-Cloned Code?","","2011","","","28–33","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985410;http://dx.doi.org/10.1145/1985404.1985410","10.1145/1985404.1985410","It is still a debated question whether cloned code causes increased maintenance efforts. If cloned code is more stable than non-cloned code, i.e. it is changed less often, it will require less maintenance efforts. The more stable cloned code is, the longer it will not have been changed, so the stability can be estimated through the code's age. This paper presents a study on the average age of cloned code. For three large open source systems, the age of every line of source code is computed as the date of the last change in that line. In addition, every line is categorized whether it belongs to cloned code as detected by a clone detector. The study shows that on average, cloned code is older than non-cloned code. Moreover, if a file has cloned code, the average age of the cloned code of the file is lower than the average age of the non-cloned code in the same file. The results support the previous findings that cloned code is more stable than non-cloned code.","software evolution, mining software archives, clone detection","","IWSC '11"
"Conference Paper","Schugerl P","Scalable Clone Detection Using Description Logic","","2011","","","47–53","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985413;http://dx.doi.org/10.1145/1985404.1985413","10.1145/1985404.1985413","The semantic web is slowly transforming the web as we know it into a machine understandable pool of information that can be consumed and reasoned about by various clients. Source-code is no exception to this trend and various communities have proposed standards to share code as linked data. With the availability of large amounts of open source code published in publically accessible repositories and the introduction of massively horizontally scaling frameworks and cloud computing infrastructure, a new era of software mining across information silos is reshaping the software engineering landscape. The so far unreachable goal of analyzing code at a global level, and therefore detecting global software clones, has become manageable. Description logic and semantic web reasoners have so far only plaid a minor role in this transformation and are mainly used to model source code data. In this paper, we introduce a clone detection algorithm that uses a semantic web reasoner and is based on the Hadoop map-reduce framework that can scale horizontally to a large amount of data. We also define a novel and compact clone model that only considers control-blocks and used data types while still yielding similar clone detection results than more complex representations. In order to validate our approach we have compared our algorithm to some of the leading clone detection tools (CCFinder, JCD and Simian) and show differences in performance and detection precision.","semantic-web., code clone detection","","IWSC '11"
"Conference Paper","Hummel B,Juergens E,Steidl D","Index-Based Model Clone Detection","","2011","","","21–27","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985409;http://dx.doi.org/10.1145/1985404.1985409","10.1145/1985404.1985409","Existing algorithms for model clone detection operate in batch mode. Consequently, if a small part of a large model changes during maintenance, the entire detection needs to be recomputed to produce updated cloning information. Since this can take several hours, the lack of incremental detection algorithms hinders clone management, which requires up-to-date cloning information. In this paper we present an index-based algorithm for model clone detection that is incremental and distributable. We present a case study that demonstrates its capabilities, outline its current limitations and present directions for future work.","matlab/simulink, model clone, clone detection, data-flow","","IWSC '11"
"Conference Paper","Martin D,Cordy JR","Analyzing Web Service Similarity Using Contextual Clones","","2011","","","41–46","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985412;http://dx.doi.org/10.1145/1985404.1985412","10.1145/1985404.1985412","There are several tools and techniques developed over the past decade for detecting duplicated code in software. However, there exists a class of languages for which clone detection is ill-suited. We discovered one of these languages when we attempted to use clone detection to find similar web service operations in service descriptions written in the Web Service Description Language (WSDL). WSDL is structured in such a way that identifying units for comparison becomes a challenge. WSDL service descriptions contain specifications of one or more operations that are divided into pieces and intermingled throughout the description. In this paper, we describe a method of reorganizing them in order to leverage clone detection technology to identify similar services. We introduce the idea of contextual clones -- clones that can only be found by augmenting code fragments with related information referenced by the fragment to give it context. We demonstrate this idea for WSDL and propose other languages and situations for which contextual clones may be of interest.","web services, wsdl, clone detection techniques","","IWSC '11"
"Conference Paper","Choi E,Yoshida N,Ishio T,Inoue K,Sano T","Extracting Code Clones for Refactoring Using Combinations of Clone Metrics","","2011","","","7–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985407;http://dx.doi.org/10.1145/1985404.1985407","10.1145/1985404.1985407","Code clone detection tools may report a large number of code clones, while software developers are interested in only a subset of code clones that are relevant to software development tasks such as refactoring. Our research group has supported many software developers with the code clone detection tool CCFinder and its GUI front-end Gemini. Gemini shows clone sets (i.e., a set of code clones identical or similar to each other) with several clone metrics including their length and the number of code clones; however, it is not clear how to use those metrics to extract interesting code clones for developers. In this paper, we propose a method combining clone metrics to extract code clones for refactoring activity. We have conducted an empirical study on a web application developed by a Japanese software company. The result indicates that combinations of simple clone metric is more effective to extract refactoring candidates in detected code clones than individual clone metric.","industrial case study, code clone, refactoring","","IWSC '11"
"Conference Paper","Göde N,Harder J","Oops! . . . I Changed It Again","","2011","","","14–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985408;http://dx.doi.org/10.1145/1985404.1985408","10.1145/1985404.1985408","Duplicated passages of source code-clones-are a threat to software maintenance as modifying duplicated code causes additional change effort and bears the risk of incomplete propagation of changes to all copies. Although previous studies have investigated the consistency and threats of changing clones, changes have always been analyzed detached from another-not considering that individual clones may change more than once during their lifetime. In this paper we present our study on the patterns of consecutive changes to clones in real systems and discuss in how far they are suitable for identifying unwanted inconsistencies.","clone evolution, clone detection, software maintenance","","IWSC '11"
"Conference Paper","Lavoie T,Merlo E","Automated Type-3 Clone Oracle Using Levenshtein Metric","","2011","","","34–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985411;http://dx.doi.org/10.1145/1985404.1985411","10.1145/1985404.1985411","Clone detection techniques quality and performance evaluation require a system along with its clone oracle, that is a reference database of all accepted clones in the investigated system. Many challenges, including finding an adequate clone definition and scalability to industrial size systems, must be overcome to create good oracles. This paper presents an original method to construct clone oracles based on the Levenshtein metric. Although other oracles exist, this is the largest known oracle for type-3 clones that was created by an automated process on massive data sets. The method behind the creation of the oracle as well as actual oracles characteristics are presented. Discussion of the results in relation to other ways of building oracles is also provided along with future research possibilities.","clone benchmark, clone detection, software clones, type-3 clones","","IWSC '11"
"Conference Paper","Basit HA,Ali U,Jarzabek S","Viewing Simple Clones from Structural Clones' Perspective","","2011","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985406;http://dx.doi.org/10.1145/1985404.1985406","10.1145/1985404.1985406","In previous work, we described a technique for detecting designlevel similar program structures that we called structural clones. Structural clones are recurring configurations of simple clones (i.e., similar code fragments). In this paper, we show how structural clone analysis extends the benefits of analysis based on simple clones only. First, we present experimental results showing that in many cases simple clones participated in structural clones. In such cases, structural clones being larger than simple clones but smaller in number, allow analysts to see the ""forest from the trees"", as far as the similarity situation is concerned. We provide arguments and examples to show how the knowledge of structural clones - their location and exact similarities and differences - helps in program understanding, design recovery, maintenance, and refactoring.","code clones, high level similarities","","IWSC '11"
"Conference Paper","Tairas R,Jacob F,Gray J","Representing Clones in a Localized Manner","","2011","","","54–60","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on Software Clones","Waikiki, Honolulu, HI, USA","2011","9781450305884","","https://doi.org/10.1145/1985404.1985414;http://dx.doi.org/10.1145/1985404.1985414","10.1145/1985404.1985414","Code clones (i.e., duplicate sections of code) can be scattered throughout the source files of a program. Manually evaluating group of such clones requires observing each clone in its origina location (i.e., opening each file and finding the source location o each clone), which can be a time-consuming process. As a alternative, this paper introduces a technique that localizes th representation of code clones to provide a summary of th properties of two or more clones in one location. In our approach the results of a clone detection tool are analyzed in an automate manner to determine the properties (i.e., similarities an differences) of the clones. These properties are visualized directl within the source editor. The localized representation is realize as part of the features of an Eclipse plug-in called CeDAR.","representation, code clones, visualization","","IWSC '11"
"Conference Paper","Nugroho A,Visser J,Kuipers T","An Empirical Model of Technical Debt and Interest","","2011","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Workshop on Managing Technical Debt","Waikiki, Honolulu, HI, USA","2011","9781450305860","","https://doi.org/10.1145/1985362.1985364;http://dx.doi.org/10.1145/1985362.1985364","10.1145/1985362.1985364","Cunningham introduced the metaphor of technical debt as guidance for software developers that must trade engineering quality against short-term goals.We revisit the technical debt metaphor, and translate it into terms that can help IT executives better understand their IT investments. An approach is proposed to quantify debts (cost to fix technical quality issues) and interest (extra cost spent on maintenance due to technical quality issues). Our approach is based on an empirical assessment method of software quality developed at the Software Improvement Group (SIG). The core part of the technical debt calculation is constructed on the basis of empirical data of 44 systems that are currently being monitored by SIG.In a case study, we apply the approach to a real system, and discuss how the results provide useful insights on important questions related to IT investment such as the return on investment (ROI) in software quality improvement.","effort, maintenance, software economics, estimation, cost, measurement","","MTD '11"
"Conference Paper","Hemel A,Kalleberg KT,Vermaas R,Dolstra E","Finding Software License Violations through Binary Code Clone Detection","","2011","","","63–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985453;http://dx.doi.org/10.1145/1985441.1985453","10.1145/1985441.1985453","Software released in binary form frequently uses third-party packages without respecting their licensing terms. For instance, many consumer devices have firmware containing the Linux kernel, without the suppliers following the requirements of the GNU General Public License. Such license violations are often accidental, e.g., when vendors receive binary code from their suppliers with no indication of its provenance. To help find such violations, we have developed the Binary Analysis Tool (BAT), a system for code clone detection in binaries. Given a binary, such as a firmware image, it attempts to detect cloning of code from repositories of packages in source and binary form. We evaluate and compare the effectiveness of three of BAT's clone detection techniques: scanning for string literals, detecting similarity through data compression, and detecting similarity by computing binary deltas.","binary analysis, repository mining, code clone detection, firmware","","MSR '11"
"Conference Paper","Kim H,Jung Y,Kim S,Yi K","MeCC: Memory Comparison-Based Clone Detector","","2011","","","301–310","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985835;http://dx.doi.org/10.1145/1985793.1985835","10.1145/1985793.1985835","In this paper, we propose a new semantic clone detection technique by comparing programs' abstract memory states, which are computed by a semantic-based static analyzer.Our experimental study using three large-scale open source projects shows that our technique can detect semantic clones that existing syntactic- or semantic-based clone detectors miss. Our technique can help developers identify inconsistent clone changes, find refactoring candidates, and understand software evolution related to semantic clones.","clone detection, abstract interpretation, software maintenance, static analysis","","ICSE '11"
"Conference Paper","German DM,Davies J","Apples vs. Oranges? An Exploration of the Challenges of Comparing the Source Code of Two Software Systems","","2011","","","246–249","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985483;http://dx.doi.org/10.1145/1985441.1985483","10.1145/1985441.1985483","We attempt to compare the source code of two Java IDE systems: Netbeans and Eclipse. The result of this experiment shows that many factors, if ignored, could risk a bias in the results, and we posit various observations that should be taken into consideration to minimize such risk.","eclipse, netbeans","","MSR '11"
"Conference Paper","Jhi YC,Wang X,Jia X,Zhu S,Liu P,Wu D","Value-Based Program Characterization and Its Application to Software Plagiarism Detection","","2011","","","756–765","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985899;http://dx.doi.org/10.1145/1985793.1985899","10.1145/1985793.1985899","Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (1) most of them cannot handle advanced obfuscation techniques; (2) the methods based on source code analysis are less practical since the source code of suspicious programs is typically not available until strong evidences are collected; and (3) those depending on the features of specific operating systems or programming languages have limited applicability.Based on an observation that some critical runtime values are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.","dynamic code identification, software plagiarism detection","","ICSE '11"
"Conference Paper","Biegel B,Soetens QD,Hornig W,Diehl S,Demeyer S","Comparison of Similarity Metrics for Refactoring Detection","","2011","","","53–62","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985452;http://dx.doi.org/10.1145/1985441.1985452","10.1145/1985441.1985452","Identifying refactorings in software archives has been an active research topic in the last decade, mainly because it is a prerequisite for various software evolution analyses (e.g., error detection, capturing intent of change, capturing and replaying changes, and relating refactorings and software metrics). Many of these techniques rely on similarity measures to identify structurally equivalent code, however, up until now the effect of this similarity measure on the performance of the refactoring identification algorithm is largely unexplored. In this paper we replicate a well-known experiment from Weißgerber and Diehl, plugging in three different similarity measures (text-based, AST-based, token-based). We look at the overlap of the results obtained by the different metrics, and we compare the results using recall and the computation time. We conclude that the different result sets have a large overlap and that the three metrics perform with a comparable quality.","code clones, refactoring, mining software repositories, replication experiment, similarity metrics, software evolution","","MSR '11"
"Conference Paper","Canfora G,Cerulo L,Cimitile M,Di Penta M","Social Interactions around Cross-System Bug Fixings: The Case of FreeBSD and OpenBSD","","2011","","","143–152","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985463;http://dx.doi.org/10.1145/1985441.1985463","10.1145/1985441.1985463","Cross-system bug fixing propagation is frequent among systems having similar characteristics, using a common framework, or, in general, systems with cloned source code fragments. While previous studies showed that clones tend to be properly maintained within a single system, very little is known about cross-system bug management.This paper describes an approach to mine explicitly documented cross-system bug fixings, and to relate their occurrences to social characteristics of contributors discussing through the project mailing lists--e.g., degree, betweenness, and brokerage--as well as to the contributors' activity on source code.The paper reports results of an empirical study carried out on FreeBSD and OpenBSD kernels. The study shows that the phenomenon of cross-system bug fixing between these two projects occurs often, despite the limited overlap of contributors. The study also shows that cross-system bug fixings mainly involve contributors with the highest degree, betweenness and brokerage level, as well as contributors that change the source code more than others.","social network analysis, empirical study, code migration, bug fixing","","MSR '11"
"Conference Paper","Davies J,German DM,Godfrey MW,Hindle A","Software Bertillonage: Finding the Provenance of an Entity","","2011","","","183–192","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985468;http://dx.doi.org/10.1145/1985441.1985468","10.1145/1985441.1985468","Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components -- such as external libraries or cloned source code -- is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of fingerprints. As an example, we have developed a fast, simple, and approximate technique called anchored signature matching for identifying library version information within a given Java application. This technique involves a type of structured signature matching performed against a database of candidates drawn from the Maven2 repository, a 150GB collection of open source Java libraries. An exploratory case study using a proprietary e-commerce Java application illustrates that the approach is both feasible and effective.","bertillonage, provenance, code fingerprints, code evolution","","MSR '11"
"Conference Paper","Göde N,Koschke R","Frequency and Risks of Changes to Clones","","2011","","","311–320","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985836;http://dx.doi.org/10.1145/1985793.1985836","10.1145/1985793.1985836","Code Clones - duplicated source fragments - are said to increase maintenance effort and to facilitate problems caused by inconsistent changes to identical parts. While this is certainly true for some clones and certainly not true for others, it is unclear how many clones are real threats to the system's quality and need to be taken care of. Our analysis of clone evolution in mature software projects shows that most clones are rarely changed and the number of unintentional inconsistent changes to clones is small. We thus have to carefully select the clones to be managed to avoid unnecessary effort managing clones with no risk potential.","clone evolution, clone detection, software maintenance","","ICSE '11"
"Conference Paper","Nguyen TT,Nguyen HV,Nguyen HA,Nguyen TN","Aspect Recommendation for Evolving Software","","2011","","","361–370","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985843;http://dx.doi.org/10.1145/1985793.1985843","10.1145/1985793.1985843","Cross-cutting concerns are unavoidable and create difficulties in the development and maintenance of large-scale systems. In this paper, we present a novel approach that identifies certain groups of code units that potentially share some cross-cutting concerns and recommends them for creating and updating aspects. Those code units, called concern peers, are detected based on their similar interactions (similar calling relations in similar contexts, either internally or externally). The recommendation is applicable to both the aspectization of non-aspect-oriented programs (i.e. for aspect creation), and the evolution of aspect-oriented programs (i.e. for aspect updating). The empirical evaluation on several real-world software systems shows that our approach is scalable and provides useful recommendations.","aspect mining, concern peer, cross-cutting concern","","ICSE '11"
"Conference Paper","Wong S,Cai Y,Kim M,Dalton M","Detecting Software Modularity Violations","","2011","","","411–420","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985850;http://dx.doi.org/10.1145/1985793.1985850","10.1145/1985793.1985850","This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches.","design structure matrix, modularity violation detection, bad code smells, refactoring","","ICSE '11"
"Conference Paper","Kawrykow D,Robillard MP","Non-Essential Changes in Version Histories","","2011","","","351–360","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985842;http://dx.doi.org/10.1145/1985793.1985842","10.1145/1985793.1985842","Numerous techniques involve mining change data captured in software archives to assist engineering efforts, for example to identify components that tend to evolve together. We observed that important changes to software artifacts are sometimes accompanied by numerous non-essential modifications, such as local variable refactorings, or textual differences induced as part of a rename refactoring. We developed a tool-supported technique for detecting non-essential code differences in the revision histories of software systems. We used our technique to investigate code changes in over 24,000 change sets gathered from the change histories of seven long-lived open-source systems. We found that up to 15.5% of a system's method updates were due solely to non-essential differences. We also report on numerous observations on the distribution of non-essential differences in change history and their potential impact on change-based analyses.","differencing algorithms, software change analysis, mining software repositories","","ICSE '11"
"Conference Paper","Deissenboeck F,Heinemann L,Herrmannsdoerfer M,Lochmann K,Wagner S","The Quamoco Tool Chain for Quality Modeling and Assessment","","2011","","","1007–1009","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985977;http://dx.doi.org/10.1145/1985793.1985977","10.1145/1985793.1985977","Continuous quality assessment is crucial for the long-term success of evolving software. On the one hand, code analysis tools automatically supply quality indicators, but do not provide a complete overview of software quality. On the other hand, quality models define abstract characteristics that influence quality, but are not operationalized. Currently, no tool chain exists that integrates code analysis tools with quality models. To alleviate this, the Quamoco project provides a tool chain to both define and assess software quality. The tool chain consists of a quality model editor and an integration with the quality assessment toolkit ConQAT. Using the editor, we can define quality models ranging from abstract characteristics down to operationalized measures. From the quality model, a ConQAT configuration can be generated that can be used to automatically assess the quality of a software system.","quality assessment, tool chain, quality modeling","","ICSE '11"
"Conference Paper","Davies J","Measuring Subversions: Security and Legal Risk in Reused Software Artifacts","","2011","","","1149–1151","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1986025;http://dx.doi.org/10.1145/1985793.1986025","10.1145/1985793.1986025","A software system often includes a set of library dependencies and other software artifacts necessary for the system's proper operation. However, long-term maintenance problems related to reused software can gradually emerge over the lifetime of the deployed system. In our exploratory study we propose a manual technique to locate documented security and legal problems in a set of reused software artifacts. We evaluate our technique with a case study of 81 Java libraries found in a proprietary e-commerce web application. Using our approach we discovered both a potential legal problem with one library, and a second library that was affected by a known security vulnerability.These results support our larger thesis: software reuse entails long-term maintenance costs. In future work we strive to develop automated techniques by which developers, managers, and other software stakeholders can measure, address, and minimize these costs over the lifetimes of their software assets.","maintenance, security, licensing, reuse","","ICSE '11"
"Conference Paper","Fokaefs M,Tsantalis N,Stroulia E,Chatzigeorgiou A","JDeodorant: Identification and Application of Extract Class Refactorings","","2011","","","1037–1039","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985989;http://dx.doi.org/10.1145/1985793.1985989","10.1145/1985793.1985989","Evolutionary changes in object-oriented systems can result in large, complex classes, known as ""God Classes"". In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from ""God Classes"" and automatically apply the refactoring chosen by the developer.","object-oriented programming, clustering, software reengineering, refactoring","","ICSE '11"
"Conference Paper","Parnin C,Bird C,Murphy-Hill E","Java Generics Adoption: How New Features Are Introduced, Championed, or Ignored","","2011","","","3–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Working Conference on Mining Software Repositories","Waikiki, Honolulu, HI, USA","2011","9781450305747","","https://doi.org/10.1145/1985441.1985446;http://dx.doi.org/10.1145/1985441.1985446","10.1145/1985441.1985446","Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured whether generics actually provide such relief. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 20 popular open source Java programs, traversing more than 500 million lines of code in the process. We evaluate five hypotheses, each based on assertions made by prior researchers, about how Java developers use generics. For example, our results suggest that generics do not significantly reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers.","languages, java, generics, post-mortem analysis","","MSR '11"
"Conference Paper","Stolee KT,Elbaum S","Refactoring Pipe-like Mashups for End-User Programmers","","2011","","","81–90","Association for Computing Machinery","New York, NY, USA","Proceedings of the 33rd International Conference on Software Engineering","Waikiki, Honolulu, HI, USA","2011","9781450304450","","https://doi.org/10.1145/1985793.1985805;http://dx.doi.org/10.1145/1985793.1985805","10.1145/1985793.1985805","Mashups are becoming increasingly popular as end users are able to easily access, manipulate, and compose data from many web sources. We have observed, however, that mashups tend to suffer from deficiencies that propagate as mashups are reused. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on end-user programmers and observe that users generally prefer mashups without smells. We then introduce refactorings targeting those smells, reducing the complexity of the mashup programs, increasing their abstraction, updating broken data sources and dated components, and standardizing their structures to fit the community development patterns. Our assessment of a large sample of mashups shows that smells are present in 81% of them and that the proposed refactorings can reduce the number of smelly mashups to 16%, illustrating the potential of refactoring to support the thousands of end users programming mashups.","refactoring, end user software engineering, web mashups","","ICSE '11"
"Conference Paper","Villavicencio G","A Bottom-up Approach to Understand Functional Programs","","2011","","","111–120","Association for Computing Machinery","New York, NY, USA","Proceedings of The Fourth International C* Conference on Computer Science and Software Engineering","Montreal, Quebec, Canada","2011","9781450306263","","https://doi.org/10.1145/1992896.1992910;http://dx.doi.org/10.1145/1992896.1992910","10.1145/1992896.1992910","One affective way to carry out a program comprehension process is by refactoring the source code. In this paper we explore this approach in the functional programming paradigm, on Haskell programs specifically. As result, we have identified many correlations between the traditional (procedural and object-oriented) program comprehension process and the so called understanding-oriented refactorings, in the functional programming context. Besides, we have identified a catalog of refactorings used to improve program efficiency which, applied in reverse order, are valuable for program understanding purposes. Coupled to these refactorings, there is a calculational process by (means of) which we obtain a full formal description of program functionality. All together, a bottom-up program comprehension strategy in the functional setting is described.","program comprehension, refactorings, formal methods","","C3S2E '11"
"Journal Article","Tip F,Fuhrer RM,Kieżun A,Ernst MD,Balaban I,De Sutter B","Refactoring Using Type Constraints","ACM Trans. Program. Lang. Syst.","2011","33","3","","Association for Computing Machinery","New York, NY, USA","","","2011-05","","0164-0925","https://doi.org/10.1145/1961204.1961205;http://dx.doi.org/10.1145/1961204.1961205","10.1145/1961204.1961205","Type constraints express subtype relationships between the types of program expressions, for example, those relationships that are required for type correctness. Type constraints were originally proposed as a convenient framework for solving type checking and type inference problems. This paper shows how type constraints can be used as the basis for practical refactoring tools. In our approach, a set of type constraints is derived from a type-correct program P. The main insight behind our work is the fact that P constitutes just one solution to this constraint system, and that alternative solutions may exist that correspond to refactored versions of P. We show how a number of refactorings for manipulating types and class hierarchies can be expressed naturally using type constraints. Several refactorings in the standard distribution of Eclipse are based on our work.","program transformation, Refactoring, type constraints","",""
"Conference Paper","Lahiri M,Tantipathananandh C,Warungu R,Rubenstein DI,Berger-Wolf TY","Biometric Animal Databases from Field Photographs: Identification of Individual Zebra in the Wild","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st ACM International Conference on Multimedia Retrieval","Trento, Italy","2011","9781450303361","","https://doi.org/10.1145/1991996.1992002;http://dx.doi.org/10.1145/1991996.1992002","10.1145/1991996.1992002","We describe an algorithmic and experimental approach to a fundamental problem in field ecology: computer-assisted individual animal identification. We use a database of noisy photographs taken in the wild to build a biometric database of individual animals differentiated by their coat markings. A new image of an unknown animal can then be queried by its coat markings against the database to determine if the animal has been observed and identified before. Our algorithm, called StripeCodes, efficiently extracts simple image features and uses a dynamic programming algorithm to compare images. We test its accuracy against two different classes of methods: Eigenface, which is based on algebraic techniques, and matching multi-scale histograms of differential image features, an approach from signal processing. StripeCodes performs better than all competing methods for our dataset, and scales well with database size.","biometrics, image databases, edit distance, ecology","","ICMR '11"
"Conference Paper","Gray I,Audsley NC","Targeting Complex Embedded Architectures by Combining the Multicore Communications API (Mcapi) with Compile-Time Virtualisation","","2011","","","51–60","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2011 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems","Chicago, IL, USA","2011","9781450305556","","https://doi.org/10.1145/1967677.1967685;http://dx.doi.org/10.1145/1967677.1967685","10.1145/1967677.1967685","Within the domain of embedded systems, hardware architectures are commonly characterised by application-specific heterogeneity. Systems may contain multiple dissimilar processing elements, non-standard memory architectures, and custom hardware elements. The programming of such systems is a considerable challenge, not only because of the need to exploit large degrees of parallelism but also because hardware architectures change from system to system. To solve this problem, this paper proposes the novel combination of a new industry standard for communication across multicore architectures (MCAPI), with a minimal-overhead technique for targeting complex architectures with standard programming languages (Compile-Time Virtualisation).The Multicore Association have proposed MCAPI as an industry standard for on-chip communications. MCAPI abstracts the on-chip physical communication to provide the application with logical point-to-point unidirectional channels between nodes (software thread, hardware core, etc.). Compile-Time Virtualisation is used to provide an extremely lightweight implementation of MCAPI, that supports a much wider range of architectures than its specification normally considers. Overall, this unique combination enhances programmability by abstracting on-chip communication whilst also exposing critical parts of the target architecture to the programming language.","embedded, multicore, mcapi, virtualisation","","LCTES '11"
"Journal Article","Gray I,Audsley NC","Targeting Complex Embedded Architectures by Combining the Multicore Communications API (Mcapi) with Compile-Time Virtualisation","SIGPLAN Not.","2011","46","5","51–60","Association for Computing Machinery","New York, NY, USA","","","2011-04","","0362-1340","https://doi.org/10.1145/2016603.1967685;http://dx.doi.org/10.1145/2016603.1967685","10.1145/2016603.1967685","Within the domain of embedded systems, hardware architectures are commonly characterised by application-specific heterogeneity. Systems may contain multiple dissimilar processing elements, non-standard memory architectures, and custom hardware elements. The programming of such systems is a considerable challenge, not only because of the need to exploit large degrees of parallelism but also because hardware architectures change from system to system. To solve this problem, this paper proposes the novel combination of a new industry standard for communication across multicore architectures (MCAPI), with a minimal-overhead technique for targeting complex architectures with standard programming languages (Compile-Time Virtualisation).The Multicore Association have proposed MCAPI as an industry standard for on-chip communications. MCAPI abstracts the on-chip physical communication to provide the application with logical point-to-point unidirectional channels between nodes (software thread, hardware core, etc.). Compile-Time Virtualisation is used to provide an extremely lightweight implementation of MCAPI, that supports a much wider range of architectures than its specification normally considers. Overall, this unique combination enhances programmability by abstracting on-chip communication whilst also exposing critical parts of the target architecture to the programming language.","embedded, virtualisation, multicore, mcapi","",""
"Conference Paper","Bryant R,Tumanov A,Irzak O,Scannell A,Joshi K,Hiltunen M,Lagar-Cavilla A,de Lara E","Kaleidoscope: Cloud Micro-Elasticity via VM State Coloring","","2011","","","273–286","Association for Computing Machinery","New York, NY, USA","Proceedings of the Sixth Conference on Computer Systems","Salzburg, Austria","2011","9781450306348","","https://doi.org/10.1145/1966445.1966471;http://dx.doi.org/10.1145/1966445.1966471","10.1145/1966445.1966471","We introduce cloud micro-elasticity, a new model for cloud Virtual Machine (VM) allocation and management. Current cloud users over-provision long-lived VMs with large memory footprints to better absorb load spikes, and to conserve performance-sensitive caches. Instead, we achieve elasticity by swiftly cloning VMs into many transient, short-lived, fractional workers to multiplex physical resources at a much finer granularity. The memory of a micro-elastic clone is a logical replica of the parent VM state, including caches, yet its footprint is proportional to the workload, and often a fraction of the nominal maximum. We enable micro-elasticity through a novel technique dubbed VM state coloring, which classifies VM memory into sets of semantically-related regions, and optimizes the propagation, allocation and deduplication of these regions. Using coloring, we build Kaleidoscope and empirically demonstrate its ability to create micro-elastic cloned servers. We model the impact of micro-elasticity on a demand dataset from AT&T's cloud, and show that fine-grained multiplexing yields infrastructure reductions of 30% relative to state-of-the art techniques for managing elastic clouds.","cloud computing, virtualization","","EuroSys '11"
"Conference Paper","Hage J,Rademaker P,van Vugt N","Plagiarism Detection for Java: A Tool Comparison","","2011","","","33–46","Open Universiteit, Heerlen","Heerlen, NLD","Computer Science Education Research Conference","Heerlen, Netherlands","2011","","","","","In this paper we compare five tools for detecting plagiarism in Java source code texts: JPlag, Marble, moss, Plaggie, and sim. The tools are compared with respect to their features and performance. For the performance comparison we carried out two experiments: to compare the sensitivity of the tools for different plagiarism techniques we have applied the tools to a set of intentionally plagiarised programs. To get a picture of the precision of the tools, we have run the tools on several incarnations of a student assignment and compared the top 10's of the results.","Java, experimental evaluation, source code plagiarism","","CSERC '11"
"Journal Article","Canfora G,Di Penta M,Cerulo L","Achievements and Challenges in Software Reverse Engineering","Commun. ACM","2011","54","4","142–151","Association for Computing Machinery","New York, NY, USA","","","2011-04","","0001-0782","https://doi.org/10.1145/1924421.1924451;http://dx.doi.org/10.1145/1924421.1924451","10.1145/1924421.1924451","Deeply understanding the intricacies of software must always come before any considerations for modifying it.","","",""
"Conference Paper","Biggers LR,Kraft NA","Quantifying the Similiarities between Source Code Lexicons","","2011","","","80–85","Association for Computing Machinery","New York, NY, USA","Proceedings of the 49th Annual Southeast Regional Conference","Kennesaw, Georgia","2011","9781450306867","","https://doi.org/10.1145/2016039.2016067;http://dx.doi.org/10.1145/2016039.2016067","10.1145/2016039.2016067","Several recent static analysis techniques automate software understanding activities by extracting textual information from source code and applying information retrieval models to the extracted corpora. These source code retrieval techniques show efficacy, but the literature provides no guidance regarding configuration of their constituent processes. For example, the literature provides conflicting information regarding the benefit of extracting comments and string literals along with identifiers such as method or variable names. In this paper we present an initial investigation into the similarities between three source code lexicons described in the literature: identifiers, comments, and string literals. We address three research questions using a case study of six open source Java projects. The results indicate that methods uniquely contain from 30% to 60% of the projects' terms, whereas the comments uniquely contain from 22% to 45% of the terms. Future work includes analyzing the extent to which comments and string literals introduce domain terms rather than non-domain terms.","software evolution and maintenance, static analysis, program comprehension, information retrieval","","ACM-SE '11"
"Conference Paper","Jacob F,Gray J,Sun Y,Bangalore P","A Platform-Independent Tool for Modeling Parallel Programs","","2011","","","138–143","Association for Computing Machinery","New York, NY, USA","Proceedings of the 49th Annual Southeast Regional Conference","Kennesaw, Georgia","2011","9781450306867","","https://doi.org/10.1145/2016039.2016079;http://dx.doi.org/10.1145/2016039.2016079","10.1145/2016039.2016079","Programming languages that can utilize the underlying parallel architecture in shared memory, distributed memory or Graphics Processing Units (GPUs) are used extensively for solving scientific problems. However, from our observation of studying multiple parallel programs from various domains, such programming languages have a substantial amount of sequential code mixed with the parallel code. When rewriting the parallel code for another platform, the same sequential code is often reused without much modification. Although this is a common occurrence, existing tools and programming environments do not offer much support for this process. In this paper, we introduce a tool named PPmodel, which was designed and implemented to assist programmers in separating the core computation from the details of a specific parallel architecture. Using PPmodel, a programmer can identify and retarget the parallel section of a program to execute in a different platform. With PPmodel, a programmer is better enabled to focus on the parallel section of interest, while ignoring other parallel and sequential sections in a program. The tool is explained by example execution of the parallel section of an OpenMP program for the circuit satisfiability problem in a cluster using the Message Passing Interface (MPI).","parallel programming, model-driven engineering","","ACM-SE '11"
"Conference Paper","Yokomori R,Siy H,Yoshida N,Noro M,Inoue K","Measuring the Effects of Aspect-Oriented Refactoring on Component Relationships: Two Case Studies","","2011","","","215–226","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development","Porto de Galinhas, Brazil","2011","9781450306058","","https://doi.org/10.1145/1960275.1960301;http://dx.doi.org/10.1145/1960275.1960301","10.1145/1960275.1960301","Aspect-oriented refactoring is a promising technique for improving modularity and reducing complexity of existing software systems through encapsulating crosscutting concerns. As complexity of a system is often linked to the degree to which its components are connected, we investigate in this paper the impact of such refactoring activities on component relationships. We analyze two aspect-refactoring projects to determine circumstances when such activities are effective at reducing component relationships and when they are not. We measure two kinds of relationships between components, use and clone relations. We compare how these metrics changed between the original and the refactored system. Our findings indicate that aspect-oriented refactoring is successful in improving the modularity and complexity of the base code. However, we obtain mixed results when aspects are accounted for. Based on these results, we also discuss constraints to the technology as well as other design considerations that may limit the effectiveness of aspect-oriented refactoring on actual systems.","aspect-oriented programming, use-relation analysis, refactoring, coupling, code clone analysis","","AOSD '11"
"Conference Paper","Liebig J,Kästner C,Apel S","Analyzing the Discipline of Preprocessor Annotations in 30 Million Lines of C Code","","2011","","","191–202","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development","Porto de Galinhas, Brazil","2011","9781450306058","","https://doi.org/10.1145/1960275.1960299;http://dx.doi.org/10.1145/1960275.1960299","10.1145/1960275.1960299","The C preprocessor cpp is a widely used tool for implementing variable software. It enables programmers to express variable code (which may even crosscut the entire implementation) with conditional compilation. The C preprocessor relies on simple text processing and is independent of the host language (C, C++, Java, and so on). Language-independent text processing is powerful and expressive - programmers can make all kinds of annotations in the form of #ifdefs - but can render unpreprocessed code difficult to process automatically by tools, such as refactoring, concern management, and variability-aware type checking. We distinguish between disciplined annotations, which align with the underlying source-code structure, and undisciplined annotations, which do not align with the structure and hence complicate tool development. This distinction raises the question of how frequently programmers use undisciplined annotations and whether it is feasible to change them to disciplined annotations to simplify tool development and to enable programmers to use a wide variety of tools in the first place. By means of an analysis of 40 medium-sized to large-sized C programs, we show empirically that programmers use cpp mostly in a disciplined way: about 84% of all annotations respect the underlying source-code structure. Furthermore, we analyze the remaining undisciplined annotations, identify patterns, and discuss how to transform them into a disciplined form.","crosscutting concerns, virtual separation of concerns, preprocessor, conditional compilation, ifdef","","AOSD '11"
"Conference Paper","Figueiredo E,Garcia A,Maia M,Ferreira G,Nunes C,Whittle J","On the Impact of Crosscutting Concern Projection on Code Measurement","","2011","","","81–92","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development","Porto de Galinhas, Brazil","2011","9781450306058","","https://doi.org/10.1145/1960275.1960287;http://dx.doi.org/10.1145/1960275.1960287","10.1145/1960275.1960287","Many concern metrics have been defined to quantify properties of crosscutting concerns, such as scattering, tangling, and dedication. To quantify these properties, concern metrics directly rely on the projection (assignment) of concerns into source code. Although concern identification tools have emerged over the last years, they are still rarely used in practice to support concern projection and, therefore, it is a task often performed manually. This means that the results of concern metrics are likely to be influenced by how accurately programmers assign concerns to code elements. Even though concern assignment is an important and long-standing problem in software engineering, its impact on accurate measures of crosscutting concerns has never been studied and quantified. This paper presents a series of 5 controlled experiments to quantify and analyse the impact of concern projection on crosscutting concern measures. A set of 80 participants from 4 different institutions projected 10 concern instances into the source code of two software systems. We analyse the accuracy of concern projections independently made by developers, and their impact on a set of 12 concern metrics. Our results suggest that: (i) programmers are conservative when projecting crosscutting concerns, (ii) all concern metrics suffer with such conservative behaviour, and (iii) fine-grained tangling measures are more sensitive to different concern projections than coarse-grained scattering metrics.","concern projection, concern metrics, crosscutting concerns","","AOSD '11"
"Conference Paper","Macia Bertran I,Garcia A,von Staa A","An Exploratory Study of Code Smells in Evolving Aspect-Oriented Systems","","2011","","","203–214","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development","Porto de Galinhas, Brazil","2011","9781450306058","","https://doi.org/10.1145/1960275.1960300;http://dx.doi.org/10.1145/1960275.1960300","10.1145/1960275.1960300","Although aspect-oriented programming (AOP) aims to improve software maintainability, developers can unwittingly introduce code smells in their programs. A code smell is any symptom in the source code that possibly indicates a deeper maintainability problem. Even though a few code smells for AOP have been reported in the literature, there is no evidence if and how they occur in evolving software projects. There is also little knowledge on their actual impact on maintenance effort, such as required refactorings or corrective changes in later software releases. This paper presents an exploratory analysis of code smells recurrently observed in a set of evolving aspect-oriented systems. We analyzed instances of code smells previously reported in the literature and newly-revealed ones. Our study involved in total 18 releases of 3 evolving aspect-oriented systems from different domains. The outcome of our evaluation suggests that previously-documented AOP smells might not occur as often as claimed. Our analysis also revealed that: (1) newly-discovered code smells might occur more often than well-known ones, and (2) the former ones seemed to be consistently associated with non-trivial refactorings and corrective changes.","aspect-oriented programming, software maintenance, exploratory study, code smells","","AOSD '11"
"Conference Paper","Silva Filho RS,Bronsard F,Hasling WM","Experiences Documenting and Preserving Software Constraints Using Aspects","","2011","","","7–18","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development Companion","Porto de Galinhas, Brazil","2011","9781450306065","","https://doi.org/10.1145/1960314.1960321;http://dx.doi.org/10.1145/1960314.1960321","10.1145/1960314.1960321","Software systems are increasingly being built as compositions of reusable artifacts (components, frameworks, toolkits, plug-ins, APIs, etc) that have non-trivial usage constraints in the form of interface contracts, underlying assumptions and design composition rules. Satisfying these constraints is challenging: they are often not well documented; or they are difficult to integrate into the software development process in ways that allow their identification by developers; or they may not be enforced by existing tools and development environments. Aspect-Oriented Programming has been advocated as an approach to represent and enforce software constraints in code artifacts. Aspects can be used to detect constraint violations, or more pro-actively, to ensure that the constraints are satisfied without requiring the developer's attention. This paper discusses our experience using aspects to document and enforce software constraints in an industrial application, specifically TDE/UML, a model-driven software testing tool developed at SIEMENS. We present an analysis of common constraints found in our case study, a set of primitive aspects developed to help the enforcement of software constraints, and show how AOP has been incorporated into existing software development and governance approaches in the TDE/UML project. We conclude with a discussion of strengths and limitations of AspectJ in supporting these constraints.","aspect-oriented programming, design documentation, architectural constraints, software architecture","","AOSD '11"
"Conference Paper","Lämmel R,Pek E,Starek J","Large-Scale, AST-Based API-Usage Analysis of Open-Source Java Projects","","2011","","","1317–1324","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2011 ACM Symposium on Applied Computing","TaiChung, Taiwan","2011","9781450301138","","https://doi.org/10.1145/1982185.1982471;http://dx.doi.org/10.1145/1982185.1982471","10.1145/1982185.1982471","Research on API migration and language conversion can be informed by empirical data about API usage. For instance, such data may help with designing and defending mapping rules for API migration in terms of relevance and applicability. We describe an approach to large-scale API-usage analysis of open-source Java projects, which we also instantiate for the Source-Forge open-source repository in a certain way. Our approach covers checkout, building, tagging with metadata, fact extraction, analysis, and synthesis with a large degree of automation. Fact extraction relies on resolved (type-checked) ASTs. We describe a few examples of API-usage analysis; they are motivated by API migration. These examples are concerned with analysing API footprint (such as the numbers of distinct APIs used in a project), API coverage (such as the percentage of methods of an API used in a corpus), and framework-like vs. class-library-like usage.","","","SAC '11"
"Conference Paper","Moret P,Binder W,Tanter É","Polymorphic Bytecode Instrumentation","","2011","","","129–140","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Conference on Aspect-Oriented Software Development","Porto de Galinhas, Brazil","2011","9781450306058","","https://doi.org/10.1145/1960275.1960292;http://dx.doi.org/10.1145/1960275.1960292","10.1145/1960275.1960292","Bytecode instrumentation is a widely used technique to implement aspect weaving and dynamic analyses in virtual machines such as the Java Virtual Machine. Aspect weavers and other instrumentations are usually developed independently and combining them often requires significant engineering effort, if at all possible. In this paper we introduce polymorphic bytecode instrumentation (PBI), a simple but effective technique that allows dynamic dispatch amongst several, possibly independent instrumentations.PBI enables complete bytecode coverage, that is, any method with a bytecode representation can be instrumented. We illustrate further benefits of PBI with three case studies. First, we provide an implementation of execution levels for AspectJ, which avoid infinite regression and unwanted interference between aspects. Second, we present a framework for adaptive dynamic analysis, where the analysis to be performed can be changed at runtime by the user. Third, we describe how PBI can be used to support a form of dynamic mixin layers. We provide thorough performance evaluations with dynamic analysis aspects applied to standard benchmarks. We show that PBI-based execution levels are much faster than control flow pointcuts to avoid interference between aspects, and that their efficient integration in a practical aspect language is possible. We also demonstrate that PBI enables adaptive dynamic analysis tools that are more reactive to user inputs than existing tools that rely on dynamic aspect-oriented programming with runtime weaving.","dynamic program analysis, modularity constructs, bytecode instrumentation, aspect-oriented programming, java virtual machine, mixin layers","","AOSD '11"
"Journal Article","Majumdar D,Kanjilal A,Bhattacharya S","Separation of Scattered Concerns: A Graph Based Approach for Aspect Mining","SIGSOFT Softw. Eng. Notes","2011","36","2","1–11","Association for Computing Machinery","New York, NY, USA","","","2011-03","","0163-5948","https://doi.org/10.1145/1943371.1943387;http://dx.doi.org/10.1145/1943371.1943387","10.1145/1943371.1943387","Aspect Mining is a dynamic area of research in the field of Software Engineering. Aspects are concerns that are intermingled with other concerns thereby reducing the understandability, maintainability and scalability of the code. The concept of Separation of Concerns (SoC) is often achieved untill the Design Phase, but gets difficult in the later phases of the software development life cycle (SDLC). During program maintenance the maintenance team is left with an aggregation of procedures and variables, both of which may be generically called user-defined tokens. This paper proposes a graph-based approach to address the problem of SoC during program maintenance. This is done by the removal of some source code elements (e.g., user-defined-tokens), which can be responsible for tangled concerns and complex code. These user-definedtokens can be treated separately under the Aspect Oriented Programming paradigm. The paper proposes a graphical-model, which represents a procedural program and defines a mathematical- model to identify and remove the tangled and interleaving code-fragments. Thereafter these code fragments are traced back to the requirements engineering level through a formal traceability model. This process yields the corresponding user requirements that are associated with these scattered code fragments. These identified user requirements are put forward as Aspects, to be handled or re-engineered under the Aspect Oriented Programming paradigm.","separation-of-concerns, relational algebra, aspect mining, user-defined tokens","",""
"Conference Paper","Han TD,Abdelrahman TS","Reducing Branch Divergence in GPU Programs","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth Workshop on General Purpose Processing on Graphics Processing Units","Newport Beach, California, USA","2011","9781450305693","","https://doi.org/10.1145/1964179.1964184;http://dx.doi.org/10.1145/1964179.1964184","10.1145/1964179.1964184","Branch divergence has a significant impact on the performance of GPU programs. We propose two novel software-based optimizations, called iteration delaying and branch distribution that aim to reduce branch divergence. Iteration delaying targets a divergent branch enclosed by a loop within a kernel. It improves performance by executing loop iterations that take the same branch direction and delaying those that take the other direction until later iterations. Branch distribution reduces the length of divergent code by factoring out structurally similar code from the branch paths. We conduct a preliminary evaluation of the two optimizations using both synthetic benchmarks and a highly-optimized real-world application. Our evaluation shows that they improve the performance of the synthetic benchmarks by as much as 30% and 80% respectively, and that of the real-world application by 12% and 16% respectively.","data parallel programming, branch divergence, GPGPU","","GPGPU-4"
"Conference Paper","Sharma A,Sarangdevot SS","Investigating the Application of AOP Methodology in Development of Bioinformatics Software Using Eclipse-AJDT Environment","","2011","","","731–736","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference & Workshop on Emerging Trends in Technology","Mumbai, Maharashtra, India","2011","9781450304498","","https://doi.org/10.1145/1980022.1980182;http://dx.doi.org/10.1145/1980022.1980182","10.1145/1980022.1980182","The application of AOP methodology has been investigated in the development of Bioinformatics Software -- Bioseqsearch, using Eclipse-AJDT environment. The software aims to reveal the biological significance of an unknown sequence using similarity search through biological databases using NCBI BLAST via internet, thus deciphering the structure and biological function. Crosscutting concerns have been identified and modularized into aspects, thus reducing the complexity of the design due to elimination of code scattering and tangling. The impact of using this methodology on various quality factors of the software has been examined. The study assesses the usefulness of AOP methodology in design and implementation of real world bioinformatics software projects.","Eclipse-AJDT, bioinformatics software, aspect-oriented programming, separation of concerns","","ICWET '11"
"Conference Paper","Lim JS,Ji JH,Cho HG,Woo G","Plagiarism Detection among Source Codes Using Adaptive Local Alignment of Keywords","","2011","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Ubiquitous Information Management and Communication","Seoul, Korea","2011","9781450305716","","https://doi.org/10.1145/1968613.1968643;http://dx.doi.org/10.1145/1968613.1968643","10.1145/1968613.1968643","This paper proposes a new method for detecting plagiarized pairs of source codes among a large set of source codes. The typical algorithms for detecting code plagiarism, which are largely exploited up to now, are based on Greedy-String Tiling or on local alignments of the two strings. This paper introduces a variant of the local alignment, namely, the adaptive local alignment, which exploits an adaptive similarity matrix. Each entry of the adaptive similarity matrix is the logarithm of the probabilities of the keywords based on the frequencies in a given set of programs. We experimented with this method using a set of programs submitted to more than 10 real programming contests. According to the experimental results, the distribution of the adaptive local alignment is more sensitive than that of the previous local alignments that used a fixed similarity matrix (+1 for match, −1 for mismatch, and −2 for gap), and the performance of the adaptive local alignment is superior to Greedy-String Tiling for detecting various plagiarism cases.","software similarity, similarity measurement, adaptive local alignment, program plagiarism detection, plagiarism","","ICUIMC '11"
"Conference Paper","Jenista JC,Eom YH,Demsky BC","OoOJava: Software out-of-Order Execution","","2011","","","57–68","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming","San Antonio, TX, USA","2011","9781450301190","","https://doi.org/10.1145/1941553.1941563;http://dx.doi.org/10.1145/1941553.1941563","10.1145/1941553.1941563","Developing parallel software using current tools can be challenging. Even experts find it difficult to reason about the use of locks and often accidentally introduce race conditions and deadlocks into parallel software. OoOJava is a compiler-assisted approach that leverages developer annotations along with static analysis to provide an easy-to-use deterministic parallel programming model. OoOJava extends Java with a task annotation that instructs the compiler to consider a code block for out-of-order execution. OoOJava executes tasks as soon as their data dependences are resolved and guarantees that the execution of an annotated program preserves the exact semantics of the original sequential program. We have implemented OoOJava and achieved an average speedup of 16.6x on our ten benchmarks.","deterministic parallel programming, out-of-order execution","","PPoPP '11"
"Journal Article","Jenista JC,Eom YH,Demsky BC","OoOJava: Software out-of-Order Execution","SIGPLAN Not.","2011","46","8","57–68","Association for Computing Machinery","New York, NY, USA","","","2011-02","","0362-1340","https://doi.org/10.1145/2038037.1941563;http://dx.doi.org/10.1145/2038037.1941563","10.1145/2038037.1941563","Developing parallel software using current tools can be challenging. Even experts find it difficult to reason about the use of locks and often accidentally introduce race conditions and deadlocks into parallel software. OoOJava is a compiler-assisted approach that leverages developer annotations along with static analysis to provide an easy-to-use deterministic parallel programming model. OoOJava extends Java with a task annotation that instructs the compiler to consider a code block for out-of-order execution. OoOJava executes tasks as soon as their data dependences are resolved and guarantees that the execution of an annotated program preserves the exact semantics of the original sequential program. We have implemented OoOJava and achieved an average speedup of 16.6x on our ten benchmarks.","deterministic parallel programming, out-of-order execution","",""
"Journal Article","Lagar-Cavilla HA,Whitney JA,Bryant R,Patchin P,Brudno M,de Lara E,Rumble SM,Satyanarayanan M,Scannell A","SnowFlock: Virtual Machine Cloning as a First-Class Cloud Primitive","ACM Trans. Comput. Syst.","2011","29","1","","Association for Computing Machinery","New York, NY, USA","","","2011-02","","0734-2071","https://doi.org/10.1145/1925109.1925111;http://dx.doi.org/10.1145/1925109.1925111","10.1145/1925109.1925111","A basic building block of cloud computing is virtualization. Virtual machines (VMs) encapsulate a user’s computing environment and efficiently isolate it from that of other users. VMs, however, are large entities, and no clear APIs exist yet to provide users with programatic, fine-grained control on short time scales.We present SnowFlock, a paradigm and system for cloud computing that introduces VM cloning as a first-class cloud abstraction. VM cloning exploits the well-understood and effective semantics of UNIX fork. We demonstrate multiple usage models of VM cloning: users can incorporate the primitive in their code, can wrap around existing toolchains via scripting, can encapsulate the API within a parallel programming framework, or can use it to load-balance and self-scale clustered servers.VM cloning needs to be efficient to be usable. It must efficiently transmit VM state in order to avoid cloud I/O bottlenecks. We demonstrate how the semantics of cloning aid us in realizing its efficiency: state is propagated in parallel to multiple VM clones, and is transmitted during runtime, allowing for optimizations that substantially reduce the I/O load. We show detailed microbenchmark results highlighting the efficiency of our optimizations, and macrobenchmark numbers demonstrating the effectiveness of the different usage models of SnowFlock.","Virtualization, cloud computing","",""
"Conference Paper","Vardoulakis D,Shivers O","Ordering Multiple Continuations on the Stack","","2011","","","13–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the 20th ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation","Austin, Texas, USA","2011","9781450304856","","https://doi.org/10.1145/1929501.1929504;http://dx.doi.org/10.1145/1929501.1929504","10.1145/1929501.1929504","Passing multiple continuation arguments to a function in CPS form allows one to encode a wide variety of direct-style control constructs, such as conditionals, exceptions, and multi-return function calls. We show that, with a simple syntactic restriction on the CPS language, one can prove that these multi-continuation arguments can be compiled into stack frames in the traditional manner. The restriction comes with no loss in expressive power, since we can still encode the same control mechanisms.In addition, we show that tail calls can be generalized efficiently for many continuations because the run-time check to determine which continuation to pop to can be avoided with a simple static analysis. A prototype implementation in Scheme48 shows that our analysis is very precise.","continuation-passing style, flow analysis, tail recursion","","PEPM '11"
"Conference Paper","Harland J","Towards Methods for Discovering Universal Turing Machines (or How Universal Unicorns Can Be Discovered, Not Created)","","2011","","","151–160","Australian Computer Society, Inc.","AUS","Proceedings of the Seventeenth Computing: The Australasian Theory Symposium - Volume 119","Perth, Australia","2011","9781920682989","","","","Universal Turing machines are a well-known concept in computer science. Most often, universal Turing machines are constructed by humans, and designed with particular features in mind. This is especially true of recent efforts to find small universal Turing machines, as these are often the product of sophisticated human reasoning. In this paper we take a different approach, in that we investigate how we can search through a number of Turing machines and recognise universal ones. This means that we have to examine very carefully the concepts involved, including the notion of what it means for a Turing machine to be universal, and what implications there are for the way that Turing machines are coded as input strings.","","","CATS '11"
"Conference Paper","Harland J","Towards Methods for Discovering Universal Turing Machines (or How Universal Unicorns Can Be Discovered, Not Created)","","2011","","","151–160","Australian Computer Society, Inc.","AUS","Proceedings of the Seventeenth Computing on The Australasian Theory Symposium - Volume 119","Perth, Australia","2011","9781920682989","","","","Universal Turing machines are a well-known concept in computer science. Most often, universal Turing machines are constructed by humans, and designed with particular features in mind. This is especially true of recent efforts to find small universal Turing machines, as these are often the product of sophisticated human reasoning. In this paper we take a different approach, in that we investigate how we can search through a number of Turing machines and recognise universal ones. This means that we have to examine very carefully the concepts involved, including the notion of what it means for a Turing machine to be universal, and what implications there are for the way that Turing machines are coded as input strings.","","","CATS 2011"
"Conference Paper","Corney M,Lister R,Teague D","Early Relational Reasoning and the Novice Programmer: Swapping as the ""Hello World"" of Relational Reasoning","","2011","","","95–104","Australian Computer Society, Inc.","AUS","Proceedings of the Thirteenth Australasian Computing Education Conference - Volume 114","Perth, Australia","2011","9781920682941","","","","We report on a longitudinal research study of the development of novice programmers in their first semester of programming. In the third week, almost half of our sample of students could not answer an explain-in-plain-English question, for code consisting of just three assignment statements, which swapped the values in two variables. We regard code that swaps the values of two variables as the simplest case of where a programming student can manifest a SOLO relational response. Our results demonstrate that the problems many students face with understanding code can begin very early, on relatively trivial code. However, using traditional programming exercises, these problems often go undetected until late in the semester. New approaches are required to detect and fix these problems earlier.","novice programmer, chunking, SOLO","","ACE '11"
"Conference Paper","Lee MW,Roh JW,Hwang SW,Kim S","Instant Code Clone Search","","2010","","","167–176","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering","Santa Fe, New Mexico, USA","2010","9781605587912","","https://doi.org/10.1145/1882291.1882317;http://dx.doi.org/10.1145/1882291.1882317","10.1145/1882291.1882317","In this paper, we propose a scalable instant code clone search engine for large-scale software repositories. While there are commercial code search engines available, they treat software as text and often fail to find semantically related code. Meanwhile, existing tools for semantic code clone searches take a ""post-mortem"" approach involving the detection of clones ""after"" the code development is completed, and hence, fail to return the results instantly. In clear contrast, we combine the strength of these two lines of existing research, by supporting instant code clone detection. To achieve this goal, we propose scalable indexing structures on vector abstractions of code. Our proposed algorithms allow developers to detect clones of a given code segment among the 1.7 million code segments from 492 open source projects in sub-second response times, without compromising the accuracy obtained by a state-of-the-art tool.","code search, clone detection","","FSE '10"
"Conference Paper","Gabel M,Su Z","A Study of the Uniqueness of Source Code","","2010","","","147–156","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering","Santa Fe, New Mexico, USA","2010","9781605587912","","https://doi.org/10.1145/1882291.1882315;http://dx.doi.org/10.1145/1882291.1882315","10.1145/1882291.1882315","This paper presents the results of the first study of the uniqueness of source code. We define the uniqueness of a unit of source code with respect to the entire body of written software, which we approximate with a corpus of 420 million lines of source code. Our high-level methodology consists of examining a collection of 6,000 software projects and measuring the degree to which each project can be `assembled' solely from portions of this corpus, thus providing a precise measure of `uniqueness' that we call syntactic redundancy. We parameterized our study over a variety of variables, the most important of which being the level of granularity at which we view source code. Our suite of experiments together consumed approximately four months of CPU time, providing quantitative answers to the following questions: at what levels of granularity is software unique, and at a given level of granularity, how unique is software? While we believe these questions to be of intrinsic interest, we discuss possible applications to genetic programming and developer productivity tools.","source code, large scale study, software uniqueness","","FSE '10"
"Conference Paper","Bajracharya SK,Ossher J,Lopes CV","Leveraging Usage Similarity for Effective Retrieval of Examples in Code Repositories","","2010","","","157–166","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering","Santa Fe, New Mexico, USA","2010","9781605587912","","https://doi.org/10.1145/1882291.1882316;http://dx.doi.org/10.1145/1882291.1882316","10.1145/1882291.1882316","Developers often learn to use APIs (Application Programming Interfaces) by looking at existing examples of API usage. Code repositories contain many instances of such usage of APIs. However, conventional information retrieval techniques fail to perform well in retrieving API usage examples from code repositories. This paper presents Structural Semantic Indexing (SSI), a technique to associate words to source code entities based on similarities of API usage. The heuristic behind this technique is that entities (classes, methods, etc.) that show similar uses of APIs are semantically related because they do similar things. We evaluate the effectiveness of SSI in code retrieval by comparing three SSI based retrieval schemes with two conventional baseline schemes. We evaluate the performance of the retrieval schemes by running a set of 20 candidate queries against a repository containing 222,397 source code entities from 346 jars belonging to the Eclipse framework. The results of the evaluation show that SSI is effective in improving the retrieval of examples in code repositories.","api usage, code search, software information retrieval, ssi, structural semantic indexing","","FSE '10"
"Conference Paper","Le Goues C,Forrest S,Weimer W","The Case for Software Evolution","","2010","","","205–210","Association for Computing Machinery","New York, NY, USA","Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research","Santa Fe, New Mexico, USA","2010","9781450304276","","https://doi.org/10.1145/1882362.1882406;http://dx.doi.org/10.1145/1882362.1882406","10.1145/1882362.1882406","Many software systems exceed our human ability to comprehend and manage, and they continue to contain unacceptable errors. This is an unintended consequence of Moore's Law, which has led to increases in system size, complexity, and interconnectedness. Yet, software is still primarily created, modified, and maintained by humans. The interactions among heterogeneous programs, machines and human operators has reached a level of complexity rivaling that of some biological ecosystems. By viewing software as an evolving complex system, researchers could incorporate biologically inspired mechanisms and employ the quantitative analysis methods of evolutionary biology. This approach could improve our understanding and analysis of software; it could lead to robust methods for automatically writing, debugging and improving code; and it could improve predictions about functional and structural transitions as scale increases. In the short term, an evolutionary perspective challenges several research assumptions, enabling advances in error detection, correction, and prevention.","evolutionary computation, genetic programming, software engineering, program repair","","FoSER '10"
"Conference Paper","Brown N,Cai Y,Guo Y,Kazman R,Kim M,Kruchten P,Lim E,MacCormack A,Nord R,Ozkaya I,Sangwan R,Seaman C,Sullivan K,Zazworka N","Managing Technical Debt in Software-Reliant Systems","","2010","","","47–52","Association for Computing Machinery","New York, NY, USA","Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research","Santa Fe, New Mexico, USA","2010","9781450304276","","https://doi.org/10.1145/1882362.1882373;http://dx.doi.org/10.1145/1882362.1882373","10.1145/1882362.1882373","Delivering increasingly complex software-reliant systems demands better ways to manage the long-term effects of short-term expedients. The technical debt metaphor is gaining significant traction in the agile development community as a way to understand and communicate such issues. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a ""debt"": on which ""interest"" has to be paid and which the ""principal"" should be repaid at some point for the long-term health of the project. We argue that the software engineering research community has an opportunity to study and improve this concept. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. Therefore, we propose managing technical debt as a part of the future research agenda for the software engineering field.","design decision trade-off, cost-benefit analysis, technical debt, software metrics, large-scale system development","","FoSER '10"
"Conference Paper","Arnold KC,Lieberman H","Embracing Ambiguity","","2010","","","1–6","Association for Computing Machinery","New York, NY, USA","Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research","Santa Fe, New Mexico, USA","2010","9781450304276","","https://doi.org/10.1145/1882362.1882364;http://dx.doi.org/10.1145/1882362.1882364","10.1145/1882362.1882364","Software helps people fulfill their goals, but development tools lack understanding of those goals. But if development tools did understand how software artifacts relate to higher-level intents and goals, they could help developers reuse code, solve problems, and develop systems that are more robust and easier to use. In this paper, we suggest that supporting software development at a stage before concrete formalization is an area of opportunity for software engineering research. We discuss three aspects that are both core challenges and opportunities for this research area: handling ambiguity, understanding human situations, and flexible reflection about failure, and identify research results suggesting that substantial progress can be made on these problems within a decade. We believe that this research will make it easier to develop software that is more broadly useful and robust, even in the face of everyday uncertainty and failure.","ambiguity, informal representation","","FoSER '10"
"Conference Paper","Memon A,Porter A,Sussman A","Community-Based, Collaborative Testing and Analysis","","2010","","","239–244","Association for Computing Machinery","New York, NY, USA","Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research","Santa Fe, New Mexico, USA","2010","9781450304276","","https://doi.org/10.1145/1882362.1882412;http://dx.doi.org/10.1145/1882362.1882412","10.1145/1882362.1882412","This article proposes a research agenda aimed at enabling optimized testing and analysis processes and tools to support component-based software development communities. We hypothesize that de facto communities---sets of projects that provide, maintain and integrate many shared infrastructure components---are commonplace. Currently, community members, often unknown to each other, tend to work in isolation, duplicating work, failing to learn from each other's effort, and missing opportunities to efficiently improve the common infrastructure. We further hypothesize that as software integration continues to become the predominant mode of software development, there will be increasing value in tools and techniques that empower these communities to coordinate and optimize their development efforts, and to generate and broadly share information. Such tools and techniques will greatly improve the robustness, quality and usability of the common infrastructure which, in turn, will greatly reduce the time and effort needed to produce and use the end systems that are the true goal of the entire community.","component-based software development communities, testing and analysis","","FoSER '10"
"Journal Article","Kim J,Chou PH","Energy-Efficient Progressive Remote Update for Flash-Based Firmware of Networked Embedded Systems","ACM Trans. Des. Autom. Electron. Syst.","2010","16","1","","Association for Computing Machinery","New York, NY, USA","","","2010-11","","1084-4309","https://doi.org/10.1145/1870109.1870116;http://dx.doi.org/10.1145/1870109.1870116","10.1145/1870109.1870116","Firmware update over a network connection is an essential but expensive feature for many embedded systems due to not only the relatively high power consumption and limited bandwidth, but also page-granular erasure before rewriting to flash memory. This work proposes a page-level, link-time technique that minimizes not only the size of patching scripts but also perturbation to the firmware memory, over the entire sequence of updates in the system’s lifetime. We propose a tool that first clusters functions to minimize caller-callee dependency across pages, and then orders the functions within each page to minimize intrapage perturbation. Experimental results show our technique to reduce the energy consumption of firmware update by 30--42% over the state-of-the-art. Most importantly, this is the first work that has ever shown to evolve well over 41 revisions of a real-world open-source real-time operating system.","embedded systems, NOR flash memory, diff, High-level analysis, progressive code update, page, clycomatic complexity","",""
"Journal Article","Lang F,Salaün G,Hérilier R,Kramer J,Magee J","Translating FSP into LOTOS and Networks of Automata","Form. Asp. Comput.","2010","22","6","681–711","Springer-Verlag","Berlin, Heidelberg","","","2010-11","","0934-5043","https://doi.org/10.1007/s00165-009-0133-8;http://dx.doi.org/10.1007/s00165-009-0133-8","10.1007/s00165-009-0133-8","Many process calculi have been proposed since Robin Milner and Tony Hoare opened the way more than 25 years ago. Although they are based on the same kernel of operators, most of them are incompatible in practice. We aim at reducing the gap between process calculi, and especially making possible the joint use of underlying tool support. Finite state processes (FSP) is a widely used calculus equipped with Ltsa, a graphical and user-friendly tool. Language of temporal ordering specification (Lotos) is the only process calculus that has led to an international standard, and is supported by the Cadp verification toolbox. We propose a translation of FSP sequential processes into Lotos. Since FSP composite processes (i.e., parallel compositions of processes) are hard to encode directly in Lotos, they are translated into networks of automata which are another input language accepted by Cadp. Hence, it is possible to use jointly Ltsa and Cadp to validate FSP specifications. Our approach is completely automated by a translator tool.","Verification, Process algebra, Communicating automata, Lotos, FSP, Parallel composition, Automated translation","",""
"Conference Paper","Murphy-Hill E,Black AP","An Interactive Ambient Visualization for Code Smells","","2010","","","5–14","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Symposium on Software Visualization","Salt Lake City, Utah, USA","2010","9781450300285","","https://doi.org/10.1145/1879211.1879216;http://dx.doi.org/10.1145/1879211.1879216","10.1145/1879211.1879216","Code smells are characteristics of software that indicate that code may have a design problem. Code smells have been proposed as a way for programmers to recognize the need for restructuring their software. Because code smells can go unnoticed while programmers are working, tools called smell detectors have been developed to alert programmers to the presence of smells in their code, and to help them understand the cause of those smells. In this paper, we propose a novel smell detector called Stench Blossom that provides an interactive ambient visualization designed to first give programmers a quick, high-level overview of the smells in their code, and then, if they wish, to help in understanding the sources of those code smells. We also describe a laboratory experiment with 12 programmers that tests several hypotheses about our tool. Our findings suggest that programmers can use our tool effectively to identify smells and to make refactoring judgements. This is partly because the tool serves as a memory aid, and partly because it is more reliable and easier to use than heuristics for analyzing smells.","usability, software, code smells, refactoring","","SOFTVIS '10"
"Conference Paper","Adamoli A,Hauswirth M","Trevis: A Context Tree Visualization & Analysis Framework and Its Use for Classifying Performance Failure Reports","","2010","","","73–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Symposium on Software Visualization","Salt Lake City, Utah, USA","2010","9781450300285","","https://doi.org/10.1145/1879211.1879224;http://dx.doi.org/10.1145/1879211.1879224","10.1145/1879211.1879224","When developers profile their applications to identify performance problems, they normally use profilers producing calling context trees. A calling context tree (CCT) represents the caller-callee relationships of a program. A dynamically collected CCT provides method coverage information. The CCTs produced by profilers also include method hotness information. Trevis, our context tree visualization and analysis framework, allows users to visualize, compare, cluster, and intersect CCTs produced by profilers. We evaluate Trevis in the context of a novel profiling tool called FlyBy. FlyBy runs transparently on an end-user's computer and continuously samples the applications' call stack. When the user perceives the application as sluggish, she presses a ""Was Slow!"" button to tell FlyBy to file a performance failure report. The report contains the CCT based on the call stack samples FlyBy gathered over the last few seconds before the user pressed the button. We show how Trevis allows us to visualize and classify FlyBy bug reports.","perceptible performance, tree visualization and analysis","","SOFTVIS '10"
"Conference Paper","Mehnert H","Extending Dylan's Type System for Better Type Inference and Error Detection","","2010","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 International Conference on Lisp","Reno/Tahoe, Nevada, USA","2010","9781450304702","","https://doi.org/10.1145/1869643.1869645;http://dx.doi.org/10.1145/1869643.1869645","10.1145/1869643.1869645","Whereas dynamic typing enables rapid prototyping and easy experimentation, static typing provides early error detection and better compile time optimization. Gradual typing [26] provides the best of both worlds. This paper shows how to define and implement gradual typing in Dylan, traditionally a dynamically typed language. Dylan poses several special challenges for gradual typing, such as multiple return values, variable-arity methods and generic functions (multiple dispatch).In this paper Dylan is extended with function types and parametric polymorphism. We implemented the type system and aunification-based type inference algorithm in the mainstream Dylan compiler. As case study we use the Dylan standard library (roughly 32000 lines of code), which witnesses that the implementation generates faster code with fewer errors. Some previously undiscovered errors in the Dylan library were revealed.","dynamic typing, gradual typing, type inference, multimethods, dylan, generic functions, object-orientation, static typing, lisp, multiple dispatch, compilers","","ILC '10"
"Conference Paper","Gabel M,Yang J,Yu Y,Goldszmidt M,Su Z","Scalable and Systematic Detection of Buggy Inconsistencies in Source Code","","2010","","","175–190","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Reno/Tahoe, Nevada, USA","2010","9781450302036","","https://doi.org/10.1145/1869459.1869475;http://dx.doi.org/10.1145/1869459.1869475","10.1145/1869459.1869475","Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.","bug detection, static analysis, clone detection","","OOPSLA '10"
"Journal Article","Gabel M,Yang J,Yu Y,Goldszmidt M,Su Z","Scalable and Systematic Detection of Buggy Inconsistencies in Source Code","SIGPLAN Not.","2010","45","10","175–190","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1932682.1869475;http://dx.doi.org/10.1145/1932682.1869475","10.1145/1932682.1869475","Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.","static analysis, bug detection, clone detection","",""
"Conference Paper","Chatterji D,Massengill B,Oslin J,Carver JC,Kraft NA","Measuring the Efficacy of Code Clone Information: An Empirical Study","","2010","","","","Association for Computing Machinery","New York, NY, USA","Evaluation and Usability of Programming Languages and Tools","Reno, Nevada","2010","9781450305471","","https://doi.org/10.1145/1937117.1937121;http://dx.doi.org/10.1145/1937117.1937121","10.1145/1937117.1937121","Much recent research effort has been devoted to designing efficient code clone detection algorithms and tools. However, there has been little human-based empirical study of how the output of those tools is used by developers when performing maintenance tasks. In this study 43 computer science graduate students completed a bug localization task in which a clone was present while researchers observed their activities. The goal of the study was to understand how those developers use clone information to perform this task. The results of this study showed that participants who used the clone information correctly, i.e. they first identified a defect then used it to look for clones of the defect, were more effective than participants who used the information incorrectly. The results also showed that participants who had industrial experience were more effective than those without industrial experience.","","","PLATEAU '10"
"Conference Paper","Merkle B","Stop the Software Architecture Erosion: Building Better Software Systems","","2010","","","129–138","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion","Reno/Tahoe, Nevada, USA","2010","9781450302401","","https://doi.org/10.1145/1869542.1869563;http://dx.doi.org/10.1145/1869542.1869563","10.1145/1869542.1869563","In lots of software projects unfortunately an architectural erosion happens over time. Modules which were independent, become connected, plug-ins finally depend on each other, and in general the architecture gets violated more and more. In this paper we will discuss how to avoid such architecture- and design-erosion and how an already eroded system can be fixed again. We will look at three different level of static analysis and examine architectural analysis in detail. Also typical use cases for architectural analysis are examined, followed by a collection of requirements for powerful tool support. The eclipse platform serves as case study and we look if, and how far architectural erosion happened there and if it can be fixed. Finally we discuss pros and cons of architectural analysis and conclude with an out view.","static code analysis, design, architecture, erosion, refactoring, simulation, software","","OOPSLA '10"
"Conference Paper","Sindhgatta R,Narendra NC,Sengupta B","Software Evolution in Agile Development: A Case Study","","2010","","","105–114","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion","Reno/Tahoe, Nevada, USA","2010","9781450302401","","https://doi.org/10.1145/1869542.1869560;http://dx.doi.org/10.1145/1869542.1869560","10.1145/1869542.1869560","The agile development method (ADM) is characterized by continuous feedback and change, and a software system developed using ADMevolves continuously through short iterations. Empirical studies on evolution of software following agile development method have been sparse. Most studies on software evolution have been performed on systems built using traditional (waterfall) development methods or using the open source development approach. This paper summarizes our study on the evolution of an enterprise software system following ADM. We evaluated key characteristics of evolution in the light of Lehman's laws of software evolution dealing with continuous change and growth, self-regulation and conservation, increasing complexity and declining quality. Our study indicates that most laws of evolution are followed by the system. We also present our observations on agile practices such as collective code ownership, test driven development and collaboration when the team is distributed.","evolution, agile methods, scrum","","OOPSLA '10"
"Conference Paper","Arnold KC,Lieberman H","Managing Ambiguity in Programming by Finding Unambiguous Examples","","2010","","","877–884","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Reno/Tahoe, Nevada, USA","2010","9781450302036","","https://doi.org/10.1145/1869459.1869531;http://dx.doi.org/10.1145/1869459.1869531","10.1145/1869459.1869531","We propose a new way to raise the level of discourse in the programming process: permit ambiguity, but manage it by linking it to unambiguous examples. This allows programming environments to work with informal descriptions that lack precise semantics, such as natural language descriptions or conceptual diagrams, without requiring programmers to formulate their ideas in a formal language first. As an example of this idea, we present Zones, a code search and reuse interface that connects code with ambiguous natural language statements about its purpose. The backend, called ProcedureSpace, relates purpose statements, static code analysis features, and natural language background knowledge. ProcedureSpace can search for code given statements of purpose or vice versa, and can find code that was never annotated or commented. Since completed Zones searches become annotations, system coverage grows with user interaction. Users in a preliminary study found that reasoning jointly over natural language and programming language helped them reuse code.","common sense, reuse, ambiguity, informal representation, natural language, blending","","OOPSLA '10"
"Journal Article","Arnold KC,Lieberman H","Managing Ambiguity in Programming by Finding Unambiguous Examples","SIGPLAN Not.","2010","45","10","877–884","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1932682.1869531;http://dx.doi.org/10.1145/1932682.1869531","10.1145/1932682.1869531","We propose a new way to raise the level of discourse in the programming process: permit ambiguity, but manage it by linking it to unambiguous examples. This allows programming environments to work with informal descriptions that lack precise semantics, such as natural language descriptions or conceptual diagrams, without requiring programmers to formulate their ideas in a formal language first. As an example of this idea, we present Zones, a code search and reuse interface that connects code with ambiguous natural language statements about its purpose. The backend, called ProcedureSpace, relates purpose statements, static code analysis features, and natural language background knowledge. ProcedureSpace can search for code given statements of purpose or vice versa, and can find code that was never annotated or commented. Since completed Zones searches become annotations, system coverage grows with user interaction. Users in a preliminary study found that reasoning jointly over natural language and programming language helped them reuse code.","informal representation, common sense, natural language, reuse, ambiguity, blending","",""
"Conference Paper","Barzilay O,Yehudai A,Hazzan O","Developers Attentiveness to Example Usage","","2010","","","","Association for Computing Machinery","New York, NY, USA","Human Aspects of Software Engineering","Reno, Nevada","2010","9781450305433","","https://doi.org/10.1145/1938595.1938599;http://dx.doi.org/10.1145/1938595.1938599","10.1145/1938595.1938599","As part of our research on the use of code examples in professional programming, we focus on the developer's awareness and attentiveness to example usage. We identify three types of lack of attentiveness: lack of awareness that causes professional developers to use examples only in certain contexts but not in others, lack of attentiveness to examples of different scales, and lack of attentiveness to the variety of purposes examples may serve. We present a qualitative research followed by a focus group case study that illustrate our findings.","attentiveness, example usage, example embedding ecosystem, qualitative methodology, examples, empirical software engineering research, software activity, awareness","","HAoSE '10"
"Conference Paper","Mercadal J,Enard Q,Consel C,Loriant N","A Domain-Specific Approach to Architecturing Error Handling in Pervasive Computing","","2010","","","47–61","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Reno/Tahoe, Nevada, USA","2010","9781450302036","","https://doi.org/10.1145/1869459.1869465;http://dx.doi.org/10.1145/1869459.1869465","10.1145/1869459.1869465","The challenging nature of error handling constantly escalates as a growing number of environments consists of networked devices and software components. In these environments, errors cover a uniquely large spectrum of situations related to each layer ranging from hardware to distributed platforms, to software components. Handling errors becomes a daunting task for programmers, whose outcome is unpredictable. Scaling up error handling requires to raise the level of abstraction beyond the code level and the try-catch construct, approaching error handling at the software architecture level.We propose a novel approach that relies on an Architecture Description Language (ADL), which is extended with error-handling declarations. To further raise the level of abstraction, our approach revolves around a domain-specific architectural pattern commonly used in pervasive computing. Error handling is decomposed into components dedicated to platform-wide, error-recovery strategies. At the application level, descriptions of functional components include declarations dedicated to error handling.We have implemented a compiler for an ADL extended with error-handling declarations. It produces customized programming frameworks that drive and support the programming of error handling. Our approach has been validated with a variety of applications for building automation.","pervasive computing, architecture description languages, domain-specific languages, exception","","OOPSLA '10"
"Journal Article","Mercadal J,Enard Q,Consel C,Loriant N","A Domain-Specific Approach to Architecturing Error Handling in Pervasive Computing","SIGPLAN Not.","2010","45","10","47–61","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1932682.1869465;http://dx.doi.org/10.1145/1932682.1869465","10.1145/1932682.1869465","The challenging nature of error handling constantly escalates as a growing number of environments consists of networked devices and software components. In these environments, errors cover a uniquely large spectrum of situations related to each layer ranging from hardware to distributed platforms, to software components. Handling errors becomes a daunting task for programmers, whose outcome is unpredictable. Scaling up error handling requires to raise the level of abstraction beyond the code level and the try-catch construct, approaching error handling at the software architecture level.We propose a novel approach that relies on an Architecture Description Language (ADL), which is extended with error-handling declarations. To further raise the level of abstraction, our approach revolves around a domain-specific architectural pattern commonly used in pervasive computing. Error handling is decomposed into components dedicated to platform-wide, error-recovery strategies. At the application level, descriptions of functional components include declarations dedicated to error handling.We have implemented a compiler for an ADL extended with error-handling declarations. It produces customized programming frameworks that drive and support the programming of error handling. Our approach has been validated with a variety of applications for building automation.","domain-specific languages, architecture description languages, pervasive computing, exception","",""
"Conference Paper","Kell S","Component Adaptation and Assembly Using Interface Relations","","2010","","","322–340","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Reno/Tahoe, Nevada, USA","2010","9781450302036","","https://doi.org/10.1145/1869459.1869487;http://dx.doi.org/10.1145/1869459.1869487","10.1145/1869459.1869487","Software's expense owes partly to frequent reimplementation of similar functionality and partly to maintenance of patches, ports or components targeting evolving interfaces. More modular non-invasive approaches are unpopular because they entail laborious wrapper code. We propose Cake, a rule-based language describing compositions using interface relations. To evaluate it, we compare several existing wrappers with reimplemented Cake versions, finding the latter to be simpler and better modularised.","assembly, interoperation, adaptation, composition, interoperability, components, cake","","OOPSLA '10"
"Journal Article","Kell S","Component Adaptation and Assembly Using Interface Relations","SIGPLAN Not.","2010","45","10","322–340","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1932682.1869487;http://dx.doi.org/10.1145/1932682.1869487","10.1145/1932682.1869487","Software's expense owes partly to frequent reimplementation of similar functionality and partly to maintenance of patches, ports or components targeting evolving interfaces. More modular non-invasive approaches are unpopular because they entail laborious wrapper code. We propose Cake, a rule-based language describing compositions using interface relations. To evaluate it, we compare several existing wrappers with reimplemented Cake versions, finding the latter to be simpler and better modularised.","composition, components, assembly, interoperability, cake, interoperation, adaptation","",""
"Conference Paper","Schaefer M,de Moor O","Specifying and Implementing Refactorings","","2010","","","286–301","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications","Reno/Tahoe, Nevada, USA","2010","9781450302036","","https://doi.org/10.1145/1869459.1869485;http://dx.doi.org/10.1145/1869459.1869485","10.1145/1869459.1869485","Modern IDEs for object-oriented languages like Java provide support for a basic set of simple automated refactorings whose behaviour is easy to describe intuitively. It is, however, surprisingly difficult to specify their behaviour in detail. In particular, the popular precondition-based approach tends to produce somewhat unwieldy descriptions if advanced features of the object language are taken into account. This has resulted in refactoring implementations that are complex, hard to understand, and even harder to maintain, yet these implementations themselves are the only precise ""specification"" of many refactorings. We have in past work advocated a different approach based on several complementary notions of dependencies that guide the implementation, and on the concept of microrefactorings that structure it. We show in this work that these concepts are powerful enough to provide high-level specifications of many of the refactorings implemented in Eclipse. These specifications are precise enough to serve as the basis of a clean-room reimplementation of these refactorings that is very compact, yet matches Eclipse's for features and outperforms it in terms of correctness.","specification, refactoring, language extensions","","OOPSLA '10"
"Journal Article","Schaefer M,de Moor O","Specifying and Implementing Refactorings","SIGPLAN Not.","2010","45","10","286–301","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1932682.1869485;http://dx.doi.org/10.1145/1932682.1869485","10.1145/1932682.1869485","Modern IDEs for object-oriented languages like Java provide support for a basic set of simple automated refactorings whose behaviour is easy to describe intuitively. It is, however, surprisingly difficult to specify their behaviour in detail. In particular, the popular precondition-based approach tends to produce somewhat unwieldy descriptions if advanced features of the object language are taken into account. This has resulted in refactoring implementations that are complex, hard to understand, and even harder to maintain, yet these implementations themselves are the only precise ""specification"" of many refactorings. We have in past work advocated a different approach based on several complementary notions of dependencies that guide the implementation, and on the concept of microrefactorings that structure it. We show in this work that these concepts are powerful enough to provide high-level specifications of many of the refactorings implemented in Eclipse. These specifications are precise enough to serve as the basis of a clean-room reimplementation of these refactorings that is very compact, yet matches Eclipse's for features and outperforms it in terms of correctness.","specification, language extensions, refactoring","",""
"Conference Paper","LaToza TD,Myers BA","Hard-to-Answer Questions about Code","","2010","","","","Association for Computing Machinery","New York, NY, USA","Evaluation and Usability of Programming Languages and Tools","Reno, Nevada","2010","9781450305471","","https://doi.org/10.1145/1937117.1937125;http://dx.doi.org/10.1145/1937117.1937125","10.1145/1937117.1937125","To build new tools and programming languages that make it easier for professional software developers to create, debug, and understand code, it is helpful to better understand the questions that developers ask during coding activities. We surveyed professional software developers and asked them to list hard-to-answer questions that they had recently asked about code. 179 respondents reported 371 questions. We then clustered these questions into 21 categories and 94 distinct questions. The most frequently reported categories dealt with intent and rationale -- what does this code do, what is it intended to do, and why was it done this way? Many questions described very specific situations -- e.g., what does the code do when an error occurs, how to refactor without breaking callers, or the implications of a specific change on security. These questions revealed opportunities for both existing research tools to help developers and for developing new languages and tools that make answering these questions easier.","program comprehension, developer questions","","PLATEAU '10"
"Conference Paper","Schulze S,Apel S,Kästner C","Code Clones in Feature-Oriented Software Product Lines","","2010","","","103–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the Ninth International Conference on Generative Programming and Component Engineering","Eindhoven, The Netherlands","2010","9781450301541","","https://doi.org/10.1145/1868294.1868310;http://dx.doi.org/10.1145/1868294.1868310","10.1145/1868294.1868310","Some limitations of object-oriented mechanisms are known to cause code clones (e.g., extension using inheritance). Novel programming paradigms such as feature-oriented programming (FOP) aim at alleviating these limitations. However, it is an open issue whether FOP is really able to avoid code clones or whether it even facilitates (FOP-related) clones. To address this issue, we conduct an empirical analysis on ten feature-oriented software product lines with respect to code cloning. We found that there is a considerable number of clones in feature-oriented software product lines and that a large fraction of these clones is FOP-related (i.e., caused by limitations of feature-oriented mechanisms). Based on our results, we initiate a discussion on the reasons for FOP-related clones and on how to cope with them. We show by means of examples how such clones can be removed by applying refactorings.","software product lines, feature-oriented programming, refactoring, code clones","","GPCE '10"
"Journal Article","Schulze S,Apel S,Kästner C","Code Clones in Feature-Oriented Software Product Lines","SIGPLAN Not.","2010","46","2","103–112","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1942788.1868310;http://dx.doi.org/10.1145/1942788.1868310","10.1145/1942788.1868310","Some limitations of object-oriented mechanisms are known to cause code clones (e.g., extension using inheritance). Novel programming paradigms such as feature-oriented programming (FOP) aim at alleviating these limitations. However, it is an open issue whether FOP is really able to avoid code clones or whether it even facilitates (FOP-related) clones. To address this issue, we conduct an empirical analysis on ten feature-oriented software product lines with respect to code cloning. We found that there is a considerable number of clones in feature-oriented software product lines and that a large fraction of these clones is FOP-related (i.e., caused by limitations of feature-oriented mechanisms). Based on our results, we initiate a discussion on the reasons for FOP-related clones and on how to cope with them. We show by means of examples how such clones can be removed by applying refactorings.","refactoring, software product lines, feature-oriented programming, code clones","",""
"Conference Paper","Rompf T,Odersky M","Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs","","2010","","","127–136","Association for Computing Machinery","New York, NY, USA","Proceedings of the Ninth International Conference on Generative Programming and Component Engineering","Eindhoven, The Netherlands","2010","9781450301541","","https://doi.org/10.1145/1868294.1868314;http://dx.doi.org/10.1145/1868294.1868314","10.1145/1868294.1868314","Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used.We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process.We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.","code generation, language virtualization, domain-specific languages, multi-stage programming","","GPCE '10"
"Journal Article","Rompf T,Odersky M","Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs","SIGPLAN Not.","2010","46","2","127–136","Association for Computing Machinery","New York, NY, USA","","","2010-10","","0362-1340","https://doi.org/10.1145/1942788.1868314;http://dx.doi.org/10.1145/1942788.1868314","10.1145/1942788.1868314","Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used.We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process.We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.","language virtualization, multi-stage programming, domain-specific languages, code generation","",""
"Conference Paper","Kapova L,Goldschmidt T,Happe J,Reussner RH","Domain-Specific Templates for Refinement Transformations","","2010","","","69–78","Association for Computing Machinery","New York, NY, USA","Proceedings of the First International Workshop on Model-Driven Interoperability","Oslo, Norway","2010","9781450302920","","https://doi.org/10.1145/1866272.1866282;http://dx.doi.org/10.1145/1866272.1866282","10.1145/1866272.1866282","Model transformations are a major instrument of model-driven software development. Especially in declarative transformation approaches, the structuring of transformations depends to a large extent on the structure of the source models and the generated artefacts. In many cases, similar code is written for transformations that deal with the same source or target metamodel. Writing such transformations can be simplified significantly if re-occurring parts within the transformation rules can be specified in a reusable way. Current approaches to transformation development include means for transformation reuse as well as inheritance. However, modularisation along the boundaries of different parts of domain metamodels is still lacking. Furthermore, the possibilities to reuse transformation fragments that re-occur in multiple transformations is limited. In this paper, we introduce domain-specific templates for refinement transformations with well-defined variation points. Transformation templates are based on known design patterns and enable a modular specification of refinement transformations and thus yield a simpler definition of transformations that can be grasped more easily and developed more efficiently. In addition, we present a real-world case study of transformation templates in the context of component based software architectures. The case study gives insight into the application of the presented approach.","higher-order transformations, templates, refinement transformations, software architecture","","MDI '10"
"Conference Paper","Arts T,Thompson S","From Test Cases to FSMs: Augmented Test-Driven Development and Property Inference","","2010","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th ACM SIGPLAN Workshop on Erlang","Baltimore, Maryland, USA","2010","9781450302531","","https://doi.org/10.1145/1863509.1863511;http://dx.doi.org/10.1145/1863509.1863511","10.1145/1863509.1863511","This paper uses the inference of finite state machines from EUnit test suites for Erlang programs to make two contributions. First, we show that the inferred FSMs provide feedback on the adequacy of the test suite that is developed incrementally during the test-driven development of a system. This is novel because the feedback we give is independent of the implementation of the system.Secondly, we use FSM inference to develop QuickCheck properties for testing state-based systems. This has the effect of transforming a fixed set of tests into a property which can be tested using randomly generated data, substantially widening the coverage and scope of the tests.","unit test, property, inference, erlang, quickcheck, finite-state machine, test-driven development, eunit, tdd","","Erlang '10"
"Conference Paper","Terei DA,Chakravarty MM","An LlVM Backend for GHC","","2010","","","109–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third ACM Haskell Symposium on Haskell","Baltimore, Maryland, USA","2010","9781450302524","","https://doi.org/10.1145/1863523.1863538;http://dx.doi.org/10.1145/1863523.1863538","10.1145/1863523.1863538","In the presence of ever-changing computer architectures, high-quality optimising compiler backends are moving targets that require specialist knowledge and sophisticated algorithms. In this paper, we explore a new backend for the Glasgow Haskell Compiler (GHC) that leverages the Low Level Virtual Machine (LLVM), a new breed of compiler written explicitly for use by other compiler writers, not high-level programmers, that promises to enable outsourcing of low-level and architecture-dependent aspects of code generation. We discuss the conceptual challenges and our backend design. We also provide an extensive quantitative evaluation of the performance of the backend and of the code it produces.","llvm, backend, ghc","","Haskell '10"
"Journal Article","Terei DA,Chakravarty MM","An LlVM Backend for GHC","SIGPLAN Not.","2010","45","11","109–120","Association for Computing Machinery","New York, NY, USA","","","2010-09","","0362-1340","https://doi.org/10.1145/2088456.1863538;http://dx.doi.org/10.1145/2088456.1863538","10.1145/2088456.1863538","In the presence of ever-changing computer architectures, high-quality optimising compiler backends are moving targets that require specialist knowledge and sophisticated algorithms. In this paper, we explore a new backend for the Glasgow Haskell Compiler (GHC) that leverages the Low Level Virtual Machine (LLVM), a new breed of compiler written explicitly for use by other compiler writers, not high-level programmers, that promises to enable outsourcing of low-level and architecture-dependent aspects of code generation. We discuss the conceptual challenges and our backend design. We also provide an extensive quantitative evaluation of the performance of the backend and of the code it produces.","llvm, backend, ghc","",""
"Conference Paper","Rebêlo HM,Lima R,Kulesza U,Coelho R,Mota A,Ribeiro M,Araújo JE","The Contract Enforcement Aspect Pattern","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Latin American Conference on Pattern Languages of Programs","Salvador, Bahia, Brazil","2010","9781450302609","","https://doi.org/10.1145/2581507.2581513;http://dx.doi.org/10.1145/2581507.2581513","10.1145/2581507.2581513","The most fundamental motivation for employing contracts in the development of OO applications is to improve the reliability. Contract enforcement is a well-known established technique in object-oriented (OO) programming. However, the need to intercept well defined points in the execution of a program to check design constraints makes the enforcement of contracts a crosscutting concern. Thus, contract enforcement code is interwined with the business code, hindering maintenance. Moreover, because of the difficulty in separating contract enforcement code and business code, the former is often duplicated across several different places within a system. In this paper we present the Contract Enforcement Aspect pattern, which documents an aspect-oriented solution for the modularization of the contract concern.","design by contract, aspect-oriented programming","","SugarLoafPLoP '10"
"Conference Paper","Hotta K,Sano Y,Higo Y,Kusumoto S","Is Duplicate Code More Frequently Modified than Non-Duplicate Code in Software Evolution? An Empirical Study on Open Source Software","","2010","","","73–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the Joint ERCIM Workshop on Software Evolution (EVOL) and International Workshop on Principles of Software Evolution (IWPSE)","Antwerp, Belgium","2010","9781450301282","","https://doi.org/10.1145/1862372.1862390;http://dx.doi.org/10.1145/1862372.1862390","10.1145/1862372.1862390","Various kinds of research efforts have been performed on the basis that the presence of duplicate code has a negative impact on software evolution. A typical example is that, if we modify a code fragment that has been duplicated to other code fragments, it is necessary to consider whether the other code fragments have to be modified simultaneously or not. In this research, in order to investigate how much the presence of duplicate code is related to software evolution, we defined a new indicator, modification frequency. The indicator is a quantitative measure, and it allows us to objectively compare the maintainability of duplicate code and non-duplicate code. We conducted an experiment on 15 open source software systems, and the result showed that the presence of duplicate code does not have a negative impact on software evolution.","empirical study, software maintenance, duplicate code","","IWPSE-EVOL '10"
"Conference Paper","Pham NH,Nguyen TT,Nguyen HA,Nguyen TN","Detection of Recurring Software Vulnerabilities","","2010","","","447–456","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering","Antwerp, Belgium","2010","9781450301169","","https://doi.org/10.1145/1858996.1859089;http://dx.doi.org/10.1145/1858996.1859089","10.1145/1858996.1859089","Software security vulnerabilities are discovered on an almost daily basis and have caused substantial damage. Aiming at supporting early detection and resolution for them, we have conducted an empirical study on thousands of vulnerabilities and found that many of them are recurring due to software reuse. Based on the knowledge gained from the study, we developed SecureSync, an automatic tool to detect recurring software vulnerabilities on the systems that reuse source code or libraries. The core of SecureSync includes two techniques to represent and compute the similarity of vulnerable code across different systems. The evaluation for 60 vulnerabilities on 176 releases of 119 open-source software systems shows that SecureSync is able to detect recurring vulnerabilities with high accuracy and to identify 90 releases having potentially vulnerable code that are not reported or fixed yet, even in mature systems. A couple of cases were actually confirmed by their developers.","recurring vulnerabilities, software security, software reuse","","ASE '10"
"Conference Paper","Wang X,Lo D,Cheng J,Zhang L,Mei H,Yu JX","Matching Dependence-Related Queries in the System Dependence Graph","","2010","","","457–466","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering","Antwerp, Belgium","2010","9781450301169","","https://doi.org/10.1145/1858996.1859091;http://dx.doi.org/10.1145/1858996.1859091","10.1145/1858996.1859091","In software maintenance and evolution, it is common that developers want to apply a change to a number of similar places. Due to the size and complexity of the code base, it is challenging for developers to locate all the places that need the change. A main challenge in locating the places that need the change is that, these places share certain common dependence conditions but existing code searching techniques can hardly handle dependence relations satisfactorily. In this paper, we propose a technique that enables developers to make queries involving dependence conditions and textual conditions on the system dependence graph of the program. We carried out an empirical evaluation on four searching tasks taken from the development history of two real-world projects. The results of our evaluation indicate that, compared with code-clone detection, our technique is able to locate many required code elements that code-clone detection cannot locate, and compared with text search, our technique is able to effectively reduce false positives without losing any required code elements.","code search, system dependence graph, graph indexing","","ASE '10"
"Conference Paper","Shang W,Adams B,Hassan AE","An Experience Report on Scaling Tools for Mining Software Repositories Using MapReduce","","2010","","","275–284","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering","Antwerp, Belgium","2010","9781450301169","","https://doi.org/10.1145/1858996.1859050;http://dx.doi.org/10.1145/1858996.1859050","10.1145/1858996.1859050","The need for automated software engineering tools and techniques continues to grow as the size and complexity of studied systems and analysis techniques increase. Software engineering researchers often scale their analysis techniques using specialized one-off solutions, expensive infrastructures, or heuristic techniques (e.g., search-based approaches). However, such efforts are not reusable and are often costly to maintain. The need for scalable analysis is very prominent in the Mining Software Repositories (MSR) field, which specializes in the automated recovery and analysis of large data stored in software repositories. In this paper, we explore the scaling of automated software engineering analysis techniques by reusing scalable analysis platforms from the web field. We use three representative case studies from the MSR field to analyze the potential of the MapReduce platform to scale MSR tools with minimal effort. We document our experience such that other researchers could benefit from them. We find that many of the web field's guidelines for using the MapReduce platform need to be modified to better fit the characteristics of software engineering problems.","cloud computing, mining software repositories, mapreduce","","ASE '10"
"Conference Paper","Duley A,Spandikow C,Kim M","A Program Differencing Algorithm for Verilog HDL","","2010","","","477–486","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering","Antwerp, Belgium","2010","9781450301169","","https://doi.org/10.1145/1858996.1859093;http://dx.doi.org/10.1145/1858996.1859093","10.1145/1858996.1859093","During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as diff is inherently limited because existing program differencing tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents Vdiff, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. We evaluated Vdiff on two open source hardware design projects. The evaluation result shows that Vdiff is very accurate, with overall 96.8% precision and 97.3% recall when using manually classified differences as a basis of comparison.","change types, verilog, empirical study, program differencing, hardware description languages","","ASE '10"
"Conference Paper","Sindhgatta R,Narendra NC,Sengupta B,Visweswariah K,Ryman AG","Timesheet Assistant: Mining and Reporting Developer Effort","","2010","","","265–274","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering","Antwerp, Belgium","2010","9781450301169","","https://doi.org/10.1145/1858996.1859049;http://dx.doi.org/10.1145/1858996.1859049","10.1145/1858996.1859049","Timesheets are an important instrument used to track time spent by team members in a software project on the tasks assigned to them. In a typical project, developers fill timesheets manually on a periodic basis. This is often tedious, time consuming and error prone. Over or under reporting of time spent on tasks causes errors in billing development costs to customers and wrong estimation baselines for future work, which can have serious business consequences. In order to assist developers in filling their timesheets accurately, we present a tool called Timesheet Assistant (TA) that non-intrusively mines developer activities and uses statistical analysis on historical data to estimate the actual effort the developer may have spent on individual assigned tasks. TA further helps the developer or project manager by presenting the details of the activities along with effort data so that the effort may be seen in the context of the actual work performed. We report on an empirical study of TA in a software maintenance project at IBM that provides preliminary validation of its feasibility and usefulness. Some of the limitations of the TA approach and possible ways to address those are also discussed.","development activity, timesheet, mining, estimation","","ASE '10"
"Conference Paper","Higgins BD,Reda A,Alperovich T,Flinn J,Giuli TJ,Noble B,Watson D","Intentional Networking: Opportunistic Exploitation of Mobile Network Diversity","","2010","","","73–84","Association for Computing Machinery","New York, NY, USA","Proceedings of the Sixteenth Annual International Conference on Mobile Computing and Networking","Chicago, Illinois, USA","2010","9781450301817","","https://doi.org/10.1145/1859995.1860005;http://dx.doi.org/10.1145/1859995.1860005","10.1145/1859995.1860005","Mobile devices face a diverse and dynamic set of networking options. Using those options to the fullest requires knowledge of application intent. This paper describes Intentional Networking, a simple but powerful mechanism for handling network diversity. Applications supply a declarative label for network transmissions, and the system matches transmissions to the most appropriate network. The system may also defer and re-order opportunistic transmissions subject to application-supplied mutual exclusion and ordering constraints. We have modified three applications to use Intentional Networking: BlueFS, a distributed file system for pervasive computing, Mozilla's Thunderbird e-mail client, and a vehicular participatory sensing application. We evaluated the performance of these applications using measurements obtained by driving a vehicle through WiFi and cellular 3G network coverage. Compared to an idealized solution that makes optimal use of all aggregated available networks but that lacks knowledge of application intent, Intentional Networking improves the latency of interactive messages from 48% to 13x, while adding no more than 7% throughput overhead.","wireless network selection, application-aware networking","","MobiCom '10"
"Conference Paper","Lavazza L,Morasca S,Taibi D,Tosi D","Predicting OSS Trustworthiness on the Basis of Elementary Code Assessment","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","Bolzano-Bozen, Italy","2010","9781450300391","","https://doi.org/10.1145/1852786.1852834;http://dx.doi.org/10.1145/1852786.1852834","10.1145/1852786.1852834","Background. Open Source Software (OSS) provides increasingly serious and viable alternatives to traditional closed source software. The number of OSS users is continuously growing, as is the number of potential users that are interested in evaluating the quality of OSS. The latter would greatly benefit from simple methods for evaluating the trustworthiness of OSS.Objective. This paper aims at finding a quantitative relationship between the perceived quality of OSS and a few simple objective measures.Method. the users' and developers' evaluations of trustworthiness and reliability of OSS products were collected and correlated to static code measures, called ""Elementary Code Assessment"" rules, which check very simple rules that well-written code should satisfy.Results. The result of the analysis is a set of quantitative models that link static measures of the source code to perceivable qualities of OSS. These models can be used by: 1) end-users and developers that would like to reuse existing OSS products and components, to evaluate the level of trustworthiness and reliability that can be expected based on the characteristics of code; 2) developers of OSS products, who can set code quality targets based on the level of trustworthiness and reliability they want to achieve.Conclusions. The perceivable quality of OSS seems to be predictable on the basis of simple static code measures. However, only a part of the many measures produced by tools appears actually correlated to the quality of software that are perceivable by users.","source code analysis, static analysis, trustworthiness of open-source software, elementary code assessment (ECA) rules","","ESEM '10"
"Conference Paper","Falessi D,Cantone G,Canfora G","A Comprehensive Characterization of NLP Techniques for Identifying Equivalent Requirements","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","Bolzano-Bozen, Italy","2010","9781450300391","","https://doi.org/10.1145/1852786.1852810;http://dx.doi.org/10.1145/1852786.1852810","10.1145/1852786.1852810","Though very important in software engineering, linking artifacts of the same type (clone detection) or of different types (traceability recovery) is extremely tedious, error-prone and requires significant effort. Past research focused on supporting analysts with mechanisms based on Natural Language Processing (NLP) to identify candidate links. Because a plethora of NLP techniques exists, and their performances vary among contexts, it is important to characterize them according to the provided level of support. The aim of this paper is to characterize a comprehensive set of NLP techniques according to the provided level of support to human analysts in detecting equivalent requirements. The characterization consists on a case study, featuring real requirements, in the context of an Italian company in the defense and aerospace domain. The major result from the case study is that simple NLP are more precise than complex ones.","case study, natural language processing, requirements","","ESEM '10"
"Conference Paper","Schumacher J,Zazworka N,Shull F,Seaman C,Shaw M","Building Empirical Support for Automated Code Smell Detection","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement","Bolzano-Bozen, Italy","2010","9781450300391","","https://doi.org/10.1145/1852786.1852797;http://dx.doi.org/10.1145/1852786.1852797","10.1145/1852786.1852797","Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the ""smelly"" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These results lead to the conclusion that an automated metric-based pre-selection decreases the effort spent on manual code inspections. Automatic detection accompanied by a manual review increases the overall confidence in the results of metric-based classifiers.","maintainability, god class, code inspection, code smells, empirical study","","ESEM '10"
"Conference Paper","Li Y,Abousamra A,Melhem R,Jones AK","Compiler-Assisted Data Distribution for Chip Multiprocessors","","2010","","","501–512","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques","Vienna, Austria","2010","9781450301787","","https://doi.org/10.1145/1854273.1854335;http://dx.doi.org/10.1145/1854273.1854335","10.1145/1854273.1854335","Data access latency, a limiting factor in the performance of chip multiprocessors, grows significantly with the number of cores in non-uniform cache architectures with distributed cache banks. To mitigate this effect, it is necessary to leverage the data access locality and choose an optimum data placement. Achieving this is especially challenging when other constraints such as cache capacity, coherence messages and runtime overhead need to be considered. This paper presents a compiler-based approach used for analyzing data access behavior in multi-threaded applications. The proposed experimental compiler framework employs novel compilation techniques to discover and represent multi-threaded memory access patterns (MMAPs). At run time, symbolic MMAPs are resolved and used by a partitioning algorithm to choose a partition of allocated memory blocks among the forked threads in the analyzed application. This partition is used to enforce data ownership by associating the data with the core that executes the thread owning the data. We demonstrate how this information can be used in an experimental architecture to accelerate applications. In particular, our compiler assisted approach shows a 20% speedup over shared caching and 5% speedup over the closest runtime approximation, ""first touch"".","data distribution, compiler-assisted caching, partitioning","","PACT '10"
"Conference Paper","Zhang Y,Lee JW,Johnson NP,August DI","DAFT: Decoupled Acyclic Fault Tolerance","","2010","","","87–98","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques","Vienna, Austria","2010","9781450301787","","https://doi.org/10.1145/1854273.1854289;http://dx.doi.org/10.1145/1854273.1854289","10.1145/1854273.1854289","Higher transistor counts, lower voltage levels, and reduced noise margin increase the susceptibility of multicore processors to transient faults. Redundant hardware modules can detect such errors, but software transient fault detection techniques are more appealing for their low cost and flexibility. Recent software proposals double register pressure or memory usage, or are too slow in the absence of hardware extensions, preventing widespread acceptance. This paper presents DAFT, a fast, safe, and memory efficient transient fault detection framework for commodity multicore systems. DAFT replicates computation across multiple cores and schedules fault detection off the critical path. Where possible, values are speculated to be correct and only communicated to the redundant thread at essential program points. DAFT is implemented in the LLVM compiler framework and evaluated using SPEC CPU2000 and SPEC CPU2006 benchmarks on a commodity multicore system. Results demonstrate DAFT's high performance and broad fault coverage. Speculation allows DAFT to reduce the perfor- mance overhead of software redundant multithreading from an average of 200% to 38% with no degradation of fault coverage.","transient fault, multicore, speculation","","PACT '10"
"Conference Paper","Störrle H","Towards Clone Detection in UML Domain Models","","2010","","","285–293","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth European Conference on Software Architecture: Companion Volume","Copenhagen, Denmark","2010","9781450301794","","https://doi.org/10.1145/1842752.1842805;http://dx.doi.org/10.1145/1842752.1842805","10.1145/1842752.1842805","Code clones are a substantial problem for code based development; it is not surprising that model clones are a problem for model based development, too. Past experience with model version control vs. code version control suggests, however, that there are radically different problems and techniques on the code level than on the model level. In this paper, we explore the problems and possibilities associated with detecting clones in UML domain models. In particular, we have designed a number of algorithms and heuristics to detect clones, and have implemented them in the MQlone tool (read as ""m clone""). However, all of the approaches we have studied so far have their weak spots, so that we cannot present a universal solution. Further work is required to refine and combine the approaches studied here, and test them on larger populations of models. While the related work faces similar unsolved problems our approach is the first one to deal with UML as a whole rather than with Matlab/Simulink or individual UML model types.","clone detection, UML","","ECSA '10"
"Conference Paper","Weyns D,Capilla R","Current and Emerging Topics in Software Architecture (ECSA 2010 Workshops Summary)","","2010","","","59–62","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth European Conference on Software Architecture: Companion Volume","Copenhagen, Denmark","2010","9781450301794","","https://doi.org/10.1145/1842752.1842769;http://dx.doi.org/10.1145/1842752.1842769","10.1145/1842752.1842769","Since 2004 in St. Andrews (Scotland, U.K.), ECSA the European Conference on Software Architecture (formerly EWSA, the European Workshop on Software Architecture) has been considered as an important meeting point for researchers and practitioners on the topic of software architecture. ECSA has matured from a workshop format to a full software engineering conference in the subfield of software architecture.This year, ECSA has become more ambitious and expanded its scope and schedule up to four full days. The program includes a series of tutorials, a doctoral mentoring program, and four full-day workshops. New and existing software challenges have led to a variety of trends in software architecture research, which makes the conference and workshops more attractive and promotes the discussion on current and emerging topics.Based on the scientific and technical interest of the topics, the innovativeness of workshop topics, and the capacity of the conference workshop program, the workshop co-chairs selected four workshops from the nine submitted proposals. We summarize the aims and goals of each workshop and the contributions accepted for the four workshops:• 2nd International Workshop on Software Ecosystems (EcoSys). Piers Campbell, Faheem Ahmed, Jan Bosch, Sliger Jansen.• 1st International Workshop on Measurability of Security in Software Architectures (MeSSa). Reijo Savola, Teemu Kranstén, Antti Evesti.• 8th Nordic Workshop on Model Driven Software Engineering (NW-MODE). Andrzej Wasowski, Dragos Truscan, Ludwik Kuzniarz.• 1st International Workshop on Variability in Software Product Line Architectures (VARI-ARCH). Alexander Helleboogh, Paris Avgeriou, Nelis Boucke, Patryck Heymans.The ECSA 2010 Workshop co-chairs would like to thanks all workshop organizers for their effort and enthusiasm to attract submission in different software architecture research topics and make the ECSA 2010 workshops a success.","","","ECSA '10"
"Conference Paper","Adams A,Talvala EV,Park SH,Jacobs DE,Ajdin B,Gelfand N,Dolson J,Vaquero D,Baek J,Tico M,Lensch HP,Matusik W,Pulli K,Horowitz M,Levoy M","The Frankencamera: An Experimental Platform for Computational Photography","","2010","","","","Association for Computing Machinery","New York, NY, USA","ACM SIGGRAPH 2010 Papers","Los Angeles, California","2010","9781450302104","","https://doi.org/10.1145/1833349.1778766;http://dx.doi.org/10.1145/1833349.1778766","10.1145/1833349.1778766","Although there has been much interest in computational photography within the research and photography communities, progress has been hampered by the lack of a portable, programmable camera with sufficient image quality and computing power. To address this problem, we have designed and implemented an open architecture and API for such cameras: the Frankencamera. It consists of a base hardware specification, a software stack based on Linux, and an API for C++. Our architecture permits control and synchronization of the sensor and image processing pipeline at the microsecond time scale, as well as the ability to incorporate and synchronize external hardware like lenses and flashes. This paper specifies our architecture and API, and it describes two reference implementations we have built. Using these implementations we demonstrate six computational photography applications: HDR viewfinding and capture, low-light viewfinding and capture, automated acquisition of extended dynamic range panoramas, foveal imaging, IMU-based hand shake detection, and rephotography. Our goal is to standardize the architecture and distribute Frankencameras to researchers and students, as a step towards creating a community of photographer-programmers who develop algorithms, applications, and hardware for computational cameras.","computational photography, programmable cameras","","SIGGRAPH '10"
"Journal Article","Adams A,Talvala EV,Park SH,Jacobs DE,Ajdin B,Gelfand N,Dolson J,Vaquero D,Baek J,Tico M,Lensch HP,Matusik W,Pulli K,Horowitz M,Levoy M","The Frankencamera: An Experimental Platform for Computational Photography","ACM Trans. Graph.","2010","29","4","","Association for Computing Machinery","New York, NY, USA","","","2010-07","","0730-0301","https://doi.org/10.1145/1778765.1778766;http://dx.doi.org/10.1145/1778765.1778766","10.1145/1778765.1778766","Although there has been much interest in computational photography within the research and photography communities, progress has been hampered by the lack of a portable, programmable camera with sufficient image quality and computing power. To address this problem, we have designed and implemented an open architecture and API for such cameras: the Frankencamera. It consists of a base hardware specification, a software stack based on Linux, and an API for C++. Our architecture permits control and synchronization of the sensor and image processing pipeline at the microsecond time scale, as well as the ability to incorporate and synchronize external hardware like lenses and flashes. This paper specifies our architecture and API, and it describes two reference implementations we have built. Using these implementations we demonstrate six computational photography applications: HDR viewfinding and capture, low-light viewfinding and capture, automated acquisition of extended dynamic range panoramas, foveal imaging, IMU-based hand shake detection, and rephotography. Our goal is to standardize the architecture and distribute Frankencameras to researchers and students, as a step towards creating a community of photographer-programmers who develop algorithms, applications, and hardware for computational cameras.","programmable cameras, computational photography","",""
"Conference Paper","Quintella F,Soares LP,Raposo AB","DWeb3D: A Toolkit for Developing X3D Applications in a Simplified Environment","","2010","","","45–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Web 3D Technology","Los Angeles, California","2010","9781450302098","","https://doi.org/10.1145/1836049.1836056;http://dx.doi.org/10.1145/1836049.1836056","10.1145/1836049.1836056","The X3D standard is open, supported by an international consortium, mature and in constant development, but with a low adoption rate. In this work, X3D qualities and problems are discussed and correlated with other solutions. In this process it was detected some necessities in current applications and the complexity of X3D to deal with these issues. As an attempt to demonstrate that the complexity of X3D in some aspects may be reduced, the DWeb3D toolkit was built. DWeb3d is a toolkit to help the development of dynamic X3D applications, showing that it is possible to simplify the development process, possibly increasing the access to developers in this area. The toolkit provides tools to deal with publishing, synchronism, interactivity, multiple users management and disk persistence.","distributed environments, scene graph, virtual reality","","Web3D '10"
"Conference Paper","McMinn P,Stevenson M,Harman M","Reducing Qualitative Human Oracle Costs Associated with Automatically Generated Test Data","","2010","","","1–4","Association for Computing Machinery","New York, NY, USA","Proceedings of the First International Workshop on Software Test Output Validation","Trento, Italy","2010","9781450301381","","https://doi.org/10.1145/1868048.1868049;http://dx.doi.org/10.1145/1868048.1868049","10.1145/1868048.1868049","Due to the frequent non-existence of an automated oracle, test cases are often evaluated manually in practice. However, this fact is rarely taken into account by automatic test data generators, which seek to maximise a program's structural coverage only. The test data produced tends to be of a poor fit with the program's operational profile. As a result, each test case takes longer for a human to check, because the scenarios that arbitrary-looking data represent require time and effort to understand. This short paper proposes methods to extracting knowledge from programmers, source code and documentation and its incorporation into the automatic test data generation process so as to inject the realism required to produce test cases that are quick and easy for a human to comprehend and check. The aim is to reduce the so-called qualitative human oracle costs associated with automatic test data generation. The potential benefits of such an approach are demonstrated with a simple case study.","","","STOV '10"
"Conference Paper","Downey C,Zhang M,Browne WN","New Crossover Operators in Linear Genetic Programming for Multiclass Object Classification","","2010","","","885–892","Association for Computing Machinery","New York, NY, USA","Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation","Portland, Oregon, USA","2010","9781450300728","","https://doi.org/10.1145/1830483.1830644;http://dx.doi.org/10.1145/1830483.1830644","10.1145/1830483.1830644","Genetic programming (GP) has been successfully applied to solving multiclass classification problems, but the performance of GP classifiers still lags behind that of alternative techniques. This paper investigates an alternative form of GP, Linear GP (LGP), which demonstrates great promise as a classifier as the division of classes is inherent in this technique. By combining biological inspiration with detailed knowledge of program structure two new crossover operators that significantly improve performance are developed. The first is a new crossover operator that mimics biological crossover between alleles, which helps reduce the disruptive effect on building blocks of information. The second is an extension of the first where a heuristic is used to predict offspring fitness guiding search to promising solutions.","genetic programming, classification, crossover operator","","GECCO '10"
"Journal Article","Duala-Ekoko E,Robillard MP","Clone Region Descriptors: Representing and Tracking Duplication in Source Code","ACM Trans. Softw. Eng. Methodol.","2010","20","1","","Association for Computing Machinery","New York, NY, USA","","","2010-07","","1049-331X","https://doi.org/10.1145/1767751.1767754;http://dx.doi.org/10.1145/1767751.1767754","10.1145/1767751.1767754","Source code duplication, commonly known as code cloning, is considered an obstacle to software maintenance because changes to a cloned region often require consistent changes to other regions of the source code. Research has provided evidence that the elimination of clones may not always be practical, feasible, or cost-effective. We present a clone management approach that describes clone regions in a robust way that is independent from the exact text of clone regions or their location in a file, and that provides support for tracking clones in evolving software. Our technique relies on the concept of abstract clone region descriptors (CRDs), which describe clone regions using a combination of their syntactic, structural, and lexical information. We present our definition of CRDs, and describe a clone tracking system capable of producing CRDs from the output of different clone detection tools, notifying developers of modifications to clone regions, and supporting updates to the documented clone relationships. We evaluated the performance and usefulness of our approach across three clone detection tools and five subject systems, and the results indicate that CRDs are a practical and robust representation for tracking code clones in evolving software.","clone management, refactoring, clone detection, code clones, Source code duplication","",""
"Journal Article","Steimann F,Pawlitzki T,Apel S,Kästner C","Types and Modularity for Implicit Invocation with Implicit Announcement","ACM Trans. Softw. Eng. Methodol.","2010","20","1","","Association for Computing Machinery","New York, NY, USA","","","2010-07","","1049-331X","https://doi.org/10.1145/1767751.1767752;http://dx.doi.org/10.1145/1767751.1767752","10.1145/1767751.1767752","Through implicit invocation, procedures are called without explicitly referencing them. Implicit announcement adds to this implicitness by not only keeping implicit which procedures are called, but also where or when—under implicit invocation with implicit announcement, the call site contains no signs of that, or what it calls. Recently, aspect-oriented programming has popularized implicit invocation with implicit announcement as a possibility to separate concerns that lead to interwoven code if conventional programming techniques are used. However, as has been noted elsewhere, as currently implemented it establishes strong implicit dependencies between components, hampering independent software development and evolution. To address this problem, we present a type-based modularization of implicit invocation with implicit announcement that is inspired by how interfaces and exceptions are realized in Java. By extending an existing compiler and by rewriting several programs to make use of our proposed language constructs, we found that the imposed declaration clutter tends to be moderate; in particular, we found that, for general applications of implicit invocation with implicit announcement, fears that programs utilizing our form of modularization become unreasonably verbose are unjustified.","modularity, typing, aspect-oriented programming, publish/subscribe, event-driven programming, Implicit invocation","",""
"Conference Paper","de Kruijf M,Nomura S,Sankaralingam K","Relax: An Architectural Framework for Software Recovery of Hardware Faults","","2010","","","497–508","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th Annual International Symposium on Computer Architecture","Saint-Malo, France","2010","9781450300537","","https://doi.org/10.1145/1815961.1816026;http://dx.doi.org/10.1145/1815961.1816026","10.1145/1815961.1816026","As technology scales ever further, device unreliability is creating excessive complexity for hardware to maintain the illusion of perfect operation. In this paper, we consider whether exposing hardware fault information to software and allowing software to control fault recovery simplifies hardware design and helps technology scaling.The combination of emerging applications and emerging many-core architectures makes software recovery a viable alternative to hardware-based fault recovery. Emerging applications tend to have few I/O and memory side-effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub-computations with small qualitative impact. Software recovery can harness these properties in ways that hardware recovery cannot.We describe Relax, an architectural framework for software recovery of hardware faults. Relax includes three core components: (1) an ISA extension that allows software to mark regions of code for software recovery, (2) a hardware organization that simplifies reliability considerations and provides energy efficiency with hardware recovery support removed, and (3) software support for compilers and programmers to utilize the Relax ISA. Applying Relax to counter the effects of process variation, our results show a 20% energy efficiency improvement for PARSEC applications with only minimal source code changes and simpler hardware.","reliability, software recovery","","ISCA '10"
"Journal Article","de Kruijf M,Nomura S,Sankaralingam K","Relax: An Architectural Framework for Software Recovery of Hardware Faults","SIGARCH Comput. Archit. News","2010","38","3","497–508","Association for Computing Machinery","New York, NY, USA","","","2010-06","","0163-5964","https://doi.org/10.1145/1816038.1816026;http://dx.doi.org/10.1145/1816038.1816026","10.1145/1816038.1816026","As technology scales ever further, device unreliability is creating excessive complexity for hardware to maintain the illusion of perfect operation. In this paper, we consider whether exposing hardware fault information to software and allowing software to control fault recovery simplifies hardware design and helps technology scaling.The combination of emerging applications and emerging many-core architectures makes software recovery a viable alternative to hardware-based fault recovery. Emerging applications tend to have few I/O and memory side-effects, which limits the amount of information that needs checkpointing, and they allow discarding individual sub-computations with small qualitative impact. Software recovery can harness these properties in ways that hardware recovery cannot.We describe Relax, an architectural framework for software recovery of hardware faults. Relax includes three core components: (1) an ISA extension that allows software to mark regions of code for software recovery, (2) a hardware organization that simplifies reliability considerations and provides energy efficiency with hardware recovery support removed, and (3) software support for compilers and programmers to utilize the Relax ISA. Applying Relax to counter the effects of process variation, our results show a 20% energy efficiency improvement for PARSEC applications with only minimal source code changes and simpler hardware.","software recovery, reliability","",""
"Conference Paper","Chudá D,Kováčová B","Checking Plagiarism in E-Learning","","2010","","","419–424","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies","Sofia, Bulgaria","2010","9781450302432","","https://doi.org/10.1145/1839379.1839453;http://dx.doi.org/10.1145/1839379.1839453","10.1145/1839379.1839453","In e-learning is very important the checking plagiarism of students homework submissions. This paper proposes methods and tools for the plagiarism detection. The paper focuses only on the source code plagiarism. The analysis phase was the basis for our method proposal which has ambitions to bring something new and to make the plagiarism detection more effective. The method was implemented in a tool and its results and advantages are also presented.","detection method for plagiarised programming code, side-by-side visualisation of plagiarised programming code, plagiarism plagiarised programming code","","CompSysTech '10"
"Conference Paper","Guevara E","NoWaC: A Large Web-Based Corpus for Norwegian","","2010","","","1–7","Association for Computational Linguistics","USA","Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop","Los Angeles, California","2010","","","","","In this paper we introduce the first version of noWaC, a large web-based corpus of Bokm\ral Norwegian currently containing about 700 million tokens. The corpus has been built by crawling, downloading and processing web documents in the .no top-level internet domain. The procedure used to collect the noWaC corpus is largely based on the techniques described by Ferraresi et al. (2008). In brief, first a set of ""seed"" URLs containing documents in the target language is collected by sending queries to commercial search engines (Google and Yahoo). The obtained seeds (overall 6900 URLs) are then used to start a crawling job using the Heritrix web-crawler limited to the .no domain. The downloaded documents are then processed in various ways in order to build a linguistic corpus (e.g. filtering by document size, language identification, duplicate and near duplicate detection, etc.).","","","WAC-6 '10"
"Conference Paper","Linderman MD,Bruggner R,Athalye V,Meng TH,Bani Asadi N,Nolan GP","High-Throughput Bayesian Network Learning Using Heterogeneous Multicore Computers","","2010","","","95–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th ACM International Conference on Supercomputing","Tsukuba, Ibaraki, Japan","2010","9781450300186","","https://doi.org/10.1145/1810085.1810101;http://dx.doi.org/10.1145/1810085.1810101","10.1145/1810085.1810101","Aberrant intracellular signaling plays an important role in many diseases. The causal structure of signal transduction networks can be modeled as Bayesian Networks (BNs), and computationally learned from experimental data. However, learning the structure of Bayesian Networks (BNs) is an NP-hard problem that, even with fast heuristics, is too time consuming for large, clinically important networks (20--50 nodes). In this paper, we present a novel graphics processing unit (GPU)-accelerated implementation of a Monte Carlo Markov Chain-based algorithm for learning BNs that is up to 7.5-fold faster than current general-purpose processor (GPP)-based implementations.The GPU-based implementation is just one of several implementations within the larger application, each optimized for a different input or machine configuration. We describe the methodology we use to build an extensible application, assembled from these variants, that can target a broad range of heterogeneous systems, e.g., GPUs, multicore GPPs. Specifically we show how we use the Merge programming model to efficiently integrate, test and intelligently select among the different potential implementations.","GPU, MCMC, Bayesian networks","","ICS '10"
"Conference Paper","Meskens J,Luyten K,Coninx K","Jelly: A Multi-Device Design Environment for Managing Consistency across Devices","","2010","","","289–296","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Advanced Visual Interfaces","Roma, Italy","2010","9781450300766","","https://doi.org/10.1145/1842993.1843044;http://dx.doi.org/10.1145/1842993.1843044","10.1145/1842993.1843044","When creating applications that should be available on multiple computing platforms, designers have to cope with different design tools and user interface toolkits. Incompatibilities between these design tools and toolkits make it hard to keep multi-device user interfaces consistent. This paper presents Jelly, a flexible design environment that can target a broad set of computing devices and toolkits. Jelly enables designers to copy parts of a user interface from one device to another and to maintain the different user interfaces in concert using linked editing. Our approach lowers the burden of designing multi-device user interfaces by eliminating the need to switch between different design tools and by providing tool support for keeping the user interfaces consistent across different platforms and toolkits.","multi-platform GUI design, GUI builder, design tools","","AVI '10"
"Conference Paper","Payer M,Gross TR","Generating Low-Overhead Dynamic Binary Translators","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd Annual Haifa Experimental Systems Conference","Haifa, Israel","2010","9781605589084","","https://doi.org/10.1145/1815695.1815724;http://dx.doi.org/10.1145/1815695.1815724","10.1145/1815695.1815724","Dynamic (on the fly) binary translation is an important part of many software systems. In this paper we discuss how to combine efficient translation with the generation of efficient code, while providing a high-level table-driven user interface that simplifies the generation of the binary translator (BT).The translation actions of the BT are specified in high-level abstractions that are compiled into translation tables; these tables control the runtime program translation. This table generator allows a compact description of changes in the translated code.We use fastBT, a table-based dynamic binary translator that uses a code cache and various optimizations for indirect control transfers to illustrate the design tradeoffs in binary translators. We present an analysis of the most challenging sources of overhead and describe optimizations to further reduce these penalties. Keys to the good performance are a configurable inlining mechanism and adaptive self-modifying optimizations for indirect control transfers.","dynamic instrumentation, dynamic translation, binary translation, optimization","","SYSTOR '10"
"Conference Paper","Laadan O,Nieh J","Operating System Virtualization: Practice and Experience","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd Annual Haifa Experimental Systems Conference","Haifa, Israel","2010","9781605589084","","https://doi.org/10.1145/1815695.1815717;http://dx.doi.org/10.1145/1815695.1815717","10.1145/1815695.1815717","Operating system (OS) virtualization can provide a number of important benefits, including transparent migration of applications, server consolidation, online OS maintenance, and enhanced system security. However, the construction of such a system presents a myriad of challenges, even for the most cautious developer, that if overlooked may result in a weak, incomplete virtualization. We present a detailed discussion of key implementation issues in providing OS virtualization in a commodity OS, including system call interposition, virtualization state management, and race conditions. We discuss our experiences in implementing such functionality across two major versions of Linux entirely in a loadable kernel module without any kernel modification. We present experimental results on both uniprocessor and multiprocessor systems that demonstrate the ability of our approach to provide fine-grain virtualization with very low overhead.","operating systems, virtualization","","SYSTOR '10"
"Conference Paper","Deissenboeck F,Hummel B,Juergens E,Pfaehler M,Schaetz B","Model Clone Detection in Practice","","2010","","","57–64","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808909;http://dx.doi.org/10.1145/1808901.1808909","10.1145/1808901.1808909","Cloned code is considered harmful for two reasons: (1) multiple, possibly unnecessary, duplicates of code increase maintenance costs and, (2) inconsistent changes to cloned code can create faults and, hence, lead to incorrect program behavior. Likewise, duplicated parts of models are problematic in model-based development. Recently, we and other authors proposed multiple approaches to automatically identify duplicates in graphical models. While it has been demonstrated that these approaches work in principal, a number of challenges remain for application in industrial practice. Based on an industrial case study undertaken with the BMW Group, this paper details on these challenges and presents solutions to the most pressing ones, namely scalability and relevance of the results. Moreover, we present tool support that eases the evaluation of detection results and thereby helps to make clone detection a standard technique in model-based quality assurance.","data-flow models, clone detection","","IWSC '10"
"Conference Paper","Juergens E,Göde N","Achieving Accurate Clone Detection Results","","2010","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808902;http://dx.doi.org/10.1145/1808901.1808902","10.1145/1808901.1808902","Many existing clone detection approaches produce substantial amounts of clones that are irrelevant to software engineers. These false positives significantly hinder adoption of clone detection in practice and can lead to inaccurate research conclusions. In this paper, we propose clone coupling as an explicit criterion for the relevance of clones and outline a method for clone detection tailoring. The results of a large industrial case study indicate that it can significantly increase clone detection accuracy.","clone detection, tailoring, software maintenance, assessment","","IWSC '10"
"Conference Paper","Davis IJ,Godfrey MW","Clone Detection by Exploiting Assembler","","2010","","","77–78","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808913;http://dx.doi.org/10.1145/1808901.1808913","10.1145/1808901.1808913","In this position paper, we describe work-in-progress in detecting source code clones by means of analyzing and comparing the assembler that is produced when the source code is compiled.","software, assembler, clone detection, Java, C, C++","","IWSC '10"
"Conference Paper","Funaro M,Braga D,Campi A,Ghezzi C","A Hybrid Approach (Syntactic and Textual) to Clone Detection","","2010","","","79–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808914;http://dx.doi.org/10.1145/1808901.1808914","10.1145/1808901.1808914","Code clone detection has been so far tackled with several approaches (mainly textual, syntactic, and semantic), each performing best w.r.t. specific metrics and against consolidated but specific benchmarks. Hybrid approaches have been recently proposed as well. This paper proposes a novel hybrid (syntactic, textual) approach using the abstract syntax tree to identify clone candidates and textual methods to discard false positives. The novelty of the approach is in the combination of two well-grounded techniques, in a way that has not been explored yet.The paper shows the result of empirical evaluation methods and hints of a prototype implementation. Initial experimental results show that the approach is effective and can find clones that were not detected by state of the art approaches.","abstract syntax tree, clone detection","","IWSC '10"
"Conference Paper","Chilowicz M,Duris E,Roussel G","Towards a Multi-Scale Approach for Source Code Approximate Match Report","","2010","","","89–90","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808919;http://dx.doi.org/10.1145/1808901.1808919","10.1145/1808901.1808919","Finding exact clones in source code can be efficiently handled using classical exact substring or subtree pattern matching techniques inspired from genomics applications. These methods may be wisely employed as a foundation to sketch new techniques highlighting duplicated code chunks presenting minor edits or more extensive modifications at a higher structural scale. The main goal is to improve recall of small near matches and to aggregate them into larger ones to provide a more global view of similarities with a reasonable complexity. These concerns are essential to be able to address a large database of source code projects.","clones, source code similarity, software plagiarism","","IWSC '10"
"Conference Paper","Krinke J,Gold N,Jia Y,Binkley D","Distinguishing Copies from Originals in Software Clones","","2010","","","41–48","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808907;http://dx.doi.org/10.1145/1808901.1808907","10.1145/1808901.1808907","Cloning is widespread in today's systems where automated assistance is required to locate cloned code. Although the evolution of clones has been studied for many years, no attempt has been made so far to automatically distinguish the original source code leading to cloned copies. This paper presents an approach to classify the clones of a clone pair based on the version information available in version control systems. This automatic classification attempts to distinguish the original from the copy. It allows for the fact that the clones may be modified and thus consist of lines coming from different versions. An evaluation, based on two case studies, shows that when comments are ignored and a small tolerance is accepted, for the majority of clone pairs the proposed approach can automatically distinguish between the original and the copy.","software evolution, clone detection, mining software archives","","IWSC '10"
"Conference Paper","Yoshida N,Hattori T,Inoue K","Finding Similar Defects Using Synonymous Identifier Retrieval","","2010","","","49–56","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808908;http://dx.doi.org/10.1145/1808901.1808908","10.1145/1808901.1808908","When we encounter a defect in one part of a program, it is very important to find other parts of the program that may contain similar defects. In this paper, we propose a novel system to find similar defects in the large collection of source code. This system takes a code fragment containing a defect as the query input, and returns code fragments containing the same or synonymous identifiers which appear in the input fragment. Case studies with two open source systems and their defect data show the advantages of the proposed retrieval system, compared to the code-clone based retrievals.","natural language processing, code retrieval, defect detection","","IWSC '10"
"Conference Paper","Gold N,Krinke J,Harman M,Binkley D","Issues in Clone Classification for Dataflow Languages","","2010","","","83–84","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808916;http://dx.doi.org/10.1145/1808901.1808916","10.1145/1808901.1808916","While clone detection and classification research for textual source code is well-established, clones in visual dataflow languages have only recently received attention. The accepted existing clone classification framework does not adequately capture the nature of clones in the latter kind of programs. In this article, we propose a new classification framework for clone types that may be found in dataflow programs. It parallels the scheme for textual languages but accounts for the differences in syntax and semantics present in graphical languages.","clone classification, clone detection","","IWSC '10"
"Conference Paper","Higo Y,Tanaka K,Kusumoto S","Toward Identifying Inter-Project Clone Sets for Building Useful Libraries","","2010","","","87–88","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808918;http://dx.doi.org/10.1145/1808901.1808918","10.1145/1808901.1808918","The present paper discusses how clone sets can be generated from an very large amount of source code. The knowledge of clone sets can help to manage software asset. For example, we can figure out the state of the asset easier, or we can build more useful libraries based on the knowledge.","code clone, reengineering for libraries","","IWSC '10"
"Conference Paper","Jacob F,Hou D,Jablonski P","Actively Comparing Clones inside the Code Editor","","2010","","","9–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808903;http://dx.doi.org/10.1145/1808901.1808903","10.1145/1808901.1808903","Tool support for code clones can improve software quality and maintainability. While significant research has been done in locating clones in existing source code, there has been less of a research focus on proactively tracking and supporting copy-paste-modify operations, even though copying and pasting is a major source of clone formation and the resulting clones are then often modified. We designed and implemented a programming editor, based on the Eclipse integrated development environment, named CSeR (Code Segment Reuse), which keeps a record of copy-and-paste-induced clones and then tracks and visualizes the changes made to a clone with distinct colors. The core of CSeR is an algorithm that actively compares two clones for detailed differences as a programmer edits either one of them. This edit-based comparison algorithm is unique to CSeR and produces more immediate, accurate, and natural results than other differencing tools.","software maintenance, copy-and-paste programming, code clone, Java, differencing tools, software evolution, Eclipse integrated development environment, code comparison","","IWSC '10"
"Conference Paper","Lavoie T,Eilers-Smith M,Merlo E","Challenging Cloning Related Problems with GPU-Based Algorithms","","2010","","","25–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808905;http://dx.doi.org/10.1145/1808901.1808905","10.1145/1808901.1808905","Graphics Processing Unit (GPU) have been around for a while. Although they are primarily used for high-end 3D graphics processing, their use is now acknowledged for general massive parallel computing. This paper presents an original technique based on [10] to compute many instances of the longest common subsequence problem on a generic GPU architecture using classic DP-matching [7]. Application of this algorithm has been found useful to address the problem of filtering false positives produced by metrics-based clone detection methods. Experimental results of this application are presented along with a discussion of possibilities of using GPUs for other cloning related problems.","parallel computing, open source analysis, clone detection, string matching, GPU computing, software similarity","","IWSC '10"
"Conference Paper","Jarzabek S,Xue Y","Are Clones Harmful for Maintenance?","","2010","","","73–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808911;http://dx.doi.org/10.1145/1808901.1808911","10.1145/1808901.1808911","We often find clones in semantically related programs parts. This semantic relationship, not clones, is a prime reason for maintenance problems, as semantically related program parts must be changed in sync no matter of clones.Only clones resulting from bad design hinder program maintainability. Long-lived essential clones that should not or cannot be eliminated are mostly neutral in respect to maintenance effort, and in some cases even help in maintenance. On the other hand, the presence of such clones signifies program parts that often are more difficult to maintain than clone-free parts.","similarity patterns, software clones, clone detection","","IWSC '10"
"Conference Paper","Göde N","Clone Removal: Fact or Fiction?","","2010","","","33–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808906;http://dx.doi.org/10.1145/1808901.1808906","10.1145/1808901.1808906","Despite ongoing research in the field of code duplication, clone research has not yet investigated when and how developers remove clones. We think knowing how developers select candidates for removal and what techniques they use to eliminate duplication is essential to provide efficient clone management tools. Our empirical results show a significant discrepancy between clones detected by a state-of-the-art clone detector and duplication removed by developers. We believe it is necessary to have a better understanding of how developers approach and remove duplication to improve clone detectors as well as clone refactoring tools.","clone removal, code clones, software maintenance","","IWSC '10"
"Conference Paper","Lozano A,Wermelinger M","Tracking Clones' Imprint","","2010","","","65–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808910;http://dx.doi.org/10.1145/1808901.1808910","10.1145/1808901.1808910","Cloning imprint is the lasting effect of cloning on applications. This paper aims to analyze the clone imprint over time, in terms of the extension of cloning, the persistence of clones in methods, and the stability of cloned methods. Such level of detail requires an improvement in the clone tracking algorithms previously proposed, which is also presented.We found that cloned methods are cloned most of their lifetime, cloned methods have a higher density of changes, and that changes in cloned methods tend to be customizations to the clone environment.","maintenance, extension, impact, empirical software engineering, changeability, stability, clones, persistence, cloning, mining software repositories","","IWSC '10"
"Conference Paper","Roy CK,Cordy JR","Are Scripting Languages Really Different?","","2010","","","17–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808904;http://dx.doi.org/10.1145/1808901.1808904","10.1145/1808901.1808904","Scripting languages such as Python, Perl, Ruby and PHP are increasingly important in new software systems as web technology becomes a dominant force. These languages are often spoken of as having different properties, in particular with respect to cloning, and the question arises whether the observations made based on traditional languages also apply to them. In this paper we present a first experiment in measuring the cloning properties of open source software systems written in the Python scripting language using the NiCad clone detector. We compare our results for Python with previous observations of C, C#, and Java, and discover that perhaps scripting languages are not so different after all.","code clones, empirical study, Python, scripting languages","","IWSC '10"
"Conference Paper","Harder J,Göde N","Quo Vadis, Clone Management?","","2010","","","85–86","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Workshop on Software Clones","Cape Town, South Africa","2010","9781605589800","","https://doi.org/10.1145/1808901.1808917;http://dx.doi.org/10.1145/1808901.1808917","10.1145/1808901.1808917","Although potential benefits are frequently mentioned in the literature, empirical evidence is missing to concretize the costs and benefits of clone management. Without precise estimation of the expected gain, clone management will hardly ever become a self-evident part of serious software development, as for example version control systems are. A study on clones in our own code showed a complex relationship between costs and benefits, which, in addition, differs between individual clones. We conclude that empirical evidence, that adheres the diversity of clones, is needed to support the adoption of clone management in practice.","software maintenance, cost-benefit analysis, clone management","","IWSC '10"
"Conference Paper","Rosenblum NE,Miller BP,Zhu X","Extracting Compiler Provenance from Program Binaries","","2010","","","21–28","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering","Toronto, Ontario, Canada","2010","9781450300827","","https://doi.org/10.1145/1806672.1806678;http://dx.doi.org/10.1145/1806672.1806678","10.1145/1806672.1806678","We present a novel technique that identifies the source compiler of program binaries, an important element of program provenance. Program provenance answers fundamental questions of malware analysis and software forensics, such as whether programs are generated by similar tool chains; it also can allow development of debugging, performance analysis, and instrumentation tools specific to particular compilers. We formulate compiler identification as a structured learning problem, automatically building models to recognize sequences of binary code generated by particular compilers. We evaluate our techniques on a large set of real-world test binaries, showing that our models identify the source compiler of binary code with over 90% accuracy, even in the presence of interleaved code from multiple compilers. A case study demonstrates the use of inferred compiler provenance to augment stripped binary parsing, reducing parsing errors by 18%.","program provenance, static binary analysis, forensics","","PASTE '10"
"Conference Paper","Counsell S,Hierons RM,Hamza H,Black S,Durrand M","Is a Strategy for Code Smell Assessment Long Overdue?","","2010","","","32–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ICSE Workshop on Emerging Trends in Software Metrics","Cape Town, South Africa","2010","9781605589763","","https://doi.org/10.1145/1809223.1809228;http://dx.doi.org/10.1145/1809223.1809228","10.1145/1809223.1809228","Code smells reflect code decay and, as such, developers should seek to eradicate such smells through application of 'deodorant' in the form of one or more refactorings. However, a dearth of studies exploring code smells either theoretically or empirically suggests that there are reasons why smell eradication is neither being applied in anger, nor the subject of significant research. In this paper, we present three studies as supporting evidence for this claim. The first is an analysis of a set of five, open-source Java systems, the second an empirical study of a sub-system of a proprietary, C# web-based application and the third, a theoretical enumeration of smell-related refactorings. Key findings of the study were first, that developers seemed to avoid eradicating superficially 'simple' smells in favor of more 'obscure' ones; second, a wide range of conflicts and anomalies soon emerged when trying to identify smelly code. Finally, perceived effort to eradicate a smell may be a key factor. The study highlights the need for a clearer research strategy on the issue of code smells and all aspects of their identification and measurement.","code smells, Java, refactoring, C#, empirical studies","","WETSoM '10"
"Conference Paper","Hou D,Pletcher DM","Towards a Better Code Completion System by API Grouping, Filtering, and Popularity-Based Ranking","","2010","","","26–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering","Cape Town, South Africa","2010","9781605589749","","https://doi.org/10.1145/1808920.1808926;http://dx.doi.org/10.1145/1808920.1808926","10.1145/1808920.1808926","Nowadays, programmers spend much of their workday dealing with code libraries and frameworks that are bloated with APIs. One common way of interacting with APIs is through Code Completion inside the code editor. By default, Code Completion presents in a popup pane, in alphabetical order or by relevance, all accessible members available in the apparent type and supertypes of a receiver expression. This default behavior for Code Completion should and can be further improved because (1) not all public methods are APIs and presenting non-API public members to a programmer is misleading, (2) certain APIs are meant to be accessible only in some limited contexts but not others, (3) the alphabetical order separates otherwise logically related APIs, making it hard to see their connection and to work with, and (4) commonly used APIs are often presented long after much less used APIs due to suboptimal API sorting strategies. BCC (Better Code Completion) addresses these problems by enhancing Code Completion so that programmers can control how specific API elements should be sorted, filtered, and grouped. We report our preliminary validation results from testing BCC with Java projects that make use of the AWT/Swing APIs. For one large project, the BCC approach reduces by over ninety percent the total number of APIs that a programmer would have to scroll through using Eclipse's Code Completion before settling on the desired ones.","","","RSSE '10"
"Conference Paper","Sheth S,Arora N,Murphy C,Kaiser G","The WeHelp Reference Architecture for Community-Driven Recommender Systems","","2010","","","46–47","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering","Cape Town, South Africa","2010","9781605589749","","https://doi.org/10.1145/1808920.1808930;http://dx.doi.org/10.1145/1808920.1808930","10.1145/1808920.1808930","Recommender systems have become increasingly popular. Most research on recommender systems has focused on recommendation algorithms. There has been relatively little research, however, in the area of generalized system architectures for recommendation systems. In this paper, we introduce weHelp - a reference architecture for social recommender systems. Our architecture is designed to be application and domain agnostic, but we briefly discuss here how it applies to recommender systems for software engineering.","recommender systems, reference architecture","","RSSE '10"
"Conference Paper","Guimarães ML,Rito-Silva A","Towards Real-Time Integration","","2010","","","56–63","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering","Cape Town, South Africa","2010","9781605589664","","https://doi.org/10.1145/1833310.1833320;http://dx.doi.org/10.1145/1833310.1833320","10.1145/1833310.1833320","Today, most developers work in parallel inside private workspaces to ensure stability during programming, but this provokes isolation from what co-workers are doing. Isolation may result in conflicts, which must be detected as early as possible to avoid bugs and expensive rework. Currently, frequent integration and awareness are used for early conflict detection. Frequent integration detects actual conflicts using automated builds, although only when merging checked in changes. Awareness informs about ongoing changes in private workspaces, however developers must find actual conflicts by themselves.This paper proposes the novel concept of real-time integration. This automates the detection of actual conflicts emerging during programming, involving checked in and ongoing changes, and affecting two or more developers. Cooperative resolution of conflicts is explicitly supported by sharing fine-grained changes among private workspaces. Software versions will also improve quality as integration builds are run before checking in.","early conflict detection, cooperative work, real-time integration, continuous integration","","CHASE '10"
"Conference Paper","Juergens E,Deissenboeck F,Feilkas M,Hummel B,Schaetz B,Wagner S,Domann C,Streit J","Can Clone Detection Support Quality Assessments of Requirements Specifications?","","2010","","","79–88","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810308;http://dx.doi.org/10.1145/1810295.1810308","10.1145/1810295.1810308","Due to their pivotal role in software engineering, considerable effort is spent on the quality assurance of software requirements specifications. As they are mainly described in natural language, relatively few means of automated quality assessment exist. However, we found that clone detection, a technique widely applied to source code, is promising to assess one important quality aspect in an automated way, namely redundancy that stems from copy&paste operations. This paper describes a large-scale case study that applied clone detection to 28 requirements specifications with a total of 8,667 pages. We report on the amount of redundancy found in real-world specifications, discuss its nature as well as its consequences and evaluate in how far existing code clone detection approaches can be applied to assess the quality of requirements specifications in practice.","requirements specification, redundancy, clone detection","","ICSE '10"
"Conference Paper","Inoue K,Jarzabek S,Cordy JR,Koshke R","Fourth International Workshop on Software Clones (IWSC)","","2010","","","465–466","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810431;http://dx.doi.org/10.1145/1810295.1810431","10.1145/1810295.1810431","Software clones are identical or similar pieces of code. They are often the result of copy--and--paste activities as ad-hoc code reuse by programmers. Software clones research is of high relevance for the industry. Many researchers have reported high rates of code cloning in both industrial and open-source systems.In this workshop we will explore lines of research that evaluate code clone detection methods, reason about ways to remove clones, assess the effect of clones on maintainablity, track clones' evolution, and investigate the root causes of clones.","software clone, code clone detection, software maintenance","","ICSE '10"
"Conference Paper","Pham NH,Nguyen TT,Nguyen HA,Wang X,Nguyen AT,Nguyen TN","Detecting Recurring and Similar Software Vulnerabilities","","2010","","","227–230","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810336;http://dx.doi.org/10.1145/1810295.1810336","10.1145/1810295.1810336","New software security vulnerabilities are discovered on almost daily basis and it is vital to be able to identify and resolve them as early as possible. Fortunately, many software vulnerabilities are recurring or very similar, thus, one could effectively detect and fix a vulnerability in a system by consulting the similar vulnerabilities and fixes from other systems. In this paper, we propose, SecureSync, an automatic approach to detect and provide suggested resolutions for recurring software vulnerabilities on multiple systems sharing/using similar code or API libraries. The core of SecureSync includes a usage model and a mapping algorithm for matching vulnerable code across different systems, a model for the comparison of vulnerability reports, and a tracing technique from a report to corresponding source code. Our preliminary evaluation with case studies showed the potential usefulness of SecureSync.","","","ICSE '10"
"Conference Paper","Nguyen TT,Nguyen HA,Pham NH,Al-Kofahi J,Nguyen TN","Recurring Bug Fixes in Object-Oriented Programs","","2010","","","315–324","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1806799.1806847;http://dx.doi.org/10.1145/1806799.1806847","10.1145/1806799.1806847","Previous research confirms the existence of recurring bug fixes in software systems. Analyzing such fixes manually, we found that a large percentage of them occurs in code peers, the classes/methods having the similar roles in the systems, such as providing similar functions and/or participating in similar object interactions. Based on graph-based representation of object usages, we have developed several techniques to identify code peers, recognize recurring bug fixes, and recommend changes for code units from the bug fixes of their peers. The empirical evaluation on several open-source projects shows that our prototype, FixWizard, is able to identify recurring bug fixes and provide fixing recommendations with acceptable accuracy.","","","ICSE '10"
"Conference Paper","Guo Y,Seaman C,Zazworka N,Shull F","Domain-Specific Tailoring of Code Smells: An Empirical Study","","2010","","","167–170","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810321;http://dx.doi.org/10.1145/1810295.1810321","10.1145/1810295.1810321","Code smells refer to commonly occurring patterns in source code that indicate poor programming practices or code decay. Detecting code smells helps developers find design problems that can cause trouble in future maintenance. Detection rules for code smells, based on software metrics, have been proposed, but they do not take domain-specific characteristics into consideration. In this study we investigate whether such generic heuristics can be tailored to include domain-specific factors. Input into these domain-specific heuristics comes from an iterative empirical field study in a software maintenance project. The results yield valuable insight into code smell detection.","empirical study, code smells, domain-specific","","ICSE '10"
"Conference Paper","Deissenboeck F,Heinemann L,Hummel B,Juergens E","Flexible Architecture Conformance Assessment with ConQAT","","2010","","","247–250","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810343;http://dx.doi.org/10.1145/1810295.1810343","10.1145/1810295.1810343","The architecture of software systems is known to decay if no counter-measures are taken. In order to prevent this architectural erosion, the conformance of the actual system architecture to its intended architecture needs to be assessed and controlled; ideally in a continuous manner. To support this, we present the architecture conformance assessment capabilities of our quality analysis framework ConQAT. In contrast to other tools, ConQAT is not limited to the assessment of use-dependencies between software components. Its generic architectural model allows the assessment of various types of dependencies found between different kinds of artifacts. It thereby provides the necessary tool-support for flexible architecture conformance assessment in diverse contexts.","","","ICSE '10"
"Conference Paper","Kornstaedt A,Reiswich E","Staying Afloat in an Expanding Sea of Choices: Emerging Best Practices for Eclipse Rich Client Platform Development","","2010","","","59–67","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2","Cape Town, South Africa","2010","9781605587196","","https://doi.org/10.1145/1810295.1810305;http://dx.doi.org/10.1145/1810295.1810305","10.1145/1810295.1810305","The Eclipse Rich Client Platform attracts considerable attention for being a promising candidate for providing the component model Java never had. This is even truer since the incorporation of OSGi for providing services within the framework. However, the rapid sequence of new versions and the continuous growth of features lead to a discussion that almost exclusively focused on technological aspects while leaving application developers in the midst of a sea of sometimes conflicting choices of how to implement their business-oriented applications. This lack of guidance leads to systems with vastly different architectures (or lack thereof) which often force complete rewrites when further development steps are to be taken. The best practices and architectural blueprints that provide this guidance in the field of object- or service-orientation haven't emerged yet. In this experience report, we render our observations made in several projects over the last years about the challenges that cooperating teams of application developers face when using RCP. We provide a first business-oriented architectural blue-print and best practices that have helped us greatly to overcome these challenges.","rich client platform, component-based architecture, RCP, plug-in, bundles, Eclipse, OSGi","","ICSE '10"
"Conference Paper","Jacob F,Tairas R","Code Template Inference Using Language Models","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 48th Annual Southeast Regional Conference","Oxford, Mississippi","2010","9781450300643","","https://doi.org/10.1145/1900008.1900143;http://dx.doi.org/10.1145/1900008.1900143","10.1145/1900008.1900143","This paper investigates the use of a natural language processing technique that automatically detects project-specific code templates (i.e., frequently used code blocks), which can be made available to software developers within an integrated development environment. During software development, programmers often and in some cases unknowingly rewrite the same code block that represents some functionality. These frequently used code blocks can inform the existence and possible use of code templates. Many existing code editors support code templates, but programmers are expected to manually define these templates and subsequently add them as templates in the editor. Furthermore, the support of editors to provide templates based on the editing context is still limited. The use of n-gram language models within the context of software development is described and evaluated to overcome these restrictions. The technique can search for project-specific code templates and present these templates to the programmer based on the current editing context.","source code, n-gram language model, template, auto complete","","ACM SE '10"
"Conference Paper","Li W,Zhang Y","An Efficient Code Update Scheme for DSP Applications in Mobile Embedded Systems","","2010","","","105–114","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGPLAN/SIGBED 2010 Conference on Languages, Compilers, and Tools for Embedded Systems","Stockholm, Sweden","2010","9781605589534","","https://doi.org/10.1145/1755888.1755904;http://dx.doi.org/10.1145/1755888.1755904","10.1145/1755888.1755904","DSP processors usually provide dedicated address generation units (AGUs) to assist address computation. By carefully allocating variables in the memory, DSP compilers take advantage of AGUs and generate efficient code with compact size and improved performance. However, DSP applications running on mobile embedded systems often need to be updated after their initial releases. Studies showed that small changes at the source code level may significantly change the variable layout in the memory and thus the binary code, which causes large energy overheads to mobile embedded systems that patch through wireless or satellite communication, and often pecuniary burden to the users.In this paper, we propose an update-conscious code update scheme to effectively reduce patch size. It first performs incremental offset assignment based on a recent variable coalescing heuristic, and then summarizes the code difference using two types of update primitives. Our experimental results showed that using update-conscious code update can greatly improve code similarity and thus reduce the update script sizes.","context-aware script, incremental coalescing general offset assignment (icgoa), context-unaware script, incremental coalescing simple offset assignment (icsoa)","","LCTES '10"
"Journal Article","Li W,Zhang Y","An Efficient Code Update Scheme for DSP Applications in Mobile Embedded Systems","SIGPLAN Not.","2010","45","4","105–114","Association for Computing Machinery","New York, NY, USA","","","2010-04","","0362-1340","https://doi.org/10.1145/1755951.1755904;http://dx.doi.org/10.1145/1755951.1755904","10.1145/1755951.1755904","DSP processors usually provide dedicated address generation units (AGUs) to assist address computation. By carefully allocating variables in the memory, DSP compilers take advantage of AGUs and generate efficient code with compact size and improved performance. However, DSP applications running on mobile embedded systems often need to be updated after their initial releases. Studies showed that small changes at the source code level may significantly change the variable layout in the memory and thus the binary code, which causes large energy overheads to mobile embedded systems that patch through wireless or satellite communication, and often pecuniary burden to the users.In this paper, we propose an update-conscious code update scheme to effectively reduce patch size. It first performs incremental offset assignment based on a recent variable coalescing heuristic, and then summarizes the code difference using two types of update primitives. Our experimental results showed that using update-conscious code update can greatly improve code similarity and thus reduce the update script sizes.","context-unaware script, context-aware script, incremental coalescing simple offset assignment (icsoa), incremental coalescing general offset assignment (icgoa)","",""
"Conference Paper","Brandt J,Dontcheva M,Weskamp M,Klemmer SR","Example-Centric Programming: Integrating Web Search into the Development Environment","","2010","","","513–522","Association for Computing Machinery","New York, NY, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","Atlanta, Georgia, USA","2010","9781605589299","","https://doi.org/10.1145/1753326.1753402;http://dx.doi.org/10.1145/1753326.1753402","10.1145/1753326.1753402","The ready availability of online source-code examples has fundamentally changed programming practices. However, current search tools are not designed to assist with programming tasks and are wholly separate from editing tools. This paper proposes that embedding a task-specific search engine in the development environment can significantly reduce the cost of finding information and thus enable programmers to write better code more easily. This paper describes the design, implementation, and evaluation of Blueprint, a Web search interface integrated into the Adobe Flex Builder development environment that helps users locate example code. Blueprint automatically augments queries with code context, presents a code-centric view of search results, embeds the search experience into the editor, and retains a link between copied code and its source. A comparative laboratory study found that Blueprint enables participants to write significantly better code and find example code significantly faster than with a standard Web browser. Analysis of three months of usage logs with 2,024 users suggests that task-specific search interfaces can significantly change how and when people search the Web.","example-centric development","","CHI '10"
"Conference Paper","Hartmann B,MacDougall D,Brandt J,Klemmer SR","What Would Other Programmers Do: Suggesting Solutions to Error Messages","","2010","","","1019–1028","Association for Computing Machinery","New York, NY, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","Atlanta, Georgia, USA","2010","9781605589299","","https://doi.org/10.1145/1753326.1753478;http://dx.doi.org/10.1145/1753326.1753478","10.1145/1753326.1753478","Interpreting compiler errors and exception messages is challenging for novice programmers. Presenting examples of how other programmers have corrected similar errors may help novices understand and correct such errors. This paper introduces HelpMeOut, a social recommender system that aids the debugging of error messages by suggesting solutions that peers have applied in the past. HelpMeOut comprises IDE instrumentation to collect examples of code changes that fix errors; a central database that stores fix reports from many users; and a suggestion interface that, given an error, queries the database for a list of relevant fixes and presents these to the programmer. We report on implementations of this architecture for two programming languages. An evaluation with novice programmers found that the technique can suggest useful fixes for 47% of errors after 39 person-hours of programming in an instrumented environment.","debugging, recommender systems","","CHI '10"
"Conference Paper","Assogba Y,Donath J","Share: A Programming Environment for Loosely Bound Cooperation","","2010","","","961–970","Association for Computing Machinery","New York, NY, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","Atlanta, Georgia, USA","2010","9781605589299","","https://doi.org/10.1145/1753326.1753469;http://dx.doi.org/10.1145/1753326.1753469","10.1145/1753326.1753469","We introduce a programming environment entitled Share that is designed to encourage loosely bound cooperation between individuals within communities of practice through the sharing of code. Loosely bound cooperation refers to the opportunity community members have to assist and share resources with one another while maintaining their autonomy and independent practice. We contrast this model with forms of collaboration that enable large numbers of distributed individuals to collaborate on large scale works where they are guided by a shared vision of what they are collectively trying to achieve. We hypothesize that providing fine-grained, publicly visible attribution of code sharing activity within a community can provide socially motivated encouragement for code sharing. We present an overview of the design of our tool and the objectives that guided its design and a discussion of a small-scale deployment of our prototype among members of a particular community of practice.","open source, social software, visualization, community, collaboration, cooperaion, programming, computer suppoerted cooperative work","","CHI '10"
"Conference Paper","Klint P,van der Storm T,Vinju J","On the Impact of DSL Tools on the Maintainability of Language Implementations","","2010","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth Workshop on Language Descriptions, Tools and Applications","Paphos, Cyprus","2010","9781450300636","","https://doi.org/10.1145/1868281.1868291;http://dx.doi.org/10.1145/1868281.1868291","10.1145/1868281.1868291","Does the use of DSL tools improve the maintainability of language implementations compared to implementations from scratch? We present empirical results on aspects of maintainability of six implementations of the same DSL using different languages (Java, JavaScript, C#) and DSL tools (ANTLR, OMeta, Microsoft ""M""). Our evaluation indicates that the maintainability of language implementations is indeed higher when constructed using DSL tools.","language engineering, domain specific languages, maintainability, tools","","LDTA '10"
"Conference Paper","Scanniello G,D'Amico A,D'Amico C,D'Amico T","An Approach for Architectural Layer Recovery","","2010","","","2198–2202","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ACM Symposium on Applied Computing","Sierre, Switzerland","2010","9781605586397","","https://doi.org/10.1145/1774088.1774551;http://dx.doi.org/10.1145/1774088.1774551","10.1145/1774088.1774551","In this paper we present an approach to identify software layers for the understanding and evolution of object oriented software systems. The approach first identifies relations between the classes and then uses the Kleinberg algorithm to group them into layers. Additionally to assess the approach and the underlying techniques, the paper also presents a prototype of a supporting tool to identify layers within Java software systems. To assess the feasibility of both the approach and the system prototype, the results from a case study conducted on an open source Java software system are presented and discussed.","software maintenance, software clustering, class diagram, architecture recovery","","SAC '10"
"Conference Paper","Gu Q","Efficient Code Diversification for Network Reprogramming in Sensor Networks","","2010","","","145–150","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third ACM Conference on Wireless Network Security","Hoboken, New Jersey, USA","2010","9781605589237","","https://doi.org/10.1145/1741866.1741890;http://dx.doi.org/10.1145/1741866.1741890","10.1145/1741866.1741890","As sensors in a network are mostly homogeneous in software and hardware, a captured sensor can easily expose its code and data to attackers and further threaten the whole network. To increase the survivability of a sensor network, code diversification has been shown to be an effective solution. However, disseminating many diversified code images is very costly in current network reprogramming systems, as it does not take advantage of the epidemic propagation of network reprogramming. New mechanisms need to be studied for integrating code diversification with network reprogramming. This paper proposes an efficient code diversification scheme for network reprogramming in sensor networks. The scheme uses Deluge to disseminate code images of sensor applications that carry diversification information and allows sensors to randomize the layout of their own executables. Such diversification can defeat a wide range of attacks that exploit the knowledge of code layout, as no sensors have the same code layout in their executables. Except the cost determined by the code size, the computational overhead of diversification in sensors can be reduced to 60%, while sacrificing only 10% of security.","network reprogramming, deluge, code diversity, code security, tinyos","","WiSec '10"
"Conference Paper","Eurviriyanukul K,Paton NW,Fernandes AA,Lynden SJ","Adaptive Join Processing in Pipelined Plans","","2010","","","183–194","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Extending Database Technology","Lausanne, Switzerland","2010","9781605589459","","https://doi.org/10.1145/1739041.1739066;http://dx.doi.org/10.1145/1739041.1739066","10.1145/1739041.1739066","In adaptive query processing, the way in which a query is evaluated is changed in the light of feedback obtained from the environment during query evaluation. Such feedback may, for example, establish that misleading selectivity estimates were used when the query was compiled, leading to the optimizer choosing an inappropriate join order or unsuitable join algorithms. This paper describes how joins can be reordered, and the join algorithms used replaced, while they are being evaluated in pipelined plans. Where joins are reordered and/or replaced during their evaluation, the approach avoids duplicating work that has already been carried out, by resuming from where the previous plan left off. The approach has been evaluated empirically, and shown to be effective for improving query performance in the light of misleading selectivity estimates.","","","EDBT '10"
"Conference Paper","Geoffray N,Thomas G,Lawall J,Muller G,Folliot B","VMKit: A Substrate for Managed Runtime Environments","","2010","","","51–62","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments","Pittsburgh, Pennsylvania, USA","2010","9781605589107","","https://doi.org/10.1145/1735997.1736006;http://dx.doi.org/10.1145/1735997.1736006","10.1145/1735997.1736006","Managed Runtime Environments (MREs), such as the JVM and the CLI, form an attractive environment for program execution, by providing portability and safety, via the use of a bytecode language and automatic memory management, as well as good performance, via just-in-time (JIT) compilation. Nevertheless, developing a fully featured MRE, including e.g. a garbage collector and JIT compiler, is a herculean task. As a result, new languages cannot easily take advantage of the benefits of MREs, and it is difficult to experiment with extensions of existing MRE based languages.This paper describes and evaluates VMKit, a first attempt to build a common substrate that eases the development of high-level MREs. We have successfully used VMKit to build two MREs: a Java Virtual Machine and a Common Language Runtime. We provide an extensive study of the lessons learned in developing this infrastructure, and assess the ease of implementing new MREs or MRE extensions and the resulting performance. In particular, it took one of the authors only one month to develop a Common Language Runtime using VMKit. VMKit furthermore has performance comparableto the well established open source MREs Cacao, Apache Harmony and Mono, and is 1.2 to 3 times slower than JikesRVM on most of the Dacapo benchmarks.","vmkit, virtual machine, just in time compiler","","VEE '10"
"Journal Article","Geoffray N,Thomas G,Lawall J,Muller G,Folliot B","VMKit: A Substrate for Managed Runtime Environments","SIGPLAN Not.","2010","45","7","51–62","Association for Computing Machinery","New York, NY, USA","","","2010-03","","0362-1340","https://doi.org/10.1145/1837854.1736006;http://dx.doi.org/10.1145/1837854.1736006","10.1145/1837854.1736006","Managed Runtime Environments (MREs), such as the JVM and the CLI, form an attractive environment for program execution, by providing portability and safety, via the use of a bytecode language and automatic memory management, as well as good performance, via just-in-time (JIT) compilation. Nevertheless, developing a fully featured MRE, including e.g. a garbage collector and JIT compiler, is a herculean task. As a result, new languages cannot easily take advantage of the benefits of MREs, and it is difficult to experiment with extensions of existing MRE based languages.This paper describes and evaluates VMKit, a first attempt to build a common substrate that eases the development of high-level MREs. We have successfully used VMKit to build two MREs: a Java Virtual Machine and a Common Language Runtime. We provide an extensive study of the lessons learned in developing this infrastructure, and assess the ease of implementing new MREs or MRE extensions and the resulting performance. In particular, it took one of the authors only one month to develop a Common Language Runtime using VMKit. VMKit furthermore has performance comparableto the well established open source MREs Cacao, Apache Harmony and Mono, and is 1.2 to 3 times slower than JikesRVM on most of the Dacapo benchmarks.","virtual machine, just in time compiler, vmkit","",""
"Conference Paper","Palix N,Lawall J,Muller G","Tracking Code Patterns over Multiple Software Versions with Herodotos","","2010","","","169–180","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th International Conference on Aspect-Oriented Software Development","Rennes and Saint-Malo, France","2010","9781605589589","","https://doi.org/10.1145/1739230.1739250;http://dx.doi.org/10.1145/1739230.1739250","10.1145/1739230.1739250","An important element of understanding a software code base is to identify the repetitive patterns of code it contains and how these evolve over time. Some patterns are useful to the software, and may be modularized. Others are detrimental to the software, such as patterns that represent defects. In this case, it is useful to study the occurrences of such patterns, to identify properties such as when and why they are introduced, how long they persist, and the reasons why they are corrected.To enable studying pattern occurrences over time, we propose a tool, Herodotos, that semi-automatically tracks pattern occurrences over multiple versions of a software project, independent of other changes in the source files. Guided by a user-provided configuration file, Herodotos builds various graphs showing the evolution of the pattern occurrences and computes some statistics. We have evaluated this approach on the history of a representative range of open source projects over the last three years. For each project, we track several kinds of defects that have been found by pattern matching. This tracking is done automatically in 99% of the occurrences. The results allow us to compare the evolution of the selected projects and defect kinds over time.","Herodotos, bug tracking, history of pattern occurrences","","AOSD '10"
"Conference Paper","Feng S,Gupta S,Ansari A,Mahlke S","Shoestring: Probabilistic Soft Error Reliability on the Cheap","","2010","","","385–396","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems","Pittsburgh, Pennsylvania, USA","2010","9781605588391","","https://doi.org/10.1145/1736020.1736063;http://dx.doi.org/10.1145/1736020.1736063","10.1145/1736020.1736063","Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with ""shoestring"" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6%. This reliability improvement comes at a modest performance overhead of 15.8%.","compiler analysis, error detection, fault injection","","ASPLOS XV"
"Journal Article","Feng S,Gupta S,Ansari A,Mahlke S","Shoestring: Probabilistic Soft Error Reliability on the Cheap","SIGARCH Comput. Archit. News","2010","38","1","385–396","Association for Computing Machinery","New York, NY, USA","","","2010-03","","0163-5964","https://doi.org/10.1145/1735970.1736063;http://dx.doi.org/10.1145/1735970.1736063","10.1145/1735970.1736063","Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with ""shoestring"" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6%. This reliability improvement comes at a modest performance overhead of 15.8%.","compiler analysis, fault injection, error detection","",""
"Journal Article","Feng S,Gupta S,Ansari A,Mahlke S","Shoestring: Probabilistic Soft Error Reliability on the Cheap","SIGPLAN Not.","2010","45","3","385–396","Association for Computing Machinery","New York, NY, USA","","","2010-03","","0362-1340","https://doi.org/10.1145/1735971.1736063;http://dx.doi.org/10.1145/1735971.1736063","10.1145/1735971.1736063","Aggressive technology scaling provides designers with an ever increasing budget of cheaper and faster transistors. Unfortunately, this trend is accompanied by a decline in individual device reliability as transistors become increasingly susceptible to soft errors. We are quickly approaching a new era where resilience to soft errors is no longer a luxury that can be reserved for just processors in high-reliability, mission-critical domains. Even processors used in mainstream computing will soon require protection. However, due to tighter profit margins, reliable operation for these devices must come at little or no cost. This paper presents Shoestring, a minimally invasive software solution that provides high soft error coverage with very little overhead, enabling its deployment even in commodity processors with ""shoestring"" reliability budgets. Leveraging intelligent analysis at compile time, and exploiting low-cost, symptom-based error detection, Shoestring is able to focus its efforts on protecting statistically-vulnerable portions of program code. Shoestring effectively applies instruction duplication to protect only those segments of code that, when subjected to a soft error, are likely to result in user-visible faults without first exhibiting symptomatic behavior. Shoestring is able to recover from an additional 33.9% of soft errors that are undetected by a symptom-only approach, achieving an overall user-visible failure rate of 1.6%. This reliability improvement comes at a modest performance overhead of 15.8%.","fault injection, error detection, compiler analysis","",""
"Conference Paper","Rama GM","A Desiderata for Refactoring-Based Software Modularity Improvement","","2010","","","93–102","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd India Software Engineering Conference","Mysore, India","2010","9781605589220","","https://doi.org/10.1145/1730874.1730893;http://dx.doi.org/10.1145/1730874.1730893","10.1145/1730874.1730893","There exists number of large business critical software systems of recent vintage that are becoming increasingly difficult to maintain. These systems, written in newer languages such as C and Java are fast becoming legacy and showing the same symptoms of modularity deterioration reminiscent of older legacy systems written in Cobol and PL1. However, this problem has not received much attention from the software modularization community. In this paper, we argue that the modularization needs of these relatively newer systems, which generally have some modular structure, is significantly different from the needs of older, mainly monolithic, legacy systems. We emphasize the need for incrementally improving modularity and propose a software refactoring based approach to solve this problem, thereby, uniting hitherto two disparate threads of research - software modularization and software refactoring. As part of this refactoring based modularity improvement approach, we identify a set of modularity smells and specify how to detect these smells in poorly modularized software systems. A validation of these modularity smells is carried out using several open source systems.","software maintenance, preventive maintenance, software refactoring, software modularity, code smells","","ISEC '10"
"Conference Paper","Gross PA,Herstand MS,Hodges JW,Kelleher CL","A Code Reuse Interface for Non-Programmer Middle School Students","","2010","","","219–228","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th International Conference on Intelligent User Interfaces","Hong Kong, China","2010","9781605585154","","https://doi.org/10.1145/1719970.1720001;http://dx.doi.org/10.1145/1719970.1720001","10.1145/1719970.1720001","We describe a code reuse tool for use in the Looking Glass IDE, the successor to Storytelling Alice [17], which enables middle school students with little to no programming experience to reuse functionality they find in programs written by others. Users (1) record a feature to reuse, (2) find code responsible for the feature, (3) abstract the code into a reusable Actionscript by describing object ""roles,"" and (4) integrate the Actionscript into another program. An exploratory study with middle school students indicates they can successfully reuse code. Further, 36 of the 47 users appropriated new programming constructs through the process of reuse.","non-programmer, storytelling alice, looking glass, code reuse, middle school, end user","","IUI '10"
"Conference Paper","Monnier S,Haguenauer D","Singleton Types Here, Singleton Types There, Singleton Types Everywhere","","2010","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM SIGPLAN Workshop on Programming Languages Meets Program Verification","Madrid, Spain","2010","9781605588902","","https://doi.org/10.1145/1707790.1707792;http://dx.doi.org/10.1145/1707790.1707792","10.1145/1707790.1707792","Singleton types are often considered a poor man's substitute for dependent types. But their generalization in the form of GADTs has found quite a following. The main advantage of singleton types and GADTs is to preserve the so-called phase distinction, which seems to be so important to make use of the usual compilation techniques.Of course, they considerably restrict the programmers, which often leads them to duplicate code at both the term and type levels, so as to reflect at the type level what happens at the term level, in order to be able to reason about it.In this article, we show how to automate such a duplication while eliminating the problematic dependencies. More specifically, we show how to compile the Calculus of Constructions into λH, a non-dependently-typed language, while still preserving all the typing information. Since λH has been shown to be amenable to type preserving CPS and closure conversion, it shows a way to preserve types when doing code extraction and more generally when using all the common compiler techniques.","singleton types, dependent types, certified compilation","","PLPV '10"
"Conference Paper","Brown C,Thompson S","Clone Detection and Elimination for Haskell","","2010","","","111–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2010 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation","Madrid, Spain","2010","9781605587271","","https://doi.org/10.1145/1706356.1706378;http://dx.doi.org/10.1145/1706356.1706378","10.1145/1706356.1706378","Duplicated code is a well known problem in software maintenance and refactoring. Code clones tend to increase program size and several studies have shown that duplicated code makes maintenance and code understanding more complex and time consuming.This paper presents a new technique for the detection and removal of duplicated Haskell code. The system is implemented within the refactoring framework of the Haskell Refactorer (HaRe), and uses an Abstract Syntax Tree (AST) based approach. Detection of duplicate code is automatic, while elimination is semi-automatic, with the user managing the clone removal. After presenting the system, an example is given to show how it works in practice.","duplicated code, generalisation, haskell, program analysis, hare, refactoring, program transformation","","PEPM '10"
"Conference Paper","Dömer R","Computer-Aided Recoding for Multi-Core Systems","","2010","","","713–716","IEEE Press","Taipei, Taiwan","Proceedings of the 2010 Asia and South Pacific Design Automation Conference","","2010","9781605588377","","","","The design of embedded computing systems faces a serious productivity gap due to the increasing complexity of their hardware and software components. One solution to address this problem is the modeling at higher levels of abstraction. However, manually writing proper executable system models is challenging, error-prone, and very time-consuming. We aim to automate critical coding tasks in the creation of system models.This paper outlines a novel modeling technique called computer-aided recoding which automates the process of writing abstract models of embedded systems by use of advanced computer-aided design (CAD) techniques. Using an interactive, designer-controlled approach with automated source code transformations, our omputer-aided recoding technique derives an executable parallel system model directly from available sequential reference code. Specifically, we describe three sets of source code transformations that create structural hierarchy, expose potential parallelism, and create explicit communication and synchronization.As a result, system modeling is significantly streamlined. Our experimental results demonstrate the shortened design time and higher productivity.","","","ASPDAC '10"
"Conference Paper","Martin JP,Hicks M,Costa M,Akritidis P,Castro M","Dynamically Checking Ownership Policies in Concurrent c/C++ Programs","","2010","","","457–470","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","Madrid, Spain","2010","9781605584799","","https://doi.org/10.1145/1706299.1706351;http://dx.doi.org/10.1145/1706299.1706351","10.1145/1706299.1706351","Concurrent programming errors arise when threads share data incorrectly. Programmers often avoid these errors by using synchronization to enforce a simple ownership policy: data is either owned exclusively by a thread that can read or write the data, or it is read owned by a set of threads that can read but not write the data. Unfortunately, incorrect synchronization often fails to enforce these policies and memory errors in languages like C and C++ can violate these policies even when synchronization is correct.In this paper, we present a dynamic analysis for checking ownership policies in concurrent C and C++ programs despite memory errors. The analysis can be used to find errors in commodity multi-threaded programs and to prevent attacks that exploit these errors. We require programmers to write ownership assertions that describe the sharing policies used by different parts of the program. These policies may change over time, as may the policies' means of enforcement, whether it be locks, barriers, thread joins, etc. Our compiler inserts checks in the program that signal an error if these policies are violated at runtime. We evaluated our tool on several benchmark programs. The run-time overhead was reasonable: between 0 and 49% with an average of 26%. We also found the tool easy to use: the total number of ownership assertions is small, and the asserted specification and implementation can be debugged together by running the instrumented program and addressing the errors that arise. Our approach enjoys a pleasing modular soundness property: if a thread executes a sequence of statements on variables it owns, the statements are serializable within a valid execution, and thus their effects can be reasoned about in isolation from other threads in the program.","security, dynamic analysis, tools, testing, concurrency, debugging","","POPL '10"
"Journal Article","Martin JP,Hicks M,Costa M,Akritidis P,Castro M","Dynamically Checking Ownership Policies in Concurrent c/C++ Programs","SIGPLAN Not.","2010","45","1","457–470","Association for Computing Machinery","New York, NY, USA","","","2010-01","","0362-1340","https://doi.org/10.1145/1707801.1706351;http://dx.doi.org/10.1145/1707801.1706351","10.1145/1707801.1706351","Concurrent programming errors arise when threads share data incorrectly. Programmers often avoid these errors by using synchronization to enforce a simple ownership policy: data is either owned exclusively by a thread that can read or write the data, or it is read owned by a set of threads that can read but not write the data. Unfortunately, incorrect synchronization often fails to enforce these policies and memory errors in languages like C and C++ can violate these policies even when synchronization is correct.In this paper, we present a dynamic analysis for checking ownership policies in concurrent C and C++ programs despite memory errors. The analysis can be used to find errors in commodity multi-threaded programs and to prevent attacks that exploit these errors. We require programmers to write ownership assertions that describe the sharing policies used by different parts of the program. These policies may change over time, as may the policies' means of enforcement, whether it be locks, barriers, thread joins, etc. Our compiler inserts checks in the program that signal an error if these policies are violated at runtime. We evaluated our tool on several benchmark programs. The run-time overhead was reasonable: between 0 and 49% with an average of 26%. We also found the tool easy to use: the total number of ownership assertions is small, and the asserted specification and implementation can be debugged together by running the instrumented program and addressing the errors that arise. Our approach enjoys a pleasing modular soundness property: if a thread executes a sequence of statements on variables it owns, the statements are serializable within a valid execution, and thus their effects can be reasoned about in isolation from other threads in the program.","debugging, dynamic analysis, tools, concurrency, security, testing","",""
"Journal Article","Välimäki N,Mäkinen V,Gerlach W,Dixit K","Engineering a Compressed Suffix Tree Implementation","ACM J. Exp. Algorithmics","2010","14","","","Association for Computing Machinery","New York, NY, USA","","","2010-01","","1084-6654","https://doi.org/10.1145/1498698.1594228;http://dx.doi.org/10.1145/1498698.1594228","10.1145/1498698.1594228","Suffix tree is one of the most important data structures in string algorithms and biological sequence analysis. Unfortunately, when it comes to implementing those algorithms and applying them to real genomic sequences, often the main memory size becomes the bottleneck. This is easily explained by the fact that while a DNA sequence of length n from alphabet Σ = A,C,G,T can be stored in n log |Σ| = 2n bits, its suffix tree occupiesO(n log n) bits. In practice, the size difference easily reaches factor 50.We report on an implementation of the compressed suffix tree very recently proposed by Sadakane (2007). The compressed suffix tree occupies space proportional to the text size, that is, O(n log |Σ|) bits, and supports all typical suffix tree operations with at most log n factor slowdown. Our experiments show that, for example, on a 10 MB DNA sequence, the compressed suffix tree takes 10% of the space of the normal suffix tree. At the same time, a representative algorithm is slowed down by factor 30.Our implementation follows the original proposal in spirit, but some internal parts are tailored toward practical implementation. Our construction algorithm has time requirement O(n log n log |Σ|) and uses closely the same space as the final structure while constructing it: on the 10MB DNA sequence, the maximum space usage during construction is only 1.5 times the final product size. As by-products, we develop a method to create Succinct Suffix Array directly from Burrows-Wheeler transform and a space-efficient version of the suffixes-insertion algorithm to build balanced parentheses representation of suffix tree from LCP information.","lowest common ancestor, wavelet tree, Burrows-Wheeler transform, biological sequence analysis, balanced parentheses, Algorithm engineering, lcp values, text indexing, text compression","",""
"Conference Paper","Cesare S,Xiang Y","Classification of Malware Using Structured Control Flow","","2010","","","61–70","Australian Computer Society, Inc.","AUS","Proceedings of the Eighth Australasian Symposium on Parallel and Distributed Computing - Volume 107","Brisbane, Australia","2010","9781920682880","","","","Malware is a pervasive problem in distributed computer and network systems. Identification of malware variants provides great benefit in early detection. Control flow has been proposed as a characteristic that can be identified across variants, resulting in flowgraph based malware classification. Static analysis is widely used for the classification but can be ineffective if malware undergoes a code packing transformation to hide its real content. This paper proposes a novel algorithm for constructing a control flow graph signature using the decompilation technique of structuring. Similarity between structured graphs can be quickly determined using string edit distances. To reverse the code packing transformation, a fast application level emulator is proposed. To demonstrate the effectiveness of the automated unpacking and flowgraph based classification, we implement a complete system and evaluate it using synthetic and real malware. The evaluation shows our system is highly effective in terms of accuracy in revealing all the hidden code, execution time for unpacking, and accuracy in classification.","unpacking, malware, structured control flow, network security","","AusPDC '10"
"Conference Paper","Hirayama T","Let's Make a Tennis Game! Introduction to Game Programming","","2009","","","","Association for Computing Machinery","New York, NY, USA","ACM SIGGRAPH ASIA 2009 Courses","Yokohama, Japan","2009","9781450379311","","https://doi.org/10.1145/1665817.1665827;http://dx.doi.org/10.1145/1665817.1665827","10.1145/1665817.1665827","In recent years, games have become more sophisticated and demanding, with high-level technologies such as AI, physics, and graphics. At the same time, the knowledge required to be a game programmer is becoming increasingly unclear. This course attempts to clarify this situation by guiding non-game programmers through the development process for a simple tennis game and providing an overview of game-programming concepts. The course begins with a 2D game requiring minimal preliminary knowledge and then adds more advanced elements such as 3D CG, audio, effects, interface, cameras, and shaders. The shader section of the course covers the graphics-engine architecture required to speed up the rendering without sacrificing the look by taking advantage of the game design. The same idea was used in Virtua Tennis 3. This course is unique in that it focuses on the process of development by adding elements one by one, rather than explaining elements of a finished game separately. Attendees learn how a game gets closer to its complete form step by step, assuring that a game can be developed by one person, provided its scale is small enough.","","","SIGGRAPH ASIA '09"
"Conference Paper","Wang X,Jhi YC,Zhu S,Liu P","Behavior Based Software Theft Detection","","2009","","","280–290","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM Conference on Computer and Communications Security","Chicago, Illinois, USA","2009","9781605588940","","https://doi.org/10.1145/1653662.1653696;http://dx.doi.org/10.1145/1653662.1653696","10.1145/1653662.1653696","Along with the burst of open source projects, software theft (or plagiarism) has become a very serious threat to the healthiness of software industry. Software birthmark, which represents the unique characteristics of a program, can be used for software theft detection. We propose a system call dependence graph based software birthmark called SCDG birthmark, and examine how well it reflects unique behavioral characteristics of a program. To our knowledge, our detection system based on SCDG birthmark is the first one that is capable of detecting software component theft where only partial code is stolen. We demonstrate the strength of our birthmark against various evasion techniques, including those based on different compilers and different compiler optimization levels as well as two state-of-the-art obfuscation tools. Unlike the existing work that were evaluated through small or toy software, we also evaluate our birthmark on a set of large software. Our results show that SCDG birthmark is very practical and effective in detecting software theft that even adopts advanced evasion techniques.","software birthmark, software plagiarism, dynamic analysis, software theft","","CCS '09"
"Conference Paper","Lee HS,Doh KG","Tree-Pattern-Based Duplicate Code Detection","","2009","","","7–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM First International Workshop on Data-Intensive Software Management and Mining","Hong Kong, China","2009","9781605588100","","https://doi.org/10.1145/1651309.1651312;http://dx.doi.org/10.1145/1651309.1651312","10.1145/1651309.1651312","This paper presents a tree-pattern-based method of automatically and accurately finding code clones in program files. Duplicate tree-patterns are first collected by anti-unification algorithm and redundancy-free exhaustive comparisons, and then finally clustered. The algorithm is designed in such a way that the same comparison is not repeated for speed, while thoroughly examining every possible pairs of tree patterns for accuracy. Our method maintains the syntax structure of code in tree-pattern clusters, which gives the flexibility of finding different types of clones while keeping the precision.","reverse engineering, tree-pattern, clone detection, software maintenance","","DSMM '09"
"Journal Article","Harman M,Binkley D,Gallagher K,Gold N,Krinke J","Dependence Clusters in Source Code","ACM Trans. Program. Lang. Syst.","2009","32","1","","Association for Computing Machinery","New York, NY, USA","","","2009-11","","0164-0925","https://doi.org/10.1145/1596527.1596528;http://dx.doi.org/10.1145/1596527.1596528","10.1145/1596527.1596528","A dependence cluster is a set of program statements, all of which are mutually inter-dependent. This article reports a large scale empirical study of dependence clusters in C program source code. The study reveals that large dependence clusters are surprisingly commonplace. Most of the 45 programs studied have clusters of dependence that consume more than 10% of the whole program. Some even have clusters consuming 80% or more. The widespread existence of clusters has implications for source code analyses such as program comprehension, software maintenance, software testing, reverse engineering, reuse, and parallelization.","Dependence, program slicing, program comprehension","",""
"Conference Paper","Hou D,Jacob F,Jablonski P","Exploring the Design Space of Proactive Tool Support for Copy-and-Paste Programming","","2009","","","188–202","IBM Corp.","USA","Proceedings of the 2009 Conference of the Center for Advanced Studies on Collaborative Research","Ontario, Canada","2009","","","https://doi.org/10.1145/1723028.1723051;http://dx.doi.org/10.1145/1723028.1723051","10.1145/1723028.1723051","Programmers copy and paste code for various reasons, leaving similar code fragments (clones) in the software systems. As a result, they must spend effort and attention in tracking, understanding, and correctly adapting and evolving clones. Ideally, an integrated environment should be used to fully and seamlessly assist programmers in all these activities. Unlike clone-detection-based tools, such environments should support and manage clones proactively as clones are created and evolved. In this paper, an initial design space for such environments is sketched based on our experience with a prototype named CnP. The features and design decisions taken in CnP as well as remaining design problems are identified. To assess the potential of the developed features and identify new design input, code clones in several real-world systems are examined, and their implications on the design (and redesign) of proactive clone tools are discussed.","","","CASCON '09"
"Journal Article","Li W,Zhang Y,Yang J,Zheng J","Towards Update-Conscious Compilation for Energy-Efficient Code Dissemination in WSNs","ACM Trans. Archit. Code Optim.","2009","6","4","","Association for Computing Machinery","New York, NY, USA","","","2009-10","","1544-3566","https://doi.org/10.1145/1596510.1596512;http://dx.doi.org/10.1145/1596510.1596512","10.1145/1596510.1596512","Postdeployment code dissemination in wireless sensor networks (WSN) is challenging, as the code has to be transmitted via energy-expensive wireless communication. In this article, we propose novel update-conscious compilation (UCC) techniques to achieve energy efficiency. By integrating the compilation decisions in generating the old binary, an update-conscious compiler strives to match the old decisions, which improves the binary code similarity, reduces the amount of transmitted data to remote sensors, and thus, consumes less energy. In this article, we develop update-conscious register allocation and data layout algorithms. Our experimental results show great improvements over the traditional, update-oblivious approaches.","sensor networks, code dissemination, Register allocation","",""
"Conference Paper","Inoue H,Nakatani T","How a Java VM Can Get More from a Hardware Performance Monitor","","2009","","","137–154","Association for Computing Machinery","New York, NY, USA","Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications","Orlando, Florida, USA","2009","9781605587660","","https://doi.org/10.1145/1640089.1640100;http://dx.doi.org/10.1145/1640089.1640100","10.1145/1640089.1640100","This paper describes our sampling-based profiler that exploits a processor's HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the Java VM. Our profiler provides two novel features: Java-level event profiling and lightweight context-sensitive event profiling. For Java events, we propose new techniques to leverage the sampling facility of the HPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve a smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations with the HPM, which can only sample hardware events such as executed instructions or cache misses, we correlate the object creations with the store instructions for Java object headers. For the lock activity profile, we introduce an instrumentation-based technique, called ProbeNOP, which uses a special NOP instruction whose executions are counted by the HPM. For the context-sensitive event profiling, we propose a new technique called CallerChaining, which detects the calling context of HPM events based on the call stack depth (the value of the stack frame pointer). We show that it can detect the calling contexts in many programs including a large commercial application. Our proposed techniques enable both programmers and runtime systems to get more valuable information from the HPM to understand and optimize the programs without adding significant runtime overhead.","hardware performance monitor, profiling, calling context","","OOPSLA '09"
"Journal Article","Inoue H,Nakatani T","How a Java VM Can Get More from a Hardware Performance Monitor","SIGPLAN Not.","2009","44","10","137–154","Association for Computing Machinery","New York, NY, USA","","","2009-10","","0362-1340","https://doi.org/10.1145/1639949.1640100;http://dx.doi.org/10.1145/1639949.1640100","10.1145/1639949.1640100","This paper describes our sampling-based profiler that exploits a processor's HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the Java VM. Our profiler provides two novel features: Java-level event profiling and lightweight context-sensitive event profiling. For Java events, we propose new techniques to leverage the sampling facility of the HPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve a smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations with the HPM, which can only sample hardware events such as executed instructions or cache misses, we correlate the object creations with the store instructions for Java object headers. For the lock activity profile, we introduce an instrumentation-based technique, called ProbeNOP, which uses a special NOP instruction whose executions are counted by the HPM. For the context-sensitive event profiling, we propose a new technique called CallerChaining, which detects the calling context of HPM events based on the call stack depth (the value of the stack frame pointer). We show that it can detect the calling contexts in many programs including a large commercial application. Our proposed techniques enable both programmers and runtime systems to get more valuable information from the HPM to understand and optimize the programs without adding significant runtime overhead.","hardware performance monitor, calling context, profiling","",""
"Conference Paper","Voigt T,Eriksson J,Österlind F,Sauter R,Aschenbruck N,Marrón PJ,Reynolds V,Shu L,Visser O,Koubaa A,Köpke A","Towards Comparable Simulations of Cooperating Objects and Wireless Sensor Networks","","2009","","","","ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)","Brussels, BEL","Proceedings of the Fourth International ICST Conference on Performance Evaluation Methodologies and Tools","Pisa, Italy","2009","9789639799707","","https://doi.org/10.4108/ICST.VALUETOOLS2009.7651;http://dx.doi.org/10.4108/ICST.VALUETOOLS2009.7651","10.4108/ICST.VALUETOOLS2009.7651","Simulators are indispensable tools to support the development and testing of cooperating objects such as wireless sensor networks (WSN). However, it is often not possible to compare the results of different simulation tools. Thus, the goal of this paper is the specification of a generic simulation platform for cooperating objects. We propose a platform that consists of a set of simulators that together fulfill desired simulator properties. We show that to achieve comparable results the use of a common specification language for the software-under-test is not feasible. Instead, we argue that using common input formats for the simulated environment and common output formats for the results is useful. This again motivates that a simulation tool consisting of a set of existing simulators that are able to use common scenario-input and can produce common output which will bring us a step closer to the vision of achieving comparable simulation results.","","","VALUETOOLS '09"
"Conference Paper","Wu N,Wen M,Wu W,Ren J,Su H,Xun C,Zhang C","Streaming HD H.264 Encoder on Programmable Processors","","2009","","","371–380","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th ACM International Conference on Multimedia","Beijing, China","2009","9781605586083","","https://doi.org/10.1145/1631272.1631324;http://dx.doi.org/10.1145/1631272.1631324","10.1145/1631272.1631324","Programmable processors have great advantage over dedicated ASIC design under intense time-to-market pressure. However, real-time encoding of high-definition (HD) H.264 video (up to 1080p) is a challenge to most existing programmable processors. On the other hand, model-based design is widely accepted in developing complex media program. Stream model, an emerging model-based programming method, shows surprising efficiency on many compute-intensive domains especially for media processing. On the basis, this paper proposes a set of streaming techniques for H.264 encoding, and then develops all of the code based on the X264 reference code. Our streaming H.264 encoder is a pure software implementation completely written in high-level language without special hardware/algorithm support. Real execution results show that our encoder achieves significant speedup over the original X264 encoder on various programmable architectures: on X86 CoreTM2 E8200 the speedup is 1.8x, on MIPS 4KEc the speedup is 3.7x, on TMS320 C6416 DSP the speedup is 5.5x, on stream processor STORM-SP16 G220 the speedup is 6.1x. Especially, on STORM processor, the streaming encoder achieves the performance of 30.6 frames per second for a 1080P HD sequence, satisfying the real-time requirement. These indicate that streaming is extremely efficient for this kind of media workload. Our work is also applicable for other media processing applications, and provides architecture insights into dedicated ASIC or FPGA HD H.264 encoders.","1080P HD, H.264 encoder, real-time, programmable, stream","","MM '09"
"Conference Paper","Shao B,Vasudevan N,Edwards SA","Compositional Deadlock Detection for Rendezvous Communication","","2009","","","59–66","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventh ACM International Conference on Embedded Software","Grenoble, France","2009","9781605586274","","https://doi.org/10.1145/1629335.1629344;http://dx.doi.org/10.1145/1629335.1629344","10.1145/1629335.1629344","Concurrent programming languages are growing in importance with the advent of multi-core systems. However, concurrent programs suffer from problems, such as data races and deadlock, absent from sequential programs. Unfortunately, traditional race and deadlock detection techniques fail on both large programs and small programs with complex behaviors.In this paper, we present a compositional deadlock detection technique for a concurrent language--SHIM--in which tasks run asynchronously and communicate using synchronous CSP-style rendezvous. Although SHIM guarantees the absence of data races, a SHIM program may still deadlock if the communication protocol is violated. Our previous work used NuSMV, a symbolic model checker, to detect deadlock in a SHIM program, but it did not scale well with the size of the problem. In this work, we take an incremental, divide-and-conquer approach to deadlock detection.In practice, we find our procedure is faster and uses less memory than the existing technique, especially on large programs, making our algorithm a practical part of the compilation chain.","concurrency, deadlock, divide-and-conquer, static analysis, SHIM","","EMSOFT '09"
"Conference Paper","Kandemir M,Zhang Y,Muralidhara SP,Ozturk O,Narayanan SH","Slicing Based Code Parallelization for Minimizing Inter-Processor Communication","","2009","","","87–96","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems","Grenoble, France","2009","9781605586267","","https://doi.org/10.1145/1629395.1629409;http://dx.doi.org/10.1145/1629395.1629409","10.1145/1629395.1629409","One of the critical problems in distributed memory multi-core architectures is scalable parallelization that minimizes inter-processor communication. Using the concept of iteration space slicing, this paper presents a new code parallelization scheme for data-intensive applications. This scheme targets distributed memory multi-core architectures, and formulates the problem of data-computation distribution (partitioning) across parallel processors using slicing such that, starting with the partitioning of the output arrays, it iteratively determines the partitions of other arrays as well as iteration spaces of the loop nests in the application code. The goal is to minimize inter-processor data communications. Based on this iteration space slicing based formulation of the problem, we also propose a solution scheme. The proposed data-computation scheme is evaluated using six data-intensive benchmark programs. In our experimental evaluation, we also compare this scheme against three alternate data-computation distribution schemes. The results obtained are very encouraging, indicating around 10% better speedup, with 16 processors, over the next-best scheme when averaged over all benchmark codes we tested.","code analysis and optimization, parallelizing compilers, automatic code parallelization, iteration space slicing","","CASES '09"
"Conference Paper","Nilsson J,Löwe W,Hall J,Nivre J","Parsing Formal Languages Using Natural Language Parsing Techniques","","2009","","","49–60","Association for Computational Linguistics","USA","Proceedings of the 11th International Conference on Parsing Technologies","Paris, France","2009","","","","","Program analysis tools used in software maintenance must be robust and ought to be accurate. Many data-driven parsing approaches developed for natural languages are robust and have quite high accuracy when applied to parsing of software. We show this for the programming languages Java, C/C++, and Python. Further studies indicate that post-processing can almost completely remove the remaining errors. Finally, the training data for instantiating the generic data-driven parser can be generated automatically for formal languages, as opposed to the manually development of treebanks for natural languages. Hence, our approach could improve the robustness of software maintenance tools, probably without showing a significant negative effect on their accuracy.","","","IWPT '09"
"Conference Paper","Apel S,Liebig J,Kästner C,Kuhlemann M,Leich T","An Orthogonal Access Modifier Model for Feature-Oriented Programming","","2009","","","27–33","Association for Computing Machinery","New York, NY, USA","Proceedings of the First International Workshop on Feature-Oriented Software Development","Denver, Colorado, USA","2009","9781605585673","","https://doi.org/10.1145/1629716.1629723;http://dx.doi.org/10.1145/1629716.1629723","10.1145/1629716.1629723","In feature-oriented programming (FOP), a programmer decomposes a program in terms of features. Ideally, features are implemented modularly so that they can be developed in isolation. Access control is an important ingredient to attain feature modularity as it provides mechanisms to hide and expose internal details of a module's implementation. But developers of contemporary feature-oriented languages did not consider access control mechanisms so far. The absence of a well-defined access control model for FOP breaks the encapsulation of feature code and leads to unexpected and undefined program behaviors as well as inadvertent type errors, as we will demonstrate. The reason for these problems is that common object-oriented modifiers, typically provided by the base language, are not expressive enough for FOP and interact in subtle ways with feature-oriented language mechanisms. We raise awareness of this problem, propose three feature-oriented modifiers for access control, and present an orthogonal access modifier model.","orthogonal access modifier model, feature-oriented programming","","FOSD '09"
"Conference Paper","Akai S,Chiba S","Extending AspectJ for Separating Regions","","2009","","","45–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighth International Conference on Generative Programming and Component Engineering","Denver, Colorado, USA","2009","9781605584942","","https://doi.org/10.1145/1621607.1621616;http://dx.doi.org/10.1145/1621607.1621616","10.1145/1621607.1621616","Synchronization is a good candidate for an aspect in aspect-oriented programming (AOP) since programmers have to choose the best granularity of synchronization for the underlying hardware to obtain the best execution performance. If synchronization is an aspect, programmers can change the synchronization code independently of the rest of the program when the program runs on different hardware. However, existing AOP languages such as AspectJ have problems. They cannot select an arbitrary code region as a join point. Moreover, they cannot enforce weaving of a synchronization aspect. Since it is an alternative feature in feature modeling, at least one of available synchronization aspects must be woven. Otherwise, the program would be thread-unsafe. Since an aspect in AspectJ is inherently optional, programmers must be responsible for weaving it. To solve these problems, this paper proposes two new constructs for AspectJ, regioncut and assertions for advice. Regioncut selects arbitrary code region as a join point and assertion for advice enforces weaving a mandatory advice. We implemented these constructs by extending the AspectBench compiler. We evaluated the design of our constructs by applying them to two open-source software products, Javassist and Hadoop.","aspect-oriented programming, feature-oriented programming, synchronization, region","","GPCE '09"
"Journal Article","Akai S,Chiba S","Extending AspectJ for Separating Regions","SIGPLAN Not.","2009","45","2","45–54","Association for Computing Machinery","New York, NY, USA","","","2009-10","","0362-1340","https://doi.org/10.1145/1837852.1621616;http://dx.doi.org/10.1145/1837852.1621616","10.1145/1837852.1621616","Synchronization is a good candidate for an aspect in aspect-oriented programming (AOP) since programmers have to choose the best granularity of synchronization for the underlying hardware to obtain the best execution performance. If synchronization is an aspect, programmers can change the synchronization code independently of the rest of the program when the program runs on different hardware. However, existing AOP languages such as AspectJ have problems. They cannot select an arbitrary code region as a join point. Moreover, they cannot enforce weaving of a synchronization aspect. Since it is an alternative feature in feature modeling, at least one of available synchronization aspects must be woven. Otherwise, the program would be thread-unsafe. Since an aspect in AspectJ is inherently optional, programmers must be responsible for weaving it. To solve these problems, this paper proposes two new constructs for AspectJ, regioncut and assertions for advice. Regioncut selects arbitrary code region as a join point and assertion for advice enforces weaving a mandatory advice. We implemented these constructs by extending the AspectBench compiler. We evaluated the design of our constructs by applying them to two open-source software products, Javassist and Hadoop.","region, aspect-oriented programming, synchronization, feature-oriented programming","",""
"Conference Paper","Sagonas K,Avgerinos T","Automatic Refactoring of Erlang Programs","","2009","","","13–24","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th ACM SIGPLAN Conference on Principles and Practice of Declarative Programming","Coimbra, Portugal","2009","9781605585680","","https://doi.org/10.1145/1599410.1599414;http://dx.doi.org/10.1145/1599410.1599414","10.1145/1599410.1599414","This paper describes the design goals and current status of tidier, a software tool that tidies Erlang source code, making it cleaner, simpler, and often also more efficient. In contrast to other refactoring tools, tidier is completely automatic and is not tied to any particular editor or IDE. Instead, tidier comes with a suite of code transformations that can be selected by its user via command-line options and applied in bulk on a set of modules or entire applications using a simple command. Alternatively, users can use tidier's GUI to inspect one by one the transformations that will be performed on their code and manually select only those that they fancy. We have used tidier to clean up various applications of Erlang/OTP and have tested it on many open source Erlang code bases of significant size. We briefly report our experiences and show opportunities for tidier's current set of transformations on existing Erlang code out there. As a by-product, our paper also documents what we believe are good coding practices in Erlang. Last but not least, our paper describes in detail the automatic code cleanup methodology we advocate and a set of refactorings which are general enough to be applied, as is or with only small modifications, to the source code of programs written in Haskell or Clean and possibly even in non-functional languages.","refactoring, program transformation, code cleanup, erlang, code simplification","","PPDP '09"
"Conference Paper","Avgerinos T,Sagonas K","Cleaning up Erlang Code is a Dirty Job but Somebody's Gotta Do It","","2009","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM SIGPLAN Workshop on ERLANG","Edinburgh, Scotland","2009","9781605585079","","https://doi.org/10.1145/1596600.1596602;http://dx.doi.org/10.1145/1596600.1596602","10.1145/1596600.1596602","This paper describes opportunities for automatically modernizing Erlang applications, cleaning them up, eliminating certain bad smells from their code and occasionally also improving their performance. In addition, we present concrete examples of code improvements and our experiences from using a software tool with these capabilities, tidier, on Erlang code bases of significant size.","code cleanup, code simplification, program transformation, erlang, refactoring","","ERLANG '09"
"Conference Paper","Newton RR,Ko T","Experience Report: Embedded, Parallel Computer-Vision with a Functional DSL","","2009","","","59–64","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming","Edinburgh, Scotland","2009","9781605583327","","https://doi.org/10.1145/1596550.1596562;http://dx.doi.org/10.1145/1596550.1596562","10.1145/1596550.1596562","This paper presents our experience using a domain-specific functional language, WaveScript, to build embedded sensing applications used in scientific research. We focus on a recent computervision application for detecting birds in their natural environment. The application was ported from a prototype in C++. In reimplementing the application, we gained a much cleaner factoring of its functionality (through higher-order functions and better interfaces to libraries) and a near-linear parallel speed-up with no additional effort. These benefits are offset by one substantial downside: the lack of familiarity with the language of the original vision researchers, who understandably tried to use the language in the familiar way they use C++ and thus ran into various problems.","computer vision, stream processing languages","","ICFP '09"
"Journal Article","Newton RR,Ko T","Experience Report: Embedded, Parallel Computer-Vision with a Functional DSL","SIGPLAN Not.","2009","44","9","59–64","Association for Computing Machinery","New York, NY, USA","","","2009-08","","0362-1340","https://doi.org/10.1145/1631687.1596562;http://dx.doi.org/10.1145/1631687.1596562","10.1145/1631687.1596562","This paper presents our experience using a domain-specific functional language, WaveScript, to build embedded sensing applications used in scientific research. We focus on a recent computervision application for detecting birds in their natural environment. The application was ported from a prototype in C++. In reimplementing the application, we gained a much cleaner factoring of its functionality (through higher-order functions and better interfaces to libraries) and a near-linear parallel speed-up with no additional effort. These benefits are offset by one substantial downside: the lack of familiarity with the language of the original vision researchers, who understandably tried to use the language in the familiar way they use C++ and thus ran into various problems.","computer vision, stream processing languages","",""
"Conference Paper","Binder W,Ansaloni D,Villazón A,Moret P","Parallelizing Calling Context Profiling in Virtual Machines on Multicores","","2009","","","111–120","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Conference on Principles and Practice of Programming in Java","Calgary, Alberta, Canada","2009","9781605585987","","https://doi.org/10.1145/1596655.1596672;http://dx.doi.org/10.1145/1596655.1596672","10.1145/1596655.1596672","The Calling Context Tree (CCT) is a prevailing datastructure for calling context profiling. As generating a complete CCT reflecting every method call is expensive, recent research has focused on efficiently approximating the CCT with sampling techniques. However, for tasks such as debugging, testing, and reverse engineering, complete and accurate CCTs are often needed. In this paper, we introduce the ParCCT, a novel approach to parallelizing application code and CCT generation on multicores. Each thread maintains a shadow stack and generates ""packets"" of method calls and returns that correspond to partial CCTs. Each packet includes a copy of the shadow stack, indicating the calling context of the first method call in the packet. Hence, packets are independent of each other and can be processed out-of-order and in parallel in order to update the CCT. Our portable and extensible implementation targets standard Java Virtual Machines, thanks to instrumentation techniques that ensure complete bytecode coverage and efficiently support custom calling context representations. The ParCCT is more than 110% faster than a primitive, non-parallel approach to CCT construction, when more than two cores are available. This speedup stems both from reduced contention and from parallelization.","aspect-oriented programming, calling context profiling, bytecode instrumentation, calling context tree, concurrency, multicores, Java virtual machine","","PPPJ '09"
"Conference Paper","Hanna P,O'Neill I,Stewart D,Qasemizadeh B","Development of a Java-Based Unified and Flexible Natural Language Discourse System","","2009","","","21–29","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Conference on Principles and Practice of Programming in Java","Calgary, Alberta, Canada","2009","9781605585987","","https://doi.org/10.1145/1596655.1596659;http://dx.doi.org/10.1145/1596655.1596659","10.1145/1596655.1596659","This paper outlines the design and development of a Java-based, unified and flexible natural language dialogue system that enables users to interact using natural language, e.g. speech. A number of software development issues are considered with the aim of designing an architecture that enables different discourse components to be readily and flexibly combined in a manner that permits information to be easily shared. Use of XML schemas assists this component interaction. The paper describes how a range of Java language features were employed to support the development of the architecture, providing an illustration of how a modern programming language makes tractable the development of a complex dialogue system.","human computer interaction, dialogue management, spoken dialogue systems","","PPPJ '09"
"Conference Paper","Ganesan D,Lindvall M,Ackermann C,McComas D,Bartholomew M","Verifying Architectural Design Rules of the Flight Software Product Line","","2009","","","161–170","Carnegie Mellon University","USA","Proceedings of the 13th International Software Product Line Conference","San Francisco, California, USA","2009","","","","","This paper presents experiences of verifying architectural design rules of the NASA Core Flight Software (CFS) product line implementation. The goal is to check whether the implementation is consistent with the CFS' architectural rules derived from the developer's guide. The results indicate that consistency checking helps a) identifying architecturally significant deviations that were eluded during code reviews, b) clarifying the design rules to the team, and c) assessing the overall implementation quality. Furthermore, it helps connecting business goals to architectural principles, and to the implementation. This paper is the first step in the definition of a method for analyzing and evaluating product line implementations from an architecture-centric perspective.","flight software, architectural rules, implemented architecture, business goals","","SPLC '09"
"Conference Paper","Pérez J,Crespo Y","Perspectives on Automated Correction of Bad Smells","","2009","","","99–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops","Amsterdam, The Netherlands","2009","9781605586786","","https://doi.org/10.1145/1595808.1595827;http://dx.doi.org/10.1145/1595808.1595827","10.1145/1595808.1595827","Keeping a software system conformant with a desired architecture and consistent with good design principles is a recurring task during the software evolution process. Deviations from good design principles can manifest in the form of bad smells: problems in the system's structure that can negatively affect software quality factors.Many authors have worked in identifying bad smells and in removing them with refactorings: tools have been built to suggest refactorings; successful approaches to detect bad smells have been developed, etc.. We present a comprehensive and historical review on this subject, in order to model the current state of the art and to identify the open challenges, current trends and research opportunities.We also propose a technique based on automated planning, aimed at taking one step forward in the automatic improvement of a system's structure. This proposal will allow computing complex refactoring sequences which can be directed to the achievement of a certain objective, such as the correction of bad smells.","bad smells, automated planning, refactoring","","IWPSE-Evol '09"
"Conference Paper","Ashok B,Joy J,Liang H,Rajamani SK,Srinivasa G,Vangala V","DebugAdvisor: A Recommender System for Debugging","","2009","","","373–382","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering","Amsterdam, The Netherlands","2009","9781605580012","","https://doi.org/10.1145/1595696.1595766;http://dx.doi.org/10.1145/1595696.1595766","10.1145/1595696.1595766","In large software development projects, when a programmer is assigned a bug to fix, she typically spends a lot of time searching (in an ad-hoc manner) for instances from the past where similar bugs have been debugged, analyzed and resolved. Systematic search tools that allow the programmer to express the context of the current bug, and search through diverse data repositories associated with large projects can greatly improve the productivity of debugging This paper presents the design, implementation and experience from such a search tool called DebugAdvisor.The context of a bug includes all the information a programmer has about the bug, including natural language text, textual rendering of core dumps, debugger output etc. Our key insight is to allow the programmer to collate this entire context as a query to search for related information. Thus, DebugAdvisor allows the programmer to search using a fat query, which could be kilobytes of structured and unstructured data describing the contextual information for the current bug. Information retrieval in the presence of fat queries and variegated data repositories, all of which contain a mix of structured and unstructured data is a challenging problem. We present novel ideas to solve this problem.We have deployed DebugAdvisor to over 100 users inside Microsoft. In addition to standard metrics such as precision and recall, we present extensive qualitative and quantitative feedback from our users.","search, recommendation systems, debugging","","ESEC/FSE '09"
"Conference Paper","Nguyen TT,Nguyen HA,Pham NH,Al-Kofahi JM,Nguyen TN","Graph-Based Mining of Multiple Object Usage Patterns","","2009","","","383–392","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering","Amsterdam, The Netherlands","2009","9781605580012","","https://doi.org/10.1145/1595696.1595767;http://dx.doi.org/10.1145/1595696.1595767","10.1145/1595696.1595767","The interplay of multiple objects in object-oriented programming often follows specific protocols, for example certain orders of method calls and/or control structure constraints among them that are parts of the intended object usages. Unfortunately, the information is not always documented. That creates long learning curve, and importantly, leads to subtle problems due to the misuse of objects.In this paper, we propose GrouMiner, a novel graph-based approach for mining the usage patterns of one or multiple objects. GrouMiner approach includes a graph-based representation for multiple object usages, a pattern mining algorithm, and an anomaly detection technique that are efficient, accurate, and resilient to software changes. Our experiments on several real-world programs show that our prototype is able to find useful usage patterns with multiple objects and control structures, and to translate them into user-friendly code skeletons to assist developers in programming. It could also detect the usage anomalies that caused yet undiscovered defects and code smells in those programs.","object usage, groum, api usage, anomaly, graph mining, clone, pattern","","ESEC/FSE '09"
"Conference Paper","Zimmermann T,Nagappan N,Gall H,Giger E,Murphy B","Cross-Project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process","","2009","","","91–100","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering","Amsterdam, The Netherlands","2009","9781605580012","","https://doi.org/10.1145/1595696.1595713;http://dx.doi.org/10.1145/1595696.1595713","10.1145/1595696.1595713","Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted.","decision trees, cross-project, prediction quality, churn, logistic regression, defect prediction","","ESEC/FSE '09"
"Conference Paper","Venables A,Tan G,Lister R","A Closer Look at Tracing, Explaining and Code Writing Skills in the Novice Programmer","","2009","","","117–128","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fifth International Workshop on Computing Education Research Workshop","Berkeley, CA, USA","2009","9781605586151","","https://doi.org/10.1145/1584322.1584336;http://dx.doi.org/10.1145/1584322.1584336","10.1145/1584322.1584336","The way in which novice programmers learn to write code is of considerable interest to computing education researchers. One research approach to understanding how beginners acquire their programming abilities has been to look at student performance in exams. Lopez et al. (2008) analyzed student responses to an end-of-first-semester exam. They found two types of questions accounted for 46% of the variance on the code writing portion of the same exam. One of those types of question required students to trace iterative code, while the other type required students to explain what a piece of code did. In this paper, we investigate whether the results by Lopez et al. may be generally indicative of something about novice programmers, or whether their results are just an artifact of their particular exam. We studied student responses to our own exam and our results are broadly consistent with Lopez et al. However, we did find that some aspects of their model are sensitive to the particular exam questions used. Specifically, we found that student performance on explaining code was hard to characterize, and the strength of the relationship between explaining and code writing is particularly sensitive to the specific questions asked. Additionally, we found Lopez et al.'s use of a Rasch model to be unnecessary, which will make it far easier for others to conduct similar research.","hierarchy, comprehension, tracing, novice programmers, CS1","","ICER '09"
"Conference Paper","McDonnell R,Larkin M,Hernández B,Rudomin I,O'Sullivan C","Eye-Catching Crowds: Saliency Based Selective Variation","","2009","","","","Association for Computing Machinery","New York, NY, USA","ACM SIGGRAPH 2009 Papers","New Orleans, Louisiana","2009","9781605587264","","https://doi.org/10.1145/1576246.1531361;http://dx.doi.org/10.1145/1576246.1531361","10.1145/1576246.1531361","Populated virtual environments need to be simulated with as much variety as possible. By identifying the most salient parts of the scene and characters, available resources can be concentrated where they are needed most. In this paper, we investigate which body parts of virtual characters are most looked at in scenes containing duplicate characters or clones. Using an eye-tracking device, we recorded fixations on body parts while participants were asked to indicate whether clones were present or not. We found that the head and upper torso attract the majority of first fixations in a scene and are attended to most. This is true regardless of the orientation, presence or absence of motion, sex, age, size, and clothing style of the character. We developed a selective variation method to exploit this knowledge and perceptually validated our method. We found that selective colour variation is as effective at generating the illusion of variety as full colour variation. We then evaluated the effectiveness of four variation methods that varied only salient parts of the characters. We found that head accessories, top texture and face texture variation are all equally effective at creating variety, whereas facial geometry alterations are less so. Performance implications and guidelines are presented.","virtual humans, crowd rendering, eye-tracking","","SIGGRAPH '09"
"Journal Article","McDonnell R,Larkin M,Hernández B,Rudomin I,O'Sullivan C","Eye-Catching Crowds: Saliency Based Selective Variation","ACM Trans. Graph.","2009","28","3","","Association for Computing Machinery","New York, NY, USA","","","2009-07","","0730-0301","https://doi.org/10.1145/1531326.1531361;http://dx.doi.org/10.1145/1531326.1531361","10.1145/1531326.1531361","Populated virtual environments need to be simulated with as much variety as possible. By identifying the most salient parts of the scene and characters, available resources can be concentrated where they are needed most. In this paper, we investigate which body parts of virtual characters are most looked at in scenes containing duplicate characters or clones. Using an eye-tracking device, we recorded fixations on body parts while participants were asked to indicate whether clones were present or not. We found that the head and upper torso attract the majority of first fixations in a scene and are attended to most. This is true regardless of the orientation, presence or absence of motion, sex, age, size, and clothing style of the character. We developed a selective variation method to exploit this knowledge and perceptually validated our method. We found that selective colour variation is as effective at generating the illusion of variety as full colour variation. We then evaluated the effectiveness of four variation methods that varied only salient parts of the characters. We found that head accessories, top texture and face texture variation are all equally effective at creating variety, whereas facial geometry alterations are less so. Performance implications and guidelines are presented.","eye-tracking, crowd rendering, virtual humans","",""
"Conference Paper","Godlin B,Strichman O","Regression Verification","","2009","","","466–471","Association for Computing Machinery","New York, NY, USA","Proceedings of the 46th Annual Design Automation Conference","San Francisco, California","2009","9781605584973","","https://doi.org/10.1145/1629911.1630034;http://dx.doi.org/10.1145/1629911.1630034","10.1145/1629911.1630034","Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are two main reasons for this claim: it circumvents the problem of specifying what the program should do, and in many cases it is computationally easier. We study theoretical and practical aspects of this problem, which we call regression verification.","equivalence checking, software verification","","DAC '09"
"Conference Paper","Jiang L,Su Z","Automatic Mining of Functionally Equivalent Code Fragments via Random Testing","","2009","","","81–92","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth International Symposium on Software Testing and Analysis","Chicago, IL, USA","2009","9781605583389","","https://doi.org/10.1145/1572272.1572283;http://dx.doi.org/10.1145/1572272.1572283","10.1145/1572272.1572283","Similar code may exist in large software projects due to some common software engineering practices, such as copying and pasting code and n-version programming. Although previous work has studied syntactic equivalence and small-scale, coarse-grained program-level and function-level semantic equivalence, it is not known whether significant fine-grained, code-level semantic duplications exist. Detecting such semantic equivalence is also desirable because it can enable many applications such as code understanding, maintenance, and optimization.In this paper, we introduce the first algorithm to automatically mine functionally equivalent code fragments of arbitrary size - down to an executable statement. Our notion of functional equivalence is based on input and output behavior. Inspired by Schwartz's randomized polynomial identity testing, we develop our core algorithm using automated random testing: (1) candidate code fragments are automatically extracted from the input program; and (2) random inputs are generated to partition the code fragments based on their output values on the generated inputs. We implemented the algorithm and conducted a large-scale empirical evaluation of it on the Linux kernel 2.6.24. Our results show that there exist many functionally equivalent code fragments that are syntactically different (i.e., they are unlikely due to copying and pasting code). The algorithm also scales to million-line programs; it was able to analyze the Linux kernel with several days of parallel processing.","code clones, functional equivalence, random testing","","ISSTA '09"
"Conference Paper","Sæbjørnsen A,Willcock J,Panas T,Quinlan D,Su Z","Detecting Code Clones in Binary Executables","","2009","","","117–128","Association for Computing Machinery","New York, NY, USA","Proceedings of the Eighteenth International Symposium on Software Testing and Analysis","Chicago, IL, USA","2009","9781605583389","","https://doi.org/10.1145/1572272.1572287;http://dx.doi.org/10.1145/1572272.1572287","10.1145/1572272.1572287","Large software projects contain significant code duplication, mainly due to copying and pasting code. Many techniques have been developed to identify duplicated code to enable applications such as refactoring, detecting bugs, and protecting intellectual property. Because source code is often unavailable, especially for third-party software, finding duplicated code in binaries becomes particularly important. However, existing techniques operate primarily on source code, and no effective tool exists for binaries.In this paper, we describe the first practical clone detection algorithm for binary executables. Our algorithm extends an existing tree similarity framework based on clustering of characteristic vectors of labeled trees with novel techniques to normalize assembly instructions and to accurately and compactly model their structural information. We have implemented our technique and evaluated it on Windows XP system binaries totaling over 50 million assembly instructions. Results show that it is both scalable and precise: it analyzed Windows XP system binaries in a few hours and produced few false positives. We believe our technique is a practical, enabling technology for many applications dealing with binary code.","software tools, clone detection, binary analysis","","ISSTA '09"
"Conference Paper","Bolz CF,Cuni A,Fijalkowski M,Rigo A","Tracing the Meta-Level: PyPy's Tracing JIT Compiler","","2009","","","18–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th Workshop on the Implementation, Compilation, Optimization of Object-Oriented Languages and Programming Systems","Genova, Italy","2009","9781605585413","","https://doi.org/10.1145/1565824.1565827;http://dx.doi.org/10.1145/1565824.1565827","10.1145/1565824.1565827","We attempt to apply the technique of Tracing JIT Compilers in the context of the PyPy project, i.e., to programs that are interpreters for some dynamic languages, including Python. Tracing JIT compilers can greatly speed up programs that spend most of their time in loops in which they take similar code paths. However, applying an unmodified tracing JIT to a program that is itself a bytecode interpreter results in very limited or no speedup. In this paper we show how to guide tracing JIT compilers to greatly improve the speed of bytecode interpreters. One crucial point is to unroll the bytecode dispatch loop, based on two kinds of hints provided by the implementer of the bytecode interpreter. We evaluate our technique by applying it to two PyPy interpreters: one is a small example, and the other one is the full Python interpreter.","","","ICOOOLPS '09"
"Conference Paper","Miura M,Sugihara T,Kunifuji S","Anchor Garden: An Interactive Workbenchfor Basic Data Concept Learningin Object Oriented Programming Languages","","2009","","","141–145","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Annual ACM SIGCSE Conference on Innovation and Technology in Computer Science Education","Paris, France","2009","9781605583815","","https://doi.org/10.1145/1562877.1562925;http://dx.doi.org/10.1145/1562877.1562925","10.1145/1562877.1562925","We propose Anchor Garden (AG), an interactive workbench software for learning fundamentals of data structures with the concepts of type, variable, object, and their relations in a strongly typed object-oriented programming language (OOPL) such as Java or C#. Learners can approach the basic data-handling concepts of OOPLs by direct manipulation of graphical models in AG. In addition, the learner can correlate his/her manipulation and notation of source code because AG automatically generates appropriate source-code corresponding to this manipulation. Experimental results showed a tendency of a learning effect with AG and high correlations between concept understanding and programming ability. Thus, AG has the potential to enhance the programming ability of novice programmers.","CS1, data type, object-orientation, reference","","ITiCSE '09"
"Journal Article","Miura M,Sugihara T,Kunifuji S","Anchor Garden: An Interactive Workbenchfor Basic Data Concept Learningin Object Oriented Programming Languages","SIGCSE Bull.","2009","41","3","141–145","Association for Computing Machinery","New York, NY, USA","","","2009-07","","0097-8418","https://doi.org/10.1145/1595496.1562925;http://dx.doi.org/10.1145/1595496.1562925","10.1145/1595496.1562925","We propose Anchor Garden (AG), an interactive workbench software for learning fundamentals of data structures with the concepts of type, variable, object, and their relations in a strongly typed object-oriented programming language (OOPL) such as Java or C#. Learners can approach the basic data-handling concepts of OOPLs by direct manipulation of graphical models in AG. In addition, the learner can correlate his/her manipulation and notation of source code because AG automatically generates appropriate source-code corresponding to this manipulation. Experimental results showed a tendency of a learning effect with AG and high correlations between concept understanding and programming ability. Thus, AG has the potential to enhance the programming ability of novice programmers.","reference, object-orientation, data type, CS1","",""
"Journal Article","Mishra A,Misra AK","Component Assessment and Proactive Model for Support of Dynamic Integration in Self Adaptive System","SIGSOFT Softw. Eng. Notes","2009","34","4","1–9","Association for Computing Machinery","New York, NY, USA","","","2009-07","","0163-5948","https://doi.org/10.1145/1543405.1543418;http://dx.doi.org/10.1145/1543405.1543418","10.1145/1543405.1543418","Component Based Software Engineering (CBSE) is a paradigm in use by most of the software developers. In a multi component system, each component is a probable point of malfunction. Typical work to make such system more vigorous and safe are both brittle and time intense. A model has been designed for self-adaptive system that automates the component integration process at runtime by accessing the equivalent component from diversified set of components that may be needed in future. The proposed general model is for proactive adaptation, which pre-fetch the component from the pre available repository. This model integrates the caching technique to reduce the amount of time that has been spent during search of best-fitted component to replace the required one, when a system fails to respond due to component failure. To pre-fetch the required component we perform the component assessment on the basis of numerical metadata for each component present in the repository. In the computation of numerical metadata we design an algorithm which uses the concept of Abstract Syntax Tree. To ensure the consistency in the system after the modification, we have used a technique which is used in GUI based component architecture model. To asses the component at run-time, we provide prototype in .Net technology using its attribute feature, which support run-time component evolution without its execution.","pre-fetch, component assessment, proactive, numerical metadata, assertion","",""
"Conference Paper","Wilkinson S,Watson I","Thread and Execution-Context Specific Barriers via Dynamic Method Versioning","","2009","","","48–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th Workshop on the Implementation, Compilation, Optimization of Object-Oriented Languages and Programming Systems","Genova, Italy","2009","9781605585413","","https://doi.org/10.1145/1565824.1565831;http://dx.doi.org/10.1145/1565824.1565831","10.1145/1565824.1565831","The insertion of read and write barriers into managed code is a typical runtime compilation task of a Virtual Machine. As part of our current work in applying Thread-Level Speculation (TLS) to Java, we insert a high density of barriers that are conditionally executed based on the identity of the running thread and current execution context. Rather than perform runtime tests, it is more profitable for our TLS system to maintain thread and execution-context specific versions of methods that are compiled with unconditional barriers, and then rely on modified dispatch semantics to ensure conditional execution.In this paper, we extract the method versioning system from our TLS implementation and present it in a general form, which we call Dynamic Method Versioning (DMV). DMV allows thread and execution-context specific versions of Java methods to be dynamically generated and compiled, with inter-version dispatch managed by a runtime policy. We describe our technique via its implementation within the Jikes Research Virtual Machine, and present initial measurements of its runtime overheads.","","","ICOOOLPS '09"
"Conference Paper","Bettini L,Capecchi S,Damiani F","A Mechanism for Flexible Dynamic Trait Replacement","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Workshop on Formal Techniques for Java-like Programs","Genova, Italy","2009","9781605585406","","https://doi.org/10.1145/1557898.1557907;http://dx.doi.org/10.1145/1557898.1557907","10.1145/1557898.1557907","Dynamic trait replacement is a programming language feature for changing the objects' behavior at runtime by replacing some of the objects' methods. In previous work on dynamic trait replacement for JAVA-like languages, the object's methods that may be replaced must correspond exactly to a named trait used in the object's class definition. In this paper we propose the notion of replaceable: a programming language feature that decouples trait replacement operation code and class declaration code, thus making it possible refactoring classes and/or performing unanticipated trait replacement operations without invalidating existing code.","type system, trait, featherweight Java","","FTfJP '09"
"Conference Paper","Lister R,Fidge C,Teague D","Further Evidence of a Relationship between Explaining, Tracing and Writing Skills in Introductory Programming","","2009","","","161–165","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Annual ACM SIGCSE Conference on Innovation and Technology in Computer Science Education","Paris, France","2009","9781605583815","","https://doi.org/10.1145/1562877.1562930;http://dx.doi.org/10.1145/1562877.1562930","10.1145/1562877.1562930","This paper reports on a replication of earlier studies into a possible hierarchy of programming skills. In this study, the students from whom data was collected were at a university that had not provided data for earlier studies. Also, the students were taught the programming language ""Python"", which had not been used in earlier studies. Thus this study serves as a test of whether the findings in the earlier studies were specific to certain institutions, student cohorts, and programming languages. Also, we used a non-parametric approach to the analysis, rather than the linear approach of earlier studies. Our results are consistent with the earlier studies. We found that students who cannot trace code usually cannot explain code, and also that students who tend to perform reasonably well at code writing tasks have also usually acquired the ability to both trace code and explain code.","novice programmers, comprehension, CS1, tracing, taxonomy","","ITiCSE '09"
"Journal Article","Lister R,Fidge C,Teague D","Further Evidence of a Relationship between Explaining, Tracing and Writing Skills in Introductory Programming","SIGCSE Bull.","2009","41","3","161–165","Association for Computing Machinery","New York, NY, USA","","","2009-07","","0097-8418","https://doi.org/10.1145/1595496.1562930;http://dx.doi.org/10.1145/1595496.1562930","10.1145/1595496.1562930","This paper reports on a replication of earlier studies into a possible hierarchy of programming skills. In this study, the students from whom data was collected were at a university that had not provided data for earlier studies. Also, the students were taught the programming language ""Python"", which had not been used in earlier studies. Thus this study serves as a test of whether the findings in the earlier studies were specific to certain institutions, student cohorts, and programming languages. Also, we used a non-parametric approach to the analysis, rather than the linear approach of earlier studies. Our results are consistent with the earlier studies. We found that students who cannot trace code usually cannot explain code, and also that students who tend to perform reasonably well at code writing tasks have also usually acquired the ability to both trace code and explain code.","comprehension, tracing, CS1, novice programmers, taxonomy","",""
"Conference Paper","Muzahid A,Suárez D,Qi S,Torrellas J","SigRace: Signature-Based Data Race Detection","","2009","","","337–348","Association for Computing Machinery","New York, NY, USA","Proceedings of the 36th Annual International Symposium on Computer Architecture","Austin, TX, USA","2009","9781605585260","","https://doi.org/10.1145/1555754.1555797;http://dx.doi.org/10.1145/1555754.1555797","10.1145/1555754.1555797","Detecting data races in parallel programs is important for both software development and production-run diagnosis. Recently, there have been several proposals for hardware-assisted data race detection. Such proposals typically modify the L1 cache and cache coherence protocol messages, and largely lose their capability when lines get displaced or invalidated from the cache. To eliminate these shortcomings, this paper proposes a novel, different approach to hardware-assisted data race detection. The approach, called SigRace, relies on hardware address signatures. As a processor runs, the addresses of the data that it accesses are automatically encoded in signatures. At certain times, the signatures are automatically passed to a hardware module that intersects them with those of other processors. If the intersection is not null, a data race may have occurred.This paper presents the architecture of SigRace, an implementation, and its software interface. With SigRace, caches and coherence protocol messages are unmodified. Moreover, cache lines can be displaced and invalidated with no effect. Our experiments show that SigRace is significantly more effective than a state-of-the-art conventional hardware-assisted race detector. SigRace finds on average 29% more static races and 107% more dynamic races. Moreover, if we inject data races, SigRace finds 150% more static races than the conventional scheme.","data race, signature, happened-before, SigRace, concurrency defect, timestamp","","ISCA '09"
"Journal Article","Muzahid A,Suárez D,Qi S,Torrellas J","SigRace: Signature-Based Data Race Detection","SIGARCH Comput. Archit. News","2009","37","3","337–348","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0163-5964","https://doi.org/10.1145/1555815.1555797;http://dx.doi.org/10.1145/1555815.1555797","10.1145/1555815.1555797","Detecting data races in parallel programs is important for both software development and production-run diagnosis. Recently, there have been several proposals for hardware-assisted data race detection. Such proposals typically modify the L1 cache and cache coherence protocol messages, and largely lose their capability when lines get displaced or invalidated from the cache. To eliminate these shortcomings, this paper proposes a novel, different approach to hardware-assisted data race detection. The approach, called SigRace, relies on hardware address signatures. As a processor runs, the addresses of the data that it accesses are automatically encoded in signatures. At certain times, the signatures are automatically passed to a hardware module that intersects them with those of other processors. If the intersection is not null, a data race may have occurred.This paper presents the architecture of SigRace, an implementation, and its software interface. With SigRace, caches and coherence protocol messages are unmodified. Moreover, cache lines can be displaced and invalidated with no effect. Our experiments show that SigRace is significantly more effective than a state-of-the-art conventional hardware-assisted race detector. SigRace finds on average 29% more static races and 107% more dynamic races. Moreover, if we inject data races, SigRace finds 150% more static races than the conventional scheme.","signature, concurrency defect, SigRace, happened-before, timestamp, data race","",""
"Conference Paper","Yang X,Cooprider N,Regehr J","Eliminating the Call Stack to Save RAM","","2009","","","60–69","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems","Dublin, Ireland","2009","9781605583563","","https://doi.org/10.1145/1542452.1542461;http://dx.doi.org/10.1145/1542452.1542461","10.1145/1542452.1542461","Most programming languages support a call stack in the programming model and also in the runtime system.We show that for applications targeting low-power embedded microcontrollers (MCUs), RAM usage can be significantly decreased by partially or completely eliminating the runtime callstack. We present flattening, a transformation that absorbs a function into its caller, replacing function invocations and returns with jumps. Unlike inlining, flattening does not duplicate the bodies of functions that have multiple callsites. Applied aggressively, flattening results in stack elimination. Flattening is most useful in conjunction with a lifting transformation that moves global variables into a local scope.Flattening and lifting can save RAM. However, even more benefit can be obtained by adapting the compiler to cope with properties of flattened code. First, we show that flattening adds false paths that confuse a standard live variables analysis. The resulting problems can be mitigated by breaking spurious live-range conflicts between variables using information from the unflattened callgraph. Second, we show that the impact of high register pressure due to flattened and lifted code, and consequent spills out of the register allocator, can be mitigated by improving a compiler's stack layout optimizations. We have implemented both of these improvements in GCC, and have implemented flattening and lifting as source-to-source transformations. On a collection of applications for the AVR family of 8-bit MCUs, we show that total RAM usage can be reduced by 20% by compiling flattened and lifted programs with our improved GCC.","sensor networks, embedded software, memory allocation, compiler optimization, stack liveness, memory optimizations","","LCTES '09"
"Journal Article","Yang X,Cooprider N,Regehr J","Eliminating the Call Stack to Save RAM","SIGPLAN Not.","2009","44","7","60–69","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0362-1340","https://doi.org/10.1145/1543136.1542461;http://dx.doi.org/10.1145/1543136.1542461","10.1145/1543136.1542461","Most programming languages support a call stack in the programming model and also in the runtime system.We show that for applications targeting low-power embedded microcontrollers (MCUs), RAM usage can be significantly decreased by partially or completely eliminating the runtime callstack. We present flattening, a transformation that absorbs a function into its caller, replacing function invocations and returns with jumps. Unlike inlining, flattening does not duplicate the bodies of functions that have multiple callsites. Applied aggressively, flattening results in stack elimination. Flattening is most useful in conjunction with a lifting transformation that moves global variables into a local scope.Flattening and lifting can save RAM. However, even more benefit can be obtained by adapting the compiler to cope with properties of flattened code. First, we show that flattening adds false paths that confuse a standard live variables analysis. The resulting problems can be mitigated by breaking spurious live-range conflicts between variables using information from the unflattened callgraph. Second, we show that the impact of high register pressure due to flattened and lifted code, and consequent spills out of the register allocator, can be mitigated by improving a compiler's stack layout optimizations. We have implemented both of these improvements in GCC, and have implemented flattening and lifting as source-to-source transformations. On a collection of applications for the AVR family of 8-bit MCUs, we show that total RAM usage can be reduced by 20% by compiling flattened and lifted programs with our improved GCC.","embedded software, memory allocation, memory optimizations, sensor networks, stack liveness, compiler optimization","",""
"Conference Paper","Rafkind J,Wick A,Regehr J,Flatt M","Precise Garbage Collection for C","","2009","","","39–48","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 International Symposium on Memory Management","Dublin, Ireland","2009","9781605583471","","https://doi.org/10.1145/1542431.1542438;http://dx.doi.org/10.1145/1542431.1542438","10.1145/1542431.1542438","Magpie is a source-to-source transformation for C programs that enables precise garbage collection, where precise means that integers are not confused with pointers, and the liveness of a pointer is apparent at the source level. Precise GC is primarily useful for long-running programs and programs that interact with untrusted components. In particular, we have successfully deployed precise GC in the C implementation of a language run-time system that was originally designed to use conservative GC. We also report on our experience in transforming parts of the Linux kernel to use precise GC instead of manual memory management.","c programming language, conservative, garbage collection, precise, accurate","","ISMM '09"
"Conference Paper","Milev A,Mutkov V","Research over Correlation Properties of Families Optical Orthogonal Codes","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing","Ruse, Bulgaria","2009","9781605589862","","https://doi.org/10.1145/1731740.1731813;http://dx.doi.org/10.1145/1731740.1731813","10.1145/1731740.1731813","This papers deals with optical orthogonal codes (OOC) and their auto -- correlation function (ACF) and cross-correlation function (CCF). The studying of optical orthogonal codes is motivated by their application in optical code-division-multiple-access (OCDMA) system. The main object of the research is periodical OOC and their families. Correlation properties of similar periodical OOC have been investigated. Special attention was given to OOC which can be constructed by using nontrivial transformation of initial known OOC. Many characteristics are given and evaluated. It is shown that the conducted research could be used to find different families OOC and their combination. In addition to this every one of them can be chosen to be implemented in OCDMA system.","optical orthogonal codes, optical CDMA system","","CompSysTech '09"
"Conference Paper","Marino D,Musuvathi M,Narayanasamy S","LiteRace: Effective Sampling for Lightweight Data-Race Detection","","2009","","","134–143","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation","Dublin, Ireland","2009","9781605583921","","https://doi.org/10.1145/1542476.1542491;http://dx.doi.org/10.1145/1542476.1542491","10.1145/1542476.1542491","Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude.In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70% of data races by sampling less than 2% of memory accesses in a given program execution.","sampling, dynamic data race detection, concurrency bugs","","PLDI '09"
"Journal Article","Marino D,Musuvathi M,Narayanasamy S","LiteRace: Effective Sampling for Lightweight Data-Race Detection","SIGPLAN Not.","2009","44","6","134–143","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0362-1340","https://doi.org/10.1145/1543135.1542491;http://dx.doi.org/10.1145/1543135.1542491","10.1145/1543135.1542491","Data races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude.In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70% of data races by sampling less than 2% of memory accesses in a given program execution.","sampling, dynamic data race detection, concurrency bugs","",""
"Conference Paper","Kawaguchi M,Rondon P,Jhala R","Type-Based Data Structure Verification","","2009","","","304–315","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation","Dublin, Ireland","2009","9781605583921","","https://doi.org/10.1145/1542476.1542510;http://dx.doi.org/10.1145/1542476.1542510","10.1145/1542476.1542510","We present a refinement type-based approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel type-based mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in dsolve, which uses liquid types to verify ocaml programs. We present experiments that show that our type-based approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binary-search-ordering, and acyclicity by more than an order of magnitude.","hindley-milner, type inference, dependent types, predicate abstraction","","PLDI '09"
"Journal Article","Kawaguchi M,Rondon P,Jhala R","Type-Based Data Structure Verification","SIGPLAN Not.","2009","44","6","304–315","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0362-1340","https://doi.org/10.1145/1543135.1542510;http://dx.doi.org/10.1145/1543135.1542510","10.1145/1543135.1542510","We present a refinement type-based approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel type-based mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in dsolve, which uses liquid types to verify ocaml programs. We present experiments that show that our type-based approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binary-search-ordering, and acyclicity by more than an order of magnitude.","dependent types, type inference, hindley-milner, predicate abstraction","",""
"Conference Paper","Xu G,Arnold M,Mitchell N,Rountev A,Sevitsky G","Go with the Flow: Profiling Copies to Find Runtime Bloat","","2009","","","419–430","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation","Dublin, Ireland","2009","9781605583921","","https://doi.org/10.1145/1542476.1542523;http://dx.doi.org/10.1145/1542476.1542523","10.1145/1542476.1542523","Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods, and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent, performance experts analyze deployed applications and regularly find gains of 2x or more.Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Bloat results from a pile-up of seemingly harmless decisions. Each adds temporary objects and method calls, and often copies values between those temporary objects. While data copies are not the entirety of bloat, we have observed that they are excellent indicators of regions of excessive activity. By optimizing copies, one is likely to remove the objects that carry copied values, and the method calls that allocate and populate them.We introduce copy profiling, a technique that summarizes runtime activity in terms of chains of data copies. A flat copy profile counts copies by method. We show how flat profiles alone can be helpful. In many cases, diagnosing a problem requires data flow context. Tracking and making sense of raw copy chains does not scale, so we introduce a summarizing abstraction called the copy graph. We implement three clients analyses that, using the copy graph, expose common patterns of bloat, such as finding hot copy chains and discovering temporary data structures. We demonstrate, with examples from a large-scale commercial application and several benchmarks, that copy profiling can be used by a programmer to quickly find opportunities for large performance gains.","copy graph, memory bloat, heap analysis, profiling","","PLDI '09"
"Journal Article","Xu G,Arnold M,Mitchell N,Rountev A,Sevitsky G","Go with the Flow: Profiling Copies to Find Runtime Bloat","SIGPLAN Not.","2009","44","6","419–430","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0362-1340","https://doi.org/10.1145/1543135.1542523;http://dx.doi.org/10.1145/1543135.1542523","10.1145/1543135.1542523","Many large-scale Java applications suffer from runtime bloat. They execute large volumes of methods, and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent, performance experts analyze deployed applications and regularly find gains of 2x or more.Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Bloat results from a pile-up of seemingly harmless decisions. Each adds temporary objects and method calls, and often copies values between those temporary objects. While data copies are not the entirety of bloat, we have observed that they are excellent indicators of regions of excessive activity. By optimizing copies, one is likely to remove the objects that carry copied values, and the method calls that allocate and populate them.We introduce copy profiling, a technique that summarizes runtime activity in terms of chains of data copies. A flat copy profile counts copies by method. We show how flat profiles alone can be helpful. In many cases, diagnosing a problem requires data flow context. Tracking and making sense of raw copy chains does not scale, so we introduce a summarizing abstraction called the copy graph. We implement three clients analyses that, using the copy graph, expose common patterns of bloat, such as finding hot copy chains and discovering temporary data structures. We demonstrate, with examples from a large-scale commercial application and several benchmarks, that copy profiling can be used by a programmer to quickly find opportunities for large performance gains.","copy graph, heap analysis, memory bloat, profiling","",""
"Conference Paper","Oiwa Y","Implementation of the Memory-Safe Full ANSI-C Compiler","","2009","","","259–269","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation","Dublin, Ireland","2009","9781605583921","","https://doi.org/10.1145/1542476.1542505;http://dx.doi.org/10.1145/1542476.1542505","10.1145/1542476.1542505","This paper describes a completely memory-safe compiler for C language programs that is fully compatible with the ANSI C specification.Programs written in C often suffer from nasty errors due to dangling pointers and buffer overflow. Such errors in Internet server programs are often exploited by malicious attackers to crack an entire system. The origin of these errors is usually corruption of in-memory data structures caused by out-of-bound array accesses. Usual C compilers do not provide any protection against such out-of-bound access, although many other languages such as Java and ML do provide such protection. There have been several proposals for preventing such memory corruption from various aspects: runtime buffer overrun detectors, designs for new C-like languages, and compilers for (subsets of) the C language. However, as far as we know, none of them have achieved full memory protection and full compatibility with the C language specification at the same time.We propose the most powerful solution to this problem ever presented. We have developed Fail-Safe C, a memory-safe implementation of the full ANSI C language. It detects and disallows all unsafe operations, yet conforms to the full ANSI C standard (including casts and unions). This paper introduces several techniques--both compile-time and runtime--to reduce the overhead of runtime checks, while still maintaining 100% memory safety. This compiler lets programmers easily make their programs safe without heavy rewriting or porting of their code. It also supports many of the ""dirty tricks"" commonly used in many existing C programs, which do not strictly conform to the standard specification. In this paper, we demonstrate several real-world server programs that can be processed by our compiler and present technical details and benchmark results for it.","memory safety, c language","","PLDI '09"
"Journal Article","Oiwa Y","Implementation of the Memory-Safe Full ANSI-C Compiler","SIGPLAN Not.","2009","44","6","259–269","Association for Computing Machinery","New York, NY, USA","","","2009-06","","0362-1340","https://doi.org/10.1145/1543135.1542505;http://dx.doi.org/10.1145/1543135.1542505","10.1145/1543135.1542505","This paper describes a completely memory-safe compiler for C language programs that is fully compatible with the ANSI C specification.Programs written in C often suffer from nasty errors due to dangling pointers and buffer overflow. Such errors in Internet server programs are often exploited by malicious attackers to crack an entire system. The origin of these errors is usually corruption of in-memory data structures caused by out-of-bound array accesses. Usual C compilers do not provide any protection against such out-of-bound access, although many other languages such as Java and ML do provide such protection. There have been several proposals for preventing such memory corruption from various aspects: runtime buffer overrun detectors, designs for new C-like languages, and compilers for (subsets of) the C language. However, as far as we know, none of them have achieved full memory protection and full compatibility with the C language specification at the same time.We propose the most powerful solution to this problem ever presented. We have developed Fail-Safe C, a memory-safe implementation of the full ANSI C language. It detects and disallows all unsafe operations, yet conforms to the full ANSI C standard (including casts and unions). This paper introduces several techniques--both compile-time and runtime--to reduce the overhead of runtime checks, while still maintaining 100% memory safety. This compiler lets programmers easily make their programs safe without heavy rewriting or porting of their code. It also supports many of the ""dirty tricks"" commonly used in many existing C programs, which do not strictly conform to the standard specification. In this paper, we demonstrate several real-world server programs that can be processed by our compiler and present technical details and benchmark results for it.","c language, memory safety","",""
"Conference Paper","Chellappa S,Franchetti F,Püeschel M","Computer Generation of Fast Fourier Transforms for the Cell Broadband Engine","","2009","","","26–35","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd International Conference on Supercomputing","Yorktown Heights, NY, USA","2009","9781605584980","","https://doi.org/10.1145/1542275.1542285;http://dx.doi.org/10.1145/1542275.1542285","10.1145/1542275.1542285","The Cell BE is a multicore processor with eight vector accelerators (called SPEs) that implement explicit cache management through direct memory access engines. While the Cell has an impressive floating point peak performance, programming and optimizing for it is difficult as it requires explicit memory management, multithreading, streaming, and vectorization. We address this problem for the discrete Fourier transform (DFT) by extending Spiral, a program generation system, to automatically generate highly optimized implementations for the Cell. The extensions include multi-SPE parallelization and explicit memory streaming, both performed at a high abstraction level using rewriting systems operating on Spiral's internal domain-specific language. Further, we support latency and throughput optimizations, single and double precision, and different data formats. The performance of Spiral's computer generated code is comparable with and sometimes better than existing DFT implementations, where available.","cell be, parallelization, program generation, dft, multicore, automatic performance tuning, performance library, multibuffering, streaming, fast fourier transform","","ICS '09"
"Conference Paper","Wagner S","A Bayesian Network Approach to Assess and Predict Software Quality Using Activity-Based Quality Models","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Predictor Models in Software Engineering","Vancouver, British Columbia, Canada","2009","9781605586342","","https://doi.org/10.1145/1540438.1540447;http://dx.doi.org/10.1145/1540438.1540447","10.1145/1540438.1540447","Assessing and predicting the complex concept of software quality is still challenging in practice as well as research. Activity-based quality models break down this complex concept into more concrete definitions, more precisely facts about the system, process and environment and their impact on activities performed on and with the system. However, these models lack an operationalisation that allows to use them in assessment and prediction of quality. Bayesian Networks (BN) have been shown to be a viable means for assessment and prediction incorporating variables with uncertainty. This paper describes how activity-based quality models can be used to derive BN models for quality assessment and prediction. The proposed approach is demonstrated in a proof of concept using publicly available data.","activity-based quality models, Bayesian networks, quality prediction, quality assessment","","PROMISE '09"
"Conference Paper","Ben Asher Y,Rotem N","The Effect of Unrolling and Inlining for Python Bytecode Optimizations","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference","Haifa, Israel","2009","9781605586236","","https://doi.org/10.1145/1534530.1534550;http://dx.doi.org/10.1145/1534530.1534550","10.1145/1534530.1534550","In this study, we consider bytecode optimizations for Python, a programming language which combines object-oriented concepts with features of scripting languages, such as dynamic dictionaries. Due to its design nature, Python is relatively slow compared to other languages. It operates through compiling the code into powerful bytecode instructions that are executed by an interpreter. Python's speed is limited due to its interpreter design, and thus there is a significant need to optimize the language. In this paper, we discuss one possible approach and limitations in optimizing Python based on bytecode transformations. In the first stage of the proposed optimizer, the bytecode is expanded using function inline and loop unrolling. The second stage of transformations simplifies the bytecode by applying a complete set of data-flow optimizations, including constant propagation, algebraic simplifications, dead code elimination, copy propagation, common sub expressions elimination, loop invariant code motion and strength reduction. While these optimizations are known and their implementation mechanism (data flow analysis) is well developed, they have not been successfully implemented in Python due to its dynamic features which prevent their use. In this work we attempt to understand the dynamic features of Python and how these features affect and limit the implementation of these optimizations. In particular, we consider the significant effects of first unrolling and then inlining on the ability to apply the remaining optimizations. The results of our experiments indicate that these optimizations can indeed be implemented and dramatically improve execution times.","dynamic languages, Python, optimizations, bytecode","","SYSTOR '09"
"Journal Article","Strong R,Mudigonda J,Mogul JC,Binkert N,Tullsen D","Fast Switching of Threads between Cores","SIGOPS Oper. Syst. Rev.","2009","43","2","35–45","Association for Computing Machinery","New York, NY, USA","","","2009-04","","0163-5980","https://doi.org/10.1145/1531793.1531801;http://dx.doi.org/10.1145/1531793.1531801","10.1145/1531793.1531801","We address the software costs of switching threads between cores in a multicore processor. Fast core switching enables a variety of potential improvements, such as thread migration for thermal management, fine-grained load balancing, and exploiting asymmetric multicores, where performance asymmetry creates opportunities for more efficient resource utilization. Successful exploitation of these opportunities demands low core-switching costs. We describe our implementation of core switching in the Linux kernel, as well as software changes that can decrease switching costs. We use detailed simulations to evaluate several alternative implementations. We also explore how some simple architectural variations can reduce switching costs. We evaluate system efficiency using both real (but symmetric) hardware, and simulated asymmetric hardware, using both microbenchmarks and realistic applications.","","",""
"Conference Paper","Leupers R,Ha S,Vajda A,Dömer R,Bekooij M,Nohl A","Programming MPSoC Platforms: Road Works Ahead!","","2009","","","1584–1589","European Design and Automation Association","Leuven, BEL","Proceedings of the Conference on Design, Automation and Test in Europe","Nice, France","2009","9783981080155","","","","This paper summarizes a special session on multi-core/multi-processor system-on-chip (MPSoC) programming challenges. The current trend towards MPSoC platforms in most computing domains does not only mean a radical change in computer architecture. Even more important from a SW developer's viewpoint, at the same time the classical sequential von Neumann programming model needs to be overcome. Efficient utilization of the MPSoC HW resources demands for radically new models and corresponding SW development tools, capable of exploiting the available parallelism and guaranteeing bug-free parallel SW. While several standards are established in the high-performance computing domain (e.g. OpenMP), it is clear that more innovations are required for successful deployment of heterogeneous embedded MPSoC. On the other hand, at least for coming years, the freedom for disruptive programming technologies is limited by the huge amount of certified sequential code that demands for a more pragmatic, gradual tool and code replacement strategy.","","","DATE '09"
"Conference Paper","Quinlan D,Panas T","Source Code and Binary Analysis of Software Defects","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th Annual Workshop on Cyber Security and Information Intelligence Research: Cyber Security and Information Intelligence Challenges and Strategies","Oak Ridge, Tennessee, USA","2009","9781605585185","","https://doi.org/10.1145/1558607.1558653;http://dx.doi.org/10.1145/1558607.1558653","10.1145/1558607.1558653","This extended abstract presents the techniques to identify a selected set of software defects (bugs, bad practices, etc.) within both source code and binary executables. We present the results from six different static analysis tests applied on both the source code and the binary executables (with and without optimization) on three different applications. We compare the precision of the static analysis results from the source code and the binary executable forms of the same software. Ideally the results from an analysis of source code and its binary would be identical, but in practice the source code and binary representation cause slightly different techniques to be used with different amounts and types of information readily available.Our work defines a few defect analyses to support what might later be a larger collection of analyses. Our goal is to more thoroughly evaluate software quality and eliminate, as much as possible, the classic asymmetry of information about software, specifically quality as understood by the software developer vs. the software user. It is not well studied how static analysis of source code and binaries are related for purposes of evaluating general quality and our work is focused in this direction; much less are the tools for such work openly available. Our work also presents an open framework well suited for identifying general software properties of both source code and binary executables.","","","CSIIRW '09"
"Conference Paper","Brandt J,Guo PJ,Lewenstein J,Dontcheva M,Klemmer SR","Two Studies of Opportunistic Programming: Interleaving Web Foraging, Learning, and Writing Code","","2009","","","1589–1598","Association for Computing Machinery","New York, NY, USA","Proceedings of the SIGCHI Conference on Human Factors in Computing Systems","Boston, MA, USA","2009","9781605582467","","https://doi.org/10.1145/1518701.1518944;http://dx.doi.org/10.1145/1518701.1518944","10.1145/1518701.1518944","This paper investigates the role of online resources in problem solving. We look specifically at how programmers - an exemplar form of knowledge workers - opportunistically interleave Web foraging, learning, and writing code. We describe two studies of how programmers use online resources. The first, conducted in the lab, observed participants' Web use while building an online chat room. We found that programmers leverage online resources with a range of intentions: They engage in just-in-time learning of new skills and approaches, clarify and extend their existing knowledge, and remind themselves of details deemed not worth remembering. The results also suggest that queries for different purposes have different styles and durations. Do programmers' queries ""in the wild"" have the same range of intentions, or is this result an artifact of the particular lab setting? We analyzed a month of queries to an online programming portal, examining the lexical structure, refinements made, and result pages visited. Here we also saw traits that suggest the Web is being used for learning and reminding. These results contribute to a theory of online resource usage in programming, and suggest opportunities for tools to facilitate online knowledge work.","opportunistic programming, copy-and-paste, prototyping","","CHI '09"
"Conference Paper","Lagar-Cavilla HA,Whitney JA,Scannell AM,Patchin P,Rumble SM,de Lara E,Brudno M,Satyanarayanan M","SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing","","2009","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM European Conference on Computer Systems","Nuremberg, Germany","2009","9781605584829","","https://doi.org/10.1145/1519065.1519067;http://dx.doi.org/10.1145/1519065.1519067","10.1145/1519065.1519067","Virtual Machine (VM) fork is a new cloud computing abstraction that instantaneously clones a VM into multiple replicas running on different hosts. All replicas share the same initial state, matching the intuitive semantics of stateful worker creation. VM fork thus enables the straightforward creation and efficient deployment of many tasks demanding swift instantiation of stateful workers in a cloud environment, e.g. excess load handling, opportunistic job placement, or parallel computing. Lack of instantaneous stateful cloning forces users of cloud computing into ad hoc practices to manage application state and cycle provisioning. We present SnowFlock, our implementation of the VM fork abstraction. To evaluate SnowFlock, we focus on the demanding scenario of services requiring on-the-fly creation of hundreds of parallel workers in order to solve computationally-intensive queries in seconds. These services are prominent in fields such as bioinformatics, finance, and rendering. SnowFlock provides sub-second VM cloning, scales to hundreds of workers, consumes few cloud I/O resources, and has negligible runtime overhead.","virtualization, cloud computing","","EuroSys '09"
"Journal Article","Shobaki G,Wilken K,Heffernan M","Optimal Trace Scheduling Using Enumeration","ACM Trans. Archit. Code Optim.","2009","5","4","","Association for Computing Machinery","New York, NY, USA","","","2009-03","","1544-3566","https://doi.org/10.1145/1498690.1498694;http://dx.doi.org/10.1145/1498690.1498694","10.1145/1498690.1498694","This article presents the first optimal algorithm for trace scheduling. The trace is a global scheduling region used by compilers to exploit instruction-level parallelism across basic block boundaries. Several heuristic techniques have been proposed for trace scheduling, but the precision of these techniques has not been studied relative to optimality. This article describes a technique for finding provably optimal trace schedules, where optimality is defined in terms of a weighted sum of schedule lengths across all code paths in a trace. The optimal algorithm uses branch-and-bound enumeration to efficiently explore the entire solution space. Experimental evaluation of the algorithm shows that, with a time limit of 1 s per problem, 91% of the hard trace scheduling problems in the SPEC CPU 2006 Integer Benchmarks are solved optimally. For 58% of these hard problems, the optimal schedule is improved compared to that produced by a heuristic scheduler with a geometric mean improvement of 3.2% in weighted schedule length and 18% in compensation code size.","global instruction scheduling, compiler optimizations, branch-and-bound enumeration, optimal instruction scheduling, trace scheduling, instruction-level parallelism, Instruction scheduling","",""
"Conference Paper","Thorsen A,Merkey P,Manne F","Maximum Weighted Matching Using the Partitioned Global Address Space Model","","2009","","","","Society for Computer Simulation International","San Diego, CA, USA","Proceedings of the 2009 Spring Simulation Multiconference","San Diego, California","2009","","","","","Efficient parallel algorithms for problems such as maximum weighted matching are central to many areas of combinatorial scientific computing. Manne and Bisseling [13] presented a parallel approximation algorithm which is well suited to distributed memory computers. This algorithm is based on a distributed protocol due to Hoepman [9]. In the current paper, a partitioned global address space (PGAS) implementation is presented.PGAS programmers have the conveniences of using a shared memory model, which provides implicit communication between processes using normal loads and stores. Since the shared memory is partitioned according to the affinity of a process, one is also able to exploit data locality.This paper addresses the main differences between the PGAS and MPI implementations of the Manne-Bisseling algorithm. It highlights some advantages of using the PGAS model such as shorter, simpler code, similarity to the sequential algorithm, and options for fine-grained and coarse-grained communication.","UPC, partitioned global address space, unified parallel C, PGAS, maximum weighted matching","","SpringSim '09"
"Conference Paper","Chang M,Smith E,Reitmaier R,Bebenita M,Gal A,Wimmer C,Eich B,Franz M","Tracing for Web 3.0: Trace Compilation for the next Generation Web Applications","","2009","","","71–80","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments","Washington, DC, USA","2009","9781605583754","","https://doi.org/10.1145/1508293.1508304;http://dx.doi.org/10.1145/1508293.1508304","10.1145/1508293.1508304","Today's web applications are pushing the limits of modern web browsers. The emergence of the browser as the platform of choice for rich client-side applications has shifted the use of in-browser JavaScript from small scripting programs to large computationally intensive application logic. For many web applications, JavaScript performance has become one of the bottlenecks preventing the development of even more interactive client side applications. While traditional just-in-time compilation is successful for statically typed virtual machine based languages like Java, compiling JavaScript turns out to be a challenging task. Many JavaScript programs and scripts are short-lived, and users expect a responsive browser during page loading. This leaves little time for compilation of JavaScript to generate machine code.We present a trace-based just-in-time compiler for JavaScript that uses run-time profiling to identify frequently executed code paths, which are compiled to executable machine code. Our approach increases execution performance by up to 116% by decomposing complex JavaScript instructions into a simple Forth-based representation, and then recording the actually executed code path through this low-level IR. Giving developers more computational horsepower enables a new generation of innovative web applications.","javascript, forth, dynamically typed languages, tracing, type specialization, trace trees, dynamic compilation, tamarin","","VEE '09"
"Conference Paper","Ceccato M,Tonella P,Preda MD,Majumdar A","Remote Software Protection by Orthogonal Client Replacement","","2009","","","448–455","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM Symposium on Applied Computing","Honolulu, Hawaii","2009","9781605581668","","https://doi.org/10.1145/1529282.1529380;http://dx.doi.org/10.1145/1529282.1529380","10.1145/1529282.1529380","In a typical client-server scenario, a trusted server provides valuable services to a client, which runs remotely on an untrusted platform. Of the many security vulnerabilities that may arise (such as authentication and authorization), guaranteeing the integrity of the client code is one of the most difficult to address. This security vulnerability is an instance of the malicious host problem, where an adversary in control of the client's host environment tries to tamper with the client code.We propose a novel client replacement strategy to counter the malicious host problem. The client code is periodically replaced by new orthogonal clients, such that their combination with the server is functionally-equivalent to the original client-server application. The reverse engineering efforts of the adversary are deterred by the complexity of analysis of frequently changing, orthogonal program code. We use the underlying concepts of program obfuscation as a basis for formally defining and providing orthogonality. We also give preliminary empirical validation of the proposed approach.","software security, program transformation, obfuscation, clone detection, remote trusting","","SAC '09"
"Conference Paper","Lazzarini Lemos OA,Bajracharya S,Ossher J,Masiero PC,Lopes C","Applying Test-Driven Code Search to the Reuse of Auxiliary Functionality","","2009","","","476–482","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM Symposium on Applied Computing","Honolulu, Hawaii","2009","9781605581668","","https://doi.org/10.1145/1529282.1529384;http://dx.doi.org/10.1145/1529282.1529384","10.1145/1529282.1529384","Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g. compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and extracted into the workspace. In this paper we introduce the use of test cases as an interface for automating code search and reuse and evaluate its applicability and performance in the reuse of auxiliary functionality. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We present CodeGenie, an Eclipse plugin that performs TDCS using a code search engine called Sourcerer. Our evaluation presents evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality.","software reuse, source code search, test-first, TDD","","SAC '09"
"Conference Paper","Huang S,Chen Y,Zhu J,Li ZJ,Tan HF","An Optimized Change-Driven Regression Testing Selection Strategy for Binary Java Applications","","2009","","","558–565","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM Symposium on Applied Computing","Honolulu, Hawaii","2009","9781605581668","","https://doi.org/10.1145/1529282.1529403;http://dx.doi.org/10.1145/1529282.1529403","10.1145/1529282.1529403","Selective regression testing involves re-testing of software systems with a subset of the whole test suite to verify that modifications have not caused adverse impacts to existing functions complied with the requirements specifications. With the growing of globalization and individual testing services providers, many testing and development teams belong to different organizations, and often the testing teams only get a binary release of the application without access to its source code. This makes source code analysis based regression test selection strategy not applicable. Meanwhile source code analysis based approach has scalability problem for large applications, which hinders its wide application in industry. This paper presents an optimized regression testing selection strategy based on binary java file change analysis, through which the problems around binary java applications are avoided. Besides static regression test suite reduction and test prioritization factors, continuous and real time testing execution information are incorporated as fault-proneness indicator of the selected test cases to dynamically select and prioritize the regression test suites. In addition, the whole strategy is lightweight, making the regression test selection process more automated and effective. Experiments show that this strategy can guarantee the change point coverage, reveal faults quickly, and scale to industry-size regression testing scenarios under resource and time constraints.","test case prioritization, regression testing selection, binary Java application","","SAC '09"
"Conference Paper","Micikevicius P","3D Finite Difference Computation on GPUs Using CUDA","","2009","","","79–84","Association for Computing Machinery","New York, NY, USA","Proceedings of 2nd Workshop on General Purpose Processing on Graphics Processing Units","Washington, D.C., USA","2009","9781605585178","","https://doi.org/10.1145/1513895.1513905;http://dx.doi.org/10.1145/1513895.1513905","10.1145/1513895.1513905","In this paper we describe a GPU parallelization of the 3D finite difference computation using CUDA. Data access redundancy is used as the metric to determine the optimal implementation for both the stencil-only computation, as well as the discretization of the wave equation, which is currently of great interest in seismic computing. For the larger stencils, the described approach achieves the throughput of between 2,400 to over 3,000 million of output points per second on a single Tesla 10-series GPU. This is roughly an order of magnitude higher than a 4-core Harpertown CPU running a similar code from seismic industry. Multi-GPU parallelization is also described, achieving linear scaling with GPUs by overlapping inter-GPU communication with computation.","GPU, parallel algorithms, CUDA, finite difference","","GPGPU-2"
"Conference Paper","Vidal S,Abait ES,Marcos C,Casas S,Díaz Pace JA","Aspect Mining Meets Rule-Based Refactoring","","2009","","","23–27","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st Workshop on Linking Aspect Technology and Evolution","Charlottesville, Virginia, USA","2009","9781605584539","","https://doi.org/10.1145/1509847.1509852;http://dx.doi.org/10.1145/1509847.1509852","10.1145/1509847.1509852","Aspect-oriented software development allows the encapsulation of crosscutting concerns, achieving a better system modularization and, therefore, improving its maintenance. One important challenge is how to evolve an object-oriented system into an aspect-oriented one in such a way the system structure gets gradually improved. This paper describes a process to assist developers in the refactoring of object-oriented systems to aspects. To do so, we propose a tool approach that combines aspect mining techniques with a rule-base engine to apply refactorings.","software maintenance, aspect refactoring, aspect mining","","PLATE '09"
"Conference Paper","Lawall JL,Muller G,Palix N","Enforcing the Use of API Functions in Linux Code","","2009","","","7–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th Workshop on Aspects, Components, and Patterns for Infrastructure Software","Charlottesville, Virginia, USA","2009","9781605584508","","https://doi.org/10.1145/1509276.1509279;http://dx.doi.org/10.1145/1509276.1509279","10.1145/1509276.1509279","In the Linux kernel source tree, header files typically define many small functions that have a simple behavior but are critical to ensure readability, correctness, and maintainability. We have observed, however, that some Linux code does not use these functions systematically. In this paper, we propose an approach combining rule-based program matching and transformation with generative programming to generate rules for finding and fixing code fragments that should use the functions defined in header files. We illustrate our approach using an in-depth study based on four typical functions defined in the header file include/linux/usb.h.","bug-finding, linux, apis, bug-fixing","","ACP4IS '09"
"Conference Paper","Bodden E,Chen F,Rosu G","Dependent Advice: A General Approach to Optimizing History-Based Aspects","","2009","","","3–14","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development","Charlottesville, Virginia, USA","2009","9781605584423","","https://doi.org/10.1145/1509239.1509243;http://dx.doi.org/10.1145/1509239.1509243","10.1145/1509239.1509243","Many aspects for runtime monitoring are history-based: they contain pieces of advice that execute conditionally, based on the observed execution history. History-based aspects are notorious for causing high runtime overhead. Compilers can apply powerful optimizations to history-based aspects using domain knowledge. Unfortunately, current aspect languages like AspectJ impede optimizations, as they provide no means to express this domain knowledge.In this paper we present dependent advice, a novel AspectJ language extension. A dependent advice contains dependency annotations that preserve crucial domain knowledge: a dependent advice needs to execute only when its dependencies are fulfilled. Optimizations can exploit this knowledge: we present a whole-program analysis that removes advice-dispatch code from program locations at which an advice's dependencies cannot be fulfilled.Programmers often opt to have history-based aspects generated automatically, from formal specifications from model-driven development or runtime monitoring. As we show using code-generation tools for two runtime-monitoring approaches, tracematches and JavaMOP, such tools can use knowledge contained in the specification to automatically generate dependency annotations as well.Our extensive evaluation using the DaCapo benchmark suite shows that the use of dependent advice can significantly lower, sometimes even completely eliminate, the runtime overhead caused by history-based aspects, independently of the specification formalism.","runtime verification, domain-specific languages, compilation, static program analysis","","AOSD '09"
"Conference Paper","Flinn J,Giuli TJ,Higgins B,Noble B,Reda A,Watson D","The Case for Intentional Networking","","2009","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Workshop on Mobile Computing Systems and Applications","Santa Cruz, California","2009","9781605582832","","https://doi.org/10.1145/1514411.1514418;http://dx.doi.org/10.1145/1514411.1514418","10.1145/1514411.1514418","Wireless infrastructures are increasingly diverse, complex, and difficult to manage. Those who restrict themselves to homogeneous, managed campus or corporate networks are a vanishing breed. In the wild, users are confronted with many overlapping infrastructures with a broad variety of strengths and weaknesses. Such diversity of infrastructure is both a challenge and an opportunity. The challenge lies in presenting the alternatives to applications and users in a way that provides the best possible utility to both. However, by managing these many alternatives, we can provide significant benefits, exploiting multiple networks concurrently and planning future transmissions intelligently.","","","HotMobile '09"
"Journal Article","Chong S,Liu J,Myers AC,Qi X,Vikram K,Zheng L,Zheng X","Building Secure Web Applications with Automatic Partitioning","Commun. ACM","2009","52","2","79–87","Association for Computing Machinery","New York, NY, USA","","","2009-02","","0001-0782","https://doi.org/10.1145/1461928.1461949;http://dx.doi.org/10.1145/1461928.1461949","10.1145/1461928.1461949","Swift is a new, principled approach to building Web applications that are secure by construction. Modern Web applications typically implement some functionality as client-side JavaScript code, for improved interactivity. Moving code and data to the client can create security vulnerabilities, but currently there are no good methods for deciding when it is secure to do so.Swift automatically partitions application code while providing assurance that the resulting placement is secure and efficient. Application code is written as Java-like code annotated with information flow policies that specify the confidentiality and integrity of Web application information. The compiler uses these policies to automatically partition the program into JavaScript code running in the client browser and Java code running on the server. To improve interactive performance, code and data are placed on the client. However, security-critical code and data are always placed on the server. The compiler may also automatically replicate code across the client and server, to obtain both security and performance.","","",""
"Conference Paper","Schäfer M,Ekman T,de Moor O","Challenge Proposal: Verification of Refactorings","","2009","","","67–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd Workshop on Programming Languages Meets Program Verification","Savannah, GA, USA","2009","9781605583303","","https://doi.org/10.1145/1481848.1481859;http://dx.doi.org/10.1145/1481848.1481859","10.1145/1481848.1481859","Automated refactoring tools are an essential part of a software developer's toolbox. They are most useful for gradually improving large existing code bases and it is essential that they work reliably, since even a simple refactoring may affect many different parts of a program, and the programmer should not have to inspect every individual change to ensure that the transformation went as expected. Even extensively tested industrial-strength refactoring engines, however, are fraught with many bugs that lead to incorrect, non-behaviour preserving transformations. We argue that software refactoring tools are a prime candidate for mechanical verification, offering significant challenges but also the prospect of tangible benefits for real-world software development.","refactoring, mechanical verification, proof assistants","","PLPV '09"
"Conference Paper","Li H,Thompson S","Clone Detection and Removal for Erlang/OTP within a Refactoring Environment","","2009","","","169–178","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2009 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation","Savannah, GA, USA","2009","9781605583273","","https://doi.org/10.1145/1480945.1480971;http://dx.doi.org/10.1145/1480945.1480971","10.1145/1480945.1480971","A well-known bad code smell in refactoring and software maintenance is duplicated code, or code clones. A code clone is a code fragment that is identical or similar to another. Unjustified code clones increase code size, make maintenance and comprehension more difficult, and also indicate design problems such as lack of encapsulation or abstraction.This paper proposes a token and AST based hybrid approach to automatically detecting code clones in Erlang/OTP programs, underlying a collection of refactorings to support user-controlled automatic clone removal, and examines their application in substantial case studies. Both the clone detector and the refactorings are integrated within Wrangler, the refactoring tool developed at Kent for Erlang/OTP.","program transformation, duplicated code, refactoring, erlang, wrangler, program analysis","","PEPM '09"
"Conference Paper","Tongarlak MH,Ankenman B,Nelson BL,Borne L,Wolfe K","Using Simulation Early in the Design of a Fuel Injector Production Line","","2008","","","471–478","Winter Simulation Conference","Miami, Florida","Proceedings of the 40th Conference on Winter Simulation","","2008","9781424427086","","","","Delphi Corporation decided to use simulation from concept development to installation of a new multimillion dollar fuel injector production line. In this paper we describe how simulation was employed in the concept development phase to assess whether production targets required for financial viability were feasible and to identify the critical features of the line on which to focus design-improvement efforts.","","","WSC '08"
"Conference Paper","Mirgorodskiy AV,Miller BP","Diagnosing Distributed Systems with Self-Propelled Instrumentation","","2008","","","82–103","Springer-Verlag","Berlin, Heidelberg","Proceedings of the 9th ACM/IFIP/USENIX International Conference on Middleware","Leuven, Belgium","2008","9783540898559","","","","We present a three-part approach for diagnosing bugs and performance problems in production distributed environments. First, we introduce a novel execution monitoring technique that dynamically injects a fragment of code, the agent, into an application process on demand. The agent inserts instrumentation ahead of the control flow within the process and propagates into other processes, following communication events, crossing host boundaries, and collecting a distributed function-level trace of the execution. Second, we present an algorithm that separates the trace into user-meaningful activities called flows. This step simplifies manual examination and enables automated analysis of the trace. Finally, we describe our automated root cause analysis technique that compares the flows to help the analyst locate an anomalous flow and identify a function in that flow that is a likely cause of the anomaly. We demonstrate the effectiveness of our techniques by diagnosing two complex problems in the Condor distributed scheduling system.","anomaly detection, distributed debugging, trace analysis, dynamic instrumentation, performance analysis","","Middleware '08"
"Conference Paper","Taherkhani A,Malmi L,Korhonen A","Algorithm Recognition by Static Analysis and Its Application in Students' Submissions Assessment","","2008","","","88–91","Association for Computing Machinery","New York, NY, USA","Proceedings of the 8th International Conference on Computing Education Research","Koli, Finland","2008","9781605583853","","https://doi.org/10.1145/1595356.1595372;http://dx.doi.org/10.1145/1595356.1595372","10.1145/1595356.1595372","Automatic program comprehension (PC) has been extensively studied for decades. It has been studied mainly from two different points of view: understanding the functionality of a program and understanding program structure. In this paper, we address the problem of automatic algorithm recognition and introduce a method based on static analysis to recognize algorithms. We discuss the applications of the method in the context of automatic assessment to widen the scope of programming assignments that can be checked automatically.","automatic grading, program analysis, algorithm recognition","","Koli '08"
"Conference Paper","Cottrell R,Walker RJ,Denzinger J","Semi-Automating Small-Scale Source Code Reuse via Structural Correspondence","","2008","","","214–225","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering","Atlanta, Georgia","2008","9781595939951","","https://doi.org/10.1145/1453101.1453130;http://dx.doi.org/10.1145/1453101.1453130","10.1145/1453101.1453130","Developers perform small-scale reuse tasks to save time and to increase the quality of their code, but due to their small scale, the costs of such tasks can quickly outweigh their benefits. Existing approaches focus on locating source code for reuse but do not support the integration of the located code within the developer's system, thereby leaving the developer with the burden of performing integration manually. This paper presents an approach that uses the developer's context to help integrate the reused source code into the developer's own source code. The approach approximates a theoretical framework (higher-order anti-unification modulo theories), known to be undecidable in general, to determine candidate correspondences between the source code to be reused and the developer's current (incomplete) system. This approach has been implemented in a prototype tool, called Jigsaw, that identifies and evaluates candidate correspondences greedily with respect to the highest similarity. Situations involving multiple candidate correspondences with similarities above a defined threshold are presented to the developer for resolution. Two empirical evaluations were conducted: an experiment comparing the quality of Jigsaw's results against suspected cases of small-scale reuse in an industrial system; and case studies with two industrial developers to consider its practical usefulness and usability issues.","structural correspondences, jigsaw, small-scale source code reuse, semi-automation","","SIGSOFT '08/FSE-16"
"Conference Paper","Stylos J,Myers BA","The Implications of Method Placement on API Learnability","","2008","","","105–112","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering","Atlanta, Georgia","2008","9781595939951","","https://doi.org/10.1145/1453101.1453117;http://dx.doi.org/10.1145/1453101.1453117","10.1145/1453101.1453117","To better understand what makes Application Programming Interfaces (APIs) hard to use and how to improve them, recent research has begun studying programmers' strategies and use of APIs. It was found that method placement --- on which class or classes a method is placed --- can have large usability impact in object-oriented APIs. This was because programmers often start their exploration of an API from one ""main"" object, and were slower finding other objects that were not referenced in the methods of the main object. For example, while mailServer.send(mailMessage) might make sense, if programmers often begin their API explorations from the MailMessage class, then this makes it harder to find the MailServer class than the alternative mailMessage.send(mailServer). This is interesting because many real APIs place methods essential to common objects on other, helper objects. Alternate versions of three different APIs were compared, and it was found that programmers gravitated toward the same starting classes and were dramatically faster --- between 2 to 11 times --- combining multiple objects when a method on the starting class referred to the other class.","APIs, user studies, frameworks, usability, documentation, libraries","","SIGSOFT '08/FSE-16"
"Conference Paper","Verdú J,Nemirovsky M,Valero M","MultiLayer Processing - an Execution Model for Parallel Stateful Packet Processing","","2008","","","79–88","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM/IEEE Symposium on Architectures for Networking and Communications Systems","San Jose, California","2008","9781605583464","","https://doi.org/10.1145/1477942.1477954;http://dx.doi.org/10.1145/1477942.1477954","10.1145/1477942.1477954","Mostly emerging network applications comprise deep packet inspection and/or stateful capabilities. Stateful workloads present limitations that reduce the exploitation of parallelism, unlike other network applications that show marginal dependencies among packets. In addition, differences among packet processing lead to significant negative interaction between threads, especially in the memory hierarchy.We propose MultiLayer Processing (MLP) as an execution model to properly exploit the levels of parallelism of stateful applications. The goal of MLP is to increase the system throughput by increasing the synergy among threads in the memory hierarchy, and alleviating the contention in critical sections of parallel workloads. We show that MLP presents about 2.4x higher throughput than other execution models with large processor architectures.","stateful, massively multithreaded architectures, multilayer processing, Snort, deep packet inspection","","ANCS '08"
"Conference Paper","Tairas R","Clone Maintenance through Analysis and Refactoring","","2008","","","29–32","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Foundations of Software Engineering Doctoral Symposium","Atlanta, Georgia","2008","9781605583785","","https://doi.org/10.1145/1496653.1496661;http://dx.doi.org/10.1145/1496653.1496661","10.1145/1496653.1496661","The removal of duplicate code associated with code clones provides a mechanism to improve code clone maintenance by eliminating redundant code and reducing the amount of code that needs to be maintained. The research described in this paper contributes to the field of code clone maintenance by exploring techniques that determine novel factors influencing code clone analysis and refactoring, and by developing a process that unifies the phases of code clone detection, analysis, and refactoring.","analysis, maintenance, refactoring, code clones","","FSEDS '08"
"Conference Paper","Mileva YM","Learning from Deletions","","2008","","","17–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Foundations of Software Engineering Doctoral Symposium","Atlanta, Georgia","2008","9781605583785","","https://doi.org/10.1145/1496653.1496658;http://dx.doi.org/10.1145/1496653.1496658","10.1145/1496653.1496658","When somethings gets deleted from source code it has been deleted because it is wrong, no longer used or inappropriate. What does this mean for other places that still use the same feature? By mining software archives and the stored deletion information, I hope to detect project specific evolutionary patterns. This knowledge can later be used for recommendation of a substitution for the deleted element, detection and correction of unknown code defects1 and prediction of future deletions.","","","FSEDS '08"
"Conference Paper","Rupakheti CR,Hou D","An Empirical Study of the Design and Implementation of Object Equality in Java","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds","Ontario, Canada","2008","9781450378826","","https://doi.org/10.1145/1463788.1463800;http://dx.doi.org/10.1145/1463788.1463800","10.1145/1463788.1463800","Applications built on top of an existing design and implementation are in general expected to collaborate well with that design and respect all of its intent. Failure in achieving this may result in buggy, fragile, and less maintainable code in the applications. When the dependence on an existing design becomes more wide-spread, this requirement on proper extension obviously becomes even more critical. As an instance of this general problem, the design for object equality in Java as well as its extensions is examined in detail and empirically. By examining how object equality is extended in a large amount of Java code, a set of typical problems are detected and their root causes analyzed. A set of design guidelines for object equality is proposed, which, if followed, will help programers systematically design and evolve rather than hack on a solution. Examples are drawn from a case study of multiple industrial and open source projects to illustrate the identified problems and how the proposed guidelines can help solve these problems.","","","CASCON '08"
"Conference Paper","He B,Fang W,Luo Q,Govindaraju NK,Wang T","Mars: A MapReduce Framework on Graphics Processors","","2008","","","260–269","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques","Toronto, Ontario, Canada","2008","9781605582825","","https://doi.org/10.1145/1454115.1454152;http://dx.doi.org/10.1145/1454115.1454152","10.1145/1454115.1454152","We design and implement Mars, a MapReduce framework, on graphics processors (GPUs). MapReduce is a distributed programming framework originally proposed by Google for the ease of development of web search applications on a large number of commodity CPUs. Compared with CPUs, GPUs have an order of magnitude higher computation power and memory bandwidth, but are harder to program since their architectures are designed as a special-purpose co-processor and their programming interfaces are typically for graphics applications. As the first attempt to harness GPU's power for MapReduce, we developed Mars on an NVIDIA G80 GPU, which contains over one hundred processors, and evaluated it in comparison with Phoenix, the state-of-the-art MapReduce framework on multi-core CPUs. Mars hides the programming complexity of the GPU behind the simple and familiar MapReduce interface. It is up to 16 times faster than its CPU-based counterpart for six common web applications on a quad-core machine.","MapReduce, web analysis, GPGPU, multi-core processors, data parallelism, graphics processor","","PACT '08"
"Conference Paper","Schulze S,Kuhlemann M,Rosenmüller M","Towards a Refactoring Guideline Using Code Clone Classification","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Workshop on Refactoring Tools","Nashville, Tennessee","2008","9781605583396","","https://doi.org/10.1145/1636642.1636648;http://dx.doi.org/10.1145/1636642.1636648","10.1145/1636642.1636648","Evolution of software often decreases desired properties like readability and maintainability of the evolved code. The process of refactoring aims at increasing the same desired properties by restructuring the code. New paradigms like AOP allow aspect-oriented refactorings as counterparts of object-oriented refactoring with the same aim. However, it is not obvious to the user, when to use which paradigm for achieving certain goals. In this paper we present an approach of code clone classification, which advises the developer when to use a respective refactoring technique or concept.","aspect-oriented programming, clone detection, object-oriented programming, refactoring","","WRT '08"
"Conference Paper","Ribeiro M,Borba P","Recommending Refactorings When Restructuring Variabilities in Software Product Lines","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Workshop on Refactoring Tools","Nashville, Tennessee","2008","9781605583396","","https://doi.org/10.1145/1636642.1636650;http://dx.doi.org/10.1145/1636642.1636650","10.1145/1636642.1636650","When restructuring variabilities in Software Product Lines (SPL), due to the great variety of existing mechanisms - such as Inheritance, Configuration Files, Aspect-Oriented Programming etc, developers may spend time and effort to decide which mechanism to use and which refactorings should be applied. To help on this task, we propose in this paper a tool capable of recommending refactorings based on some mechanisms. By applying the recommendations of our tool, bad smells such as cloned code may be removed and the modularity of the features is improved.","software product line, refactoring, modularity","","WRT '08"
"Conference Paper","Li H,Thompson S","Tool Support for Refactoring Functional Programs","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2nd Workshop on Refactoring Tools","Nashville, Tennessee","2008","9781605583396","","https://doi.org/10.1145/1636642.1636644;http://dx.doi.org/10.1145/1636642.1636644","10.1145/1636642.1636644","We present the Haskell Refactorer, HaRe, and the Erlang Refactorer, Wrangler, as examples of fully-functional refactoring tools for functional programming languages. HaRe and Wrangler are designed to handle multi-module projects in complete languages: Haskell 98 and Erlang/OTP. They are embedded in Emacs, (gVim and Eclipse) and respect programmer layout styles.In discussing the construction of HaRe and Wrangler, we comment on the different challenges presented by Haskell and Erlang due to their differences in syntax, semantics and pragmatics. In particular, we examine the sorts of analysis that underlie our systems.","refactoring, semantics, Erlang, Haskell, Wrangler, HaRe, program analysis, program transformation","","WRT '08"
"Conference Paper","Denney E,Fischer B","Generating Customized Verifiers for Automatically Generated Code","","2008","","","77–88","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Conference on Generative Programming and Component Engineering","Nashville, TN, USA","2008","9781605582672","","https://doi.org/10.1145/1449913.1449926;http://dx.doi.org/10.1145/1449913.1449926","10.1145/1449913.1449926","Program verification using Hoare-style techniques requires many logical annotations. We have previously developed a generic annotation inference algorithm that weaves in all annotations required to certify safety properties for automatically generated code. It uses patterns to capture generator- and property-specific code idioms and property-specific meta-program fragments to construct the annotations. The algorithm is customized by specifying the code patterns and integrating them with the meta-program fragments for annotation construction. However, this is difficult since it involves tedious and error-prone low-level term manipulations.Here, we describe an approach that automates this customization task using generative techniques. It uses a small annotation schema compiler that takes a collection of high-level declarative annotation schemas tailored towards a specific code generator and safety property, and generates all customized analysis functions and glue code required for interfacing with the generic algorithm core, thus effectively creating a customized annotation inference algorithm. The compiler raises the level of abstraction and simplifies schema development and maintenance. It also takes care of some more routine aspects of formulating patterns and schemas, in particular handling of irrelevant program fragments and irrelevant variance in the program structure, which reduces the size, complexity, and number of different patterns and annotation schemas required. The improvements described here make it easier and faster to customize the system to a new safety property or a new generator, and we demonstrate this by customizing it to certify frame safety of space flight navigation code that was automatically generated from Simulink models by MathWorks' Real-Time Workshop.","hoare logic, software certification, automated code generation, automated theorem proving, program verification, logical annotations","","GPCE '08"
"Conference Paper","Baldi PF,Lopes CV,Linstead EJ,Bajracharya SK","A Theory of Aspects as Latent Topics","","2008","","","543–562","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications","Nashville, TN, USA","2008","9781605582153","","https://doi.org/10.1145/1449764.1449807;http://dx.doi.org/10.1145/1449764.1449807","10.1145/1449764.1449807","After more than 10 years, Aspect-Oriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations.We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topic-over-files and files-over-topics distributions. Aspects are latent topics with high scattering entropy.The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as general-purpose aspects across multiple projects, as well as project-specific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them.Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this new perspective makes additional progress toward the design of models and tools that facilitate software development.","aspect-oriented programming, scattering, tangling, topic models","","OOPSLA '08"
"Journal Article","Baldi PF,Lopes CV,Linstead EJ,Bajracharya SK","A Theory of Aspects as Latent Topics","SIGPLAN Not.","2008","43","10","543–562","Association for Computing Machinery","New York, NY, USA","","","2008-10","","0362-1340","https://doi.org/10.1145/1449955.1449807;http://dx.doi.org/10.1145/1449955.1449807","10.1145/1449955.1449807","After more than 10 years, Aspect-Oriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations.We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topic-over-files and files-over-topics distributions. Aspects are latent topics with high scattering entropy.The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as general-purpose aspects across multiple projects, as well as project-specific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them.Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this new perspective makes additional progress toward the design of models and tools that facilitate software development.","topic models, scattering, tangling, aspect-oriented programming","",""
"Conference Paper","Ni Y,Welc A,Adl-Tabatabai AR,Bach M,Berkowits S,Cownie J,Geva R,Kozhukow S,Narayanaswamy R,Olivier J,Preis S,Saha B,Tal A,Tian X","Design and Implementation of Transactional Constructs for C/C++","","2008","","","195–212","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications","Nashville, TN, USA","2008","9781605582153","","https://doi.org/10.1145/1449764.1449780;http://dx.doi.org/10.1145/1449764.1449780","10.1145/1449764.1449780","This paper presents a software transactional memory system that introduces first-class C++ language constructs for transactional programming. We describe new C++ language extensions, a production-quality optimizing C++ compiler that translates and optimizes these extensions, and a high-performance STM runtime library. The transactional language constructs support C++ language features including classes, inheritance, virtual functions, exception handling, and templates. The compiler automatically instruments the program for transactional execution and optimizes TM overheads. The runtime library implements multiple execution modes and implements a novel STM algorithm that supports both optimistic and pessimistic concurrency control. The runtime switches a transaction's execution mode dynamically to improve performance and to handle calls to precompiled functions and I/O libraries. We present experimental results on 8 cores (two quad-core CPUs) running a set of 20 non-trivial parallel programs. Our measurements show that our system scales well as the numbers of cores increases and that our compiler and runtime optimizations improve scalability.","transactional memory, C/C++","","OOPSLA '08"
"Journal Article","Ni Y,Welc A,Adl-Tabatabai AR,Bach M,Berkowits S,Cownie J,Geva R,Kozhukow S,Narayanaswamy R,Olivier J,Preis S,Saha B,Tal A,Tian X","Design and Implementation of Transactional Constructs for C/C++","SIGPLAN Not.","2008","43","10","195–212","Association for Computing Machinery","New York, NY, USA","","","2008-10","","0362-1340","https://doi.org/10.1145/1449955.1449780;http://dx.doi.org/10.1145/1449955.1449780","10.1145/1449955.1449780","This paper presents a software transactional memory system that introduces first-class C++ language constructs for transactional programming. We describe new C++ language extensions, a production-quality optimizing C++ compiler that translates and optimizes these extensions, and a high-performance STM runtime library. The transactional language constructs support C++ language features including classes, inheritance, virtual functions, exception handling, and templates. The compiler automatically instruments the program for transactional execution and optimizes TM overheads. The runtime library implements multiple execution modes and implements a novel STM algorithm that supports both optimistic and pessimistic concurrency control. The runtime switches a transaction's execution mode dynamically to improve performance and to handle calls to precompiled functions and I/O libraries. We present experimental results on 8 cores (two quad-core CPUs) running a set of 20 non-trivial parallel programs. Our measurements show that our system scales well as the numbers of cores increases and that our compiler and runtime optimizations improve scalability.","transactional memory, C/C++","",""
"Conference Paper","Bachmann P","Deferred Cancellation: A Behavioral Pattern","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th Conference on Pattern Languages of Programs","Nashville, Tennessee, USA","2008","9781605581514","","https://doi.org/10.1145/1753196.1753218;http://dx.doi.org/10.1145/1753196.1753218","10.1145/1753196.1753218","People who design their own pool of worker threads [33, pp 290--298] or processes have to consider how to shut down the workers again or how to dynamically adapt the number of workers to varying load. Especially with regard to application termination you may have the choice between an immediate destruction of the pool and a more graceful shutdown. The pattern proposed helps to portably implement such termination and load adaptation mechanisms that assume you voted for the second choice. The main area of application are the internals of active objects [40] and similar designs that delegate work to a pool of threads or processes to execute service requests asynchronously from their actual invocation.For the pattern proposed we identified usage examples in popular existing applications or libraries.Both a real world example and sample code accompany the pattern presentation. This sample code is in C++.The presentation of the pattern follows the style well known from [11] and [44]. This pattern is based upon other patterns. Typographic conventions for references to other patterns are similar to [3]. A Glossary provides thumbnails of many of these patterns.","patterns, destructor, actor, reliability, portability","","PLoP '08"
"Conference Paper","Sagonas K,Luna D","Gradual Typing of Erlang Programs: A Wrangler Experience","","2008","","","73–82","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th ACM SIGPLAN Workshop on ERLANG","Victoria, BC, Canada","2008","9781605580654","","https://doi.org/10.1145/1411273.1411284;http://dx.doi.org/10.1145/1411273.1411284","10.1145/1411273.1411284","Currently most Erlang programs contain no or very little type information. This sometimes makes them unreliable, hard to use, and difficult to understand and maintain. In this paper we describe our experiences from using static analysis tools to gradually add type information to a medium sized Erlang application that we did not write ourselves: the code base of Wrangler. We carefully document the approach we followed, the exact steps we took, and discuss possible difficulties that one is expected to deal with and the effort which is required in the process. We also show the type of software defects that are typically brought forward, the opportunities for code refactoring and improvement, and the expected benefits from embarking in such a project. We have chosen Wrangler for our experiment because the process is better explained on a code base which is small enough so that the interested reader can retrace its steps, yet large enough to make the experiment quite challenging and the experiences worth writing about. However, we have also done something similar on large parts of Erlang/OTP. The result can partly be seen in the source code of Erlang/OTP R12B-3.","contracts, dialyzer, erlang, software defect detection","","ERLANG '08"
"Conference Paper","Li H,Thompson S,Orosz G,Tóth M","Refactoring with Wrangler, Updated: Data and Process Refactorings, and Integration with Eclipse","","2008","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th ACM SIGPLAN Workshop on ERLANG","Victoria, BC, Canada","2008","9781605580654","","https://doi.org/10.1145/1411273.1411283;http://dx.doi.org/10.1145/1411273.1411283","10.1145/1411273.1411283","Wrangler is a refactoring tool for Erlang, implemented in Erlang. This paper reports the latest developments in Wrangler, which include improved user experience, the introduction of a number of data- and process-related refactorings, and also the implementation of an Eclipse plug-in which, together with Erlide, provides refactoring support for Erlang in Eclipse.","wrangler, erlang, erlide, process, record, refactoring, slicing, eclipse, tuple","","ERLANG '08"
"Conference Paper","Nagy T,Nagyné Víg A","Erlang Testing and Tools Survey","","2008","","","21–28","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th ACM SIGPLAN Workshop on ERLANG","Victoria, BC, Canada","2008","9781605580654","","https://doi.org/10.1145/1411273.1411277;http://dx.doi.org/10.1145/1411273.1411277","10.1145/1411273.1411277","As the commercial usage of Erlang increases, so does the need for mature development and testing tools. This paper aims to evaluate the available tools with their shortcomings, strengths and commercial usability compared to common practices in other languages.To identify the needs of Erlang developers in this area we published an online survey advertising it in various media. The results of this survey and additional research in this field is presented. Through the comparison of tools and the requirements of the developers the paper identifies paths for future development.","erlang, test tools, market analysis","","ERLANG '08"
"Conference Paper","Aldrich J,Simmons RJ,Shin K","SASyLF: An Educational Proof Assistant for Language Theory","","2008","","","31–40","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 International Workshop on Functional and Declarative Programming in Education","Victoria, BC, Canada","2008","9781605580685","","https://doi.org/10.1145/1411260.1411266;http://dx.doi.org/10.1145/1411260.1411266","10.1145/1411260.1411266","Teaching and learning formal programming language theory is hard, in part because it's easy to make mistakes and hard to find them. Proof assistants can help check proofs, but their learning curve is too steep to use in most classes, and is a barrier to researchers too.In this paper we present SASyLF, an LF-based proof assistant specialized to checking theorems about programming languages and logics. SASyLF has a simple design philosophy: language and logic syntax, semantics, and meta-theory should be written as closely as possible to the way it is done on paper. We describe how we designed the SASyLF syntax to be accessible to students learning type theory, and how students can understand its semantics directly in terms of the theory they are taught in class. SASyLF can express proofs typical of an introductory graduate type theory course. SASyLF proofs are generally very explicit, but its built-in support for variable binding provides substitution properties for free and avoids awkward variable encodings. We describe preliminary experience teaching with SASyLF.","education, proof assistant, logical framework, metatheory","","FDPE '08"
"Conference Paper","Guillemette LJ,Monnier S","A Type-Preserving Compiler in Haskell","","2008","","","75–86","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming","Victoria, BC, Canada","2008","9781595939197","","https://doi.org/10.1145/1411204.1411218;http://dx.doi.org/10.1145/1411204.1411218","10.1145/1411204.1411218","There has been a lot of interest of late for programming languages that incorporate features from dependent type systems and proof assistants, in order to capture important invariants of the program in the types. This allows type-based program verification and is a promising compromise between plain old types and full blown Hoare logic proofs. The introduction of GADTs in GHC (and more recently type families) made such dependent typing available in an industry-quality implementation, making it possible to consider its use in large scale programs.We have undertaken the construction of a complete compiler for System F, whose main property is that the GHC type checker verifies mechanically that each phase of the compiler properly preserves types. Our particular focus is on ""types rather than proofs"": reasonably few annotations that do not overwhelm the actual code.We believe it should be possible to write such a type-preserving compiler with an amount of extra code comparable to what is necessary for typical typed intermediate languages, but with the advantage of static checking. We will show in this paper the remaining hurdles to reach this goal.","de Bruijn, typed assembly language, compilation, higher-order abstract syntax","","ICFP '08"
"Journal Article","Guillemette LJ,Monnier S","A Type-Preserving Compiler in Haskell","SIGPLAN Not.","2008","43","9","75–86","Association for Computing Machinery","New York, NY, USA","","","2008-09","","0362-1340","https://doi.org/10.1145/1411203.1411218;http://dx.doi.org/10.1145/1411203.1411218","10.1145/1411203.1411218","There has been a lot of interest of late for programming languages that incorporate features from dependent type systems and proof assistants, in order to capture important invariants of the program in the types. This allows type-based program verification and is a promising compromise between plain old types and full blown Hoare logic proofs. The introduction of GADTs in GHC (and more recently type families) made such dependent typing available in an industry-quality implementation, making it possible to consider its use in large scale programs.We have undertaken the construction of a complete compiler for System F, whose main property is that the GHC type checker verifies mechanically that each phase of the compiler properly preserves types. Our particular focus is on ""types rather than proofs"": reasonably few annotations that do not overwhelm the actual code.We believe it should be possible to write such a type-preserving compiler with an amount of extra code comparable to what is necessary for typical typed intermediate languages, but with the advantage of static checking. We will show in this paper the remaining hurdles to reach this goal.","typed assembly language, de Bruijn, higher-order abstract syntax, compilation","",""
"Conference Paper","Oliveira BC,Gibbons J","Scala for Generic Programmers","","2008","","","25–36","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM SIGPLAN Workshop on Generic Programming","Victoria, BC, Canada","2008","9781605580609","","https://doi.org/10.1145/1411318.1411323;http://dx.doi.org/10.1145/1411318.1411323","10.1145/1411318.1411323","Datatype-generic programming involves parametrization by the shape of data, in the form of type constructors such as ""list of"". Most approaches to datatype-generic programming are developed in the lazy functional programming language Haskell. We argue that the functional object-oriented language Scala is in many ways a better setting. Not only does Scala provide equivalents of all the necessary functional programming features (such parametric polymorphism, higher-order functions, higher-kinded type operations, and type- and constructor-classes), but it also provides the most useful features of object-oriented languages (such as subtyping, overriding, traditional single inheritance, and multiple inheritance in the form of traits). We show how this combination of features benefits datatype-generic programming, using three different approaches as illustrations.","scala, datatype-generic programming, polytypic programming","","WGP '08"
"Conference Paper","Panas T","Signature Visualization of Software Binaries","","2008","","","185–188","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM Symposium on Software Visualization","Ammersee, Germany","2008","9781605581125","","https://doi.org/10.1145/1409720.1409749;http://dx.doi.org/10.1145/1409720.1409749","10.1145/1409720.1409749","In this paper we present work on the visualization of software binaries. In particular, we utilize ROSE, an open source compiler infrastructure, to pre-process software binaries, and we apply a landscape metaphor to visualize the signature of each binary (malware). We define the signature of a binary as a metric-based layout of the functions contained in the binary. In our initial experiment, we visualize the signatures of a series of computer worms that all originate from the same line. These visualizations are useful for a number of reasons. First, the images reveal how the archetype has evolved over a series of versions of one worm. Second, one can see the distinct changes between versions. This allows the viewer to form conclusions about the development cycle of a particular worm.","binary analysis, malware visualization","","SoftVis '08"
"Conference Paper","Parnin C,Görg C,Nnadi O","A Catalogue of Lightweight Visualizations to Support Code Smell Inspection","","2008","","","77–86","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th ACM Symposium on Software Visualization","Ammersee, Germany","2008","9781605581125","","https://doi.org/10.1145/1409720.1409733;http://dx.doi.org/10.1145/1409720.1409733","10.1145/1409720.1409733","Preserving the integrity of software systems is essential in ensuring future product success. Commonly, companies allocate only a limited budget toward perfective maintenance and instead pressure developers to focus on implementing new features. Traditional techniques, such as code inspection, consume many staff resources and attention from developers. Metrics automate the process of checking for problems but produce voluminous, imprecise, and incongruent results. An opportunity exists for visualization to assist where automated measures have failed; however, current software visualization techniques only handle the voluminous aspect of data but fail to address imprecise and incongruent aspects. In this paper, we describe several techniques for visualizing possible defects reported by automated inspection tools. We propose a catalogue of lightweight visualizations that assist reviewers in weeding out false positives. We implemented the visualizations in a tool called NOSEPRINTS and present a case study on several commercial systems and open source applications in which we examined the impact of our tool on the inspection process.","lightweight visualization, code smells, code inspection, refactoring","","SoftVis '08"
"Conference Paper","Denny P,Luxton-Reilly A,Simon B","Evaluating a New Exam Question: Parsons Problems","","2008","","","113–124","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth International Workshop on Computing Education Research","Sydney, Australia","2008","9781605582160","","https://doi.org/10.1145/1404520.1404532;http://dx.doi.org/10.1145/1404520.1404532","10.1145/1404520.1404532","Common exam practice centres around two question types: code tracing (reading) and code writing. It is commonly believed that code tracing is easier than code writing, but it seems obvious that different skills are needed for each. These problems also differ in their value on an exam. Pedagogically, code tracing on paper is an authentic task whereas code writing on paper is less so. Yet, few instructors are willing to forgo the code writing question on an exam. Is there another way, easier to grade, that captures the ""problem solving through code creation process"" we wish to examine? In this paper we propose Parson's puzzle-style problems for this purpose. We explore their potential both qualitatively, through interviews, and quantitatively through a set of CS1 exams. We find notable correlation between Parsons scores and code writing scores. We find low correlation between code writing and tracing and between Parsons and tracing. We also make the case that marks from a Parsons problem make clear what students don't know (specifically, in both syntax and logic) much less ambiguously than marks from a code writing problem. We make recommendations on the design of Parsons problems for the exam setting, discuss their potential uses and urge further investigations of Parsons problems for assessment of CS1 students.","CS1, exam questions, Parsons problems, tracing, assessment, code writing","","ICER '08"
"Conference Paper","McDonnell R,Larkin M,Dobbyn S,Collins S,O'Sullivan C","Clone Attack! Perception of Crowd Variety","","2008","","","","Association for Computing Machinery","New York, NY, USA","ACM SIGGRAPH 2008 Papers","Los Angeles, California","2008","9781450301121","","https://doi.org/10.1145/1399504.1360625;http://dx.doi.org/10.1145/1399504.1360625","10.1145/1399504.1360625","When simulating large crowds, it is inevitable that the models and motions of many virtual characters will be cloned. However, the perceptual impact of this trade-off has never been studied. In this paper, we consider the ways in which an impression of variety can be created and the perceptual consequences of certain design choices. In a series of experiments designed to test people's perception of variety in crowds, we found that clones of appearance are far easier to detect than motion clones. Furthermore, we established that cloned models can be masked by color variation, random orientation, and motion. Conversely, the perception of cloned motions remains unaffected by the model on which they are displayed. Other factors that influence the ability to detect clones were examined, such as proximity, model type and characteristic motion. Our results provide novel insights and useful thresholds that will assist in creating more realistic, heterogeneous crowds.","animation, perception, variety, crowds","","SIGGRAPH '08"
"Journal Article","McDonnell R,Larkin M,Dobbyn S,Collins S,O'Sullivan C","Clone Attack! Perception of Crowd Variety","ACM Trans. Graph.","2008","27","3","1–8","Association for Computing Machinery","New York, NY, USA","","","2008-08","","0730-0301","https://doi.org/10.1145/1360612.1360625;http://dx.doi.org/10.1145/1360612.1360625","10.1145/1360612.1360625","When simulating large crowds, it is inevitable that the models and motions of many virtual characters will be cloned. However, the perceptual impact of this trade-off has never been studied. In this paper, we consider the ways in which an impression of variety can be created and the perceptual consequences of certain design choices. In a series of experiments designed to test people's perception of variety in crowds, we found that clones of appearance are far easier to detect than motion clones. Furthermore, we established that cloned models can be masked by color variation, random orientation, and motion. Conversely, the perception of cloned motions remains unaffected by the model on which they are displayed. Other factors that influence the ability to detect clones were examined, such as proximity, model type and characteristic motion. Our results provide novel insights and useful thresholds that will assist in creating more realistic, heterogeneous crowds.","animation, variety, crowds, perception","",""
"Conference Paper","Yoshida N,Ishio T,Matsushita M,Inoue K","Retrieving Similar Code Fragments Based on Identifier Similarity for Defect Detection","","2008","","","41–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Workshop on Defects in Large Software Systems","Seattle, Washington","2008","9781605580517","","https://doi.org/10.1145/1390817.1390830;http://dx.doi.org/10.1145/1390817.1390830","10.1145/1390817.1390830","Similar source code fragments, known as code clones, may involve similar defects caused by the same mistake. However, code clone detection tools cannot detect certain code fragments (e.g. modified after copy-and-pasted). To support developers who would like to detect such defects, we propose a method to retrieve similar code fragments in source code based on the similarity of identifiers between a query and a target code fragment. We present two case studies of similar defects in open source systems.","","","DEFECTS '08"
"Conference Paper","Hayase Y,Lee YY,Inoue K","A Criterion for Filtering Code Clone Related Bugs","","2008","","","37–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Workshop on Defects in Large Software Systems","Seattle, Washington","2008","9781605580517","","https://doi.org/10.1145/1390817.1390828;http://dx.doi.org/10.1145/1390817.1390828","10.1145/1390817.1390828","Software reviews are time-consuming task especially for large software systems. To reduce the efforts required, Li et al. developed CP-Miner, a code clone detection tool that detects identifier naming inconsistencies between code clones as bug candidates. However, reviewers using CP-Miner still have to assess many inconsistencies, since the tool also reports many false-positive candidates. To reduce the false-positive candidates, we propose a criterion for filtering the candidates. In our experiments, filtering with the proposed criterion removed 30% of the false-positive candidates and no true-positive candidates. This result shows that the proposed criterion helps the review task by effectively reducing the number of bug candidates.","defect mining, code clone","","DEFECTS '08"
"Conference Paper","Dor N,Lev-Ami T,Litvak S,Sagiv M,Weiss D","Customization Change Impact Analysis for Erp Professionals via Program Slicing","","2008","","","97–108","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 International Symposium on Software Testing and Analysis","Seattle, WA, USA","2008","9781605580500","","https://doi.org/10.1145/1390630.1390644;http://dx.doi.org/10.1145/1390630.1390644","10.1145/1390630.1390644","We describe a new tool that automatically identifies impact of customization changes, i.e., how changes affect software behavior. As opposed to existing static analysis tools that aim at aiding programmers or improve performance, our tool is designed for end-users without prior knowledge in programming. We utilize state-of-the-art static analysis algorithms for the programs within an Enterprise Resource Planning system (ERP). Key challenges in analyzing real world ERP programs are their significant size and the interdependency between programs. In particular, we describe and compare three customization change impact analyses for real-world programs, and a balancing algorithm built upon the three independent analyses. This paper presents PanayaImpactAnalysis (PanayaIA), a web on-demand tool, providing ERP professionals a clear view of the impact of a customization change on the system. In addition we report empirical results of PanayaIA when used by end-users on an ERP system of tens of millions LOCs.","customization change impact analysis","","ISSTA '08"
"Conference Paper","Zhang M,Hall T,Baddoo N,Wernick P","Do Bad Smells Indicate ""Trouble"" in Code?","","2008","","","43–44","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Workshop on Defects in Large Software Systems","Seattle, Washington","2008","9781605580517","","https://doi.org/10.1145/1390817.1390831;http://dx.doi.org/10.1145/1390817.1390831","10.1145/1390817.1390831","In 1999 Fowler et al. identified 22 Bad Smells in code to direct the effective refactoring. These are increasingly being used by software engineers. However, the empirical basis of using Bad Smells to direct refactoring and to address 'trouble' in code is not clear. Our project aims to empirically investigate the impact of Bad Smells on software in terms of their relationship to faults.","faults, open source, bad smells","","DEFECTS '08"
"Conference Paper","Pacheco C,Lahiri SK,Ball T","Finding Errors in .Net with Feedback-Directed Random Testing","","2008","","","87–96","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 International Symposium on Software Testing and Analysis","Seattle, WA, USA","2008","9781605580500","","https://doi.org/10.1145/1390630.1390643;http://dx.doi.org/10.1145/1390630.1390643","10.1145/1390630.1390643","We present a case study in which a team of test engineers at Microsoft applied a feedback-directed random testing tool to a critical component of the .NET architecture. Due to its complexity and high reliability requirements, the component had already been tested by 40 test engineers over five years, using manual testing and many automated testing techniques.Nevertheless, the feedback-directed random testing tool found errors in the component that eluded previous testing, and did so two orders of magnitude faster than a typical test engineer (including time spent inspecting the results of the tool). The tool also led the test team to discover errors in other testing and analysis tools, and deficiencies in previous best-practice guidelines for manual testing. Finally, we identify challenges that random testing faces for continued effectiveness, including an observed decrease in the technique's error detection rate over time.","random testing","","ISSTA '08"
"Conference Paper","Ciesielski V,Wu N,Tahaghoghi S","Evolving Similarity Functions for Code Plagiarism Detection","","2008","","","1453–1460","Association for Computing Machinery","New York, NY, USA","Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation","Atlanta, GA, USA","2008","9781605581309","","https://doi.org/10.1145/1389095.1389380;http://dx.doi.org/10.1145/1389095.1389380","10.1145/1389095.1389380","Detecting whether computer program code is a student's original work or has been copied from another student or some other source is a major problem for many universities. Detection methods based on the information retrieval concepts of indexing and similarity matching scale well to large collections of files, but require appropriate similarity functions for good performance. We have used particle swarm optimization and genetic programming to evolve similarity functions that are suited to computer program code. Using a training set of plagiarised and non-plagiarised programs we have evolved better parameter values for the previously published Okapi BM25 similarity function. We have then used genetic programming to evolve completely new similarity functions that do not conform to any predetermined structure. We found that the evolved similarity functions outperformed the human developed Okapi BM25 function. We also found that a detection system using the evolved functions was more accurate than the the best code plagiarism detection system in use today, and scales much better to large collections of files. The evolutionary computing techniques have been extremely useful in finding similarity functions that advance the state of the art in code plagiarism detection.","okapi bm25, evolutionary computing, genetic programming, particle swarm optimization, similarity function","","GECCO '08"
"Conference Paper","Sheard J,Carbone A,Lister R,Simon B,Thompson E,Whalley JL","Going SOLO to Assess Novice Programmers","","2008","","","209–213","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education","Madrid, Spain","2008","9781605580784","","https://doi.org/10.1145/1384271.1384328;http://dx.doi.org/10.1145/1384271.1384328","10.1145/1384271.1384328","This paper explores the programming knowledge of novices using Biggs' SOLO taxonomy. It builds on previous work of Lister et al. (2006) and addresses some of the criticisms of that work. The research was conducted by studying the exam scripts for 120 introductory programming students, in which three specific questions were analyzed using the SOLO taxonomy. The study reports the following four findings: when the instruction to students used by Lister et al. - ""In plain English, explain what the following segment of Java code does"" - is replaced with a less ambiguous instruction, many students still provide multistructural responses; students are relatively consistent in the SOLO level of their answers; student responses on SOLO reading tasks correlate positively with performance on writing tasks; postgraduates students manifest a higher level of thinking than undergraduates.","comprehension, CS1, novice programmers, solo taxonomy","","ITiCSE '08"
"Journal Article","Sheard J,Carbone A,Lister R,Simon B,Thompson E,Whalley JL","Going SOLO to Assess Novice Programmers","SIGCSE Bull.","2008","40","3","209–213","Association for Computing Machinery","New York, NY, USA","","","2008-06","","0097-8418","https://doi.org/10.1145/1597849.1384328;http://dx.doi.org/10.1145/1597849.1384328","10.1145/1597849.1384328","This paper explores the programming knowledge of novices using Biggs' SOLO taxonomy. It builds on previous work of Lister et al. (2006) and addresses some of the criticisms of that work. The research was conducted by studying the exam scripts for 120 introductory programming students, in which three specific questions were analyzed using the SOLO taxonomy. The study reports the following four findings: when the instruction to students used by Lister et al. - ""In plain English, explain what the following segment of Java code does"" - is replaced with a less ambiguous instruction, many students still provide multistructural responses; students are relatively consistent in the SOLO level of their answers; student responses on SOLO reading tasks correlate positively with performance on writing tasks; postgraduates students manifest a higher level of thinking than undergraduates.","solo taxonomy, novice programmers, comprehension, CS1","",""
"Conference Paper","Lovstad JS,Hughes PH","Run-Time Software Configuration for Mobile Devices Using an Evolutionary Quantifiable Deployment Model","","2008","","","189–200","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Workshop on Software and Performance","Princeton, NJ, USA","2008","9781595938732","","https://doi.org/10.1145/1383559.1383584;http://dx.doi.org/10.1145/1383559.1383584","10.1145/1383559.1383584","Current proposals for performance evaluation during development of computer systems rely on the capture of performance information from various software development models such as UML diagrams. The requirements for performance are taken into consideration at some point in the development process and evaluated using a performance model.We propose an enhanced approach using a separate, quantifiable deployment model that evolves in step with system development. This helps keep performance requirements in focus from beginning to end of the life-cycle.The system implemented to demonstrate this approach is a computer game for mobile devices that is configured dynamically for different target devices. The development process must provide for deferred, automatic performance testing and configuration of the application on a variety of devices, after the design and implementation stages are complete. The evolutionary deployment model, culminating in a run-time performance evaluation, is based on the Structure and Performance modelling paradigm SP.","modelling, configuration, development, run-time, deployment","","WOSP '08"
"Conference Paper","Ware MS,Fox CJ","Securing Java Code: Heuristics and an Evaluation of Static Analysis Tools","","2008","","","12–21","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 Workshop on Static Analysis","Tucson, Arizona","2008","9781595939241","","https://doi.org/10.1145/1394504.1394506;http://dx.doi.org/10.1145/1394504.1394506","10.1145/1394504.1394506","A secure coding standard for Java does not exist. Even if a standard did exist, it is not known how well static analysis tools could enforce it. In this work, we show how well eight static analysis tools can identify violations of a comprehensive collection of coding heuristics for increasing the quality and security of Java SE code. A new taxonomy for correlating coding heuristics with the design principles they help to achieve is also described. The taxonomy aims to make understanding, applying, and remembering both principles and heuristics easier. A significant number of secure coding violations, some of which make attacks possible, were not identified by any tool. Even if all eight tools were combined into a single tool, more than half of the violations included in the study would not be identified.","design principles, taxonomy, static analysis, secure code","","SAW '08"
"Conference Paper","Pizlo F,Petrank E,Steensgaard B","Path Specialization: Reducing Phased Execution Overheads","","2008","","","81–90","Association for Computing Machinery","New York, NY, USA","Proceedings of the 7th International Symposium on Memory Management","Tucson, AZ, USA","2008","9781605581347","","https://doi.org/10.1145/1375634.1375647;http://dx.doi.org/10.1145/1375634.1375647","10.1145/1375634.1375647","As garbage collected languages become widely used, the quest for reducing collection overheads becomes essential. In this paper, we propose a compiler optimization called path specialization that shrinks the cost of memory barriers for a wide variety of garbage collectors including concurrent, incremental, and real-time collectors. Path specialization provides a non-trivial decrease in write-barrier overheads and a drastic reduction of read-barrier overheads. It is effective when used with collectors that go through various phases each employing a different barrier behavior, and is most effective for collectors that have an idle phase, in which no barrier activity is required. We have implemented path specialization in the Bartok compiler and runtime for C# and tested it with state-of-the-art concurrent and real-time collectors, demonstrating its efficacy.","c#, memory management, write barriers, garbage collection, read barriers","","ISMM '08"
"Conference Paper","White BS,McKee SA,Quinlan D","A Projection-Based Optimization Framework for Abstractions with Application to the Unstructured Mesh Domain","","2008","","","104–113","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd Annual International Conference on Supercomputing","Island of Kos, Greece","2008","9781605581583","","https://doi.org/10.1145/1375527.1375545;http://dx.doi.org/10.1145/1375527.1375545","10.1145/1375527.1375545","Computational scientists often must choose between the greater programming productivity of high-level abstractions, such as matrices and mesh entities, and the greater execution efficiency of low-level constructs. Performance is degraded when abstraction indirection introduces overhead and hinders compiler analysis. This can be overcome by targeting the semantics, rather than the implementation, of abstractions. Raising operators specified by a domain expert project an application from an implementation space to an abstraction space, where optimizations leverage domain semantics to complement conservative analyses. Raising operators define a domain-specific intermediate representation, which optimizations target for improved portability. Following optimization, transformed code is reified as a concrete implementation via lowering operators. We have developed a framework to implement this optimization strategy, which we use to introduce two domain-specific unstructured mesh optimizations. The first uses an inspector/executor approach to avoid costly traversals over a static mesh by memoizing the relatively few references required for mathematical computations. The executor phase accesses stored entities without incurring the indirections. The second optimization lowers object-based mesh access and iteration to a low-level implementation, which uses integer-based access and iteration.","high-level abstraction semantics, rose, unstructured mesh","","ICS '08"
"Conference Paper","Traeger A,Deras I,Zadok E","DARC: Dynamic Analysis of Root Causes of Latency Distributions","","2008","","","277–288","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems","Annapolis, MD, USA","2008","9781605580050","","https://doi.org/10.1145/1375457.1375489;http://dx.doi.org/10.1145/1375457.1375489","10.1145/1375457.1375489","OSprof is a versatile, portable, and efficient profiling methodology based on the analysis of latency distributions. Although OSprof has offers several unique benefits and has been used to uncover several interesting performance problems, the latency distributions that it provides must be analyzed manually. These latency distributions are presented as histograms and contain distinct groups of data, called peaks, that characterize the overall behavior of the running code. By automating the analysis process, we make it easier to take advantage of OSprof's unique features.We have developed the Dynamic Analysis of Root Causes system (DARC), which finds root cause paths in a running program's call-graph using runtime latency analysis. A root cause path is a call-path that starts at a given function and includes the largest latency contributors to a given peak. These paths are the main causes for the high-level behavior that is represented as a peak in an OSprof histogram. DARC performs PID and call-path filtering to reduce overheads and perturbations, and can handle recursive and indirect calls. DARC can analyze preemptive behavior and asynchronous call-paths, and can also resume its analysis from a previous state, which is useful when analyzing short-running programs or specific phases of a program's execution.We present DARC and show its usefulness by analyzing behaviors that were observed in several interesting scenarios. We also show that DARC has negligible elapsed time overheads for normal use cases.","root cause, dynamic instrumentation","","SIGMETRICS '08"
"Journal Article","Traeger A,Deras I,Zadok E","DARC: Dynamic Analysis of Root Causes of Latency Distributions","SIGMETRICS Perform. Eval. Rev.","2008","36","1","277–288","Association for Computing Machinery","New York, NY, USA","","","2008-06","","0163-5999","https://doi.org/10.1145/1384529.1375489;http://dx.doi.org/10.1145/1384529.1375489","10.1145/1384529.1375489","OSprof is a versatile, portable, and efficient profiling methodology based on the analysis of latency distributions. Although OSprof has offers several unique benefits and has been used to uncover several interesting performance problems, the latency distributions that it provides must be analyzed manually. These latency distributions are presented as histograms and contain distinct groups of data, called peaks, that characterize the overall behavior of the running code. By automating the analysis process, we make it easier to take advantage of OSprof's unique features.We have developed the Dynamic Analysis of Root Causes system (DARC), which finds root cause paths in a running program's call-graph using runtime latency analysis. A root cause path is a call-path that starts at a given function and includes the largest latency contributors to a given peak. These paths are the main causes for the high-level behavior that is represented as a peak in an OSprof histogram. DARC performs PID and call-path filtering to reduce overheads and perturbations, and can handle recursive and indirect calls. DARC can analyze preemptive behavior and asynchronous call-paths, and can also resume its analysis from a previous state, which is useful when analyzing short-running programs or specific phases of a program's execution.We present DARC and show its usefulness by analyzing behaviors that were observed in several interesting scenarios. We also show that DARC has negligible elapsed time overheads for normal use cases.","root cause, dynamic instrumentation","",""
"Journal Article","Ha S,Kim S,Lee C,Yi Y,Kwon S,Joo YP","PeaCE: A Hardware-Software Codesign Environment for Multimedia Embedded Systems","ACM Trans. Des. Autom. Electron. Syst.","2008","12","3","","Association for Computing Machinery","New York, NY, USA","","","2008-05","","1084-4309","https://doi.org/10.1145/1255456.1255461;http://dx.doi.org/10.1145/1255456.1255461","10.1145/1255456.1255461","Existent hardware-software (HW-SW) codesign tools mainly focus on HW-SW cosimulation to build a virtual prototyping environment that enables software design and system verification without need of making a hardware prototype. Not only HW-SW cosimulation, but also HW-SW codesign methodology involves system specification, functional simulation, design-space exploration, and hardware-software cosynthesis. The PeaCE codesign environment is the first full-fledged HW-SW codesign environment that provides seamless codesign flow from functional simulation to system synthesis. Targeting for multimedia applications with real-time constraints, PeaCE specifies the system behavior with a heterogeneous composition of three models of computation and utilizes features of the formal models maximally during the whole design process. It is also a reconfigurable framework in the sense that third-party design tools can be integrated to build a customized tool chain. Experiments with industry-strength examples prove the viability of the proposed technique.","embedded systems, model-based design, hardware-software cosimulation, Hardware-software codesign, design-space exploration","",""
"Journal Article","Pan Z,Eigenmann R","PEAK—a Fast and Effective Performance Tuning System via Compiler Optimization Orchestration","ACM Trans. Program. Lang. Syst.","2008","30","3","","Association for Computing Machinery","New York, NY, USA","","","2008-05","","0164-0925","https://doi.org/10.1145/1353445.1353451;http://dx.doi.org/10.1145/1353445.1353451","10.1145/1353445.1353451","Compile-time optimizations generally improve program performance. Nevertheless, degradations caused by individual compiler optimization techniques are to be expected. Feedback-directed optimization orchestration systems generate optimized code versions under a series of optimization combinations, evaluate their performance, and search for the best version. One challenge to such systems is to tune program performance quickly in an exponential search space. Another challenge is to achieve high program performance, considering that optimizations interact. Aiming at these two goals, this article presents an automated performance tuning system, PEAK, which searches for the best compiler optimization combinations for the important code sections in a program. The major contributions made in this work are as follows: (1) An algorithm called Combined Elimination (CE) is developed to explore the optimization space quickly and effectively; (2) Three fast and accurate rating methods are designed to evaluate the performance of an optimized code section based on a partial execution of the program; (3) An algorithm is developed to identify important code sections as candidates for performance tuning by trading off tuning speed and tuned program performance; and (4) A set of compiler tools are implemented to automate optimization orchestration. Orchestrating optimization options in SUN Forte compilers at the whole-program level, our CE algorithm improves performance by 10.8% over the SPEC CPU2000 FP baseline setting, compared to 5.6% improved by manual tuning. Orchestrating GCC O3 optimizations, CE improves performance by 12% over O3, the highest optimization level. Applying the rating methods, PEAK reduces tuning time from 2.19 hours to 5.85 minutes on average, while achieving equal or better program performance.","Performance tuning, dynamic compilation, optimization orchestration","",""
"Conference Paper","Miyake T,Higo Y,Inoue K","A Metric-Based Approach for Reconstructing Methods in Object-Oriented Systems","","2008","","","53–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th International Workshop on Software Quality","Leipzig, Germany","2008","9781605580234","","https://doi.org/10.1145/1370099.1370112;http://dx.doi.org/10.1145/1370099.1370112","10.1145/1370099.1370112","Refactoring is an important activity to improve software quality, which tends to become worse through repetitive bug fixes and function additions. Unfortunately, it is difficult to perform appropriate refactorings because a refactoring needs certain costs, and its effects should be greater than the costs. This paper describes an approach for appropriate refactorings. The approach identifies spots to be refactored and it suggests how they should be improved. Moreover, the approach estimates the effects of the refactorings. The approach requires a lightweight source code analysis for measuring several metrics, so that it can be applied to middle- or large-scale software system. The approach can make the refactoring process more effective and efficient one.","software maintenance, refactoring, source code analysis","","WoSQ '08"
"Conference Paper","Deissenboeck F,Hummel B,Jürgens E,Schätz B,Wagner S,Girard JF,Teuchert S","Clone Detection in Automotive Model-Based Development","","2008","","","603–612","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368172;http://dx.doi.org/10.1145/1368088.1368172","10.1145/1368088.1368172","Model-based development is becoming an increasingly common development methodology. In important domains like embedded systems already major parts of the code are generated from models specified with domain-specific modelling languages. Hence, such models are nowadays an integral part of the software development and maintenance process and therefore have a major economic and strategic value for the software-developing organisations. Nevertheless almost no work has been done on a quality defect that is known to seriously hamper maintenance productivity in classic code-based development: Cloning. This paper presents an approach for the automatic detection of clones in large models as they are used in model-based development of control systems. The approach is based on graph theory and hence can be applied to most graphical data-flow languages. An industrial case study demonstrates the applicability of our approach for the detection of clones in Matlab/Simulink models that are widely used in model-based development of embedded systems in the automotive domain.","model clone, clone detection, data-flow, matlab/simulink","","ICSE '08"
"Conference Paper","Chang HF,Mockus A","Evaluation of Source Code Copy Detection Methods on Freebsd","","2008","","","61–66","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 International Working Conference on Mining Software Repositories","Leipzig, Germany","2008","9781605580241","","https://doi.org/10.1145/1370750.1370766;http://dx.doi.org/10.1145/1370750.1370766","10.1145/1370750.1370766","Studies have shown that substantial code reuse is common in open source and in commercial projects. However, the precise extent of reuse and its impact on productivity and quality are not well investigated in the open source context. Previously, we have introduced a simple-to-use method that needs only a set of file pathnames to identifies directories that share filenames and partially validated its performance on a set of closed-source projects. To evaluate this method and to improve reuse detection at the file level, we apply it and four additional file copy detection methods that utilize the underlying content of multiple versions of the source code on the FreeBSD project. The evaluation quantified unique advantages of each method and showed that the filename method detected roughly half of all reuse cases. We are still faced with a challenge to scale the content based methods to large repositories containing all versions of open source files.","cloning, clone detection, code copying, open source, version control","","MSR '08"
"Conference Paper","Duala-Ekoko E,Robillard MP","Clonetracker: Tool Support for Code Clone Management","","2008","","","843–846","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368218;http://dx.doi.org/10.1145/1368088.1368218","10.1145/1368088.1368218","Code clones are generally considered to be an obstacle to software maintenance. Research has provided evidence that it may not always be practical, feasible, or cost-effective to eliminate certain clone groups through refactoring. This paper describes CloneTracker, an Eclipse plug-in that provides support for tracking code clones in evolving software. With CloneTracker, developers can specify clone groups they wish to track, and the tool will automatically generate a clone model that is robust to changes to the source code, and can be shared with other collaborators of the project. When future modifications intersect with tracked clones, CloneTracker will notify the developer, provide support to consistently apply changes to a corresponding clone region, and provide support for updating the clone model. CloneTracker complements existing techniques by providing support for reusing knowledge about the location of clones in source code, and support for keeping track of clones when refactoring is not desirable.","software maintenance, code clone, source code analysis, simultaneous editing, refactoring","","ICSE '08"
"Conference Paper","Gabel M,Jiang L,Su Z","Scalable Detection of Semantic Clones","","2008","","","321–330","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368132;http://dx.doi.org/10.1145/1368088.1368132","10.1145/1368088.1368132","Several techniques have been developed for identifying similar code fragments in programs. These similar fragments, referred to as code clones, can be used to identify redundant code, locate bugs, or gain insight into program design. Existing scalable approaches to clone detection are limited to finding program fragments that are similar only in their contiguous syntax. Other, semantics-based approaches are more resilient to differences in syntax, such as reordered statements, related statements interleaved with other unrelated statements, or the use of semantically equivalent control structures. However, none of these techniques have scaled to real world code bases. These approaches capture semantic information from Program Dependence Graphs (PDGs), program representations that encode data and control dependencies between statements and predicates. Our definition of a code clone is also based on this representation: we consider program fragments with isomorphic PDGs to be clones.In this paper, we present the first scalable clone detection algorithm based on this definition of semantic clones. Our insight is the reduction of the difficult graph similarity problem to a simpler tree similarity problem by mapping carefully selected PDG subgraphs to their related structured syntax. We efficiently solve the tree similarity problem to create a scalable analysis. We have implemented this algorithm in a practical tool and performed evaluations on several million-line open source projects, including the Linux kernel. Compared with previous approaches, our tool locates significantly more clones, which are often more semantically interesting than simple copied and pasted code fragments.","program dependence graph, software maintenance, refactoring, clone detection","","ICSE '08"
"Conference Paper","Kästner C,Apel S,Kuhlemann M","Granularity in Software Product Lines","","2008","","","311–320","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368131;http://dx.doi.org/10.1145/1368088.1368131","10.1145/1368088.1368131","Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show how CIDE simplifies SPL development compared to traditional approaches.","software product lines, virtual separation of concerns, feature refactoring, ide","","ICSE '08"
"Conference Paper","Williams CC,Spacco JW","Branching and Merging in the Repository","","2008","","","19–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 International Working Conference on Mining Software Repositories","Leipzig, Germany","2008","9781605580241","","https://doi.org/10.1145/1370750.1370754;http://dx.doi.org/10.1145/1370750.1370754","10.1145/1370750.1370754","Two of the most complex operations version control software allows a user to perform are branching and merging. Branching provides the user the ability to create a copy of the source code to allow changes to be stored in version control but outside of the trunk. Merging provides the user the ability to copy changes from a branch to the trunk. Performing a merge can be a tedious operation and one that may be error prone. In this paper, we compare file revisions found on branches with those found on the trunk to determine when a change that is applied to a branch is moved to the trunk. This will allow us to study how developers use merges and to determine if merges are in fact more error prone than other commits.","change, subversion, diff, mining, repository","","MSR '08"
"Conference Paper","Schäfer T,Jonas J,Mezini M","Mining Framework Usage Changes from Instantiation Code","","2008","","","471–480","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368153;http://dx.doi.org/10.1145/1368088.1368153","10.1145/1368088.1368153","Framework evolution may break existing users, which need to be migrated to the new framework version. This is a tedious and error-prone process that benefits from automation. Existing approaches compare two versions of the framework code in order to find changes caused by refactorings. However, other kinds of changes exist, which are relevant for the migration. In this paper, we propose to mine framework usage change rules from already ported instantiations, the latter being applications build on top of the framework, or test cases maintained by the framework developers. Our evaluation shows that our approach finds usage changes not only caused by refactorings, but also by conceptual changes within the framework. Further, it copes well with some issues that plague tools focusing on finding refactorings such as deprecated program elements or multiple changes applied to a single program element.","migration, framework comprehension, evolution","","ICSE '08"
"Conference Paper","Scaffidi C,Myers B,Shaw M","Topes: Reusable Abstractions for Validating Data","","2008","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368090;http://dx.doi.org/10.1145/1368088.1368090","10.1145/1368088.1368090","Programmers often omit input validation when inputs can appear in many different formats or when validation criteria cannot be precisely specified. To enable validation in these situations, we present a new technique that puts valid inputs into a consistent format and that identifies ""questionable"" inputs which might be valid or invalid, so that these values can be double-checked by a person or a program. Our technique relies on the concept of a ""tope"", which is an application-independent abstraction describing how to recognize and transform values in a category of data. We present our definition of topes and describe a development environment that supports the implementation and use of topes. Experiments with web application and spreadsheet data indicate that using our technique improves the accuracy and reusability of validation code and also improves the effectiveness of subsequent data cleaning such as duplicate identification.","abstraction, data, validation","","ICSE '08"
"Conference Paper","Dagenais B,Robillard MP","Recommending Adaptive Changes for Framework Evolution","","2008","","","481–490","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368154;http://dx.doi.org/10.1145/1368088.1368154","10.1145/1368088.1368154","In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework adapts to its own changes. In a study of the evolution of the Eclipse JDT framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision, and detected non-trivial changes typically undiscovered by current refactoring detection techniques.","historical study, partial program analysis, recommendation system, adaptive changes, software evolution, origin analysis, mining software repositories, framework","","ICSE '08"
"Conference Paper","Murphy-Hill E,Black AP","Breaking the Barriers to Successful Refactoring: Observations and Tools for Extract Method","","2008","","","421–430","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368146;http://dx.doi.org/10.1145/1368088.1368146","10.1145/1368088.1368146","Refactoring is the process of changing the structure of code without changing its behavior. Refactoring can be semi-automated with tools, which should make it easier for programmers to refactor quickly and correctly. However, we have observed that many tools do a poor job of communicating errors triggered by the refactoring process and that programmers using them sometimes refactor slowly, conservatively, and incorrectly. In this paper we characterize problems with current refactoring tools, demonstrate three new tools to assist in refactoring, and report on a user study that compares these new tools against existing tools. The results of the study show that speed, accuracy, and user satisfaction can be significantly increased. From the new tools we induce a set of usability recommendations that we hope will help inspire a new generation of programmer-friendly refactoring tools.","usability, refactoring, tools, environments","","ICSE '08"
"Conference Paper","Reiss SP","Tracking Source Locations","","2008","","","11–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th International Conference on Software Engineering","Leipzig, Germany","2008","9781605580791","","https://doi.org/10.1145/1368088.1368091;http://dx.doi.org/10.1145/1368088.1368091","10.1145/1368088.1368091","Many programming tools require information to be associated with source locations. Current tools do this in different ways with different degrees of effectiveness. This paper is an investigation into the various approaches to maintaining source locations. It is based on an experiment that attempts to track a variety of locations over the evolution of a source file. The results demonstrate that relatively simple techniques can be very effective.","source lines, software evolution, tool support","","ICSE '08"
"Conference Paper","Basu K,Mishra P","A Novel Test-Data Compression Technique Using Application-Aware Bitmask and Dictionary Selection Methods","","2008","","","83–88","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th ACM Great Lakes Symposium on VLSI","Orlando, Florida, USA","2008","9781595939999","","https://doi.org/10.1145/1366110.1366132;http://dx.doi.org/10.1145/1366110.1366132","10.1145/1366110.1366132","Higher circuit densities in System-on-Chip (SOC) designs have led to enhancement in the test data volume. Larger test data size demands not only greater memory requirements, but also an increase in the testing time. Test data compression addresses this problem by reducing the test data volume without affecting the overall system performance. This paper proposes a novel test data compression technique using bitmasks which provides a significant enhancement in the compression efficiency without introducing any additional decompression penalty. The major contributions of this paper are as follows: i) it develops an efficient bitmask selection technique for test data in order to create maximum matching patterns; ii) it develops an efficient dictionary selection method which takes into account the speculated results of compressed codes and iii) it proposes a suitable code compression technique using dictionary and bitmask based code compression that can reduce the memory and time requirements. We have used our algorithm on various test data sets and compared our results with other existing test compression techniques. Our algorithm outperforms the best known existing compression technique up to 30%, giving a best possible compression of 92.2%.","decompression, test data, compression","","GLSVLSI '08"
"Conference Paper","Ishio T,Date H,Miyake T,Inoue K","Mining Application-Specific Coding Patterns for Software Maintenance","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 AOSD Workshop on Linking Aspect Technology and Evolution","Brussels, Belgium","2008","9781605581477","","https://doi.org/10.1145/1404953.1404956;http://dx.doi.org/10.1145/1404953.1404956","10.1145/1404953.1404956","A crosscutting concern is often implemented based on a coding pattern, or a particular sequence of method calls and control statements. We have applied a sequential pattern mining algorithm to capture coding patterns in Java programs. We have manually investigated the resultant patterns that involve both crosscutting concerns and implementation idioms. This paper discusses the detail of our pattern mining algorithm and reports detected crosscutting concerns.","","","LATE '08"
"Conference Paper","Bhatti MU,Ducasse S","Mining and Classification of Diverse Crosscutting Concerns","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 AOSD Workshop on Linking Aspect Technology and Evolution","Brussels, Belgium","2008","9781605581477","","https://doi.org/10.1145/1404953.1404955;http://dx.doi.org/10.1145/1404953.1404955","10.1145/1404953.1404955","Crosscutting concerns appear in software system due to the inherent inadequacy of OOP mechanisms to capture them in suitable encapsulating units. This results in scattered and tangled code. One more form of scattering and tangling may result from the absence of OOP abstractions for domain entities of a software. These non-encapsulated domain entities end up scattered and tangled, appearing as crosscutting concerns in code. Aspect mining techniques automate the task of search for possible aspects in the code and falsely attribute all the crosscutting code to aspects even when these scattered concerns point to the absence of a domain abstraction. This paper discusses the application of aspect mining in the presence crosscutting code originating from the absence of aspects and OOP abstractions. A roadmap of a possible solution is provided to distinguish these two types of code scattering.","aspect mining, crosscutting concerns, reverse engineering","","LATE '08"
"Conference Paper","Adams B,Van Rompaey B,Gibbs C,Coady Y","Aspect Mining in the Presence of the C Preprocessor","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 AOSD Workshop on Linking Aspect Technology and Evolution","Brussels, Belgium","2008","9781605581477","","https://doi.org/10.1145/1404953.1404954;http://dx.doi.org/10.1145/1404953.1404954","10.1145/1404953.1404954","In systems software, the C preprocessor is heavily used to manage variability and improve efficiency. It is the primary tool to model crosscutting concerns in a very fine-grained way, but leads to extremely tangled and scattered preprocessor code. In this paper, we explore the process of aspect mining and extraction in the context of preprocessor-driven systems. Our aim is to identify both opportunities (extracting conditional compilation into advice) and pitfalls (mining on unpreprocessed code) in migrating preprocessor code to aspects. We distill five trade-offs which give a first impression about the usefulness of replacing the preprocessor by aspects. Preprocessor-driven systems prove to be a real challenge for aspect mining, but they could become on the other hand one of the most promising applications of AOP.","","","LATE '08"
"Conference Paper","Reynolds A,Fiuczynski ME,Grimm R","On the Feasibility of an AOSD Approach to Linux Kernel Extensions","","2008","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 AOSD Workshop on Aspects, Components, and Patterns for Infrastructure Software","Brussels, Belgium","2008","9781605581422","","https://doi.org/10.1145/1404891.1404899;http://dx.doi.org/10.1145/1404891.1404899","10.1145/1404891.1404899","In previous work, we presented a domain-specific version of C, called C4, which was used for capturing extensions to the Linux 2.6 kernel using AOSD techniques as an alternative to the conventional patching approach [10, 19]. The focus of that work was on introducing new extensions represented as aspects in system software such as the Linux kernel with a focus on readablility, compatibility, performance, and the preservation of existing development workflows. However, other AOSD researchers (e.g. Lohmann et al. [8]) state that ""... Linux, as a monolithic system, provides a low number of join-points for aspects and that those available were semantically ambiguous."" This worrisome statement motivated us to study the feasibility of applying AOSD techniques to refactor existing Linux kernel extensions. To gain insight we analyzed the AOSD-ness of a large number of configurable options available in the Linux kernel and evaluated whether they could be converted into aspects---and for the AOSD fan the preliminary results are promising.","","","ACP4IS '08"
"Conference Paper","Conti M,Di Pietro R,Mancini LV,Mei A","Emergent Properties: Detection of the Node-Capture Attack in Mobile Wireless Sensor Networks","","2008","","","214–219","Association for Computing Machinery","New York, NY, USA","Proceedings of the First ACM Conference on Wireless Network Security","Alexandria, VA, USA","2008","9781595938145","","https://doi.org/10.1145/1352533.1352568;http://dx.doi.org/10.1145/1352533.1352568","10.1145/1352533.1352568","One of the most vexing problems in wireless sensor network security is the node capture attack. An adversary can capture a node from the network as the first step for further different types of attacks. For example, the adversary can collect all the cryptographic material stored in the node. Also, the node can be reprogrammed and re-deployed in the network in order to perform malicious activities. To the best of our knowledge no distributed solution has been proposed to detect a node capture in a mobile wireless sensor network. In this paper we propose an efficient and distributed solution to this problem leveraging emergent properties of mobile wireless sensor networks. In particular, we introduce two solutions: SDD, that does not require explicit information exchange between the nodes during the local detection, and CCD, a more sophisticated protocol that uses local node cooperation in addition to mobility to greatly improve performance. We also introduce a benchmark to compare these solutions with. Experimental results demonstrate the feasibility of our proposal. For instance, while the benchmark requires about 9,000 seconds to detect node captures, CDD requires less than 2,000 seconds. These results support our intuition that node mobility, in conjunction with a limited amount of local cooperation, can be used to detect emergent global properties.","distributed protocol, node capture attack detection, wireless sensor network security, resilience, efficiency, node revocation","","WiSec '08"
"Journal Article","Van Zee FG,Bientinesi P,Low TM,Geijn RA","Scalable Parallelization of FLAME Code via the Workqueuing Model","ACM Trans. Math. Softw.","2008","34","2","","Association for Computing Machinery","New York, NY, USA","","","2008-03","","0098-3500","https://doi.org/10.1145/1326548.1326552;http://dx.doi.org/10.1145/1326548.1326552","10.1145/1326548.1326552","We discuss the OpenMP parallelization of linear algebra algorithms that are coded using the Formal Linear Algebra Methods Environment (FLAME) API. This API expresses algorithms at a higher level of abstraction, avoids the use loop and array indices, and represents these algorithms as they are formally derived and presented. We report on two implementations of the workqueuing model, neither of which requires the use of explicit indices to specify parallelism. The first implementation uses the experimental taskq pragma, which may influence the adoption of a similar construct into OpenMP 3.0. The second workqueuing implementation is domain-specific to FLAME but allows us to illustrate the benefits of sorting tasks according to their computational cost prior to parallel execution. In addition, we discuss how scalable parallelization of dense linear algebra algorithms via OpenMP will require a two-dimensional partitioning of operands much like a 2D data distribution is needed on distributed memory architectures. We illustrate the issues and solutions by discussing the parallelization of the symmetric rank-k update and report impressive performance on an SGI system with 14 Itanium2 processors.","workqueuing, FLAME, scalability, parallel, SMP, OpenMP","",""
"Conference Paper","Mascarenhas F,Ierusalimschy R","Efficient Compilation of Lua for the CLR","","2008","","","217–221","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2008 ACM Symposium on Applied Computing","Fortaleza, Ceara, Brazil","2008","9781595937537","","https://doi.org/10.1145/1363686.1363743;http://dx.doi.org/10.1145/1363686.1363743","10.1145/1363686.1363743","Microsoft's Common Language Runtime offers a target environment for compiler writers that provides a managed execution environment and type system, garbage collection, access to OS services, multithreading, and a Just-In-Time compiler. But the CLR uses a statically typed intermediate language, which is a problem for efficient compilation of dynamically typed languages in general, and the Lua language in particular.This paper presents a way to implement a Lua compiler for the CLR that generates efficient code. The code this compiler generates outperforms the same code executed by the Lua interpreter and similar code generated by Microsoft's IronPython compiler. It approaches the performance of Lua code compiled to native code by LuaJIT.","clr, dynamic languages, compilers, lua","","SAC '08"
"Conference Paper","Kleanthous M,Sazeides Y","CATCH: A Mechanism for Dynamically Detecting Cache-Content-Duplication and Its Application to Instruction Caches","","2008","","","1426–1431","Association for Computing Machinery","New York, NY, USA","Proceedings of the Conference on Design, Automation and Test in Europe","Munich, Germany","2008","9783981080131","","https://doi.org/10.1145/1403375.1403720;http://dx.doi.org/10.1145/1403375.1403720","10.1145/1403375.1403720","Cache-Content-Duplication (CCD) occurs when there is a miss for a block in a cache and the entire content of the missed block is already in the cache in a block with a different tag. Caches aware of content-duplication can have lower miss rates by allowing only blocks with unique content to enter a cache. This work examines the potential of CCD for instruction caches. We show that CCD is a frequent phenomenon and that an idealized duplication-detection mechanism for instruction caches has the potential to increase performance of an out-of-order processor, with a 2-way eight instruction per block 16KB instruction cache, often by more than 5% and up to 20%. This work also proposes CATCH, a hardware based mechanism for dynamically detecting CCD. Experimental results for an out-of-order processor show that a CATCH with a 2.32KB cost usually captures 60% or more of the CCD's idealized potential.","","","DATE '08"
"Conference Paper","Bruening D,Kiriansky V","Process-Shared and Persistent Code Caches","","2008","","","61–70","Association for Computing Machinery","New York, NY, USA","Proceedings of the Fourth ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments","Seattle, WA, USA","2008","9781595937964","","https://doi.org/10.1145/1346256.1346265;http://dx.doi.org/10.1145/1346256.1346265","10.1145/1346256.1346265","Software code caches are increasingly being used to amortizethe runtime overhead of tools such as dynamic optimizers, simulators, and instrumentation engines. The additional memory consumed by these caches, along with the data structures used to manage them, limits the scalability of dynamic tool deployment. Inter-process sharing of code caches significantly improves the ability to efficiently apply code caching tools to many processes simultaneously.In this paper, we present a method of code cache sharing among processes for dynamic tools operating on native applications. Our design also supports code cache persistence for improved cold code execution in short-lived processes or long initialization sequences. Sharing raises security concerns, and we show how to achieve sharing without risk of privilege escalation and with read-only code caches and associated data structures. We evaluate process-shared and persisted code caches implemented in the DynamoRIO industrial-strength dynamic instrumentation engine, where we achieve a two-thirds reduction in both memory usage and startup time.","software code cache, tool scalability, dynamic instrumentation, binary translation","","VEE '08"
"Journal Article","Yu L,Ramaswamy S","Improving Modularity by Refactoring Code Clones: A Feasibility Study on Linux","SIGSOFT Softw. Eng. Notes","2008","33","2","","Association for Computing Machinery","New York, NY, USA","","","2008-03","","0163-5948","https://doi.org/10.1145/1350802.1350816;http://dx.doi.org/10.1145/1350802.1350816","10.1145/1350802.1350816","Modularity is an important principle of software design. It is directly associated with software understandability, maintainability, and reusability. However, as software systems evolve, old code segments are modified / removed and new code segments are added, the original modular design of the program might be distorted. One of the factors that can affect the modularity of the system is the introduction of code clones --- a portion of source code that is identical or similar to another --- in the software evolution process. This paper applies clone detection techniques to study the modularity of Linux. The code clones are first identified using an automatic tool. Then each clone set is analyzed by a domain expert to classify it into one of the three clone concern categories: singular concern, crosscutting concern, and partial concern. Different approaches to dealing with these different categories of code clones are suggested in order to improve modularity.","modularity, operating systems, refactoring, code clone, Linux","",""
"Conference Paper","Shen K,Zhong M,Dwarkadas S,Li C,Stewart C,Zhang X","Hardware Counter Driven On-the-Fly Request Signatures","","2008","","","189–200","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems","Seattle, WA, USA","2008","9781595939586","","https://doi.org/10.1145/1346281.1346306;http://dx.doi.org/10.1145/1346281.1346306","10.1145/1346281.1346306","Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g., CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly request identification and property inference allow guided operating system adaptation at request granularity (e.g., resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7%, 3%, 20%, and 41% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82% errors). Its use for resource-aware request scheduling results in a 15-70% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.","hardware counter, server system, operating system adaptation, anomaly detection, request classification","","ASPLOS XIII"
"Journal Article","Shen K,Zhong M,Dwarkadas S,Li C,Stewart C,Zhang X","Hardware Counter Driven On-the-Fly Request Signatures","SIGARCH Comput. Archit. News","2008","36","1","189–200","Association for Computing Machinery","New York, NY, USA","","","2008-03","","0163-5964","https://doi.org/10.1145/1353534.1346306;http://dx.doi.org/10.1145/1353534.1346306","10.1145/1353534.1346306","Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g., CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly request identification and property inference allow guided operating system adaptation at request granularity (e.g., resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7%, 3%, 20%, and 41% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82% errors). Its use for resource-aware request scheduling results in a 15-70% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.","request classification, anomaly detection, server system, hardware counter, operating system adaptation","",""
"Journal Article","Shen K,Zhong M,Dwarkadas S,Li C,Stewart C,Zhang X","Hardware Counter Driven On-the-Fly Request Signatures","SIGOPS Oper. Syst. Rev.","2008","42","2","189–200","Association for Computing Machinery","New York, NY, USA","","","2008-03","","0163-5980","https://doi.org/10.1145/1353535.1346306;http://dx.doi.org/10.1145/1353535.1346306","10.1145/1353535.1346306","Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g., CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly request identification and property inference allow guided operating system adaptation at request granularity (e.g., resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7%, 3%, 20%, and 41% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82% errors). Its use for resource-aware request scheduling results in a 15-70% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.","server system, hardware counter, request classification, operating system adaptation, anomaly detection","",""
"Journal Article","Shen K,Zhong M,Dwarkadas S,Li C,Stewart C,Zhang X","Hardware Counter Driven On-the-Fly Request Signatures","SIGPLAN Not.","2008","43","3","189–200","Association for Computing Machinery","New York, NY, USA","","","2008-03","","0362-1340","https://doi.org/10.1145/1353536.1346306;http://dx.doi.org/10.1145/1353536.1346306","10.1145/1353536.1346306","Today's processors provide a rich source of statistical informationon application execution through hardware counters. In this paper, we explore the utilization of these statistics as request signaturesin server applications for identifying requests and inferring high-level request properties (e.g., CPU and I/O resource needs). Our key finding is that effective request signatures may be constructed using a small amount of hardware statistics while the request is still in an early stage of its execution. Such on-the-fly request identification and property inference allow guided operating system adaptation at request granularity (e.g., resource-aware request scheduling and on-the-fly request classification). We address the challenges of selecting hardware counter metrics for signature construction and providing necessary operating system support for per-request statistics management. Our implementation in the Linux 2.6.10 kernel suggests that our approach requires low overhead suitable for runtime deployment. Our on-the-fly request resource consumption inference (averaging 7%, 3%, 20%, and 41% prediction errors for four server workloads, TPC-C, TPC-H, J2EE-based RUBiS, and a trace-driven index search, respectively) is much more accurate than the online running-average based prediction (73-82% errors). Its use for resource-aware request scheduling results in a 15-70% response time reduction for three CPU-bound applications. Its use for on-the-fly request classification and anomaly detection exhibits high accuracy for the TPC-H workload with synthetically generated anomalous requests following a typical SQL-injection attack pattern.","request classification, operating system adaptation, anomaly detection, server system, hardware counter","",""
"Journal Article","","AspectC2C: A Symmetric Aspect Extension to the C Language","SIGPLAN Not.","2008","43","2","25–32","Association for Computing Machinery","New York, NY, USA","","","2008-02","","0362-1340","https://doi.org/10.1145/1361213.1361217;http://dx.doi.org/10.1145/1361213.1361217","10.1145/1361213.1361217","By separating crosscutting concerns into modules, aspect-oriented programming (AOP) can greatly improve the maintainability, understandability and reusability of software. However, the asymmetric paradigm adopted by most AOP extensions could bring crosscutting concerns into the aspect code and thus limit the reusability of aspects.Symmetric paradigms have been proposed to alleviate such limitations, but few extensions on such paradigm target at non-object-oriented languages, such as C. In this paper, we propose a symmetric aspect extension to the C language, called AspectC2C, and discuss implementation issues and benefits of this new extension comparing to the asymmetric ones.","C language, aspect extension, symmetric paradigm, aspect-oriented programming","",""
"Conference Paper","Nita M,Grossman D,Chambers C","A Theory of Platform-Dependent Low-Level Software","","2008","","","209–220","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages","San Francisco, California, USA","2008","9781595936899","","https://doi.org/10.1145/1328438.1328465;http://dx.doi.org/10.1145/1328438.1328465","10.1145/1328438.1328465","The C language definition leaves the sizes and layouts of types partially unspecified. When a C program makes assumptions about type layout, its semantics is defined only on platforms (C compilers and the underlying hardware) on which those assumptions hold. Previous work on formalizing C-like languages has ignored this issue, either by assuming that programs do not make such assumptions or by assuming that all valid programs target only one platform. In the latter case, the platform's choices are hard-wired in the language semantics.In this paper, we present a practically-motivated model for a C-like language in which the memory layouts of types are left largely unspecified. The dynamic semantics is parameterized by a platform's layout policy and makes manifest the consequence of platform-dependent (i.e., unspecified) steps. A type-and-effect system produces a layout constraint: a logic formula encoding layout conditions under which the program is memory-safe. We prove that if a program type-checks, it is memory-safe on all platforms satisfying its constraint.Based on our theory, we have implemented a tool that discovers unportable layout assumptions in C programs. Our approach should generalize to other kinds of platform-dependent assumptions.","type safety, low-level software, casts, portability","","POPL '08"
"Journal Article","Nita M,Grossman D,Chambers C","A Theory of Platform-Dependent Low-Level Software","SIGPLAN Not.","2008","43","1","209–220","Association for Computing Machinery","New York, NY, USA","","","2008-01","","0362-1340","https://doi.org/10.1145/1328897.1328465;http://dx.doi.org/10.1145/1328897.1328465","10.1145/1328897.1328465","The C language definition leaves the sizes and layouts of types partially unspecified. When a C program makes assumptions about type layout, its semantics is defined only on platforms (C compilers and the underlying hardware) on which those assumptions hold. Previous work on formalizing C-like languages has ignored this issue, either by assuming that programs do not make such assumptions or by assuming that all valid programs target only one platform. In the latter case, the platform's choices are hard-wired in the language semantics.In this paper, we present a practically-motivated model for a C-like language in which the memory layouts of types are left largely unspecified. The dynamic semantics is parameterized by a platform's layout policy and makes manifest the consequence of platform-dependent (i.e., unspecified) steps. A type-and-effect system produces a layout constraint: a logic formula encoding layout conditions under which the program is memory-safe. We prove that if a program type-checks, it is memory-safe on all platforms satisfying its constraint.Based on our theory, we have implemented a tool that discovers unportable layout assumptions in C programs. Our approach should generalize to other kinds of platform-dependent assumptions.","portability, low-level software, casts, type safety","",""
"Journal Article","Cordeiro L,Mar C,Valentin E,Cruz F,Patrick D,Barreto R,Lucena V","An Agile Development Methodology Applied to Embedded Control Software under Stringent Hardware Constraints","SIGSOFT Softw. Eng. Notes","2008","33","1","","Association for Computing Machinery","New York, NY, USA","","","2008-01","","0163-5948","https://doi.org/10.1145/1344452.1344459;http://dx.doi.org/10.1145/1344452.1344459","10.1145/1344452.1344459","In recent years, discrete control systems play an important role in the development and advancement of modern civilization and technology. Practically every aspect of our life is affected by some type of control systems. This kind of system maybe classified as an embedded real-time system and requires rigorous methodologies to develop the software that is under stringent hardware constraints. Therefore, the proposed development methodology adapts agile principles and patterns in order to build embedded control systems focusing on the issues related to the system's constraints and safety. Strong unit testing is the foundation of the proposed methodology for ensuring timeliness and correctness. Moreover, platform-based design approach is used to balance costs and time-to-market in view of performance and functionality constraints. We conclude that the proposed methodology reduces significantly the design time and cost as well as leads to better software modularity and reliability.","organizational patterns, real-time software, embedded agile development, health care, platform-based design, agile methodologies","",""
"Journal Article","Kriens P","How OSGi Changed My Life: The Promises of the Lego Hypothesis Have yet to Materialize Fully, but They Remain a Goal Worth Pursuing","Queue","2008","6","1","44–51","Association for Computing Machinery","New York, NY, USA","","","2008-01","","1542-7730","https://doi.org/10.1145/1348583.1348594;http://dx.doi.org/10.1145/1348583.1348594","10.1145/1348583.1348594","In the early 1980s I discovered OOP (object-oriented programming) and fell in love with it, head over heels. As usual, this kind of love meant convincing management to invest in this new technology, and most important of all, send me to cool conferences. So I pitched the technology to my manager. I sketched him the rosy future, how one day we would create applications from ready-made classes. We would get those classes from a repository, put them together, and voila, a new application would be born. Today we take objects more or less for granted, but if I am honest, the pitch I gave to my manager in 1985 never really materialized. The reuse of objects never achieved the levels foreseen by people such as Brad Cox with his software-IC model, and many others, including myself. Still, this Lego hypothesis remains a grail worth pursuing.","","",""
"Journal Article","Marin M,Van Deursen A,Moonen L","Identifying Crosscutting Concerns Using Fan-In Analysis","ACM Trans. Softw. Eng. Methodol.","2007","17","1","","Association for Computing Machinery","New York, NY, USA","","","2007-12","","1049-331X","https://doi.org/10.1145/1314493.1314496;http://dx.doi.org/10.1145/1314493.1314496","10.1145/1314493.1314496","Aspect mining is a reverse engineering process that aims at finding crosscutting concerns in existing systems. This article proposes an aspect mining approach based on determining methods that are called from many different places, and hence have a high fan-in, which can be seen as a symptom of crosscutting functionality. The approach is semiautomatic, and consists of three steps: metric calculation, method filtering, and call site analysis. Carrying out these steps is an interactive process supported by an Eclipse plug-in called FINT. Fan-in analysis has been applied to three open source Java systems, totaling around 200,000 lines of code. The most interesting concerns identified are discussed in detail, which includes several concerns not previously discussed in the aspect-oriented literature. The results show that a significant number of crosscutting concerns can be recognized using fan-in analysis, and each of the three steps can be supported by tools.","crosscutting concerns, fan-in metric, Aspect-oriented programming, reverse engineering","",""
"Conference Paper","Thummalapenta S,Xie T","Parseweb: A Programmer Assistant for Reusing Open Source Code on the Web","","2007","","","204–213","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering","Atlanta, Georgia, USA","2007","9781595938824","","https://doi.org/10.1145/1321631.1321663;http://dx.doi.org/10.1145/1321631.1321663","10.1145/1321631.1321663","Programmers commonly reuse existing frameworks or libraries to reduce software development efforts. One common problem in reusing the existing frameworks or libraries is that the programmers know what type of object that they need, but do not know how to get that object with a specific method sequence. To help programmers to address this issue, we have developed an approach that takes queries of the form ""Source object type → Destination object type"" as input, and suggests relevant method-invocation sequences that can serve as solutions that yield the destination object from the source object given in the query. Our approach interacts with a code search engine (CSE) to gather relevant code samples and performs static analysis over the gathered samples to extract required sequences. As code samples are collected on demand through CSE, our approach is not limited to queries of any specific set of frameworks or libraries. We have implemented our approach with a tool called PARSEWeb, and conducted four different evaluations to show that our approach is effective in addressing programmer's queries. We also show that PARSEWeb performs better than existing related tools: Prospector and Strathcona","ranking code samples, code reuse, code examples, code search engine","","ASE '07"
"Conference Paper","Liu D,Marcus A,Poshyvanyk D,Rajlich V","Feature Location via Information Retrieval Based Filtering of a Single Scenario Execution Trace","","2007","","","234–243","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering","Atlanta, Georgia, USA","2007","9781595938824","","https://doi.org/10.1145/1321631.1321667;http://dx.doi.org/10.1145/1321631.1321667","10.1145/1321631.1321667","The paper presents a semi-automated technique for feature location in source code. The technique is based on combining information from two different sources: an execution trace, on one hand and the comments and identifiers from the source code, on the other hand. Users execute a single partial scenario, which exercises the desired feature and all executed methods are identified based on the collected trace. The source code is indexed using Latent Semantic Indexing, an Information Retrieval method, which allows users to write queries relevant to the desired feature and rank all the executed methods based on their textual similarity to the query. Two case studies on open source software (JEdit and Eclipse) indicate that the new technique has high accuracy, comparable with previously published approaches and it is easy to use as it considerably simplifies the dynamic analysis.","concept location, dynamic and static analyses, information retrieval, feature identification, program understanding","","ASE '07"
"Conference Paper","Schuler D,Dallmeier V,Lindig C","A Dynamic Birthmark for Java","","2007","","","274–283","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering","Atlanta, Georgia, USA","2007","9781595938824","","https://doi.org/10.1145/1321631.1321672;http://dx.doi.org/10.1145/1321631.1321672","10.1145/1321631.1321672","Code theft is a threat for companies that consider code asa core asset. A birthmark can help them to prove codetheft by identifying intrinsic properties of a program. Twoprograms with the same birthmark are likely to share a com-mon origin. Birthmarking works in particular for code thatwas not protected by tamper-resistant copyright notices thatotherwise could prove ownership.We propose a dynamic birthmark for Java that observes howa program uses objects provided by the Java Standard API.Such a birthmark is difficult to foil because it captures the observable semantics of a program. In an evaluation, ourAPI Birthmark reliably identified XML parsers and PNGreaders before and after obfuscating them with state-of-the-art obfuscation tools. These rendered existing birthmarksineffective, such as the Whole-Program-Path Birthmark byMyles and Collberg","birthmarking","","ASE '07"
"Conference Paper","Jablonski P,Hou D","CReN: A Tool for Tracking Copy-and-Paste Code Clones and Renaming Identifiers Consistently in the IDE","","2007","","","16–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology EXchange","Montreal, Quebec, Canada","2007","9781605580159","","https://doi.org/10.1145/1328279.1328283;http://dx.doi.org/10.1145/1328279.1328283","10.1145/1328279.1328283","Programmers often copy and paste code so that they can reuse the existing code to complete a similar task. Many times, modifications to the newly pasted code include renaming all instances of an identifier, such as a variable name, consistently throughout the fragment. When these modifications are done manually, undetected inconsistencies and errors can result in the code, for example, a single instance can be missed and mistakenly not renamed. To help programmers avoid making this type of copy-paste error, we created a tool, named CReN, to provide tracking and identifier renaming support within copy-and-paste clones in an integrated development environment (IDE). CReN tracks the code clones involved when copying and pasting occurs in the IDE and infers a set of rules based on the relationships between the identifiers in these code fragments. These rules capture the programmer's intentions, for example, that a particular group of identifiers should be renamed consistently together. Programmers can also provide feedback to improve the accuracy of the inferred rules by specifying that a particular instance of an identifier is to be renamed separately. We introduce our CReN tool, which is implemented as an Eclipse plug-in in Java.","copy-and-paste programming, consistent renaming, abstract syntax tree, code clone, Java, intent inference, Eclipse integrated development environment, error detection","","eclipse '07"
"Conference Paper","Monteiro MP,Aguiar A","Patterns for Refactoring to Aspects: An Incipient Pattern Language","","2007","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Conference on Pattern Languages of Programs","Monticello, Illinois, USA","2007","9781605584119","","https://doi.org/10.1145/1772070.1772079;http://dx.doi.org/10.1145/1772070.1772079","10.1145/1772070.1772079","Aspect-Oriented Programming is an emerging programming paradigm providing novel constructs that eliminate code scattering and tangling by modularizing crosscutting concerns in their own aspect modules. Many current aspect-oriented languages are backwards compatible extensions to existing popular languages, which opens the way to aspectize systems written in those languages. This paper contributes with the beginnings of a pattern language for refactoring existing systems into aspect-oriented versions of those systems. The pattern language covers the early assessment and decision stages: identifying latent aspects in existing systems, knowing when it is feasible to refactor to aspects and assessment of the necessary prerequisites for the refactoring process.","aspect-oriented programming, software refactoring","","PLOP '07"
"Conference Paper","Welicki L,Yoder JW,Wirfs-Brock R","Rendering Patterns for Adaptive Object-Models","","2007","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Conference on Pattern Languages of Programs","Monticello, Illinois, USA","2007","9781605584119","","https://doi.org/10.1145/1772070.1772085;http://dx.doi.org/10.1145/1772070.1772085","10.1145/1772070.1772085","An Adaptive Object-Model is an instance-based software system that represents domain-specific classes, attributes, relationships, and behavior using metadata. This paper presents three patterns for visually presenting and manipulating AOM domain entity objects.","visual rendering, adaptive object-models, patterns","","PLOP '07"
"Conference Paper","Massingill BL,Mattson TG,Sanders BA","SIMD: An Additional Pattern for PLPP (Pattern Language for Parallel Programming)","","2007","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Conference on Pattern Languages of Programs","Monticello, Illinois, USA","2007","9781605584119","","https://doi.org/10.1145/1772070.1772078;http://dx.doi.org/10.1145/1772070.1772078","10.1145/1772070.1772078","Recent trends in hardware, such as IBM's Cell Broadband Engine and GPUs that can be used for general-purpose computing, have made widely available systems for which a SIMD (Single Instruction, Multiple Data) style of data-parallel programming is appropriate. This paper presents a pattern to help software developers construct parallel programs for environments that support this style of data parallelism. In this approach, the program is viewed as a single thread of control, with implicitly parallel updates to data. This pattern is a new addition to the Pattern Language for Parallel Programming (PLPP) presented in our previous work [18, 19].","design patterns, pattern language, parallel programming, multiple data), SIMD (single instruction","","PLOP '07"
"Conference Paper","Roessler F,Geppert B","The Selex Design Pattern: Decomposing State Machines Cluttered by Message Multiplexing","","2007","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 14th Conference on Pattern Languages of Programs","Monticello, Illinois, USA","2007","9781605584119","","https://doi.org/10.1145/1772070.1772092;http://dx.doi.org/10.1145/1772070.1772092","10.1145/1772070.1772092","State machine specifications and their implementations are often complex because they have many responsibilities mixed together. A potential cause for responsibility clutter is message multiplexing, which means that one or more incoming and/or outgoing messages of the state machine contain data that belongs to different concerns. The Selex pattern untangles responsibility clutter due to message multiplexing without changing the external behavior of the state machine.","state machine, design pattern, communication protocol, message multiplexing, composition","","PLOP '07"
"Conference Paper","Cassou D,Ducasse S,Wuyts R","Redesigning with Traits: The Nile Stream Trait-Based Library","","2007","","","50–75","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2007 International Conference on Dynamic Languages: In Conjunction with the 15th International Smalltalk Joint Conference 2007","Lugano, Switzerland","2007","9781605580845","","https://doi.org/10.1145/1352678.1352682;http://dx.doi.org/10.1145/1352678.1352682","10.1145/1352678.1352682","Recently, traits have been proposed as a single inheritance backward compatible solution in which the composing entity has the control over the trait composition. Traits are fine-grained units used to compose classes, while avoiding many of the problems of multiple inheritance and mixin-based approaches.To evaluate the expressiveness of traits, some hierarchies were refactored, showing code reuse. However, such large refactorings, while valuable, may not be facing all the problems, since the hierarchies were previously expressed within single inheritance and following certain patterns. We wanted to evaluate how traits enable reuse, and what problems could be encountered when building a library using traits from scratch, taking into account that traits are units of reuse. This paper presents our work on designing a new stream library named Nile. We present the reuse that we attained using traits, and the problems we encountered.","refactoring, object-oriented programming, inheritance, traits, code reuse, smalltalk","","ICDL '07"
"Conference Paper","Leitao AM","The next 700 Programming Libraries","","2007","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2007 International Lisp Conference","Cambridge, United Kingdom","2007","9781595936189","","https://doi.org/10.1145/1622123.1622147;http://dx.doi.org/10.1145/1622123.1622147","10.1145/1622123.1622147","Modern software requirements are more diverse than before and can only be timely fulfilled via the extensive use of libraries. As a result, modern programmers spend a significant fraction of their time just mixing and matching these libraries. Programming language success becomes, then, more dependent on the quality and broadness of the accompanying libraries than on the language intrinsic characteristics.In spite of its recognized qualities, Common Lisp lags behind other languages as regards the quality and availability of its libraries. We argue that the best solution to overcome this problem is to automatically translate to Common Lisp the best libraries that are available in other languages. In this paper, we specifically address the translation of Java libraries using the Jnil translator tool and we provide a detailed explanation of the problems found and the lessons learned during the translation of a large Java library.Although many problems remain to be solved, the experiment proved the feasibility of the translation process and significantly increased our confidence in the future of Common Lisp.","","","ILC '07"
"Conference Paper","Jones J,Goel A,Rugaber S","Automating Software Evolution","","2007","","","14–16","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2007 Symposium on Science of Design","Arcata, California, USA","2007","9781605584362","","https://doi.org/10.1145/1496630.1496639;http://dx.doi.org/10.1145/1496630.1496639","10.1145/1496630.1496639","The goal of the research described in this paper is to apply teleological models and reasoning to support the automated adaptation of software. Teleology is concerned with purpose, and teleological models of software integrate descriptions of the goals of software with how those goals are realized in the software. The domain to which we are applying our ideas is computer-based strategy games. We have undertaken an initial study in which we have looked at part of the history of changes to an open-source strategy game called FreeCiv. In particular, we characterized the changes made to the game to determine the extent to which teleological approaches might apply.","automated adaptation, teleological modeling, change history, software evolution","","SoD '07"
"Conference Paper","Elssamadisy A,Whitmore J","Functional Testing: A Pattern to Follow and the Smells to Avoid","","2006","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2006 Conference on Pattern Languages of Programs","Portland, Oregon, USA","2006","9781605583723","","https://doi.org/10.1145/1415472.1415504;http://dx.doi.org/10.1145/1415472.1415504","10.1145/1415472.1415504","Functional tests are automated, business process tests co-owned by customers and developers. They are particularly useful for rescuing projects from high bug counts, delayed releases, and dissatisfied customers. Functional tests help projects by elucidating requirements, making project progress visible, and preventing bugs. We present functional testing in pattern format because it is especially expressive in conveying expert advice and enables the reader to make an informed decision regarding the applicability of the solution. The pattern presented aggregates multiple experiences with functional testing over several agile development projects. However, we have seen functional testing become more costly than its benefits, so we describe the symptoms--""smells""--of potentially costly problems. These are not problems with functional testing per se, but with the misinterpretation and mis-implementation of this practice. We draw on our experience to suggest ways of addressing these smells. Done right, functional testing successfully increases software's quality and business value.","testing, agile development practices, functional testing, patterns, acceptance testing","","PLoP '06"
"Journal Article","White WW","Sifting Through the Software Sandbox: SCM Meets QA: Source Control—It’s Not Just for Tracking Changes Anymore","Queue","2005","3","1","38–44","Association for Computing Machinery","New York, NY, USA","","","2005-02","","1542-7730","https://doi.org/10.1145/1046931.1046945;http://dx.doi.org/10.1145/1046931.1046945","10.1145/1046931.1046945","Thanks to modern SCM (software configuration management) systems, when developers work on a codeline they leave behind a trail of clues that can reveal what parts of the code have been modified, when, how, and by whom. From the perspective of QA (quality assurance) and test engineers, is this all just “data,” or is there useful information that can improve the test coverage and overall quality of a product?","","",""
"Journal Article","Wildman L,Fidge C,Carrington D","The Variety of Variables in Automated Real-Time Refinement","Form. Asp. Comput.","2003","15","2–3","258–279","Springer-Verlag","Berlin, Heidelberg","","","2003-11","","0934-5043","https://doi.org/10.1007/s00165-003-0009-2;http://dx.doi.org/10.1007/s00165-003-0009-2","10.1007/s00165-003-0009-2","The refinement calculus is a well-established theory for deriving program code from specifications. Recent research has extended the theory to handle timing requirements, as well as functional ones, and we have developed an interactive programming tool based on these extensions. Through a number of case studies completed using the tool, this paper explains how the tool helps the programmer by supporting the many forms of variables needed in the theory. These include simple state variables as in the untimed calculus, trace variables that model the evolution of properties over time, auxiliary variables that exist only to support formal reasoning, subroutine parameters, and variables shared between parallel processes.","Computer-aided programming, Real-time programming, Program refinement theory","",""
"Conference Paper","Thimbleby H","Reflections on Symmetry","","2002","","","28–33","Association for Computing Machinery","New York, NY, USA","Proceedings of the Working Conference on Advanced Visual Interfaces","Trento, Italy","2002","9781581135374","","https://doi.org/10.1145/1556262.1556265;http://dx.doi.org/10.1145/1556262.1556265","10.1145/1556262.1556265","Symmetry is routinely used in visual design, but in fact is not just a visual concept. This paper explores how deeper symmetries in user interface implementations can be 'reflected' in the design of the user interface, and make them easier to use. This deeper application of symmetry for user interface design is related to affordance, and therefore makes that concept constructively applicable. Recommendations for programming better user interfaces are suggested.""Symmetry, as wide or as narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection."" Hermann Weyl [16]","symmetry, affordance, user interface design, statechart, object orientation","","AVI '02"
"Conference Paper","Ramírez A,Larriba-Pey JL,Navarro C,Torrellas J,Valero M","Software Trace Cache","","1999","","","261–268","Association for Computing Machinery","New York, NY, USA","ACM International Conference on Supercomputing 25th Anniversary Volume","Munich, Germany","1999","9781450328401","","https://doi.org/10.1145/2591635.2667175;http://dx.doi.org/10.1145/2591635.2667175","10.1145/2591635.2667175","","","",""
"Conference Paper","Gerami CR,Shields TR,Weiland RJ","Transaction Queuing and Cylinder Logic Access in the Time, Inc. Magazine/Book/Record System","","1976","","","883–888","Association for Computing Machinery","New York, NY, USA","Proceedings of the June 7-10, 1976, National Computer Conference and Exposition","New York, New York","1976","9781450379175","","https://doi.org/10.1145/1499799.1499919;http://dx.doi.org/10.1145/1499799.1499919","10.1145/1499799.1499919","BRGE, the Time, Inc. Magazine/Book/Record online computer system manages one of the largest existing data bases directly updated online (five billion characters). System activity is managed by an extended CICS system with the ability to route and reroute transactions to appropriate terminals. A conglomerate transaction journal is maintained to serve as an audit trail and as the primary backup mechanism for restarts and recovery. When file restoration is necessary, the journal is simply used as a transaction source, and restoration is concurrent with continuing data entry. The data base is maintained on twenty-five 3330-II disks using the Cylinder Lists of Data (CLOD) file organization method, and accessed via the Cylinder Logic Access Method (CLAM). These permit both sequential and random access to a file, and handle overflow in a monolithic and extremely speedy manner.","","","AFIPS '76"
"Conference Paper","Sheldon JW,Tatum L","The IBM Card-Programmed Electronic Calculator","","1951","","","30–36","Association for Computing Machinery","New York, NY, USA","Papers and Discussions Presented at the Dec. 10-12, 1951, Joint AIEE-IRE Computer Conference: Review of Electronic Digital Computers","Philadelphia, Pennsylvania","1951","9781450378512","","https://doi.org/10.1145/1434770.1434775;http://dx.doi.org/10.1145/1434770.1434775","10.1145/1434770.1434775","Tracking a guided missile on a test range now is the only way to make sure of its performance. At one Department of Defense facility this is done by planting batteries of cameras or photo-theodolites along a 100-mile course. During its flight, the missile position is recorded by each camera at 100 frames per second, together with the camera training angles. Formerly these thousands of pictures from each of many cameras were turned over to a crew of computers, to determine just what happened. It took 2 weeks to make the calculations for a single flight. Now this is done on the International Business Machines (IBM) Card-Programmed Electronic Calculator in about 8 hours, and the tests can proceed.","","","AIEE-IRE '51"
"Conference Paper","Jain R,Gervasoni N,Ndhlovu M,Rawat S","A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th Innovations in Software Engineering Conference","Allahabad, India","2023","","","https://doi.org/10.1145/3578527.3578530;http://dx.doi.org/10.1145/3578527.3578530","10.1145/3578527.3578530","Recent years have witnessed tremendous progress in NLP-based code comprehension via deep neural networks (DNN) learning, especially Large Language Models (LLMs). While the original application of LLMs is focused on code generation, there have been attempts to extend the application to more specialized tasks, like code similarity, author attribution, code repairs, and so on. As data plays an important role in the success of any machine learning approach, researchers have also proposed several benchmarks which are coupled with a specific task at hand. It is well known in the machine learning (ML) community that the presence of biases in the dataset affects the quality of the ML algorithm in a real-world scenario. This paper evaluates several existing datasets from DNN’s application perspective. We specifically focus on training datasets of C/C++ language code. Our choice of language stems from the fact that while LLM-based techniques have been applied and evaluated on programming languages like Python, JavaScript, and Ruby, there is not much LLM research for C/C++. As a result, datasets generated synthetically or from real-world codes are in individual research work. Consequently, in the absence of a uniform dataset, such works are hard to compare with each other. In this work, we aim to achieve two main objectives– 1. propose code-centric features that are relevant to security program analysis tasks like vulnerability detection; 2. a thorough (qualitative and quantitative) examination of the existing code datasets that demonstrate the main characteristics of the individual datasets to have a clear comparison. Our evaluation finds exciting facts about existing datasets highlighting gaps that need to be addressed.","software metrics, software vulnerability, program graphs, datasets","","ISEC '23"
"Journal Article","Lubbers M,Koopman P,Ramsingh A,Singer J,Trinder P","Could Tierless Languages Reduce IoT Development Grief?","ACM Trans. Internet Things","2023","4","1","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","2691-1914","https://doi.org/10.1145/3572901;http://dx.doi.org/10.1145/3572901","10.1145/3572901","Internet of Things (IoT) software is notoriously complex, conventionally comprising multiple tiers. Traditionally an IoT developer must use multiple programming languages and ensure that the components interoperate correctly. A novel alternative is to use a single tierless language with a compiler that generates the code for each component and ensures their correct interoperation.We report a systematic comparative evaluation of two tierless language technologies for IoT stacks: one for resource-rich sensor nodes (Clean with iTask) and one for resource-constrained sensor nodes (Clean with iTask and mTask). The evaluation is based on four implementations of a typical smart campus application: two tierless and two Python-based tiered.(1) We show that tierless languages have the potential to significantly reduce the development effort for IoT systems, requiring 70% less code than the tiered implementations. Careful analysis attributes this code reduction to reduced interoperation (e.g., two embedded domain-specific languages and one paradigm versus seven languages and two paradigms), automatically generated distributed communication, and powerful IoT programming abstractions. (2) We show that tierless languages have the potential to significantly improve the reliability of IoT systems, describing how Clean iTask/mTask maintains type safety, provides higher-order failure management, and simplifies maintainability. (3) We report the first comparison of a tierless IoT codebase for resource-rich sensor nodes with one for resource-constrained sensor nodes. The comparison shows that they have similar code size (within 7%), and functional structure. (4) We present the first comparison of two tierless IoT languages, one for resource-rich sensor nodes and the other for resource-constrained sensor nodes.","IoT stacks, Tierless languages","",""
"Conference Paper","Zhang P,Wu C,Peng M,Zeng K,Yu D,Lai Y,Kang Y,Wang W,Wang Z","Khaos: The Impact of Inter-Procedural Code Obfuscation on Binary Diffing Techniques","","2023","","","55–67","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization","Montréal, QC, Canada","2023","","","https://doi.org/10.1145/3579990.3580007;http://dx.doi.org/10.1145/3579990.3580007","10.1145/3579990.3580007","Software obfuscation techniques can prevent binary diffing techniques from locating vulnerable code by obfuscating the third-party code, to achieve the purpose of protecting embedded device software. With the rapid development of binary diffing techniques, they can achieve more and more accurate function matching and identification by extracting the features within the function. This makes existing software obfuscation techniques, which mainly focus on the intra-procedural code obfuscation, no longer effective. In this paper, we propose a new inter-procedural code obfuscation mechanism Khaos, which moves the code across functions to obfuscate the function by using compilation optimizations. Two obfuscation primitives are proposed to separate and aggregate the function, which are called fission and fusion respectively. A prototype of Khaos is implemented based on the LLVM compiler and evaluated on a large number of real-world programs including SPEC CPU 2006 & 2017, CoreUtils, JavaScript engines, etc. Experimental results show that Khaos outperforms existing code obfuscations and can significantly reduce the accuracy rates of five state-of-the-art binary diffing techniques (less than 19%) with lower runtime overhead (less than 7%).","Software Protection, Obfuscation, Binary Diffing","","CGO 2023"
"Conference Paper","Martínez PA,Woodruff J,Armengol-Estapé J,Bernabé G,García JM,O'Boyle MF","Matching Linear Algebra and Tensor Code to Specialized Hardware Accelerators","","2023","","","85–97","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction","Montréal, QC, Canada","2023","","","https://doi.org/10.1145/3578360.3580262;http://dx.doi.org/10.1145/3578360.3580262","10.1145/3578360.3580262","Dedicated tensor accelerators demonstrate the importance of linear algebra in modern applications. Such accelerators have the potential for impressive performance gains, but require programmers to rewrite code using vendor APIs - a barrier to wider scale adoption. Recent work overcomes this by matching and replacing patterns within code, but such approaches are fragile and fail to cope with the diversity of real-world codes. We develop ATC, a compiler that uses program synthesis to map regions of code to specific APIs. The mapping space that ATC explores is combinatorially large, requiring the development of program classification, dynamic analysis, variable constraint generation and lexical distance matching techniques to make it tractable. We apply ATC to real-world tensor and linear algebra codes and evaluate them against four state-of-the-art approaches. We accelerate between 2.6x and 7x more programs, leading to over an order of magnitude performance improvement.","Offloading, Program synthesis, GEMM, LLVM","","CC 2023"
"Conference Paper","Rocha RC,Saumya C,Sundararajah K,Petoumenos P,Kulkarni M,O'Boyle MF","HyBF: A Hybrid Branch Fusion Strategy for Code Size Reduction","","2023","","","156–167","Association for Computing Machinery","New York, NY, USA","Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction","Montréal, QC, Canada","2023","","","https://doi.org/10.1145/3578360.3580267;http://dx.doi.org/10.1145/3578360.3580267","10.1145/3578360.3580267","Binary code size is a first-class design consideration in many computing domains and a critical factor in many more, but compiler optimizations targeting code size are few and often limited in functionality. When size reduction opportunities are left unexploited, it results in higher downstream costs such as memory, storage, bandwidth, or programmer time. We present HyBF, a framework to manage code merging techniques that target conditional branches (i.e., if-then-else) with similar code regions on both paths. While such code can be easily and profitably merged and with little control flow overhead, existing techniques generally fail to fully handle it. Our work is inspired by branch fusion, a technique for merging similar code in if-then-else statements, which is aimed at reducing thread divergence in GPUs. We introduce two new branch fusion techniques that can be applied on almost any if-then-else statement and can uncover many more code merging opportunities. The two approaches are mostly orthogonal and have different limitations and strengths. We integrate them into a single framework, HyBF, which can choose the optimal approach on per branch basis to maximize the potential of reducing code size. Our results show that we can achieve significant code savings on top of already highly optimized binaries, including state-of-the-art code size optimizations. Over 61 benchmarks, we reduce code size on 43 of them. That reduction typically ranges from a few hundred to a few thousand bytes, but for specific benchmarks it can be substantial and as high as 4.2% or 67 KB.","Code-Size Reduction, Branch Fusion, LLVM, Function Merging, Compiler Optimization","","CC 2023"
"Journal Article","Gao S,Gao C,He Y,Zeng J,Nie L,Xia X,Lyu M","Code Structure–Guided Transformer for Source Code Summarization","ACM Trans. Softw. Eng. Methodol.","2023","32","1","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1049-331X","https://doi.org/10.1145/3522674;http://dx.doi.org/10.1145/3522674","10.1145/3522674","Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.","multi-head attention, code structure, Code summary, Transformer","",""
"Journal Article","Huang Y,Guo H,Ding X,Shu J,Chen X,Luo X,Zheng Z,Zhou X","A Comparative Study on Method Comment and Inline Comment","ACM Trans. Softw. Eng. Methodol.","2023","","","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1049-331X","https://doi.org/10.1145/3582570;http://dx.doi.org/10.1145/3582570","10.1145/3582570","Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future.","code comment, comparative study, comment generation, method comment, inline comment","Just Accepted",""
"Journal Article","Xu S,Gao Y,Fan L,Liu Z,Liu Y,Ji H","LiDetector: License Incompatibility Detection for Open Source Software","ACM Trans. Softw. Eng. Methodol.","2023","32","1","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1049-331X","https://doi.org/10.1145/3518994;http://dx.doi.org/10.1145/3518994","10.1145/3518994","Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector, an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research.","Open source software, license, incompatibility detection","",""
"Journal Article","He Z,Song S,Bai Y,Luo X,Chen T,Zhang W,He P,Li H,Lin X,Zhang X","TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts","ACM Trans. Softw. Eng. Methodol.","2023","32","1","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1049-331X","https://doi.org/10.1145/3560263;http://dx.doi.org/10.1145/3560263","10.1145/3560263","Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.","smart contract, bookkeeping recognition, Ethereum, token","",""
"Journal Article","Ramírez A,Feldt R,Romero JR","A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning","ACM Trans. Softw. Eng. Methodol.","2023","32","1","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1049-331X","https://doi.org/10.1145/3511805;http://dx.doi.org/10.1145/3511805","10.1145/3511805","Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.","Regression testing, industry, machine learning, test case prioritisation, taxonomy","",""
"Journal Article","Han L,Chen T,Demartini G,Indulska M,Sadiq S","A Data-Driven Analysis of Behaviors in Data Curation Processes","ACM Trans. Inf. Syst.","2023","41","3","","Association for Computing Machinery","New York, NY, USA","","","2023-02","","1046-8188","https://doi.org/10.1145/3567419;http://dx.doi.org/10.1145/3567419","10.1145/3567419","Understanding how data workers interact with data, and various pieces of information related to data preparation, is key to designing systems that can better support them in exploring datasets. To date, however, there is a paucity of research studying the strategies adopted by data workers as they carry out data preparation activities. In this work, we investigate a specific data preparation activity, namely data quality discovery, and aim to (i) understand the behaviors of data workers in discovering data quality issues, (ii) explore what factors (e.g., prior experience) can affect their behaviors, as well as (iii) understand how these behavioral observations relate to their performance. To this end, we collect a multi-modal dataset through a data-driven experiment that relies on the use of eye-tracking technology with a purpose-designed platform built on top of iPython Notebook. The experiment results reveal that: (i) ‘copy–paste–modify’ is a typical strategy for writing code to complete tasks; (ii) proficiency in writing code has a significant impact on the quality of task performance, while perceived difficulty and efficacy can influence task completion patterns; and (iii) searching in external resources is a prevalent action that can be leveraged to achieve better performance. Furthermore, our experiment indicates that providing sample code within the system can help data workers get started with their task, and surfacing underlying data is an effective way to support exploration. By investigating data worker behaviors prior to each search action, we also find that the most common reasons that trigger external search actions are the need to seek assistance in writing or debugging code and to search for relevant code to reuse. Based on our experiment results, we showcase a systematic approach to select from the top best code snippets created by data workers and assemble them to achieve better performance than the best individual performer in the dataset. By doing so, our findings not only provide insights into patterns of interactions with various system components and information resources when performing data curation tasks, but also build effective and efficient data curation processes through data workers’ collective intelligence.","search pattern, Interaction behavior, data curation","",""
"Conference Paper","Duarte Maia JT,Figueiredo Correia F","Service Mesh Patterns","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th European Conference on Pattern Languages of Programs","Irsee, Germany","2023","9781450395946","","https://doi.org/10.1145/3551902.3551962;http://dx.doi.org/10.1145/3551902.3551962","10.1145/3551902.3551962","As the benefits and applicability of microservice architectures become better understood by the software industry, and this architecture becomes increasingly more adopted for building stable, independent and scalable cloud applications, a new set of concerns have alerted developers regarding communication between the different microservices. A service mesh tries to address this issue by creating a clear separation of concerns between application logic and the infrastructure needed for the communication between the different services. This is accomplished by abstracting the cross-cutting concerns related with communication out of the internal services making it possible to be reused by the different services. Existing literature describes a service mesh pattern and a sidecar pattern. This paper leans on these patterns and proposes six patterns found by observing the, what is commonly called, good practices. The six patterns are service mesh, shared communication library, node agent, sidecar, service mesh team and control plane per cluster.","Service Mesh, Control Plane, Design Patterns, Architectural Patterns, Data Plane, Microservices, Cloud-native","","EuroPLop '22"
"Conference Paper","Eisemann M,Bertels AK,Deimel D","The Sequential Initializer Pattern","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th European Conference on Pattern Languages of Programs","Irsee, Germany","2023","9781450395946","","https://doi.org/10.1145/3551902.3551967;http://dx.doi.org/10.1145/3551902.3551967","10.1145/3551902.3551967","This paper describes the Sequential Initializer Pattern, a creational design pattern for programming. Creational design patterns deal with the creation and initialization of objects. The Sequential Initializer Pattern provides a way to initialize an object by enforcing a series of function calls which are responsible for the initialization of the semantically different parts of the complex object. This improves readability and writeability and reduces the cognitive load of the programmer. The benefits of this pattern are numerous. It enforces correct usage by the developer, as there is only one way to initialize an object which is additionally enforced by the compiler. It is readable in a way that the intent of each passed parameter for initialization becomes clear. Dependencies and semantic connections between the parameters can be modeled as desired. We show that applying this pattern overcomes many of the hurdles of initialization of complex objects for a programmer.","design pattern, pattern, builder pattern, creational design pattern","","EuroPLop '22"
"Conference Paper","Al-Shaikh H,Vafaei A,Rahman MM,Azar KZ,Rahman F,Farahmandi F,Tehranipoor M","SHarPen: SoC Security Verification by Hardware Penetration Test","","2023","","","579–584","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th Asia and South Pacific Design Automation Conference","Tokyo, Japan","2023","9781450397834","","https://doi.org/10.1145/3566097.3567918;http://dx.doi.org/10.1145/3566097.3567918","10.1145/3566097.3567918","As modern SoC architectures incorporate many complex/heterogeneous intellectual properties (IPs), the protection of security assets has become imperative, and the number of vulnerabilities revealed is rising due to the increased number of attacks. Over the last few years, penetration testing (PT) has become an increasingly effective means of detecting software (SW) vulnerabilities. As of yet, no such technique has been applied to the detection of hardware vulnerabilities. This paper proposes a PT framework, SHarPen, for detecting hardware vulnerabilities, which facilitates the development of a SoC-level security verification framework. SHarPen proposes a formalism for performing gray-box hardware (HW) penetration testing instead of relying on coverage-based testing and provides an automation for mapping hardware vulnerabilities to logical/mathematical cost functions. SHarPen supports both simulation and FPGA-based prototyping, allowing us to automate security testing at different stages of the design process with high capabilities for identifying vulnerabilities in the targeted SoC.","BPSO, SoC security verification, cost function, penetration testing","","ASPDAC '23"
"Conference Paper","Carvalho RM,Marques AB,Andrade R,Sousa A,Dias R,Campagnoli G","An Experience of Using the GQM Approach in a Remote Environment to Define Requirements Metrics","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXI Brazilian Symposium on Software Quality","Curitiba, Brazil","2023","9781450399999","","https://doi.org/10.1145/3571473.3571491;http://dx.doi.org/10.1145/3571473.3571491","10.1145/3571473.3571491","It is well known that many problems in software development arise because the software requirements area is neglected. In this way, the quality of requirements is essential to guarantee the acceptance of the software and, thus, the success of the project. Software metrics can be used to assess the quality of products and artifacts. The literature has several metrics for the requirements area. However, to better meet the particularities of a project, the ideal is to define specific metrics. The Goal-Question-Metric (GQM) is an approach for defining metrics oriented to the organization’s goals. This paper aims to present an experience report about applying the GQM approach to define requirements metrics in a software project in the industry. Since we are going through a pandemic, we adopted GQM in the context of remote work. This paper presents the adaptations that were made, as well as the lessons learned about the application of GQM in a remote environment.","requirements, GQM, goal-question-metric, metric","","SBQS '22"
"Conference Paper","Li Y,Wen B,Zheng H","Generic O-LLVM Automatic Multi-Architecture Deobfuscation Framework Based on Symbolic Execution","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th International Conference on Advanced Information Science and System","Sanya, China","2023","9781450397933","","https://doi.org/10.1145/3573834.3574541;http://dx.doi.org/10.1145/3573834.3574541","10.1145/3573834.3574541","Nowadays, the O-LLVM obfuscation framework makes it difficult to analyze various types of malware. To address this problem, this paper proposes a multi-architecture automated deobfuscation framework GOAMD specifically for O-LLVM obfuscation technology, which can intelligently identify the differences of programs on different architectures and perform targeted deobfuscation work on them. The experimental results show that the framework has high deobfuscation accuracy and portability.","deobfuscation, O-LLVM, symbolic execution, multi-architecture","","AISS '22"
"Journal Article","Cao D,Kunkel R,Nandi C,Willsey M,Tatlock Z,Polikarpova N","Babble: Learning Better Abstractions with E-Graphs and Anti-Unification","Proc. ACM Program. Lang.","2023","7","POPL","","Association for Computing Machinery","New York, NY, USA","","","2023-01","","","https://doi.org/10.1145/3571207;http://dx.doi.org/10.1145/3571207","10.1145/3571207","Library learning compresses a given corpus of programs by extracting common structure from the corpus into reusable library functions. Prior work on library learning suffers from two limitations that prevent it from scaling to larger, more complex inputs. First, it explores too many candidate library functions that are not useful for compression. Second, it is not robust to syntactic variation in the input. We propose library learning modulo theory (LLMT), a new library learning algorithm that additionally takes as input an equational theory for a given problem domain. LLMT uses e-graphs and equality saturation to compactly represent the space of programs equivalent modulo the theory, and uses a novel e-graph anti-unification technique to find common patterns in the corpus more directly and efficiently. We implemented LLMT in a tool named babble. Our evaluation shows that babble achieves better compression orders of magnitude faster than the state of the art. We also provide a qualitative evaluation showing that babble learns reusable functions on inputs previously out of reach for library learning.","library learning, e-graphs, anti-unification","",""
"Journal Article","Cai P,Sur S","MilliPCD: Beyond Traditional Vision Indoor Point Cloud Generation via Handheld Millimeter-Wave Devices","Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","2023","6","4","","Association for Computing Machinery","New York, NY, USA","","","2023-01","","","https://doi.org/10.1145/3569497;http://dx.doi.org/10.1145/3569497","10.1145/3569497","3D Point Cloud Data (PCD) has been used in many research and commercial applications widely, such as autonomous driving, robotics, and VR/AR. But existing PCD generation systems based on RGB-D and LiDARs require robust lighting and an unobstructed field of view of the target scenes. So, they may not work properly under challenging environmental conditions. Recently, millimeter-wave (mmWave) based imaging systems have raised considerable interest due to their ability to work in dark environments. But the resolution and quality of the PCD from these mmWave imaging systems are very poor. To improve the quality of PCD, we design and implement MilliPCD, a ""beyond traditional vision"" PCD generation system for handheld mmWave devices, by integrating traditional signal processing with advanced deep learning based algorithms. We evaluate MilliPCD with real mmWave reflected signals collected from large, diverse indoor environments, and the results show improvements in the quality w.r.t. the existing algorithms, both quantitatively and qualitatively.","Wireless Sensing, Millimeter-Wave, Point Cloud Data, Graph Neural Networks","",""
"Journal Article","Lew AK,Huot M,Staton S,Mansinghka VK","ADEV: Sound Automatic Differentiation of Expected Values of Probabilistic Programs","Proc. ACM Program. Lang.","2023","7","POPL","","Association for Computing Machinery","New York, NY, USA","","","2023-01","","","https://doi.org/10.1145/3571198;http://dx.doi.org/10.1145/3571198","10.1145/3571198","Optimizing the expected values of probabilistic processes is a central problem in computer science and its applications, arising in fields ranging from artificial intelligence to operations research to statistical computing. Unfortunately, automatic differentiation techniques developed for deterministic programs do not in general compute the correct gradients needed for widely used solutions based on gradient-based optimization. In this paper, we present ADEV, an extension to forward-mode AD that correctly differentiates the expectations of probabilistic processes represented as programs that make random choices. Our algorithm is a source-to-source program transformation on an expressive, higher-order language for probabilistic computation, with both discrete and continuous probability distributions. The result of our transformation is a new probabilistic program, whose expected return value is the derivative of the original program’s expectation. This output program can be run to generate unbiased Monte Carlo estimates of the desired gradient, that can be used within the inner loop of stochastic gradient descent. We prove ADEV correct using logical relations over the denotations of the source and target probabilistic programs. Because it modularly extends forward-mode AD, our algorithm lends itself to a concise implementation strategy, which we exploit to develop a prototype in just a few dozen lines of Haskell (https://github.com/probcomp/adev).","machine learning theory, logical relations, correctness, probabilistic programming, denotational semantics, functional programming, automatic differentiation","",""
"Conference Paper","Hu Y,Zou D,Peng J,Wu Y,Shan J,Jin H","TreeCen: Building Tree Graph for Scalable Semantic Code Clone Detection","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556927;http://dx.doi.org/10.1145/3551349.3556927","10.1145/3551349.3556927","Code clone detection is an important research problem that has attracted wide attention in software engineering. Many methods have been proposed for detecting code clone, among which text-based and token-based approaches are scalable but lack consideration of code semantics, thus resulting in the inability to detect semantic code clones. Methods based on intermediate representations of codes can solve the problem of semantic code clone detection. However, graph-based methods are not practicable due to code compilation, and existing tree-based approaches are limited by the scale of trees for scalable code clone detection. In this paper, we propose TreeCen, a scalable tree-based code clone detector, which satisfies scalability while detecting semantic clones effectively. Given the source code of a method, we first extract its abstract syntax tree (AST) based on static analysis and transform it into a simple graph representation (i.e., tree graph) according to the node type, rather than using traditional heavyweight tree matching. We then treat the tree graph as a social network and adopt centrality analysis on each node to maintain the tree details. By this, the original complex tree can be converted into a 72-dimensional vector while containing comprehensive structural information of the AST. Finally, these vectors are fed into a machine learning model to train a detector and use it to find code clones. We conduct comparative evaluations on effectiveness and scalability. The experimental results show that TreeCen maintains the best performance of the other six state-of-the-art methods (i.e., SourcererCC, RtvNN, DeepSim, SCDetector, Deckard, and ASTNN) with F1 scores of 0.99 and 0.95 on BigCloneBench and Google Code Jam datasets, respectively. In terms of scalability, TreeCen is about 79 times faster than the other state-of-the-art tree-based semantic code clone detector (ASTNN), about 13 times faster than the fastest graph-based approach (SCDetector), and even about 22 times faster than the one-time trained token-based detector (RtvNN).","","","ASE '22"
"Conference Paper","Zhang J,Panthaplackel S,Nie P,Li JJ,Gligoric M","CoditT5: Pretraining for Source Code and Natural Language Editing","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556955;http://dx.doi.org/10.1145/3551349.3556955","10.1145/3551349.3556955","Pretrained language models have been shown to be effective in many software-related generation tasks; however, they are not well-suited for editing tasks as they are not designed to reason about edits. To address this, we propose a novel pretraining objective which explicitly models edits and use it to build CoditT5, a large language model for software-related editing tasks that is pretrained on large amounts of source code and natural language comments. We fine-tune it on various downstream editing tasks, including comment updating, bug fixing, and automated code review. By outperforming standard generation-based models, we demonstrate the generalizability of our approach and its suitability for editing tasks. We also show how a standard generation model and our edit-based model can complement one another through simple reranking strategies, with which we achieve state-of-the-art performance for the three downstream editing tasks.","bug fixing, editing, Pretrained language models, comment updating, automated code review","","ASE '22"
"Conference Paper","Wu Y,Feng S,Zou D,Jin H","Detecting Semantic Code Clones by Building AST-Based Markov Chains Model","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3560426;http://dx.doi.org/10.1145/3551349.3560426","10.1145/3551349.3560426","Code clone detection aims to find functionally similar code fragments, which is becoming more and more important in the field of software engineering. Many code clone detection methods have been proposed, among which tree-based methods are able to handle semantic code clones. However, these methods are difficult to scale to big code due to the complexity of tree structures. In this paper, we design Amain, a scalable tree-based semantic code clone detector by building Markov chains models. Specifically, we propose a novel method to transform the original complex tree into simple Markov chains and measure the distance of all states in these chains. After obtaining all distance values, we feed them into a machine learning classifier to train a code clone detector. To examine the effectiveness of Amain, we evaluate it on two widely used datasets namely Google Code Jam and BigCloneBench. Experimental results show that Amain is superior to nine state-of-the-art code clone detection tools (i.e., SourcererCC, RtvNN, Deckard, ASTNN, TBCNN, CDLH, FCCA, DeepSim, and SCDetector).","Abstract Syntax Tree, Semantic Code Clones, Markov Chain","","ASE '22"
"Conference Paper","Pauck F","Scaling Arbitrary Android App Analyses","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3561339;http://dx.doi.org/10.1145/3551349.3561339","10.1145/3551349.3561339","More apps are published every day and the functionality of each app increases steadily as well. Consequently app analyses are often overwhelmed when confronted with up-to-date, real-world apps. One of the biggest issues originates from the scalability of analyses with respect to libraries. Analyses, more precisely the tools implementing them, cannot distinguish the app’s code from the code of a library. Always analyzing the whole code base is the result. However, this is usually not necessary, for example, when a security property is checked, trusted libraries must not be analyzed. We propose an approach to differentiate an app’s code from a library’s code. The approach is based on clone detection and implemented in our prototype APK-Simplifier. As the evaluation shows APK-Simplifier can be employed in a cooperative analysis to remove library code and to enhance arbitrary analysis tools’ scalability. In fact, five analysis tools have been enabled to analyze five up-to-date, real-world apps they could not analyze before. Still, it is alerting that the majority of such apps remains not analyzable as also shown during evaluation.","software analysis, taint analysis, clone detection, Android, cooperative analysis, security","","ASE '22"
"Conference Paper","Eghbali A,Pradel M","CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556903;http://dx.doi.org/10.1145/3551349.3556903","10.1145/3551349.3556903","Recent years have brought a surge of work on predicting pieces of source code, e.g., for code completion, code migration, program repair, or translating natural language into code. All this work faces the challenge of evaluating the quality of a prediction w.r.t. some oracle, typically in the form of a reference solution. A common evaluation metric is the BLEU score, an n-gram-based metric originally proposed for evaluating natural language translation, but adopted in software engineering because it can be easily computed on any programming language and enables automated evaluation at scale. However, a key difference between natural and programming languages is that in the latter, completely unrelated pieces of code may have many common n-grams simply because of the syntactic verbosity and coding conventions of programming languages. We observe that these trivially shared n-grams hamper the ability of the metric to distinguish between truly similar code examples and code examples that are merely written in the same language. This paper presents CrystalBLEU, an evaluation metric based on BLEU, that allows for precisely and efficiently measuring the similarity of code. Our metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams. We evaluate CrystalBLEU on two datasets from prior work and on a new, labeled dataset of semantically equivalent programs. Our results show that CrystalBLEU can distinguish similar from dissimilar code examples 1.9–4.5 times more effectively, when compared to the original BLEU score and a previously proposed variant of BLEU for code.","Metric, BLEU, Evaluation","","ASE '22"
"Conference Paper","Mu F,Chen X,Shi L,Wang S,Wang Q","Automatic Comment Generation via Multi-Pass Deliberation","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556917;http://dx.doi.org/10.1145/3551349.3556917","10.1145/3551349.3556917","Deliberation is a common and natural behavior in human daily life. For example, when writing papers or articles, we usually first write drafts, and then iteratively polish them until satisfied. In light of such a human cognitive process, we propose DECOM, which is a multi-pass deliberation framework for automatic comment generation. DECOM consists of multiple Deliberation Models and one Evaluation Model. Given a code snippet, we first extract keywords from the code and retrieve a similar code fragment from a pre-defined corpus. Then, we treat the comment of the retrieved code as the initial draft and input it with the code and keywords into DECOM to start the iterative deliberation process. At each deliberation, the deliberation model polishes the draft and generates a new comment. The evaluation model measures the quality of the newly generated comment to determine whether to end the iterative process or not. When the iterative process is terminated, the best-generated comment will be selected as the target comment. Our approach is evaluated on two real-world datasets in Java (87K) and Python (108K), and experiment results show that our approach outperforms the state-of-the-art baselines. A human evaluation study also confirms the comments generated by DECOM tend to be more readable, informative, and useful.","Code Summarization, Deep Neural Network, Information Retrieval","","ASE '22"
"Conference Paper","Shi J,Yang Z,Xu B,Kang HJ,Lo D","Compressing Pre-Trained Models of Code into 3 MB","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556964;http://dx.doi.org/10.1145/3551349.3556964","10.1145/3551349.3556964","Although large pre-trained models of code have delivered significant advancements in various code processing tasks, there is an impediment to the wide and fluent adoption of these powerful models in software developers’ daily workflow: these large models consume hundreds of megabytes of memory and run slowly on personal devices, which causes problems in model deployment and greatly degrades the user experience. It motivates us to propose Compressor, a novel approach that can compress the pre-trained models of code into extremely small models with negligible performance sacrifice. Our proposed method formulates the design of tiny models as simplifying the pre-trained model architecture: searching for a significantly smaller model that follows an architectural design similar to the original pre-trained model. Compressor proposes a genetic algorithm (GA)-based strategy to guide the simplification process. Prior studies found that a model with higher computational cost tends to be more powerful. Inspired by this insight, the GA algorithm is designed to maximize a model’s Giga floating-point operations (GFLOPs), an indicator of the model computational cost, to satisfy the constraint of the target model size. Then, we use the knowledge distillation technique to train the small model: unlabelled data is fed into the large model and the outputs are used as labels to train the small model. We evaluate Compressor with two state-of-the-art pre-trained models, i.e., CodeBERT and GraphCodeBERT, on two important tasks, i.e., vulnerability prediction and clone detection. We use our method to compress pre-trained models to a size (3 MB), which is 160 × smaller than the original size. The results show that compressed CodeBERT and GraphCodeBERT are 4.31 × and 4.15 × faster than the original model at inference, respectively. More importantly, they maintain and of the original performance on the vulnerability prediction task. They even maintain higher ratios ( and ) of the original performance on the clone detection task.","Model Compression, Pre-Trained Models, Genetic Algorithm","","ASE '22"
"Conference Paper","Tang W,Xu Z,Liu C,Wu J,Yang S,Li Y,Luo P,Liu Y","Towards Understanding Third-Party Library Dependency in C/C++ Ecosystem","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3560432;http://dx.doi.org/10.1145/3551349.3560432","10.1145/3551349.3560432","Third-party libraries (TPLs) are frequently reused in software to reduce development cost and the time to market. However, external library dependencies may introduce vulnerabilities into host applications. The issue of library dependency has received considerable critical attention. Many package managers, such as Maven, Pip, and NPM, are proposed to manage TPLs. Moreover, a significant amount of effort has been put into studying dependencies in language ecosystems like Java, Python, and JavaScript except C/C++. Due to the lack of a unified package manager for C/C++, existing research has only few understanding of TPL dependencies in the C/C++ ecosystem, especially at large scale. Towards understanding TPL dependencies in the C/C++ ecosystem, we collect existing TPL databases, package management tools, and dependency detection tools, summarize the dependency patterns of C/C++ projects, and construct a comprehensive and precise C/C++ dependency detector. Using our detector, we extract dependencies from a large-scale database containing 24K C/C++ repositories from GitHub. Based on the extracted dependencies, we provide the results and findings of an empirical study, which aims at understanding the characteristics of the TPL dependencies. We further discuss the implications to manage dependency for C/C++ and the future research directions for software engineering researchers and developers in fields of library development, software composition analysis, and C/C++ package manager.","Package Manager, Mining Software Repositories, Third-Party Library","","ASE '22"
"Conference Paper","Khan JY,Uddin G","Automatic Code Documentation Generation Using GPT-3","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3559548;http://dx.doi.org/10.1145/3551349.3559548","10.1145/3551349.3559548","Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.","GPT-3, Machine Learning., code documentation","","ASE '22"
"Conference Paper","Yang D,Mao X,Chen L,Xu X,Lei Y,Lo D,He J","TransplantFix: Graph Differencing-Based Code Transplantation for Automated Program Repair","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556893;http://dx.doi.org/10.1145/3551349.3556893","10.1145/3551349.3556893","Automated program repair (APR) holds the promise of aiding manual debugging activities. Over a decade of evolution, a broad range of APR techniques have been proposed and evaluated on a set of real-world bug datasets. However, while more and more bugs have been correctly fixed, we observe that the growth of newly fixed bugs by APR techniques has hit a bottleneck in recent years. In this work, we explore the possibility of addressing complicated bugs by proposing TransplantFix, a novel APR technique that leverages graph differencing-based transplantation from the donor method. The key novelty of TransplantFix lies in three aspects: 1) we propose to use a graph-based differencing algorithm to distill semantic fix actions from the donor method; 2) we devise an inheritance-hierarchy-aware code search approach to identify donor methods with similar functionality; 3) we present a namespace transfer approach to effectively adapt donor code. We investigate the unique contributions of TransplantFix by conducting an extensive comparison that covers a total of 42 APR techniques and evaluating TransplantFix on 839 real-world bugs from Defects4J v1.2 and v2.0. TransplantFix presents superior results in three aspects. First, it has achieved the best performance as compared to the state-of-the-art APR techniques proposed in the last three years, in terms of the number of newly fixed bugs, reaching a 60%-300% improvement. Furthermore, not relying on any fix actions crafted manually or learned from big data, it reaches the best generalizability among all APR techniques evaluated on Defects4J v1.2 and v2.0. In addition, it shows the potential to synthesize complicated patches consisting of at most eight-line insertions at a hunk. TransplantFix presents fresh insights and a promising avenue for follow-up research towards addressing more complicated bugs.","Automated program repair, graph differencing, code transplantation","","ASE '22"
"Conference Paper","Huang Q,Yuan Z,Xing Z,Xu X,Zhu L,Lu Q","Prompt-Tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556912;http://dx.doi.org/10.1145/3551349.3556912","10.1145/3551349.3556912","Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this paper, we formulate type inference as a cloze-style fill-in-blank language task. Built on source code naturalness, our approach fine-tunes a code masked language model (MLM) as a neural knowledge base of code elements with a novel “pre-train, prompt and predict” paradigm from raw source code. Our approach is lightweight and has minimum requirements on code compilation. Unlike existing symbolic name and context matching for type inference, our prompt-tuned code MLM packs FQN syntax and usage in its parameters and supports fuzzy neural type inference. We systematically evaluate our approach on a large amount of source code from GitHub and Stack Overflow. Our results confirm the effectiveness of our approach design and the practicality for partial code type inference. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.","","","ASE '22"
"Conference Paper","Shi Y,Zhang Y,Luo T,Mao X,Yang M","Precise (Un)Affected Version Analysis for Web Vulnerabilities","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556933;http://dx.doi.org/10.1145/3551349.3556933","10.1145/3551349.3556933","Web applications are attractive attack targets given their popularity and large number of vulnerabilities. To mitigate the threat of web vulnerabilities, an important piece of information is their affected versions. However, it is non-trivial to build accurate affected version information because confirming a version as affected or unaffected requires security expertise and huge efforts, while there are usually hundreds of versions to examine. As a result, such information is maintained in a low-quality manner in almost every public vulnerability database. Therefore, it is extremely useful to have a tool that can automatically and precisely examine a large part (even if not all) of the software versions as affected or unaffected. To this end, this paper proposes a vulnerability-centric approach for precise (un)affected version analysis for web vulnerabilities. The key idea is to extract the vulnerability logic from a patch and directly use the vulnerability logic to check whether a version is (un)affected or not. Compared with existing works, our vulnerability-centric approach helps to tolerate the code changes across different software versions. We construct a high-quality dataset with 34 CVEs and 299 software versions to evaluate our approach. The results show that our approach achieves a precision of 98.15% and a recall of 85.01% in identifying (un)affected versions and significantly outperforms existing tools (e.g., V-SZZ, ReDebug, V0Finder).","OSS Security, Vulnerability Database, Web Vulnerability","","ASE '22"
"Conference Paper","Liu Y,Nie P,Legunsen O,Gligoric M","Inline Tests","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556952;http://dx.doi.org/10.1145/3551349.3556952","10.1145/3551349.3556952","Unit tests are widely used to check source code quality, but they can be too coarse-grained or ill-suited for testing individual program statements. We introduce inline tests to make it easier to check for faults in statements. We motivate inline tests through several language features and a common testing scenario in which inline tests could be beneficial. For example, inline tests can allow a developer to test a regular expression in place. We also define language-agnostic requirements for inline testing frameworks. Lastly, we implement I-Test, the first inline testing framework. I-Test works for Python and Java, and it satisfies most of the requirements. We evaluate I-Test on open-source projects by using it to test 144 statements in 31 Python programs and 37 Java programs. We also perform a user study. All nine user study participants say that inline tests are easy to write and that inline testing is beneficial. The cost of running inline tests is negligible, at 0.007x–0.014x, and our inline tests helped find two faults that have been fixed by the developers.","software testing, inline tests","","ASE '22"
"Conference Paper","Zhang L,Liu C,Xu Z,Chen S,Fan L,Chen B,Liu Y","Has My Release Disobeyed Semantic Versioning? Static Detection Based on Semantic Differencing","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556956;http://dx.doi.org/10.1145/3551349.3556956","10.1145/3551349.3556956","To enhance the compatibility in the version control of Java Third-party Libraries (TPLs), Maven adopts Semantic Versioning (SemVer) to standardize the underlying meaning of versions, but users could still confront abnormal execution and crash after upgrades even if compilation and linkage succeed. It is caused by semantic breaking (SemB) issues, such that APIs directly used by users have identical signatures but inconsistent semantics across upgrades. To strengthen compliance with SemVer rules, developers and users should be alerted of such issues. Unfortunately, it is challenging to detect them statically, because semantic changes in the internal methods of APIs are difficult to capture. Dynamic testing can confirmingly uncover some, but it is limited by inadequate coverage. To detect SemB issues over compatible upgrades (Patch and Minor) by SemVer rules, we conduct an empirical study on 180 SemB issues to understand the root causes, inspired by which, we propose Sembid (Semantic Breaking Issue Detector) to statically detect such issues of TPLs for developers and users. Since APIs are directly used by users, Sembid detects and reports SemB issues based on APIs. For a pair of APIs, Sembid walks through the call chains originating from the API to locate breaking changes by measuring semantic diff. Then, Sembid checks if the breaking changes can affect API’s output along call chains. The evaluation showed Sembid achieved recall and precision and outperformed other API checkers on SemB API detection. We also revealed Sembid detected over 3 times more SemB APIs with better coverage than unit tests, the commonly used solution. Furthermore, we carried out an empirical study on 1,629,589 APIs from 546 version pairs of top Java libraries and found there were 2 ∼ 4 times more SemB APIs than those with signature-based issues. Due to various version release strategies, of Patch version pairs and of Minor version pairs had at least one API affected by any breaking.","","","ASE '22"
"Conference Paper","Nguyen AT,Yadavally A,Nguyen TN","Next Syntactic-Unit Code Completion and Applications","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3559544;http://dx.doi.org/10.1145/3551349.3559544","10.1145/3551349.3559544","Code completion is an important feature in an IDE to improve developers’ productivity. Existing code completion approaches focus on completing the current code token, next token or statement, or code pattern. We propose AstCC, a code completion approach to suggest the next syntactic unit via an AST-based statistical language model. AstCC learns from a large code corpus to derive the next AST subtree representing a syntactic unit, and then fills in the template with the concrete variables from the current program scope. Our empirical evaluation shows that AstCC can correctly suggest the next syntactic unit in 33% of the cases, and in 62% of the cases, it correctly suggests within five candidates. We will also explain the potential applications of AstCC in automated program repair, automated test case generation, and syntactic pattern mining.","Statistical Language Model, Code Completion","","ASE '22"
"Conference Paper","Xie R,Hu T,Ye W,Zhang S","Low-Resources Project-Specific Code Summarization","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556909;http://dx.doi.org/10.1145/3551349.3556909","10.1145/3551349.3556909","Code summarization generates brief natural language descriptions of source code pieces, which can assist developers in understanding code and reduce documentation workload. Recent neural models on code summarization are trained and evaluated on large-scale multi-project datasets consisting of independent code-summary pairs. Despite the technical advances, their effectiveness on a specific project is rarely explored. In practical scenarios, however, developers are more concerned with generating high-quality summaries for their working projects. And these projects may not maintain sufficient documentation, hence having few historical code-summary pairs. To this end, we investigate low-resource project-specific code summarization, a novel task more consistent with the developers’ requirements. To better characterize project-specific knowledge with limited training samples, we propose a meta transfer learning method by incorporating a lightweight fine-tuning mechanism into a meta-learning framework. Experimental results on nine real-world projects verify the superiority of our method over alternative ones and reveal how the project-specific knowledge is learned.","meta learning, parameter efficient transfer learning, code summarization","","ASE '22"
"Conference Paper","Le Dilavrec Q,Khelladi DE,Blouin A,Jézéquel JM","HyperAST: Enabling Efficient Analysis of Software Histories at Scale","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3560423;http://dx.doi.org/10.1145/3551349.3560423","10.1145/3551349.3560423","Abstract Syntax Trees (ASTs) are widely used beyond compilers in many tools that measure and improve code quality, such as code analysis, bug detection, mining code metrics, refactoring. With the advent of fast software evolution and multistage releases, the temporal analysis of an AST history is becoming useful to understand and maintain code. However, jointly analyzing thousands versions of ASTs independently faces scalability issues, mostly combinatorial, both in terms of memory and CPU usage. In this paper, we propose a novel type of AST, called HyperAST, that enables efficient temporal code analysis on a given software history by: 1/ leveraging code redundancy through space (between code elements) and time (between versions); 2/ reusing intermediate computation results. We show how the HyperAST can be built incrementally on a set of commits to capture all multiple ASTs at once in an optimized way. We evaluated the HyperAST on a curated list of large software projects. Compared to Spoon, a state-of-the-art technique, we observed that the HyperAST outperforms it with an order-of-magnitude difference from × 6 up to × 8076 in CPU construction time and from × 12 up to × 1159 in memory footprint. While the HyperAST requires up to 2 h 22 min and 7.2 GB for the biggest project, Spoon requires up to 93 h and 31 min and 2.2 TB. The gains in construction time varied from to and the gains in memory footprint varied from to . We further compared the task of finding references of declarations with the HyperAST and Spoon. We observed on average precision and recall without a significant difference in search time.","","","ASE '22"
"Conference Paper","Tian H,Tang X,Habib A,Wang S,Liu K,Xia X,Klein J,BissyandÉ TF","Is This Change the Answer to That Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556914;http://dx.doi.org/10.1145/3551349.3556914","10.1145/3551349.3556914","Patch correctness has been the focus of automated program repair (APR) in recent years due to the propensity of APR tools to generate overfitting patches. Given a generated patch, the oracle (e.g., test suites) is generally weak in establishing correctness. Therefore, the literature has proposed various approaches of leveraging machine learning with engineered and deep learned features, or exploring dynamic execution information, to further explore the correctness of APR-generated patches. In this work, we propose a novel perspective to the problem of patch correctness assessment: a correct patch implements changes that “answer” to a problem posed by buggy behavior. Concretely, we turn the patch correctness assessment into a Question Answering problem. To tackle this problem, our intuition is that natural language processing can provide the necessary representations and models for assessing the semantic correlation between a bug (question) and a patch (answer). Specifically, we consider as inputs the bug reports as well as the natural language description of the generated patches. Our approach, Quatrain, first considers state-of-the-art commit message generation models to produce the relevant inputs associated to each generated patch. Then we leverage a neural network architecture to learn the semantic correlation between bug reports and commit messages. Experiments on a large dataset of 9 135 patches generated for three bug datasets (Defects4j, Bugs.jar and Bears) show that Quatrain achieves an AUC of 0.886 on predicting patch correctness, and recalling 93% correct patches while filtering out 62% incorrect patches. Our experimental results further demonstrate the influence of inputs quality on prediction performance. We further perform experiments to highlight that the model indeed learns the relationship between bug reports and code change descriptions for the prediction. Finally, we compare against prior work and discuss the benefits of our approach.","Question Answering, Program Repair, Machine Learning, Patch Correctness","","ASE '22"
"Conference Paper","Yin Z,Xu Y,Zhou C,Jiang Y","Empirical Study of System Resources Abused by IoT Attackers","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3556901;http://dx.doi.org/10.1145/3551349.3556901","10.1145/3551349.3556901","IoT devices have been under frequent attacks in recent years, causing severe impacts. Previous research has shown the evolution and features of some specific IoT malware families or stages of IoT attacks through offline sample analysis. However, we still lack a systematic observation of various system resources abused by active attackers and the malicious intentions behind these behaviors. This makes it difficult to design appropriate protection strategies to defend against existing attacks and possible future variants. In this paper, we fill this gap by analyzing 117,862 valid attack sessions captured by our dedicated high-interaction IoT honeypot, HoneyAsclepius, and further discover the intentions in our designed workflow. HoneyAsclepius enables high capture capability as well as continuous behavior monitoring during active attack sessions in real-time. Through a large-scale deployment, we collected 11,301,239 malicious behaviors originating from 50,594 different attackers. Based on this information, we further separate the behaviors in different attack sessions targeting distinct categories of system resources, estimate the temporal relations and summarize their malicious intentions behind. Inspired by such investigations, we present several key insights about abusive behaviors of the file, network, process, and special capability resources, and further propose practical defense strategies to better protect IoT devices.","Behavior Intention, IoT Attack, System Resource Abuse","","ASE '22"
"Conference Paper","Zhang Y,Zhang Y,Portokalidis G,Xu J","Towards Understanding the Runtime Performance of Rust","","2023","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering","Rochester, MI, USA","2023","9781450394758","","https://doi.org/10.1145/3551349.3559494;http://dx.doi.org/10.1145/3551349.3559494","10.1145/3551349.3559494","Rust is a young systems programming language, but it has gained tremendous popularity thanks to its assurance of memory safety. However, the performance of Rust has been less systematically understood, although many people are claiming that Rust is comparable to C/C++ regarding efficiency. In this paper, we aim to understand the performance of Rust, using C as the baseline. First, we collect a set of micro benchmarks where each program is implemented with both Rust and C. To ensure fairness, we manually validate that the Rust version and the C version implement the identical functionality using the same algorithm. Our measurement based on the micro benchmarks shows that Rust is in general slower than C, but the extent of the slowdown varies across different programs. On average, Rust brings a 1.77x “performance overhead” compared to C. Second, we dissect the root causes of the overhead and unveil that it is primarily incurred by run-time checks inserted by the compiler and restrictions enforced by the language design. With the run-time checks disabled and the restrictions loosened, Rust presents a performance indistinguishable from C.","","","ASE '22"
"Journal Article","Ma Y,Qi B,Xu W,Wang M,Du B,Fan H","Integrating Real-Time and Non-Real-Time Collaborative Programming: Workflow, Techniques, and Prototypes","Proc.  ACM Hum. -Comput.  Interact.","2022","7","GROUP","","Association for Computing Machinery","New York, NY, USA","","","2022-12","","","https://doi.org/10.1145/3567563;http://dx.doi.org/10.1145/3567563","10.1145/3567563","Real-time collaborative programming enables a group of programmers to edit shared source code at the same time, which significantly complements the traditional non-real-time collaborative programming supported by version control systems. However, one critical issue with this emerging technique is the lack of integration with non-real-time collaboration. Specifically, contributions from multiple programmers in a real-time collaboration session cannot be distinguished and accurately recorded in the version control system. In this study, we propose a scheme that integrates real-time and non-real-time collaborative programming with a novel workflow, and contribute enabling techniques to realize such integration. As a proof-of-concept, we have successfully implemented two prototype systems named CoEclipse and CoIDEA, which allow programmers to closely collaborate in a real-time fashion while preserving the work's compatibility with traditional non-real-time collaboration. User evaluation and performance experiments have confirmed the feasibility of the approach and techniques, demonstrated the good system performance, and presented the satisfactory usability of the prototypes.","real-time collaborative programming, integrated collaboration workflow, software development environments, colored lines algorithm, code change authorship","",""
"Conference Paper","Blanchard J,Hott JR,Berry V,Carroll R,Edmison B,Glassey R,Karnalim O,Plancher B,Russell S","Stop Reinventing the Wheel! Promoting Community Software in Computing Education","","2022","","","261–292","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 Working Group Reports on Innovation and Technology in Computer Science Education","Dublin, Ireland","2022","","","https://doi.org/10.1145/3571785.3574129;http://dx.doi.org/10.1145/3571785.3574129","10.1145/3571785.3574129","Historically, computing instructors and researchers have developed a wide variety of tools to support teaching and educational research, including exam and code testing suites and data collection solutions. However, these tools often find limited adoption beyond their creators. As a result, it is common for many of the same functionalities to be re-implemented by different instructional groups within the Computing Education community. We hypothesise that this is due in part to discoverability, availability, and adaptability challenges. Further, instructors often face institutional barriers to deployment, which can include hesitance of institutions to rely on community developed solutions that often lack a centralised authority and may be community or individually maintained. To this end, our working group explored what solutions are currently available, what instructors needed, and the reasons behind the above-mentioned phenomenon. To do so, we reviewed existing literature and surveyed the community to identify the tools that have been developed by the community; the solutions that are currently available and in use by instructors; what features are needed moving forward for classroom and research use; what support for extensions is needed to support further Computing Education research; and what institutional challenges instructors and researchers are currently facing or have faced in using community software solutions. Finally, the working group identified factors that limited adoption of solutions. This work proposes ways to integrate and improve the availability, discoverability, and dissemination of existing community projects, as well as ways to manage and overcome institutional challenges.","open source software, computing education, computing education research, community software, educational tools","","ITiCSE-WGR '22"
"Conference Paper","Ericson BJ,Denny P,Prather J,Duran R,Hellas A,Leinonen J,Miller CS,Morrison BB,Pearce JL,Rodger SH","Parsons Problems and Beyond: Systematic Literature Review and Empirical Study Designs","","2022","","","191–234","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 Working Group Reports on Innovation and Technology in Computer Science Education","Dublin, Ireland","2022","","","https://doi.org/10.1145/3571785.3574127;http://dx.doi.org/10.1145/3571785.3574127","10.1145/3571785.3574127","Programming is a complex task that requires the development of many skills including knowledge of syntax, problem decomposition, algorithm development, and debugging. Code-writing activities are commonly used to help students develop these skills, but the difficulty of writing code from a blank page can overwhelm many novices. Parsons problems offer a simpler alternative to writing code by providing scrambled code blocks that must be placed in the correct order to solve a problem. In the 16 years since their introduction to the computing education community, an expansive body of literature has emerged that documents a range of tools, novel problem variations and makes numerous claims of benefits to learners. In this work, we track the origins of Parsons problems, outline their defining characteristics, and conduct a comprehensive review of the literature to document the evidence of benefits to learners and to identify gaps that require exploration. To facilitate future work, we design empirical studies and develop associated resources that are ready for deployment at a large scale. Collectively, this review and the provided experimental resources will serve as a focal point for researchers interested in advancing our understanding of Parsons problems and their benefits to learners.","parson's puzzles, code puzzles, parsons problems, parson's problems, parsons puzzles, parson's programming puzzles","","ITiCSE-WGR '22"
"Conference Paper","Liu Y,Huang Z,Yu Y,Hussain Y,Lin L","Improving Code Completion by Sequence Features and Structural Features","","2022","","","51–58","Association for Computing Machinery","New York, NY, USA","Proceedings of the 4th World Symposium on Software Engineering","Xiamen, China","2022","9781450396950","","https://doi.org/10.1145/3568364.3568373;http://dx.doi.org/10.1145/3568364.3568373","10.1145/3568364.3568373","Code completion is essential in integrated development environments (IDEs). It has also shown intelligence in helping developers to product. Recently, neural network-based models have helped improve code completion by capturing code information from the abstract syntax tree (AST). However, these methods suffer from several issues. First, the code sequence features are not fully exploited. Second, the sequence features are not effectively combined and utilized with structural features. In this paper, we explore the effectiveness of code sequence features using relative position encoding at first. Then we combine the sequence features with structural features using an extended attention mechanism to enhance performance. We evaluate the proposed approach in two real-world datasets and find that sequence features are practically crucial for code completion. The sequence features combined with structural features enhance the code completion performance. Also, we employ Byte-Pair Encoding (BPE) to mitigate the out-of-vocabulary (OOV) issue in this task. Our best model has a 10% improvement for the mean reciprocal rank (MRR) metric compared to previous researches.","Code completion, structural features, sequential features, neural networks","","WSSE '22"
"Journal Article","Wiesen C,Becker S,Walendy R,Paar C,Rummel N","The Anatomy of Hardware Reverse Engineering: An Exploration of Human Factors during Problem Solving","ACM Trans.  Comput. -Hum.  Interact.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-12","","1073-0516","https://doi.org/10.1145/3577198;http://dx.doi.org/10.1145/3577198","10.1145/3577198","Understanding of microchips, known as Hardware Reverse Engineering (HRE), is driven by analysts’ problem solving. This work sheds light on these hitherto poorly understood problem-solving processes. We propose a methodology addressing the problem of HRE experts being unavailable for research. We developed a training enabling students to acquire intermediate levels of HRE expertise. Besides one expert, we recruited eight top-performing students from this training for our exploratory study. All participants completed a realistic HRE task involving the removal of a copyright protection mechanism from a hardware circuit. We analyzed 2445 log entries by applying an iterative open coding and developed a detailed hierarchical problem-solving model. Our exploration yielded insights into problem-solving strategies and revealed that two intermediates solved the task with a comparable solution time to the expert. We discuss that HRE problem solving may be a function of both expertise and cognitive abilities, and outline ideas for novel countermeasures.","cognitive abilities, expertise, iterative open coding, cognitive obfuscation, problem solving","Just Accepted",""
"Conference Paper","Zhong Y,Zhang X,Tao W,Zhang Y","A Systematic Literature Review of Clone Evolution","","2022","","","461–473","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Conference on Computer Science and Software Engineering","Guilin, China","2022","9781450397780","","https://doi.org/10.1145/3569966.3570091;http://dx.doi.org/10.1145/3569966.3570091","10.1145/3569966.3570091","Code clones are identical or nearly similar code fragments often introduced into software systems by programmers with software modification and maintenance. During the evolution of the software system, code clones may experience multiple changes, such as the increase in number, disappearance, location change, etc. These changes increase the difficulty of clone management and possibly introduce bugs into the software, leading to the high price of clone management and maintenance. Therefore, it is necessary to study the clone evolution. In this paper, we summarize the research works in code clone evolution in recent decades. Based on the previous review and survey, we found a total of 47 relevant papers and divided them into five categories with the help of the LDA model. We present our analysis of the current research and discussion about the possible future progress in this paper. The final result of the debate is that we believe the future work will divide into two aspects. On the one hand, developing clone management tools based on the current results become a possible direction; on the other hand, development and improvement may appear in existing tools with more theoretical support due to more knowledge of the evolutionary characteristics of clones.","systematic literature review, clone evolution, code clone","","CSSE '22"
"Journal Article","Tian H,Liu K,Li Y,Kaboré AK,Koyuncu A,Habib A,Li L,Wen J,Klein J,Bissyandé TF","The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-12","","1049-331X","https://doi.org/10.1145/3576039;http://dx.doi.org/10.1145/3576039","10.1145/3576039","A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.","Embeddings, Patch Correctness, Machine Learning, Features Combination, Explanation, Program Repair, Distributed Representation Learning","Just Accepted",""
"Journal Article","Wolter T,Barcomb A,Riehle D,Harutyunyan N","Open Source License Inconsistencies on GitHub","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-12","","1049-331X","https://doi.org/10.1145/3571852;http://dx.doi.org/10.1145/3571852","10.1145/3571852","Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository, and the licenses found in the code. We analysed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately ten percent represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not only look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.","license conflicts, license management","Just Accepted",""
"Conference Paper","Fernandes S,Aguiar A,Restivo A","A Live Environment to Improve the Refactoring Experience","","2022","","","30–37","Association for Computing Machinery","New York, NY, USA","Companion Proceedings of the 6th International Conference on the Art, Science, and Engineering of Programming","Porto, Portugal","2022","9781450396561","","https://doi.org/10.1145/3532512.3535222;http://dx.doi.org/10.1145/3532512.3535222","10.1145/3532512.3535222","Refactoring helps improve the design of software systems, making them more understandable, readable, maintainable, cleaner, and self-explanatory. Many refactoring tools allow developers to select and execute the best refactorings for their code. However, most of them lack quick and continuous feedback, support, and guidance, leading to a poor refactoring experience. To fill this gap, we are researching ways to increase liveness in refactoring. Live Refactoring consists of continuously knowing, in real-time, what and why to refactor. To explore the concept of Live Refactoring and its main components — recommendation, visualization, and application, we prototyped a Live Refactoring Environment focused on the Extract Method refactoring. With it, developers can receive recommendations about the best refactoring options and have support to apply them automatically. This work helped us reinforce the hypothesis that early and continuous refactoring feedback helps to shorten the time needed to create high-quality systems.","liveness, code smells, software visualization, code quality metrics, refactoring","","Programming '22"
"Conference Paper","Alcoz AG,Busse-Grawitz C,Marty E,Vanbever L","Reducing P4 Language's Voluminosity Using Higher-Level Constructs","","2022","","","19–25","Association for Computing Machinery","New York, NY, USA","Proceedings of the 5th International Workshop on P4 in Europe","Rome, Italy","2022","9781450399357","","https://doi.org/10.1145/3565475.3569078;http://dx.doi.org/10.1145/3565475.3569078","10.1145/3565475.3569078","Over the last years, P4 has positioned itself as the primary language for data-plane programming. Despite its constant evolution, the P4 language still ""suffers"" from one significant limitation: the voluminosity of its code. P4 applications easily reach thousands of lines of code, becoming hard to develop, debug, and maintain. The reason is twofold: P4 requires many characters to express individual concepts (verbosity), and it relies on code repetition (lack of parametrization).Today, P4 users overcome this limitation by relying on templating tools, hand-crafted scripts, and complicated macros. Unfortunately, these methods are not optimal: they make the development process difficult and do not generalize well beyond one codebase.In this work, we propose reducing the voluminosity of P4 code by introducing higher-level language constructs. We present O4, an extended version of P4, that includes three such constructs: arrays (which group same-type entities together), loops (which reduce simple repetitions), and factories (which enable code parametrization).We evaluate O4 on several state-of-the-art programs and show how, with respect to P4: (i) it reduces code volumes by up to 80%, (ii) it decreases code verbosity by 44% on average, and (iii) it cuts duplicated code by 60%. We contribute a compiler implementation that provides said benefits with just a 3.5% increase in compilation time.","programming languages, programmable networks, P4 language","","EuroP4 '22"
"Conference Paper","Ahn S,Ahn S,Koo H,Paek Y","Practical Binary Code Similarity Detection with BERT-Based Transferable Similarity Learning","","2022","","","361–374","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3567975;http://dx.doi.org/10.1145/3564625.3567975","10.1145/3564625.3567975","Binary code similarity detection (BCSD) serves as a basis for a wide spectrum of applications, including software plagiarism, malware classification, and known vulnerability discovery. However, the inference of contextual meanings of a binary is challenging due to the absence of semantic information available in source codes. Recent advances leverage the benefits of a deep learning architecture into a better understanding of underlying code semantics and the advantages of the Siamese architecture into better BCSD. In this paper, we propose BinShot, a BERT-based similarity learning architecture that is highly transferable for effective BCSD. We tackle the problem of detecting code similarity with one-shot learning (a special case of few-shot learning). To this end, we adopt a weighted distance vector with a binary cross entropy as a loss function on top of BERT. With the prototype of BinShot, our experimental results demonstrate the effectiveness, transferability, and practicality of BinShot, which is robust to detecting the similarity of previously unseen functions. We show that BinShot outperforms the previous state-of-the-art approaches for BCSD.","Similarity Detection, Binary Analysis, Deep Neural Network","","ACSAC '22"
"Conference Paper","Thapa C,Jang SI,Ahmed ME,Camtepe S,Pieprzyk J,Nepal S","Transformer-Based Language Models for Software Vulnerability Detection","","2022","","","481–496","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3567985;http://dx.doi.org/10.1145/3564625.3567985","10.1145/3564625.3567985","The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the transferability of the knowledge gained by these models in one domain to other related domains, and the closeness of natural languages to high-level programming languages, such as C/C++, this work studies how to leverage (large) transformer-based language models in detecting software vulnerabilities and how good are these models for vulnerability detection tasks. In this regard, firstly, we present a systematic (cohesive) framework that details source code translation, model preparation, and inference. Then, we perform an empirical analysis of software vulnerability datasets of C/C++ source codes having multiple vulnerabilities corresponding to the library function call, pointer usage, array usage, and arithmetic expression. Our empirical results demonstrate the good performance of the language models in vulnerability detection. Moreover, these language models have better performance metrics, such as F1-score, than the contemporary models, namely bidirectional long short term memory and bidirectional gated recurrent unit. Experimenting with the language models is always challenging due to the requirement of computing resources, platforms, libraries, and dependencies. Thus, this paper also analyses the popular platforms to efficiently fine-tune these models and present recommendations while choosing the platforms for our framework.","Software vulnerability detection, GPT-2, transformer-based models, BERT","","ACSAC '22"
"Conference Paper","Cao Y,Liang R,Chen K,Hu P","Boosting Neural Networks to Decompile Optimized Binaries","","2022","","","508–518","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3567998;http://dx.doi.org/10.1145/3564625.3567998","10.1145/3564625.3567998","Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.","","","ACSAC '22"
"Conference Paper","Luo Y,Xu W,Xu D","Compact Abstract Graphs for Detecting Code Vulnerability with GNN Models","","2022","","","497–507","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3564655;http://dx.doi.org/10.1145/3564625.3564655","10.1145/3564625.3564655","Source code representation is critical to the machine-learning-based approach to detecting code vulnerability. This paper proposes Compact Abstract Graphs (CAGs) of source code in different programming languages for predicting a broad range of code vulnerabilities with Graph Neural Network (GNN) models. CAGs make the source code representation aligned with the task of vulnerability classification and reduce the graph size to accelerate model training with minimum impact on the prediction performance. We have applied CAGs to six GNN models and large Java/C datasets with 114 vulnerability types in Java programs and 106 vulnerability types in C programs. The experiment results show that the GNN models have performed well, with accuracy ranging from 94.7% to 96.3% on the Java dataset and from 91.6% to 93.2% on the C dataset. The resultant GNN models have achieved promising performance when applied to more than 2,500 vulnerabilities collected from real-world software projects. The results also show that using CAGs for GNN models is significantly better than ASTs, CFGs (Control Flow Graphs), and PDGs (Program Dependence Graphs). A comparative study has demonstrated that the CAG-based GNN models can outperform the existing methods for machine learning-based vulnerability detection.","static code analysis, graph neural networks, machine learning, Software vulnerability","","ACSAC '22"
"Conference Paper","Picazo-Sanchez P,Eriksson B,Sabelfeld A","No Signal Left to Chance: Driving Browser Extension Analysis by Download Patterns","","2022","","","896–910","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3567988;http://dx.doi.org/10.1145/3564625.3567988","10.1145/3564625.3567988","Browser extensions are popular small applications that allow users to enrich their browsing experience. Yet browser extensions pose security concerns because they can leak user data and maliciously act on behalf of the user. Because malicious behavior can manifest dynamically, detecting malicious extensions remains a challenge for the research community, browser vendors, and web application developers. This paper identifies download patterns as a useful signal for analyzing browser extensions. We leverage machine learning for clustering extensions based on their download patterns, confirming at a large scale that many extensions follow strikingly similar download patterns. Our key insight is that the download pattern signal can be used for identifying malicious extensions. To this end, we present a novel technique to detect malicious extensions based on the public number of downloads in the Chrome Web Store. This technique fruitfully combines machine learning with security analysis, showing that the download patterns signal can be used to both directly spot malicious extensions and as input to subsequent analysis of suspicious extensions. We demonstrate the benefits of our approach on a dataset from a daily crawl of the Web Store over 6 months to track the number of downloads. We find 135 clusters and identify 61 of them to have at least 80% malicious extensions. We train our classifier and run it on a test set of 1,212 currently active extensions in the Web Store successfully detecting 326 extensions as malicious solely based on downloads. Further, we show that by combining this signal with code similarity analysis, using the 326 as a seed, we find an additional 6,579 malicious extensions.","Browser Extensions, Web Security","","ACSAC '22"
"Conference Paper","Koike Y,Katsura H,Yakura H,Kurogome Y","SLOPT: Bandit Optimization Framework for Mutation-Based Fuzzing","","2022","","","519–533","Association for Computing Machinery","New York, NY, USA","Proceedings of the 38th Annual Computer Security Applications Conference","Austin, TX, USA","2022","9781450397599","","https://doi.org/10.1145/3564625.3564659;http://dx.doi.org/10.1145/3564625.3564659","10.1145/3564625.3564659","Mutation-based fuzzing has become one of the most common vulnerability discovery solutions over the last decade. Fuzzing can be optimized when targeting specific programs, and given that, some studies have employed online optimization methods to do it automatically, i.e., tuning fuzzers for any given program in a program-agnostic manner. However, previous studies have neither fully explored mutation schemes suitable for online optimization methods, nor online optimization methods suitable for mutation schemes. In this study, we propose an optimization framework called SLOPT that encompasses both a bandit-friendly mutation scheme and mutation-scheme-friendly bandit algorithms. The advantage of SLOPT is that it can generally be incorporated into existing fuzzers, such as AFL and Honggfuzz. As a proof of concept, we implemented SLOPT-AFL++ by integrating SLOPT into AFL++ and showed that the program-agnostic optimization delivered by SLOPT enabled SLOPT-AFL++ to achieve higher code coverage than AFL++ in all of ten real-world FuzzBench programs. Moreover, we ran SLOPT-AFL++ against several real-world programs from OSS-Fuzz and successfully identified three previously unknown vulnerabilities, even though these programs have been fuzzed by AFL++ for a considerable number of CPU days on OSS-Fuzz.","grey-box fuzzing, mutation-based fuzzing, online optimization","","ACSAC '22"
"Conference Paper","Risberg Alaküla A,Hedin G,Fors N,Pop A","Property Probes: Source Code Based Exploration of Program Analysis Results","","2022","","","148–160","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering","Auckland, New Zealand","2022","9781450399197","","https://doi.org/10.1145/3567512.3567525;http://dx.doi.org/10.1145/3567512.3567525","10.1145/3567512.3567525","We present property probes, a mechanism for helping a developer interactively explore partial program analysis results in terms of the source program, and as the program is edited. A node locator data structure is introduced that maps between source code spans and program representation nodes, and that helps identify probed nodes in a robust way, after modifications to the source code. We have developed a client-server based tool supporting property probes, and argue that it is very helpful in debugging and understanding program analyses. We have evaluated our tool on several languages and analyses, including a full Java compiler and a tool for intraprocedural dataflow analysis. Our performance results show that the probe overhead is negligible even when analyzing large projects.","debugging, property probes, program analysis","","SLE 2022"
"Conference Paper","Nguyen Thanh B,Nguyen N. H. M,Le Thi My H,Nguyen Thanh B","Ml-Codesmell: A Code Smell Prediction Dataset for Machine Learning Approaches","","2022","","","368–374","Association for Computing Machinery","New York, NY, USA","Proceedings of the 11th International Symposium on Information and Communication Technology","Hanoi, Vietnam","2022","9781450397254","","https://doi.org/10.1145/3568562.3568643;http://dx.doi.org/10.1145/3568562.3568643","10.1145/3568562.3568643","In recent years, many studies on detecting code smells in source code have published datasets with limited characteristics, such as the ambiguity of code smell definitions leads to different interpretations for each code smell, the number of samples of the datasets is small, and the features of the datasets are heterogeneous. Therefore, comparing performance between detecting code smell models is challenging, and the datasets are often not reusable in other code smell detection studies. In this work, we propose the ml-Codesmell dataset created by analyzing source code and extracting massive source code metrics with many labelled code smells. The proposed dataset has been used to train and predict code smell using machine learning algorithms. Based on the high confidential F1-score in evaluation, the ml-Codesmell dataset demonstrates a strong correlation between features and labels. Regarding these advantages, the ml-Codesmell dataset is expected to be helpful for studies on detecting code smell using machine learning approaches in software development.","Code Smell Prediction, Dataset, Machine learning","","SoICT '22"
"Conference Paper","Fraivert D,Lorenz DH","Language Support for Refactorability Decay Prevention","","2022","","","122–134","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences","Auckland, New Zealand","2022","9781450399203","","https://doi.org/10.1145/3564719.3568688;http://dx.doi.org/10.1145/3564719.3568688","10.1145/3564719.3568688","Even code that is free of smells may be at high risk of forming them. In such cases, developers can either perform preventive refactoring in order to reduce this risk, or leave the code as is and perform corrective refactoring as smells emerge. In practice, however, developers usually avoid preventive refactoring during the development phase, and when code smells eventually form, other developers who are less acquainted with the code avoid the more complex corrective refactoring. As a result, a refactoring opportunity is missed, and the quality and maintainability of the code is compromised. In this work, we treat refactoring not as a single atomic action, but rather as a sequence of subactions. We divide the responsibility for these subactions between the original developer of the code, who just prepares the code for refactoring, and a future developer, who may need to carry out the actual refactoring action. To manage this division of responsibility, we introduce a set of annotations along with an annotation processor that prevents software erosion from compromising the ability to perform the refactoring action.","refactoring, extract function, preventive refactoring, move function, code reuse, refactorability decay, corrective refactoring, move method, extract method","","GPCE 2022"
"Conference Paper","Edwards J,Petricek T","Interaction vs. Abstraction: Managed Copy and Paste","","2022","","","11–19","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments","Auckland, New Zealand","2022","9781450399104","","https://doi.org/10.1145/3563836.3568723;http://dx.doi.org/10.1145/3563836.3568723","10.1145/3563836.3568723","Abstraction is at the core of programming, but it has a cost. We exhort programmers to use proper abstractions like functions but they often find it easier to copy & paste instead. Copy & paste is roundly criticized because subsequent changes to copies may have to be manually reconciled, which is easily overlooked and easily mistaken. It seems there is a conflict between the generality and reusability of abstraction with the simplicity of copying and modifying code. We suggest that this conflict arises because we are still thinking in terms of paper-based notations. Indeed the term “copy & paste” originates from the practice of physically cutting and gluing slips of paper. But an interactive programming environment can free us from the limitations of paper. We propose managed copy & paste, in which the programming environment records copy & paste operations, along with structural edit operations, so that it can track the differences between copies and reconcile them on command. These capabilities mitigate the aforementioned problems of copy & paste, allowing abstraction to be deferred or reduced. Managed copy & paste resembles version control as in git, except that it works not between versions of a program but between copies within the program. It is based on a new theory of structural editing and version control that offers precise differencing based on edit history rather than the heuristic differencing of textual version control. We informally explain this theory and demonstrate a prototype implementation of a data science notebook. Lastly, we suggest further mechanisms of gradual abstraction that could be provided by the programming environment to lessen the cognitive load of programming.","code clones, structure editing, copy & paste, version control","","PAINT 2022"
"Conference Paper","Smits J,Hartman T,Cockx J","Optimising First-Class Pattern Matching","","2022","","","74–83","Association for Computing Machinery","New York, NY, USA","Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering","Auckland, New Zealand","2022","9781450399197","","https://doi.org/10.1145/3567512.3567519;http://dx.doi.org/10.1145/3567512.3567519","10.1145/3567512.3567519","Pattern matching is a high-level notation for programs to analyse the shape of data, and can be optimised to efficient low-level instructions. The Stratego language uses first-class pattern matching, a powerful form of pattern matching that traditional optimisation techniques do not apply to directly. In this paper, we investigate how to optimise programs that use first-class pattern matching. Concretely, we show how to map first-class pattern matching to a form close to traditional pattern matching, on which standard optimisations can be applied. Through benchmarks, we demonstrate the positive effect of these optimisations on the run-time performance of Stratego programs. We conclude that the expressive power of first-class pattern matching does not hamper the optimisation potential of a language that features it.","optimisation, strategic programming, pattern matching","","SLE 2022"
"Conference Paper","Voinov P,Rigger M,Su Z","Forest: Structural Code Editing with Multiple Cursors","","2022","","","137–152","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software","Auckland, New Zealand","2022","9781450399098","","https://doi.org/10.1145/3563835.3567663;http://dx.doi.org/10.1145/3563835.3567663","10.1145/3563835.3567663","Software developers frequently refactor code. Often, a single logical refactoring change involves changing multiple related components in a source base such as renaming each occurrence of a variable or function. While many code editors can perform such common and generic refactorings, they do not support more complex refactorings or those that are specific to a given code base. For those, as a flexible---albeit less interactive---alternative, developers can write refactoring scripts that can implement arbitrarily complex logic by manipulating the program's tree representation. In this work, we present Forest, a structural code editor that aims to bridge the gap between the interactiveness of code editors and the expressiveness of refactoring scripts. While structural editors have occupied a niche as general code editors, the key insight of this work is that they enable a novel structural multi-cursor design that allows Forest to reach a similar expressiveness as refactoring scripts; Forest allows to perform a single action simultaneously in multiple program locations and thus support complex refactorings. To support interactivity, Forest provides features typical for text code editors such as writing and displaying the program through its textual representation. Our evaluation demonstrates that Forest allows performing edits similar to those from refactoring scripts, while still being interactive. We attempted to perform edits from 48 real-world refactoring scripts using Forest and found that 11 were possible, while another 17 would be possible with added features. We believe that a multi-cursor setting plays to the strengths of structural editing, since it benefits from reliable and expressive commands. Our results suggest that multi-cursor structural editors could be practical for performing small-scale specialized refactorings.","multi-cursor, refactoring, Structural editing","","Onward! 2022"
"Conference Paper","Lister R","Some Thoughts on Designing Eye Movement Studies for Novice Programmers","","2022","","","15–22","Association for Computing Machinery","New York, NY, USA","Proceedings of the Tenth International Workshop on Eye Movements in Programming","Pittsburgh, Pennsylvania","2022","9781450392891","","https://doi.org/10.1145/3524488.3527363;http://dx.doi.org/10.1145/3524488.3527363","10.1145/3524488.3527363","I first describe my three-stage model of how novices understand code. In the first stage, the novice cannot trace code. In the second stage, the novice has mastered tracing, but, crucially, that is the only skill they have mastered. It is only when novices reach the third stage that they begin to reason about code in a more general, abstract way. Most programming instructors mistakenly assume that all students begin at the third stage. Having described the three-stage model, I then explore implications of the model for the design of eye movement studies. I also provide some pieces of code that would make for interesting eye movement studies.","novice programmers, eye scanning, eye movement","","EMIP '22"
"Journal Article","Zheng Z,Chen W,Zhong Z,Chen Z,Lu Y","Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-11","","1049-331X","https://doi.org/10.1145/3571847;http://dx.doi.org/10.1145/3571847","10.1145/3571847","Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system–decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.","Blockchain, Ethereum, Ponzi Schemes, Malware Detection","Just Accepted",""
"Journal Article","Zampetti F,Tamburri DA,Panichella S,Panichella A,Canfora G,Penta MD","Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-11","","1049-331X","https://doi.org/10.1145/3571854;http://dx.doi.org/10.1145/3571854","10.1145/3571854","Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, e.g., faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This paper empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in 8 different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds, while balancing the use of Hardware-in-the-Loop (HiL) and simulators; (ii) coping with difficulties in software deployment (iii) accounting for simulators and HiL differing in their behavior; and (vi) combining hardware/software expertise in the development team. Our findings open the road towards recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.","Empirical Software Engineering, Continuous Integration and Delivery, Cyber-Physical Systems","Just Accepted",""
"Conference Paper","Li P,Guo Y,Luo Y,Wang X,Wang Z,Liu X","Graph Neural Networks Based Memory Inefficiency Detection Using Selective Sampling","","2022","","","","IEEE Press","Dallas, Texas","Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","","2022","","","","","Production software of data centers oftentimes suffers from unnecessary memory inefficiencies caused by inappropriate use of data structures, conservative compiler optimizations, and so forth. Nevertheless, whole-program monitoring tools often incur incredibly high overhead due to fine-grained memory access instrumentation. Consequently, the fine-grained monitoring tools are not viable for long-running, large-scale data center applications due to strict latency criteria (e.g., service-level agreement or SLA).To this end, this work presents a novel learning-aided system, namely Puffin, to identify three kinds of unnecessary memory operations including dead stores, silent loads and silent stores, by applying gated graph neural networks onto fused static and dynamic program semantics with respect to relative positional embedding. To deploy the system in large-scale data centers, this work explores a sampling-based detection infrastructure with high efficacy and negligible overhead. We evaluate Puffin upon the well-known SPEC CPU 2017 benchmark suite for four compilation options. Experimental results show that the proposed method is able to capture the three kinds of memory inefficiencies with as high accuracy as 96% and a reduced checking overhead by 5.66× over the state-of-the-art tool.","sampling, memory inefficiency detection, program embedding, graph neural network","","SC '22"
"Conference Paper","Wilson E,Mueller F,Pakin S","Combining Hard and Soft Constraints in Quantum Constraint-Satisfaction Systems","","2022","","","","IEEE Press","Dallas, Texas","Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis","","2022","","","","","This work presents a generalization of NchooseK, a constraint satisfaction system designed to target both quantum circuit devices and quantum annealing devices. Previously, NchooseK supported only hard constraints, which made it suitable for expressing problems in NP (e.g., 3-SAT) but not NP-hard problems (e.g., minimum vertex cover). In this paper we show how support for soft constraints can be added to the model and implementation, broadening the classes of problems that can be expressed elegantly in NchooseK without sacrificing portability across different quantum devices.Through a set of examples, we argue that this enhanced version of NchooseK enables problems to be expressed in a more concise, less error-prone manner than if these problems were encoded manually for quantum execution. We include an empirical evaluation of performance, scalability, and fidelity on both a large IBM Q system and a large D-Wave system.","quantum annealing, circuit-model quantum computing, programming models","","SC '22"
"Conference Paper","Islam MH,Paul R,Mondal M","Predicting Buggy Code Clones through Machine Learning","","2022","","","130–139","IBM Corp.","USA","Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering","Toronto, Canada","2022","9781450396165","","https://doi.org/10.1145/3564721.3564731;http://dx.doi.org/10.1145/3564721.3564731","10.1145/3564721.3564731","Code clones (similar code fragments in a code-base","Bug-proneness of code clones, Machine Learning, Clone types, Code clones","","CASCON '22"
"Conference Paper","Chakraborty S,Ahmed T,Ding Y,Devanbu PT,Ray B","NatGen: Generative Pre-Training by “Naturalizing” Source Code","","2022","","","18–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549162;http://dx.doi.org/10.1145/3540250.3549162","10.1145/3540250.3549162","Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, “Naturalizing” of source code, exploiting code’s bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code’s bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow)","Source Code Pre-training, Semantic Preserving Transformation, Source Code Transformer, Neural Network","","ESEC/FSE 2022"
"Conference Paper","Ramkisoen PK,Businge J,van Bladel B,Decan A,Demeyer S,De Roover C,Khomh F","PaReco: Patched Clones and Missed Patches among the Divergent Variants of a Software Family","","2022","","","646–658","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549112;http://dx.doi.org/10.1145/3540250.3549112","10.1145/3540250.3549112","Re-using whole repositories as a starting point for new projects is often done by maintaining a variant fork parallel to the original. However, the common artifacts between both are not always kept up to date. As a result, patches are not optimally integrated across the two repositories, which may lead to sub-optimal maintenance between the variant and the original project. A bug existing in both repositories can be patched in one but not the other (we see this as a missed opportunity) or it can be manually patched in both probably by different developers (we see this as effort duplication). In this paper we present a tool (named PaReCo) which relies on clone detection to mine cases of missed opportunity and effort duplication from a pool of patches. We analyzed 364 (source to target) variant pairs with 8,323 patches resulting in a curated dataset containing 1,116 cases of effort duplication and 1,008 cases of missed opportunities. We achieve a precision of 91%, recall of 80%, accuracy of 88%, and F1-score of 85%. Furthermore, we investigated the time interval between patches and found out that, on average, missed patches in the target variants have been introduced in the source variants 52 weeks earlier. Consequently, PaReCo can be used to manage variability in “time” by automatically identifying interesting patches in later project releases to be backported to supported earlier releases.","Bug-fixes, Software family, Github, Social coding, Effort duplication, Forking, Variants, Clone detection, Clone&own","","ESEC/FSE 2022"
"Conference Paper","Sarmah PP,Chimalakonda S","API + Code = Better Code Summary? Insights from an Exploratory Study","","2022","","","92–101","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering","Singapore, Singapore","2022","9781450398602","","https://doi.org/10.1145/3558489.3559075;http://dx.doi.org/10.1145/3558489.3559075","10.1145/3558489.3559075","Automatic code summarization techniques aid in program comprehension by generating a natural language summary from source code. Recent research in this area has seen significant developments from basic Seq2Seq models to different flavors of transformer models, which try to encode the structural components of the source code using various input representations. Apart from the source code itself, components used in source code, such as API knowledge, have previously been helpful in code summarization using recurrent neural networks (RNN). So, in this article, along with source code and its structure, we explore the importance of APIs in improving the performance of code summarization models. Our model uses a transformer-based architecture containing two encoders for two input modules, source code and API sequences, and a joint decoder to generate summaries combining the outputs of two encoders. We experimented with our proposed model on a dataset of java projects collected from GitHub containing around 87K triplets. The experiments show our model outperforms most of the existing RNN-based approaches, but the overall performance does not improve compared with the state-of-the-art approach using transformers. Thus, the results show that although API information is helpful for code summarization, we see immense scope for further research focusing on improving models and leveraging additional API knowledge for code summarization.","API sequences, transformers, source code, code summarization","","PROMISE 2022"
"Conference Paper","Jodavi M,Tsantalis N","Accurate Method and Variable Tracking in Commit History","","2022","","","183–195","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549079;http://dx.doi.org/10.1145/3540250.3549079","10.1145/3540250.3549079","Tracking program elements in the commit history of a project is essential for supporting various software maintenance, comprehension and evolution tasks. Accuracy is of paramount importance for the adoption of program element tracking tools by developers and researchers. To this end, we propose CodeTracker, a refactoring-aware tool that can generate the commit change history for method and variable declarations with a very high accuracy. More specifically, CodeTracker has 99.9% precision and recall in method tracking, surpassing the previous state-of-the-art tool, CodeShovel, with a comparable execution time. CodeTracker is the first tool of its kind that can track the change history of variables with 99.7% precision and 99.8% recall. To evaluate its accuracy in variable tracking, we extended the oracle created by Grund et al. for the evaluation of CodeShovel, with the complete change history of all 1345 variables and parameters declared in the 200 methods comprising the Grund et al. oracle. We make our tool and extended oracle publicly available to enable the replication of our experiments and facilitate future research on program element tracking techniques.","commit change history, refactoring-aware source code tracking","","ESEC/FSE 2022"
"Conference Paper","Yi X,Wu D,Jiang L,Fang Y,Zhang K,Zhang W","An Empirical Study of Blockchain System Vulnerabilities: Modules, Types, and Patterns","","2022","","","709–721","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549105;http://dx.doi.org/10.1145/3540250.3549105","10.1145/3540250.3549105","Blockchain, as a distributed ledger technology, becomes increasingly popular, especially for enabling valuable cryptocurrencies and smart contracts. However, the blockchain software systems inevitably have many bugs. Although bugs in smart contracts have been extensively investigated, security bugs of the underlying blockchain systems are much less explored. In this paper, we conduct an empirical study on blockchain’s system vulnerabilities from four representative blockchains, Bitcoin, Ethereum, Monero, and Stellar. Specifically, we first design a systematic filtering process to effectively identify 1,037 vulnerabilities and their 2,317 patches from 34,245 issues/PRs (pull requests) and 85,164 commits on GitHub. We thus build the first blockchain vulnerability dataset, which is available at https://github.com/VPRLab/BlkVulnDataset. We then perform unique analyses of this dataset at three levels, including (i) file-level vulnerable module categorization by identifying and correlating module paths across projects, (ii) text-level vulnerability type clustering by natural language processing and similarity-based sentence clustering, and (iii) code-level vulnerability pattern analysis by generating and clustering code change signatures that capture both syntactic and semantic information of patch code fragments. Our analyses reveal three key findings: (i) some blockchain modules are more susceptible than the others; notably, each of the modules related to consensus, wallet, and networking has over 200 issues; (ii) about 70% of blockchain vulnerabilities are of traditional types, but we also identify four new types specific to blockchains; and (iii) we obtain 21 blockchain-specific vulnerability patterns that capture unique blockchain attributes and statuses, and demonstrate that they can be used to detect similar vulnerabilities in other popular blockchains, such as Dogecoin, Bitcoin SV, and Zcash.","Data Mining, System Vulnerability, Blockchain Security","","ESEC/FSE 2022"
"Conference Paper","Li Y,Wang S,Nguyen TN","UTANGO: Untangling Commits with Context-Aware, Graph-Based, Code Change Clustering Learning Model","","2022","","","221–232","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549171;http://dx.doi.org/10.1145/3540250.3549171","10.1145/3540250.3549171","During software evolution, developers make several changes and commit them into the repositories. Unfortunately, many of them tangle different purposes, both hampering program comprehension and reducing separation of concerns. Automated approaches with deterministic solutions have been proposed to untangle commits. However, specifying an effective clustering criteria on the changes in a commit for untangling is challenging for those approaches. In this work, we present UTango, a machine learning (ML)-based approach that learns to untangle the changes in a commit. We develop a novel code change clustering learning model that learns to cluster the code changes, represented by the embeddings, into different groups with different concerns. We adapt the agglomerative clustering algorithm into a supervised-learning clustering model operating on the learned code change embeddings via trainable parameters and a loss function in comparing the predicted clusters and the correct ones during training. To facilitate our clustering learning model, we develop a context-aware, graph-based, code change representation learning model, leveraging Label, Graph-based Convolution Network to produce the contextualized embeddings for code changes, that integrates program dependencies and the surrounding contexts of the changes. The contexts and cloned code are also explicitly represented, helping UTango distinguish the concerns. Our empirical evaluation on C# and Java datasets with 1,612 and 14k tangled commits show that it achieves the accuracy of 28.6%– 462.5% and 13.3%–100.0% relatively higher than the state-of-the-art commit-untangling approaches for C# and Java, respectively.","Deep Learning, Commit Untangling, Code Change Embeddings","","ESEC/FSE 2022"
"Conference Paper","Garg S,Moghaddam RZ,Clement CB,Sundaresan N,Wu C","DeepDev-PERF: A Deep Learning-Based Approach for Improving Software Performance","","2022","","","948–958","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549096;http://dx.doi.org/10.1145/3540250.3549096","10.1145/3540250.3549096","Improving software performance is an important yet challenging part of the software development cycle. Today, the majority of performance inefficiencies are identified and patched by performance experts. Recent advancements in deep learning approaches and the wide-spread availability of open-source data creates a great opportunity to automate the identification and patching of performance problems. In this paper, we present DeepDev-PERF, a transformer-based approach to suggest performance improvements for C# applications. We pretrain DeepDev-PERF on English and Source code corpora, followed by finetuning for the task of generating performance improvement patches for C# applications. Our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in ‍53","Software Performance, Bug Fixing, Artificial Intelligence","","ESEC/FSE 2022"
"Conference Paper","Le-Cong T,Kang HJ,Nguyen TG,Haryono SA,Lo D,Le XB,Huynh QT","AutoPruner: Transformer-Based Call Graph Pruning","","2022","","","520–532","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549175;http://dx.doi.org/10.1145/3540250.3549175","10.1145/3540250.3549175","Constructing a static call graph requires trade-offs between soundness and precision. Program analysis techniques for constructing call graphs are unfortunately usually imprecise. To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives. In this paper, we present a novel call graph pruning technique, AutoPruner, for eliminating false positives in call graphs via both statistical semantic and structural analysis. Given a call graph constructed by traditional static analysis tools, AutoPruner takes a Transformer-based approach to capture the semantic relationships between the caller and callee functions associated with each edge in the call graph. To do so, AutoPruner fine-tunes a model of code that was pre-trained on a large corpus to represent source code based on descriptions of its semantics. Next, the model is used to extract semantic features from the functions related to each edge in the call graph. AutoPruner uses these semantic features together with the structural features extracted from the call graph to classify each edge via a feed-forward neural network. Our empirical evaluation on a benchmark dataset of real-world programs shows that AutoPruner outperforms the state-of-the-art baselines, improving on F-measure by up to 13% in identifying false-positive edges in a static call graph. Moreover, AutoPruner achieves improvements on two client analyses, including halving the false alarm rate on null pointer analysis and over 10% improvements on monomorphic call-site detection. Additionally, our ablation study and qualitative analysis show that the semantic features extracted by AutoPruner capture a remarkable amount of information for distinguishing between true and false positives.","Call Graph Pruning, Pretrained Language Model, Static Analysis, Transformer","","ESEC/FSE 2022"
"Conference Paper","Shi L,Mu F,Chen X,Wang S,Wang J,Yang Y,Li G,Xia X,Wang Q","Are We Building on the Rock? On the Importance of Data Preprocessing for Code Summarization","","2022","","","107–119","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549145;http://dx.doi.org/10.1145/3540250.3549145","10.1145/3540250.3549145","Code summarization, the task of generating useful comments given the code, has long been of interest. Most of the existing code summarization models are trained and validated on widely-used code comment benchmark datasets. However, little is known about the quality of the benchmark datasets built from real-world projects. Are the benchmark datasets as good as expected? To bridge the gap, we conduct a systematic research to assess and improve the quality of four benchmark datasets widely used for code summarization tasks. First, we propose an automated code-comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets. Then, we apply the tool to further assess the data quality of the four benchmark datasets, based on the detected noises. Finally, we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models. The results show that these data preprocessing noises widely exist in all four benchmark datasets, and removing these noisy data leads to a significant improvement on the performance of code summarization. We believe that the findings and insights will enable a better understanding of data quality in code summarization tasks, and pave the way for relevant research and practice.","Data Quality, Empirical Study, Code Summarization","","ESEC/FSE 2022"
"Conference Paper","OBrien D,Biswas S,Imtiaz S,Abdalkareem R,Shihab E,Rajan H","23 Shades of Self-Admitted Technical Debt: An Empirical Study on Machine Learning Software","","2022","","","734–746","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549088;http://dx.doi.org/10.1145/3540250.3549088","10.1145/3540250.3549088","In software development, the term “technical debt” (TD) is used to characterize short-term solutions and workarounds implemented in source code which may incur a long-term cost. Technical debt has a variety of forms and can thus affect multiple qualities of software including but not limited to its legibility, performance, and structure. In this paper, we have conducted a comprehensive study on the technical debts in machine learning (ML) based software. TD can appear differently in ML software by infecting the data that ML models are trained on, thus affecting the functional behavior of ML systems. The growing inclusion of ML components in modern software systems have introduced a new set of TDs. Does ML software have similar TDs to traditional software? If not, what are the new types of ML specific TDs? Which ML pipeline stages do these debts appear? Do these debts differ in ML tools and applications and when they get removed? Currently, we do not know the state of the ML TDs in the wild. To address these questions, we mined 68,820 self-admitted technical debts (SATD) from all the revisions of a curated dataset consisting of 2,641 popular ML repositories from GitHub, along with their introduction and removal. By applying an open-coding scheme and following upon prior works, we provide a comprehensive taxonomy of ML SATDs. Our study analyzes ML SATD type organizations, their frequencies within stages of ML software, the differences between ML SATDs in applications and tools, and quantifies the removal of ML SATDs. The findings discovered suggest implications for ML developers and researchers to create maintainable ML systems.","technical debt, open-source, machine learning, data science","","ESEC/FSE 2022"
"Conference Paper","Li L,Yang L,Jiang H,Yan J,Luo T,Hua Z,Liang G,Zuo C","AUGER: Automatically Generating Review Comments with Pre-Training Models","","2022","","","1009–1021","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549099;http://dx.doi.org/10.1145/3540250.3549099","10.1145/3540250.3549099","Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, consider- ing what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments. This paper explores a synergy of multiple practical review comments to enhance code review and proposes AUGER (AUtomatically GEnerating Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38% in ROUGE-L. 29% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.","Machine Learning, Code Review, Review Comments, Text Generation","","ESEC/FSE 2022"
"Conference Paper","Nong Y,Ou Y,Pradel M,Chen F,Cai H","Generating Realistic Vulnerabilities via Neural Code Editing: An Empirical Study","","2022","","","1097–1109","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549128;http://dx.doi.org/10.1145/3540250.3549128","10.1145/3540250.3549128","The availability of large-scale, realistic vulnerability datasets is essential both for benchmarking existing techniques and for developing effective new data-driven approaches for software security. Yet such datasets are critically lacking. A promising solution is to generate such datasets by injecting vulnerabilities into real-world programs, which are richly available. Thus, in this paper, we explore the feasibility of vulnerability injection through neural code editing. With a synthetic dataset and a real-world one, we investigate the potential and gaps of three state-of-the-art neural code editors for vulnerability injection. We find that the studied editors have critical limitations on the real-world dataset, where the best accuracy is only 10.03%, versus 79.40% on the synthetic dataset. While the graph-based editors are more effective (successfully injecting vulnerabilities in up to 34.93% of real-world testing samples) than the sequence-based one (0 success), they still suffer from complex code structures and fall short for long edits due to their insufficient designs of the preprocessing and deep learning (DL) models. We reveal the promise of neural code editing for generating realistic vulnerable samples, as they help boost the effectiveness of DL-based vulnerability detectors by up to 49.51% in terms of F1 score. We also provide insights into the gaps in current editors (e.g., they are good at deleting but not at replacing code) and actionable suggestions for addressing them (e.g., designing effective editing primitives).","datasets, benchmarking, software vulnerability, vulnerability detection, data generation, deep learning, data augmentation","","ESEC/FSE 2022"
"Conference Paper","Wan Y,Zhang S,Zhang H,Sui Y,Xu G,Yao D,Jin H,Sun L","You See What I Want You to See: Poisoning Vulnerabilities in Neural Code Search","","2022","","","1233–1245","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549153;http://dx.doi.org/10.1145/3540250.3549153","10.1145/3540250.3549153","Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset. Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50% to top 4.43%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.","backdoor attack, data poisoning, Code search, deep learning, software vulnerability","","ESEC/FSE 2022"
"Conference Paper","Peng X,Zhang C,Zhao Z,Isami A,Guo X,Cui Y","Trace Analysis Based Microservice Architecture Measurement","","2022","","","1589–1599","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3558951;http://dx.doi.org/10.1145/3540250.3558951","10.1145/3540250.3558951","Microservice architecture design highly relies on expert experience and may often result in improper service decomposition. Moreover, a microservice architecture is likely to degrade with the continuous evolution of services. Architecture measurement is thus important for the long-term evolution of microservice architectures. Due to the independent and dynamic nature of services, source code analysis based approaches cannot well capture the interactions between services. In this paper, we propose a trace analysis based microservice architecture measurement approach. We define a trace data model for microservice architecture measurement, which enables fine-grained analysis of the execution processes of requests and the interactions between interfaces and services. Based on the data model, we define 14 architectural metrics to measure the service independence and invocation chain complexity of a microservice system. We implement the approach and conduct three case studies with a student course project, an open-source microservice benchmark system, and three industrial microservice systems. The results show that our approach can well characterize the independence and invocation chain complexity of microservice architectures and help developers to identify microservice architecture issues caused by improper service decomposition and architecture degradation.","Dynamic analysis, Architecture, Tracing, Microservice","","ESEC/FSE 2022"
"Conference Paper","Shanbhag S,Chimalakonda S","Exploring the Under-Explored Terrain of Non-Open Source Data for Software Engineering through the Lens of Federated Learning","","2022","","","1610–1614","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3560883;http://dx.doi.org/10.1145/3540250.3560883","10.1145/3540250.3560883","The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.","data privacy, software engineering research, federated learning, non-open source data","","ESEC/FSE 2022"
"Conference Paper","Maddila C,Shanbhogue S,Agrawal A,Zimmermann T,Bansal C,Forsgren N,Agrawal D,Herzig K,van Deursen A","Nalanda: A Socio-Technical Graph Platform for Building Software Analytics Tools at Enterprise Scale","","2022","","","1246–1256","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3558949;http://dx.doi.org/10.1145/3540250.3558949","10.1145/3540250.3558949","Software development is information-dense knowledge work that requires collaboration with other developers and awareness of artifacts such as work items, pull requests, and file changes. With the speed of development increasing, information overload and information discovery are challenges for people developing and maintaining these systems. Finding information about similar code changes and experts is difficult for software engineers, especially when they work in large software systems or have just recently joined a project. In this paper, we build a large scale data platform named Nalanda platform to address the challenges of information overload and discovery. Nalanda contains two subsystems: (1) a large scale socio-technical graph system, named Nalanda graph system, and (2) a large scale index system, named Nalanda index system that aims at satisfying the information needs of software developers. To show the versatility of the Nalanda platform, we built two applications: (1) a software analytics application with a news feed named MyNalanda that has Daily Active Users (DAU) of 290 and Monthly Active Users (MAU) of 590, and (2) a recommendation system for related work items and pull requests that accomplished similar tasks (artifact recommendation) and a recommendation system for subject matter experts (expert recommendation), augmented by the Nalanda socio-technical graph. Initial studies of the two applications found that developers and engineering managers are favorable toward continued use of the news feed application for information discovery. The studies also found that developers agreed that a system like Nalanda artifact and expert recommendation application could reduce the time spent and the number of places needed to visit to find information.","Collaborative software development, Socio-Technical Graphs, Recommender Systems for Software Engineering, Empirical study","","ESEC/FSE 2022"
"Conference Paper","Zhang Z,Zhang H,Shen B,Gu X","Diet Code is Healthy: Simplifying Programs for Pre-Trained Models of Code","","2022","","","1073–1084","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549094;http://dx.doi.org/10.1145/3540250.3549094","10.1145/3540250.3549094","Pre-trained code representation models such as CodeBERT have demonstrated superior performance in a variety of software engineering tasks, yet they are often heavy in complexity, quadratically with the length of the input sequence. Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more attention to certain types of tokens and statements such as keywords and data-relevant statements. Based on these findings, we propose DietCode, which aims at lightweight leverage of large pre-trained models for source code. DietCode simplifies the input program of CodeBERT with three strategies, namely, word dropout, frequency filtering, and an attention-based strategy that selects statements and tokens that receive the most attention weights during pre-training. Hence, it gives a substantial reduction in the computational cost without hampering the model performance. Experimental results on two downstream tasks show that DietCode provides comparable results to CodeBERT with 40% less computational cost in fine-tuning and testing.","Pre-trained models, Learning program representations, Code intelligence, Program simplification","","ESEC/FSE 2022"
"Conference Paper","López JA,Izquierdo JL,Cuadrado JS","Using the ModelSet Dataset to Support Machine Learning in Model-Driven Engineering","","2022","","","66–70","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings","Montreal, Quebec, Canada","2022","9781450394673","","https://doi.org/10.1145/3550356.3559096;http://dx.doi.org/10.1145/3550356.3559096","10.1145/3550356.3559096","The availability of curated collections of models is essential for the application of techniques like Machine Learning (ML) and Data Analytics to MDE as well as to boost research activities. However, many applications of ML to address MDE tasks are currently limited to small datasets. In this demo paper, we will present ModelSet, a dataset composed of 5,466 Ecore models and 5,120 UML models which have been manually labelled to support ML tasks (http://modelset.github.io). ModelSet is built upon the models collected by the MAR search engine (http://mar-search.org), which provides more than 500,000 models of different types. We will describe the structure of the dataset and we will explain how to use the associated library to develop ML applications in Python. Finally, we will describe some applications which can be addressed using ModelSet.","model classification, machine learning, model-driven engineering","","MODELS '22"
"Conference Paper","Xia CS,Zhang L","Less Training, More Repairing Please: Revisiting Automated Program Repair via Zero-Shot Learning","","2022","","","959–971","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549101;http://dx.doi.org/10.1145/3540250.3549101","10.1145/3540250.3549101","Due to the promising future of Automated Program Repair (APR), researchers have proposed various APR techniques, including heuristic-based, template-based, and constraint-based techniques. Among such classic APR techniques, template-based techniques have been widely recognized as state of the art. However, such template-based techniques require predefined templates to perform repair, and their effectiveness is thus limited. To this end, researchers have leveraged the recent advances in Deep Learning to further improve APR. Such learning-based techniques typically view APR as a Neural Machine Translation problem, using the buggy/fixed code snippets as the source/target languages for translation. In this way, such techniques heavily rely on large numbers of high-quality bug-fixing commits, which can be extremely costly/challenging to construct and may limit their edit variety and context representation. In this paper, we aim to revisit the learning-based APR problem, and propose AlphaRepair, the first cloze-style (or infilling-style) APR approach to directly leveraging large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes. Our main insight is instead of modeling what a repair edit should look like (i.e., a NMT task), we can directly predict what the correct code is based on the context information (i.e., a cloze or text infilling task). Although our approach is general and can be built on various pre-trained code models, we have implemented AlphaRepair as a practical multilingual APR tool based on the recent CodeBERT model. Our evaluation of AlphaRepair on the widely used Defects4J benchmark shows for the first time that learning-based APR without any history bug fixes can already outperform state-of-the-art APR techniques. We also studied the impact of different design choices and show that AlphaRepair performs even better on a newer version of Defects4J (2.0) with 3.3X more fixes than best performing baseline, indicating that AlphaRepair can potentially avoid the dataset-overfitting issue of existing techniques. Additionally, we demonstrate the multilingual repair ability of AlphaRepair by evaluating on the QuixBugs dataset where AlphaRepair achieved the state-of-the-art results on both Java and Python versions.","Deep Learning, Zero-shot Learning, Automated Program Repair","","ESEC/FSE 2022"
"Conference Paper","Wei M,Huang Y,Wang J,Shin J,Harzevili NS,Wang S","API Recommendation for Machine Learning Libraries: How Far Are We?","","2022","","","370–381","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549124;http://dx.doi.org/10.1145/3540250.3549124","10.1145/3540250.3549124","Application Programming Interfaces (APIs) are designed to help developers build software more effectively. Recommending the right APIs for specific tasks is gaining increasing attention among researchers and developers. However, most of the existing approaches are mainly evaluated for general programming tasks using statically typed programming languages such as Java. Little is known about their practical effectiveness and usefulness for machine learning (ML) programming tasks with dynamically typed programming languages such as Python, whose paradigms are fundamentally different from general programming tasks. This is of great value considering the increasing popularity of ML and the large number of new questions appearing on question answering websites. In this work, we set out to investigate the effectiveness of existing API recommendation approaches for Python-based ML programming tasks from Stack Overflow (SO). Specifically, we conducted an empirical study of six widely-used Python-based ML libraries using two state-of-the-art API recommendation approaches, i.e., BIKER and DeepAPI. We found that the existing approaches perform poorly for two main reasons: (1) Python-based ML tasks often require significant long API sequences; and (2) there are common API usage patterns in Python-based ML programming tasks that existing approaches cannot handle. Inspired by our findings, we proposed a simple but effective frequent itemset mining-based approach, i.e., FIMAX, to boost API recommendation approaches, i.e., enhance existing API recommendation approaches for Python-based ML programming tasks by leveraging the common API usage information from SO questions. Our evaluation shows that FIMAX improves existing state-of-the-art API recommendation approaches by up to 54.3% and 57.4% in MRR and MAP, respectively. Our user study with 14 developers further demonstrates the practicality of FIMAX for API recommendation.","Python-based machine learning library, API recommendation, empirical software engineering","","ESEC/FSE 2022"
"Conference Paper","Hong Y,Tantithamthavorn C,Thongtanunam P,Aleti A","CommentFinder: A Simpler, Faster, More Accurate Code Review Comments Recommendation","","2022","","","507–519","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549119;http://dx.doi.org/10.1145/3540250.3549119","10.1145/3540250.3549119","Code review is an effective quality assurance practice, but can be labor-intensive since developers have to manually review the code and provide written feedback. Recently, a Deep Learning (DL)-based approach was introduced to automatically recommend code review comments based on changed methods. While the approach showed promising results, it requires expensive computational resource and time which limits its use in practice. To address this limitation, we propose CommentFinder – a retrieval-based approach to recommend code review comments. Through an empirical evaluation of 151,019 changed methods, we evaluate the effectiveness and efficiency of CommentFinder against the state-of-the-art approach. We find that when recommending the best-1 review comment candidate, our CommentFinder is 32% better than prior work in recommending the correct code review comment. In addition, CommentFinder is 49 times faster than the prior work. These findings highlight that our CommentFinder could help reviewers to reduce the manual efforts by recommending code review comments, while requiring less computational time.","Modern Code Review, Software Quality Assurance","","ESEC/FSE 2022"
"Conference Paper","Zhang Z,Xing Z,Xia X,Xu X,Zhu L","Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic Python Code with Pythonic Idioms","","2022","","","696–708","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549143;http://dx.doi.org/10.1145/3540250.3549143","10.1145/3540250.3549143","Compared to other programming languages (e.g., Java), Python has more idioms to make Python code concise and efficient. Although pythonic idioms are well accepted in the Python community, Python programmers are often faced with many challenges in using them, for example, being unaware of certain pythonic idioms or do not know how to use them properly. Based on an analysis of 7,638 Python repositories on GitHub, we find that non-idiomatic Python code that can be implemented with pythonic idioms occurs frequently and widely. Unfortunately, there is no tool for automatically refactoring such non-idiomatic code into idiomatic code. In this paper, we design and implement an automatic refactoring tool to make Python code idiomatic. We identify nine pythonic idioms by systematically contrasting the abstract syntax grammar of Python and Java. Then we define the syntactic patterns for detecting non-idiomatic code for each pythonic idiom. Finally, we devise atomic AST-rewriting operations and refactoring steps to refactor non-idiomatic code into idiomatic code. We test and review over 4,115 refactorings applied to 1,065 Python projects from GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to 84 projects. These evaluations confirm the high accuracy, practicality and usefulness of our refactoring tool on real-world Python code.","Pythonic Idioms, Code Refactoring, Abstract Syntax Grammar","","ESEC/FSE 2022"
"Conference Paper","Li Z,Lu S,Guo D,Duan N,Jannu S,Jenks G,Majumder D,Green J,Svyatkovskiy A,Fu S,Sundaresan N","Automating Code Review Activities by Large-Scale Pre-Training","","2022","","","1035–1047","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549081;http://dx.doi.org/10.1145/3540250.3549081","10.1145/3540250.3549081","Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real-world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review scenario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews.","deep learning, datasets, pre-training, Code review","","ESEC/FSE 2022"
"Conference Paper","Pei K,She D,Wang M,Geng S,Xuan Z,David Y,Yang J,Jana S,Ray B","NeuDep: Neural Binary Memory Dependence Analysis","","2022","","","747–759","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549147;http://dx.doi.org/10.1145/3540250.3549147","10.1145/3540250.3549147","Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries. We present a new machine-learning-based approach to predict memory dependencies by exploiting the model's learned knowledge about how binary programs execute. Our approach features (i) a self-supervised procedure that pretrains a neural net to reason over binary code and its dynamic value flows through memory addresses, followed by (ii) supervised finetuning to infer the memory dependencies statically. To facilitate efficient learning, we develop dedicated neural architectures to encode the heterogeneous inputs (i.e., code, data values, and memory addresses from traces) with specific modules and fuse them with a composition learning strategy. We implement our approach in NeuDep and evaluate it on 41 popular software projects compiled by 2 compilers, 4 optimizations, and 4 obfuscation passes. We demonstrate that NeuDep is more precise (1.5x) and faster (3.5x) than the current state-of-the-art. Extensive probing studies on security-critical reverse engineering tasks suggest that NeuDep understands memory access patterns, learns function signatures, and is able to match indirect calls. All these tasks either assist or benefit from inferring memory dependencies. Notably, NeuDep also outperforms the current state-of-the-art on these tasks.","Reverse Engineering, Large Language Models, Machine Learning for Program Analysis, Memory Dependence Analysis","","ESEC/FSE 2022"
"Conference Paper","Stănciulescu U,Yin L,Filkov V","Code, Quality, and Process Metrics in Graduated and Retired ASFI Projects","","2022","","","495–506","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549132;http://dx.doi.org/10.1145/3540250.3549132","10.1145/3540250.3549132","Recent work on open source sustainability shows that successful trajectories of projects in the Apache Software Foundation Incubator (ASFI) can be predicted early on, using a set of socio-technical measures. Because OSS projects are socio-technical systems centered around code artifacts, we hypothesize that sustainable projects may exhibit different code and process patterns than unsustainable ones, and that those patterns can grow more apparent as projects evolve over time. Here we studied the code and coding processes of over 200 ASFI projects, and found that ASFI graduated projects have different patterns of code quality and complexity than retired ones. Likewise for the coding processes – e.g., feature commits or bug-fixing commits are correlated with project graduation success. We find that minor contributors and major contributors (who contribute =95% commits) associate with graduation outcomes, implying that having also developers who contribute fewer commits are important for a project’s success. This study provides evidence that OSS projects, especially nascent ones, can benefit from introspection and instrumentation using multidimensional modeling of the whole system, including code, processes, and code quality measures, and how they are interconnected over time.","Code Quality, Open Source Sustainability","","ESEC/FSE 2022"
"Conference Paper","Shi Y,Yin Y,Wang Z,Lo D,Zhang T,Xia X,Zhao Y,Xu B","How to Better Utilize Code Graphs in Semantic Code Search?","","2022","","","722–733","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549087;http://dx.doi.org/10.1145/3540250.3549087","10.1145/3540250.3549087","Semantic code search greatly facilitates software reuse, which enables users to find code snippets highly matching user-specified natural language queries. Due to the rich expressive power of code graphs (e.g., control-flow graph and program dependency graph), both of the two mainstream research works (i.e., multi-modal models and pre-trained models) have attempted to incorporate code graphs for code modelling. However, they still have some limitations: First, there is still much room for improvement in terms of search effectiveness. Second, they have not fully considered the unique features of code graphs. In this paper, we propose a Graph-to-Sequence Converter, namely G2SC. Through converting the code graphs into lossless sequences, G2SC enables to address the problem of small graph learning using sequence feature learning and capture both the edges and nodes attribute information of code graphs. Thus, the effectiveness of code search can be greatly improved. In particular, G2SC first converts the code graph into a unique corresponding node sequence by a specific graph traversal strategy. Then, it gets a statement sequence by replacing each node with its corresponding statement. A set of carefully designed graph traversal strategies guarantee that the process is one-to-one and reversible. G2SC enables capturing rich semantic relationships (i.e., control flow, data flow, node/relationship properties) and provides learning model-friendly data transformation. It can be flexibly integrated with existing models to better utilize the code graphs. As a proof-of-concept application, we present two G2SC enabled models: GSMM (G2SC enabled multi-modal model) and GSCodeBERT (G2SC enabled CodeBERT model). Extensive experiment results on two real large-scale datasets demonstrate that GSMM and GSCodeBERT can greatly improve the state-of-the-art models MMAN and GraphCodeBERT by 92% and 22% on R@1, and 63% and 11.5% on MRR, respectively.","Graph Embedding, Semantic Code Search, Neural Networks","","ESEC/FSE 2022"
"Conference Paper","Wu Z,Le V,Tiwari A,Gulwani S,Radhakrishna A,Radiček I,Soares G,Wang X,Li Z,Xie T","NL2Viz: Natural Language to Visualization via Constrained Syntax-Guided Synthesis","","2022","","","972–983","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3549140;http://dx.doi.org/10.1145/3540250.3549140","10.1145/3540250.3549140","Recent development in NL2CODE (Natural Language to Code) research allows end-users, especially novice programmers to create a concrete implementation of their ideas such as data visualization by providing natural language (NL) instructions. An NL2CODE system often fails to achieve its goal due to three major challenges: the user's words have contextual semantics, the user may not include all details needed for code generation, and the system results are imperfect and require further refinement. To address the aforementioned three challenges for NL to Visualization, we propose a new approach and its supporting tool named NL2VIZ with three salient features: (1) leveraging not only the user's NL input but also the data and program context that the NL query is upon, (2) using hard/soft constraints to reflect different confidence levels in the constraints retrieved from the user input and data/program context, and (3) providing support for result refinement and reuse. We implement NL2VIZ in the Jupyter Notebook environment and evaluate NL2VIZ on a real-world visualization benchmark and a public dataset to show the effectiveness of NL2VIZ. We also conduct a user study involving 6 data scientist professionals to demonstrate the usability of NL2VIZ, the readability of the generated code, and NL2VIZ's effectiveness in helping users generate desired visualizations effectively and efficiently.","natural language to code, constraint, program synthesis","","ESEC/FSE 2022"
"Conference Paper","Ivers J,Nord RL,Ozkaya I,Seifried C,Timperley CS,Kessentini M","Industry Experiences with Large-Scale Refactoring","","2022","","","1544–1554","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3558954;http://dx.doi.org/10.1145/3540250.3558954","10.1145/3540250.3558954","Software refactoring plays an important role in software engineering. Developers often turn to refactoring when they want to restructure software to improve its quality without changing its external behavior. Small-scale (floss) refactoring is common in industry and is often performed by a single developer in short sessions, even though developers do much of this work manually instead of using refactoring tools. However, some refactoring efforts are much larger in scale, requiring entire teams and months or years of effort, and the role of tools in these efforts is not as well studied. In this paper, we report on a survey we conducted with developers to understand large-scale refactoring and its tool support needs. Our results from 107 industry developers demonstrate that projects commonly go through multiple large-scale refactorings, each of which requires considerable effort. Our study finds that developers use several categories of tools to support large-scale refactoring and rely more heavily on general-purpose tools like IDEs than on tools designed specifically to support refactoring. Tool support varies across the different activities, with some particularly challenging activities seeing little use of tools in practice. Furthermore, our analysis suggests significant impact is possible through advances in tool support for comprehension and testing, as well as through support for the needs of business stakeholders.","software evolution, refactoring tools, software automation, refactoring, large-scale refactoring","","ESEC/FSE 2022"
"Conference Paper","Wang L,Wang H,Luo X,Zhang T,Wang S,Liu X","Demystifying “Removed Reviews” in IOS App Store","","2022","","","1489–1499","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3558966;http://dx.doi.org/10.1145/3540250.3558966","10.1145/3540250.3558966","The app markets enable users to submit feedback for downloaded apps in the form of star ratings and text reviews, which are meant to be helpful and trustworthy for decision making to both developers and other users. App markets have released strict guidelines/policies for user review submissions. However, there has been growing evidence showing the untrustworthy and poor-quality of app reviews, making the app store review environment a shambles. Therefore, review removal is a common practice, and market maintainers have to remove undesired reviews from the market periodically in a reactive manner. Although some reports and news outlets have mentioned removed reviews, our research community still lacks the comprehensive understanding of the landscape of this kind of reviews. To fill the void, in this paper, we present a large-scale and longitudinal study of removed reviews in iOS App Store. We first collaborate with our industry partner to collect over 30 million removed reviews for 33,665 popular apps over the course of a full year in 2020. This comprehensive dataset enables us to characterize the overall landscape of removed reviews. We next investigate the practical reasons leading to the removal of policy-violating reviews, and summarize several interesting reasons, including fake reviews, offensive reviews, etc. More importantly, most of these mis-behaviors can be reflected on reviews’ basic information including the posters, narrative content, and posting time. It motivates us to design an automated approach to flag the policy-violation reviews, and our experiment result on the labelled benchmark can achieve a good performance (F1=97%). We further make an attempt to apply our approach to the large-scale industry setting, and the result suggests the promising industry usage scenario of our approach. Our approach can act as a gatekeeper to pinpoint policy-violation reviews beforehand, which will be quite effective in improving the maintenance process of app reviews in the industrial setting.","app store, iOS, removed review, user review","","ESEC/FSE 2022"
"Conference Paper","Gu T,Li X,Lu S,Tian J,Nie Y,Kuang X,Lin Z,Liu C,Liang J,Jiang Y","Group-Based Corpus Scheduling for Parallel Fuzzing","","2022","","","1521–1532","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3560885;http://dx.doi.org/10.1145/3540250.3560885","10.1145/3540250.3560885","Parallel fuzzing relies on hardware resources to guarantee test throughput and efficiency. In industrial practice, it is well known that parallel fuzzing faces the challenge of task division, but most works neglect the important process of corpus allocation. In this paper, we proposed a group-based corpus scheduling strategy to address these two issues, which has been accepted by the LLVM community. And we implement a parallel fuzzer based on this strategy called glibFuzzer. glibFuzzer first groups the global corpus into different subsets and then assigns different energy scores and different scores to them. The energy scores were mainly determined by the seed size and the length of coverage information, and the difference score can describe the degree of difference in the code covered by different subsets of seeds. In each round of key local corpus construction, the master node selects high-quality seeds by combining the two scores to improve test efficiency and avoid task conflict. To prove the effectiveness of the strategy, we conducted an extensive evaluation on the real-world programs and FuzzBench. After 4×24 CPU-hours, glibFuzzer covered 22.02% more branches and executed 19.42 times more test cases than libFuzzer in 18 real-world programs. glibFuzzer showed an average branch coverage increase of 73.02%, 55.02%, 55.86% over AFL, PAFL, UniFuzz, respectively. More importantly, glibFuzzer found over 100 unique vulnerabilities.","Parallel fuzzing, Vulnerability detection, Seed scheduling","","ESEC/FSE 2022"
"Conference Paper","Shen S,Zhu X,Dong Y,Guo Q,Zhen Y,Li G","Incorporating Domain Knowledge through Task Augmentation for Front-End JavaScript Code Generation","","2022","","","1533–1543","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","Singapore, Singapore","2022","9781450394130","","https://doi.org/10.1145/3540250.3558965;http://dx.doi.org/10.1145/3540250.3558965","10.1145/3540250.3558965","Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.","Code Generation, Domain Knowledge, Task Augmentation","","ESEC/FSE 2022"
"Conference Paper","Kang W,Son B,Heo K","TRACER: Signature-Based Static Analysis for Detecting Recurring Vulnerabilities","","2022","","","1695–1708","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security","Los Angeles, CA, USA","2022","9781450394505","","https://doi.org/10.1145/3548606.3560664;http://dx.doi.org/10.1145/3548606.3560664","10.1145/3548606.3560664","Similar software vulnerabilities recur because developers reuse existing vulnerable code, or make similar mistakes when implementing the same logic. Recently, various analysis techniques have been proposed to find syntactically recurring vulnerabilities via code reuse. However, limited attention has been devoted to semantically recurring ones that share the same vulnerable behavior in different code structures. In this paper, we present a general analysis framework, called TRACER, for detecting such recurring vulnerabilities. TRACER is based on a taint analysis that can detect various types of vulnerabilities. For a given set of known vulnerabilities, the taint analysis extracts vulnerable traces and establishes a signature database of them. When a new unseen program is analyzed, TRACER compares all potentially vulnerable traces reported by the analysis with the known vulnerability signatures. Then, TRACER reports a list of potential vulnerabilities ranked by the similarity score. We evaluate TRACER on 273 Debian packages in C/C++. Our experiment results demonstrate that TRACER is able to find 281 previously unknown vulnerabilities with 6 CVE identifiers assigned.","software security, software engineering, program analysis","","CCS '22"
"Conference Paper","De Ghein R,Abrath B,De Sutter B,Coppens B","ApkDiff: Matching Android App Versions Based on Class Structure","","2022","","","1–12","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 ACM Workshop on Research on Offensive and Defensive Techniques in the Context of Man At The End (MATE) Attacks","Los Angeles, CA, USA","2022","9781450398817","","https://doi.org/10.1145/3560831.3564257;http://dx.doi.org/10.1145/3560831.3564257","10.1145/3560831.3564257","Reverse engineering an application requires attackers to invest time and effort doing manual and automatic analyses. When a new version of the application is released, this investment could be lost completely, if all the analyses had to be re-done. The gained insights into how an application functions might be transferred from one version to the next, however, if the versions do not differ too much. Diffing tools are thus valuable to reverse engineers attempting to transfer their knowledge across versions, as well as to defenders trying to assess this attack vector, and whether or how much a new version has to be diversified. While such diffing tools exist and are in widespread use for binary applications, they are in short supply for Android apps.This paper presents ApkDiff, a tool for diffing Android apps based on the semantic features of the class structure. To evaluate our tool we selected 20 popular financial apps available in the Google Play Store, and tracked their version updates over eight months. We found that on average 79% of all classes had a unique match across version updates. When we consider only classes for which we detect explicit obfuscations being applied (by applying heuristics on their identifiers), we still find that we can find a match for 56% of the classes (ranging from 23% to 85%), suggesting that these obfuscated apps are not resilient to our matching strategies. Our results suggest that ApkDiff provides a valuable approach to diffing Android apps.","reverse engineering, bytecode, matching program versions, classes","","Checkmate '22"
"Conference Paper","Zhou Q,Wu Q,Liu D,Ji S,Lu K","Non-Distinguishable Inconsistencies as a Deterministic Oracle for Detecting Security Bugs","","2022","","","3253–3267","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security","Los Angeles, CA, USA","2022","9781450394505","","https://doi.org/10.1145/3548606.3560661;http://dx.doi.org/10.1145/3548606.3560661","10.1145/3548606.3560661","Security bugs like memory errors are constantly introduced to software programs, and recent years have witnessed an increasing number of reported security bugs. Traditional detection approaches are mainly specification-based---detecting violations against a specified rule as security bugs. This often does not work well in practice because specifications are difficult to specify and generalize, leaving complicated and new types of bugs undetected. Recent research thus leans toward deviation-based detection which finds a substantial number of similar cases and detects deviating cases as potential bugs. This, however, suffers from two other problems. First, it requires enough similar cases to find deviations and thus cannot work for custom code that does not have similar cases. Second, code-similarity analysis is probabilistic and challenging, so the detection can be unreliable. Sometimes, similar cases can normally have deviating behaviors under different contexts.In this paper, we propose a novel approach for detecting security bugs based on a new concept called Non-Distinguishable Inconsistencies (NDI). The insight is that if two code paths in a function exhibit inconsistent security states (such as being freed or initialized) that are non-distinguishable from the external, such as the callers, there is no way to recover from the inconsistency from the external, which results in a bug. Such an approach has several strengths. First, it is specification-free and thus can support complicated and new types of bugs. Second, it does not require similar cases and by its nature is deterministic. Third, the analysis is practical by minimizing complicated and lengthy data-flow analysis. We implemented NDI and applied it to well-tested programs, including the OpenSSL library, the FreeBSD kernel, the Apache httpd server, and the PHP interpreter. The results show that NDI works for both large and small programs, and it effectively found 51 new bugs, most of which are otherwise missed by the state-of-the-art detection tools.","static analysis, deterministic bug detection, non-distinguishable inconsistencies","","CCS '22"
"Conference Paper","Aycock J,Ganesh S,Biittner K,Newell PA,Therrien C","The Sincerest Form of Flattery: Large-Scale Analysis of Code Re-Use in Atari 2600 Games","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on the Foundations of Digital Games","Athens, Greece","2022","9781450397957","","https://doi.org/10.1145/3555858.3555948;http://dx.doi.org/10.1145/3555858.3555948","10.1145/3555858.3555948","The Atari 2600 was a prominent early video game console that had broad cultural impact, and possessed an extensive catalog of games that undoubtedly helped shape the fledgling game industry. How were these games created? We examine one development practice, code re-use, across a large-scale corpus of 1,984 ROM images using an analysis system we have developed. Our system allows us to study code re-use at whole-corpus granularity in addition to finer-grained views of individual developers and companies. We combine this corpus analysis with a case study: one of the co-authors was a third-party developer for Atari 2600 games in the early 1980s, providing insight into why code re-use could occur through both oral history and artifacts preserved for over forty years. Finally, we frame our results about this development practice with an interdisciplinary, bigger-picture archaeological view of humans and technology.","Atari 2600, binary reverse engineering, game development, empirical study, archaeogaming","","FDG '22"
"Conference Paper","Jemmali C,Seif El-Nasr M,Cooper S","The Effects of Adaptive Procedural Levels on Engagement and Performance in an Educational Programming Game","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on the Foundations of Digital Games","Athens, Greece","2022","9781450397957","","https://doi.org/10.1145/3555858.3555892;http://dx.doi.org/10.1145/3555858.3555892","10.1145/3555858.3555892","Learners’ backgrounds, skills, and knowledge vary as they attempt to learn a new subject. To address this variation and allow learners to progress at their own speed, many researchers are suggesting adaptive learning as a solution. Adaptive content has been successful in learning environments such as intelligent tutoring systems, but it has not been thoroughly researched within video games, especially in terms of adaptive procedural levels. In this paper, we analyze the effects of procedural levels that are generated and inserted at run-time in between pre-designed levels in the educational programming game May’s Journey. Our study with 94 Amazon Mechanical Turkers shows that players encountered fewer code-related errors in the adaptive version, however, their engagement levels were similar, if not slightly higher in the non-adaptive version.","programming game, adaptive game, educational game, debugging, adaptivity assessment","","FDG '22"
"Journal Article","Puryear B,Sprint G","Github Copilot in the Classroom: Learning to Code with AI Assistance","J. Comput. Sci. Coll.","2022","38","1","37–47","Consortium for Computing Sciences in Colleges","Evansville, IN, USA","","","2022-11","","1937-4771","","","Recent advances in deep machine learning have enabled artificial intelligence-driven development environments (AIDEs). AIDEs are programming tools that, given comments or starter code, can generate code solution suggestions. As the accuracy of these tools continues to increase, one particular AIDE from Github, Copilot, has been gaining significant attention for its performance and ease of use. The rise of Copilot suggests that code solution generation tools will soon be commonplace in both the industry and in computer science courses, with expert and novice programmers alike benefiting from using these tools. More specifically for novices, the effects of Copilot on the process of learning to code are mostly unknown. In this paper, we perform initial explorations into these effects. Using introductory computer science and data science courses, we evaluate Copilot-generated programming assignment solutions for correctness, style, skill level appropriateness, grade scores, and potential plagiarism. Our findings indicate Copilot generates mostly unique code that can solve introductory assignments with human-graded scores ranging from 68% to 95%. Based on these results, we provide recommendations for educators to help adapt their courses to incorporate new AIDE-based programming workflows.","","",""
"Journal Article","Garg P,Sengamedu SH","Synthesizing Code Quality Rules from Examples","Proc. ACM Program. Lang.","2022","6","OOPSLA2","","Association for Computing Machinery","New York, NY, USA","","","2022-10","","","https://doi.org/10.1145/3563350;http://dx.doi.org/10.1145/3563350","10.1145/3563350","Static Analysis tools have rules for several code quality issues and these rules are created by experts manually. In this paper, we address the problem of automatic synthesis of code quality rules from examples. We formulate the rule synthesis problem as synthesizing first order logic formulas over graph representations of code. We present a new synthesis algorithm RhoSynth that is based on Integer Linear Programming-based graph alignment for identifying code elements of interest to the rule. We bootstrap RhoSynth by leveraging code changes made by developers as the source of positive and negative examples. We also address rule refinement in which the rules are incrementally improved with additional user-provided examples. We validate RhoSynth by synthesizing more than 30 Java code quality rules. These rules have been deployed as part of Amazon CodeGuru Reviewer and their precision exceeds 75% based on developer feedback collected during live code-reviews within Amazon. Through comparisons with recent baselines, we show that current state-of-the-art program synthesis approaches are unable to synthesize most of these rules.","Program Synthesis","",""
"Journal Article","Sakkas G,Endres M,Guo PJ,Weimer W,Jhala R","Seq2Parse: Neurosymbolic Parse Error Repair","Proc. ACM Program. Lang.","2022","6","OOPSLA2","","Association for Computing Machinery","New York, NY, USA","","","2022-10","","","https://doi.org/10.1145/3563330;http://dx.doi.org/10.1145/3563330","10.1145/3563330","We present Seq2Parse, a language-agnostic neurosymbolic approach to automatically repairing parse errors. Seq2Parse is based on the insight that Symbolic Error Correcting (EC) Parsers can, in principle, synthesize repairs, but, in practice, are overwhelmed by the many error-correction rules that are not relevant to the particular program that requires repair. In contrast, Neural approaches are fooled by the large space of possible sequence level edits, but can precisely pinpoint the set of EC-rules that are relevant to a particular program. We show how to combine their complementary strengths by using neural methods to train a sequence classifier that predicts the small set of relevant EC-rules for an ill-parsed program, after which, the symbolic EC-parsing algorithm can make short work of generating useful repairs. We train and evaluate our method on a dataset of 1,100,000 Python programs, and show that Seq2Parse is accurate and efficient: it can parse 94% of our tests within 2.1 seconds, while generating the exact user fix in 1 out 3 of the cases; and useful: humans perceive both Seq2Parse-generated error locations and repairs to be almost as good as human-generated ones in a statistically-significant manner.","Error-Correcting Parsers, Machine Learning, Automated Program Repair","",""
"Journal Article","Sun Y,Dhandhania U,Oliveira BC","Compositional Embeddings of Domain-Specific Languages","Proc. ACM Program. Lang.","2022","6","OOPSLA2","","Association for Computing Machinery","New York, NY, USA","","","2022-10","","","https://doi.org/10.1145/3563294;http://dx.doi.org/10.1145/3563294","10.1145/3563294","A common approach to defining domain-specific languages (DSLs) is via a direct embedding into a host language. There are several well-known techniques to do such embeddings, including shallow and deep embeddings. However, such embeddings come with various trade-offs in existing programming languages. Owing to such trade-offs, many embedded DSLs end up using a mix of approaches in practice, requiring a substantial amount of code, as well as some advanced coding techniques. In this paper, we show that the recently proposed Compositional Programming paradigm and the CP language provide improved support for embedded DSLs. In CP we obtain a new form of embedding, which we call a compositional embedding, that has most of the advantages of both shallow and deep embeddings. On the one hand, compositional embeddings enable various forms of linguistic reuse that are characteristic of shallow embeddings, including the ability to reuse host-language optimizations in the DSL and add new DSL constructs easily. On the other hand, similarly to deep embeddings, compositional embeddings support definitions by pattern matching or dynamic dispatching (including dependent interpretations, transformations, and optimizations) over the abstract syntax of the DSL and have the ability to add new interpretations. We illustrate an instance of compositional embeddings with a DSL for document authoring called ExT. The DSL is highly flexible and extensible, allowing users to create various non-trivial extensions easily. For instance, ExT supports various extensions that enable the production of wiki-like documents, LaTeX documents, vector graphics or charts. The viability of compositional embeddings for ExT is evaluated with three applications.","Compositional Programming, Extensible Typesetting","",""
"Journal Article","Gong L,Zhang J,Wei M,Zhang H,Huang Z","What is the Intended Usage Context of This Model? - An Exploratory Study of Pre-Trained Models on Various Model Repositories","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-10","","1049-331X","https://doi.org/10.1145/3569934;http://dx.doi.org/10.1145/3569934","10.1145/3569934","There is a trend of researchers and practitioners to directly apply the pre-trained models to solve their specific tasks. For example, researchers in Software Engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or ML-based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse software engineering could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, though many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the Tensorflow Hub, PyTorch Hub, Model Zoo, Wolfram neural net repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that 1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set; 2) the model repositories cannot provide all our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that 1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type; 2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model; and 3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.","pre-trained models, model reuse, model contract, Software engineering for artificial intelligence","Just Accepted",""
"Journal Article","Wang H,Ma P,Wang S,Tang Q,Nie S,Wu S","Sem2vec: Semantics-Aware Assembly Tracelet Embedding","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-10","","1049-331X","https://doi.org/10.1145/3569933;http://dx.doi.org/10.1145/3569933","10.1145/3569933","Binary code similarity is the foundation of many security and software engineering applications. Recent works leverage deep neural networks (DNN) to learn a numeric vector representation (namely embeddings) of assembly functions, enabling similarity analysis in the numeric space. However, existing DNN-based techniques capture syntactic-, control flow-, or data flow-level information of assembly code, which is too coarse-grained to represent program functionality. These methods can suffer from low robustness to challenging settings such as compiler optimizations and obfuscations. We present sem2vec, a binary code embedding framework that learns from semantics. Given the control-flow graph (CFG) of an assembly function, we divide it into tracelets, denoting continuous and short execution traces that are reachable from the function entry point. We use symbolic execution to extract symbolic constraints and other auxiliary information on each tracelet. We then train masked language models to compute embeddings of symbolic execution outputs. Last, we use graph neural networks, to aggregate tracelet embeddings into the CFG-level embedding for a function. Our evaluation shows that sem2vec extracts high-quality embedding and is robust against different compilers, optimizations, architectures, and popular obfuscation methods including virtualization obfuscation. We further augment a vulnerability search application with embeddings computed by sem2vec and demonstrate a significant improvement in vulnerability search accuracy.","Symbolic Execution, Embedding, Graph Neural Network, Binary Code Similarity","Just Accepted",""
"Conference Paper","Yan L,Kim M,Hartmann B,Zhang T,Glassman EL","Concept-Annotated Examples for Library Comparison","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology","Bend, OR, USA","2022","9781450393201","","https://doi.org/10.1145/3526113.3545647;http://dx.doi.org/10.1145/3526113.3545647","10.1145/3526113.3545647","Programmers often rely on online resources—such as code examples, documentation, blogs, and Q&A forums—to compare similar libraries and select the one most suitable for their own tasks and contexts. However, this comparison task is often done in an ad-hoc manner, which may result in suboptimal choices. Inspired by Analogical Learning and Variation Theory, we hypothesize that rendering many concept-annotated code examples from different libraries side-by-side can help programmers (1) develop a more comprehensive understanding of the libraries’ similarities and distinctions and (2) make more robust, appropriate library selections. We designed a novel interactive interface, ParaLib, and used it as a technical probe to explore to what extent many side-by-side concepted-annotated examples can facilitate the library comparison and selection process. A within-subjects user study with 20 programmers shows that, when using ParaLib, participants made more consistent, suitable library selections and provided more comprehensive summaries of libraries’ similarities and differences.","visualization, Sensemaking, programming support","","UIST '22"
"Conference Paper","He X,Wang S,Xing Y,Feng P,Wang H,Li Q,Chen S,Sun K","BinProv: Binary Code Provenance Identification without Disassembly","","2022","","","350–363","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses","Limassol, Cyprus","2022","9781450397049","","https://doi.org/10.1145/3545948.3545956;http://dx.doi.org/10.1145/3545948.3545956","10.1145/3545948.3545956","Provenance identification, which is essential for binary analysis, aims to uncover the specific compiler and configuration used for generating the executable. Traditionally, the existing solutions extract syntactic, structural, and semantic features from disassembled programs and employ machine learning techniques to identify the compilation provenance of binaries. However, their effectiveness heavily relies on disassembly tools (e.g., IDA Pro) and tedious feature engineering, since it is challenging to obtain accurate assembly code, particularly, from the stripped or obfuscated binaries. In addition, the features in machine learning approaches are manually selected based on the domain knowledge of one specific architecture, which cannot be applied to other architectures. In this paper, we develop an end-to-end provenance identification system BinProv, which leverages a BERT (Bidirectional Encoder Representations from Transformers) based embedding model to learn and represent the context semantics and syntax directly from the binary code. Therefore, BinProv avoids the disassembling step and manual feature selection in provenance identification. Moreover, BinProv can distinguish the compilers and the four optimization levels (O0/O1/O2/O3) by fine-tuning the classifier model with the embedding inputs for specific provenance identification tasks. Experimental results show that BinProv achieves 92.14%, 99.4%, and 99.8% accuracy at byte sequence, function, and binary levels, respectively. We further demonstrate that BinProv works well on obfuscated binary code, suggesting that BinProv is a viable approach to remarkably mitigate the disassembler dependence in future provenance identification tasks. Finally, our case studies show that BinProv can better identify compiler helper functions and improve the performance of binary code similarity detection.","compiler, binary program, BERT, provenance, optimization level","","RAID '22"
"Conference Paper","Weideman N,Wang H,Kann T,Zahabizadeh S,Wu WC,Tandon R,Mirkovic J,Hauser C","Harm-DoS: Hash Algorithm Replacement for Mitigating Denial-of-Service Vulnerabilities in Binary Executables","","2022","","","276–291","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses","Limassol, Cyprus","2022","9781450397049","","https://doi.org/10.1145/3545948.3545967;http://dx.doi.org/10.1145/3545948.3545967","10.1145/3545948.3545967","Programs and services relying on weak hash algorithms as part of their hash table implementations are vulnerable to hash-collision denial-of-service attacks. In the context of such an attack, the attacker sends a series of program inputs leading to hash collisions. In the best case, this slows down the execution and processing for all requests, and in the worst case it renders the program or service unavailable. We propose a new binary program analysis approach to automatically detect weak hash functions and patch vulnerable binary programs, by replacing the weak hash function with a secure alternative. To verify that our mitigation strategy does not break program functionality, we design and leverage multiple stages of static analysis and symbolic execution, which demonstrate that the patched code performs equivalently to the original code, but does not suffer from the same vulnerability. We analyze 105,831 real-world programs and confirm the use of 796 weak hash functions in the same number of programs. We successfully replace 759 of these in a non-disruptive manner. The entire process is automated. Among the real-world programs analyzed, we discovered, disclosed and mitigated a zero-day hash-collision vulnerability in Reddit.","automatic vulnerability detection, automatic vulnerability mitigation, Binary program analysis","","RAID '22"
"Conference Paper","Franzen F,Holl T,Andreas M,Kirsch J,Grossklags J","Katana: Robust, Automated, Binary-Only Forensic Analysis of Linux Memory Snapshots","","2022","","","214–231","Association for Computing Machinery","New York, NY, USA","Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses","Limassol, Cyprus","2022","9781450397049","","https://doi.org/10.1145/3545948.3545980;http://dx.doi.org/10.1145/3545948.3545980","10.1145/3545948.3545980","The development and research of tools for forensically analyzing Linux memory snapshots have stalled in recent years as they cannot deal with the high degree of configurability and fail to handle security advances like structure layout randomization. Existing tools such as Volatility and Rekall require a pre-generated profile of the operating system, which is not always available, and can be invalidated by the smallest source code or configuration changes in the kernel. In this paper, we create a reference model of the control and data flow of selected representative Linux kernels. Using this model, ABI properties, and Linux’s own runtime information, we apply a configuration- and instruction-set-agnostic structural matching between the reference model and the loaded kernel to obtain enough information to drive all practically relevant forensic analyses. We implemented our approach in Katana 1, and evaluated it against Volatility. Katana is superior where no perfect profile information is available. Furthermore, we show correct functionality on an extensive set of 85 kernels with different configurations and 45 realistic snapshots taken while executing popular Linux distributions or recent versions of Android from version 8.1 to 11. Our approach translates to other CPU architectures in the Internet-of-Things (IoT) device domain such as MIPS and ARM64 as we show by analyzing a TP-Link router and a smart camera. We also successfully generalize to modified Linux kernels such as Android.","binary analysis, automated profile generation, memory forensics","","RAID '22"
"Conference Paper","Bennett G,Hall T,Bowes D","Some Automatically Generated Patches Are More Likely to Be Correct than Others: An Analysis of Defects4J Patch Features","","2022","","","46–52","Association for Computing Machinery","New York, NY, USA","Proceedings of the Third International Workshop on Automated Program Repair","Pittsburgh, Pennsylvania","2022","9781450392853","","https://doi.org/10.1145/3524459.3527348;http://dx.doi.org/10.1145/3524459.3527348","10.1145/3524459.3527348","Defects4J is a popular dataset against which many Java Automatic Program Repair (APR) tools benchmark their performance. However, recent evidence suggests that some APR tools overfit to Defects4J, producing plausible patches which are incorrect. What we do not currently know is whether there is any commonality in the features of these plausible patches that turn out not to be correct. We compare the features of Defects4J's human written patches in terms of those correctly patched by existing APR tools and those incorrectly patched. We found that 48.4% of Defects4J v1.5 have been automatically patched by existing APR tools; of which only 28.9% have been correctly patched leaving 19.5% incorrectly patched. We found patches of defects that added a method call, added a variable, or wrapped existing code with new code, such as a try/catch block were significantly associated with incorrect patches. Editing only a single line was significantly associated with correct patches. Our results suggest that current tools are weak at generating multiline patches and synthesising new code especially when wrapping existing code. Our results highlight potential future areas of development for new APR approaches, such as developing a tool that effectively repairs defects that require a try/catch block.Our replication Package is available online 1.","","","APR '22"
"Conference Paper","Kupoluyi J,Chaqfeh M,Varvello M,Coke R,Hashmi W,Subramanian L,Zaki Y","Muzeel: Assessing the Impact of JavaScript Dead Code Elimination on Mobile Web Performance","","2022","","","335–348","Association for Computing Machinery","New York, NY, USA","Proceedings of the 22nd ACM Internet Measurement Conference","Nice, France","2022","9781450392594","","https://doi.org/10.1145/3517745.3561427;http://dx.doi.org/10.1145/3517745.3561427","10.1145/3517745.3561427","To quickly create interactive web pages, developers heavily rely on (large) general-purpose JavaScript libraries. This practice bloats web pages with complex unused functions dead code which are unnecessarily downloaded and processed by the browser. The identification and the elimination of these functions is an open problem, which this paper tackles with Muzeel, a black-box approach requiring neither knowledge of the code nor execution traces. While the state-of-the-art solutions stop analyzing JavaScript when the page loads, the core design principle of Muzeel is to address the challenge of dynamically analyzing JavaScript after the page is loaded, by emulating all possible user interactions with the page, such that the used functions (executed when interactivity events fire) are accurately identified, whereas unused functions are filtered out and eliminated. We run Muzeel against 15,000 popular web pages and show that half of the 300,000 JavaScript files used in these pages have at least 70% of unused functions, accounting for 55% of the files' sizes. To assess the impact of dead code elimination on Mobile Web performance, we serve 200 Muzeel-ed pages to several Android phones and browsers, under variable network conditions. Our evaluation shows that Muzeel can speed up page loads by 25-30% thanks to a combination of lower CPU and bandwidth usage. Most importantly, we show that such savings are achieved while maintaining the pages' visual appearance and interactive functionality.","web simplification, user experience, mobile web, JavaScript","","IMC '22"
"Conference Paper","Tao C,Zhan Q,Hu X,Xia X","C4: Contrastive Cross-Language Code Clone Detection","","2022","","","413–424","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527911;http://dx.doi.org/10.1145/3524610.3527911","10.1145/3524610.3527911","During software development, developers introduce code clones by reusing existing code to improve programming productivity. Considering the detrimental effects on software maintenance and evolution, many techniques are proposed to detect code clones. Existing approaches are mainly used to detect clones written in the same programming language. However, it is common to develop programs with the same functionality but in different programming languages to support various platforms. In this paper, we propose a new approach named C4, referring to Contrastive Cross-language Code Clone detection model. It can detect cross-language clones with learned representations effectively. C4 exploits the pre-trained model CodeBERT to convert programs in different languages into high-dimensional vector representations. In addition, we fine tune the C4 model through a constrastive learning objective that can effectively recognize clone pairs and non-clone pairs. To evaluate the effectiveness of our approach, we conduct extensive experiments on the dataset proposed by CLCDSA. Experimental results show that C4 achieves scores of 0.94, 0.90, and 0.92 in terms of precision, recall and F-measure and substantially outperforms the state-of-the-art baselines.","contrastive learning, neural networks, code clone detection, cross-language","","ICPC '22"
"Conference Paper","Zhu W,Yoshida N,Kamiya T,Choi E,Takada H","MSCCD: Grammar Pluggable Clone Detection Based on ANTLR Parser Generation","","2022","","","460–470","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3529161;http://dx.doi.org/10.1145/3524610.3529161","10.1145/3524610.3529161","For various reasons, programming languages continue to multiply and evolve. It has become necessary to have a multilingual clone detection tool that can easily expand supported programming languages and detect various code clones is needed. However, research on multilingual code clone detection has not received sufficient attention. In this study, we propose MSCCD (Multilingual Syntactic Code Clone Detector), a grammar pluggable code clone detection tool that uses a parser generator to generate a code block extractor for the target language. The extractor then extracts the semantic code blocks from a parse tree. MSCCD can detect Type-3 clones at various granularities. We evaluated MSCCD's language extensibility by applying MSCCD to 20 modern languages. Sixteen languages were perfectly supported, and the remaining four were provided with the same detection capabilities at the expense of execution time. We evaluated MSCCD's recall by using BigCloneEval and conducted a manual experiment to evaluate precision. MSCCD achieved equivalent detection performance equivalent to state-of-the-art tools.","clone detection, programming language, parser generator, code clone, syntactic analysis","","ICPC '22"
"Conference Paper","Hu B,Wu Y,Peng X,Sha C,Wang X,Fu B,Zhao W","Predicting Change Propagation between Code Clone Instances by Graph-Based Deep Learning","","2022","","","425–436","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527912;http://dx.doi.org/10.1145/3524610.3527912","10.1145/3524610.3527912","Code clones widely exist in open-source and industrial software projects and are still recognized as a threat to software maintenance due to the additional effort required for the simultaneous maintenance of multiple clone instances and potential defects caused by inconsistent changes in clone instances. To alleviate the threat, it is essential to accurately and efficiently make the decisions of change propagation between clone instances. Based on an exploratory study on clone change propagation with five famous open-source projects, we find that a clone class can have both propagation-required changes and propagation-free changes and thus fine-grained change propagation decision is required. Based on the findings, we propose a graph-based deep learning approach to predict the change propagation requirements of clone instances. We develop a graph representation, named Fused Clone Program Dependency Graph (FC-PDG), to capture the textual and structural code contexts of a pair of clone instances along with the changes on one of them. Based on the representation, we design a deep learning model that uses a Relational Graph Convolutional Network (R-GCN) to predict the change propagation requirement. We evaluate the approach with a dataset constructed based on 51 open-source Java projects, which includes 24,672 pairs of matched changes and 38,041 non-matched changes. The results show that the approach achieves high precision (83.1%), recall (81.2%), and F1-score (82.1%). Our further evaluation with three other open-source projects confirms the generality of the trained clone change propagation prediction model.","","","ICPC '22"
"Conference Paper","Wang X,Wu Q,Zhang H,Lyu C,Jiang X,Zheng Z,Lyu L,Hu S","HELoC: Hierarchical Contrastive Learning of Source Code Representation","","2022","","","354–365","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527896;http://dx.doi.org/10.1145/3524610.3527896","10.1145/3524610.3527896","Abstract syntax trees (ASTs) play a crucial role in source code representation. However, due to the large number of nodes in an AST and the typically deep AST hierarchy, it is challenging to learn the hierarchical structure of an AST effectively. In this paper, we propose HELoC, a hierarchical contrastive learning model for source code representation. To effectively learn the AST hierarchy, we use contrastive learning to allow the network to predict the AST node level and learn the hierarchical relationships between nodes in a self-supervised manner, which makes the representation vectors of nodes with greater differences in AST levels farther apart in the embedding space. By using such vectors, the structural similarities between code snippets can be measured more precisely. In the learning process, a novel GNN (called Residual Self-attention Graph Neural Network, RSGNN) is designed, which enables HELoC to focus on embedding the local structure of an AST while capturing its overall structure. HELoC is self-supervised and can be applied to many source code related downstream tasks such as code classification, code clone detection, and code clustering after pre-training. Our extensive experiments demonstrate that HELoC outperforms the state-of-the-art source code representation models.","abstract syntax tree, contrastive learning, code representation","","ICPC '22"
"Conference Paper","Gao Y,Lyu C","M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization","","2022","","","24–35","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527907;http://dx.doi.org/10.1145/3524610.3527907","10.1145/3524610.3527907","Source code summarization aims to generate natural language descriptions of code snippets. Many existing studies learn the syntactic and semantic knowledge of code snippets from their token sequences and Abstract Syntax Trees (ASTs). They use the learned code representations as input to code summarization models, which can accordingly generate summaries describing source code. Traditional models traverse ASTs as sequences or split ASTs into paths as input. However, the former loses the structural properties of ASTs, and the latter destroys the overall structure of ASTs. Therefore, comprehensively capturing the structural features of ASTs in learning code representations for source code summarization remains a challenging problem to be solved. In this paper, we propose M2TS, a Multi-scale Multi-modal approach based on Transformer for source code Summarization. M2TS uses a multi-scale AST feature extraction method, which can extract the structures of ASTs more completely and accurately at multiple local and global levels. To complement missing semantic information in ASTs, we also obtain code token features, and further combine them with the extracted AST features using a cross modality fusion method that not only fuses the syntactic and contextual semantic information of source code, but also highlights the key features of each modality. We conduct experiments on two Java and one Python datasets, and the experimental results demonstrate that M2TS outperforms current state-of-the-art methods. We release our code at https://github.com/TranSMS/M2TS.","transformer, deep learning, neural network, source code summarization","","ICPC '22"
"Conference Paper","Huang Y,Xu F,Zhou H,Chen X,Zhou X,Wang T","Towards Exploring the Code Reuse from Stack Overflow during Software Development","","2022","","","548–559","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527923;http://dx.doi.org/10.1145/3524610.3527923","10.1145/3524610.3527923","As one of the most well-known programmer Q&A websites, Stack Overflow (i.e., SO) is serving tens of thousands of developers every day. Previous work has shown that many developers reuse the code snippets on SO when they find an answer (from SO) that functionally matches the programming problem they encounter in their development activities. To study how programmers reuse code on SO during project development, we conduct a comprehensive empirical study. First, to capture the development activities of programmers, we collect 342,148 modified code snippets in commits from 793 open-source Java projects, and these modified code can reflect the programming problems encountered during development. We also collect the code snippets from 1,355,617 posts on SO. Then, we employ CCFinder to detect the code clone between the modified code from commits and the code from SO, and further analyze the code reuse when programmer solves a programming problem during development. We count the code reuse ratios of the modified code snippets in the commits of each project in different years, the results show that the average code reuse ratio is 6.32%, and the maximum is 8.38%. The code reuse ratio in project commits has increased year by year, and the proportion of code reuse in the newly established project is higher than that of old projects. We also find that some projects reuse the code snippets from many years ago. Additionally, we find that experienced developers seem to be more likely to reuse the knowledge on SO. Moreover, we find that the code reuse ratio in bug-related commits (6.67%) is slightly higher than that of in non-bug-related commits (6.59%). Furthermore, we also find that the code reuse ratio (14.44%) in Java class files that have undergone multiple modifications is more than double the overall code reuse ratio (6.32%).","GitHub, code reuse, code clone, software development, stack overflow, code commit","","ICPC '22"
"Conference Paper","Zhao Y,Mo R,Zhang Y,Zhang S,Xiong P","Exploring and Understanding Cross-Service Code Clones in Microservice Projects","","2022","","","449–459","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527925;http://dx.doi.org/10.1145/3524610.3527925","10.1145/3524610.3527925","Microservice is an architecture style that decomposes complex software into loosely coupled services, which could be developed, maintained, and deployed independently. In recent years, the microservice architecture has been drawing more and more attention from both industrial and academic communities. Many companies, such as Google, Netflix, Amazon, and IBM have applied microservice architecture in their projects. Researchers have also studied microservices in different directions, such as microservices extraction, fault localization, and code quality analysis. The recent work has presented cross-service code clones are prevalent in microservice projects and have caused considerable co-modifications among different services, which undermines the independence of microservices. But there is no systematic study to reveal the underlying reasons for the emergence of such clones. In this paper, we first build a dataset consisting of 2,722 pairs of cross-service clones from 22 open-source microservice projects. Then we manually inspect the implementations of files and methods involved in cross-service clones to understand why the clones are introduced. In the file-level analysis, we categorize files into three types: DPFile (Data-processing File), DRFile (Data-related File), and DIFile (Data-irrelevant File), and have presented that DRFiles are more likely to encounter cross-service clones. For each type of files, we further classify them into specific cases. Each case describes the characteristics of involved files and why the clones happen. In the method-level analysis, we dig information from the code of involved methods. On this basis, we propose a catalog containing 4 categories with 10 subcategories of method-level implementations that result in cross-service clones. We believe our analyses have provided the fundamental knowledge of cross-service clones, which can help developers better manage and resolve such clones in microservice projects.","code analysis, manual analysis, code clone, microservice","","ICPC '22"
"Conference Paper","Goel D,Grover R,Fard FH","On the Cross-Modal Transfer from Natural Language to Code through Adapter Modules","","2022","","","71–81","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527892;http://dx.doi.org/10.1145/3524610.3527892","10.1145/3524610.3527892","Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently used in software engineering as models pre-trained on large source code corpora. Their knowledge is transferred to downstream tasks (e.g. code clone detection) via fine-tuning. In natural language processing (NLP), other alternatives for transferring the knowledge of PTLMs are explored through using adapters, compact, parameter efficient modules inserted in the layers of the PTLM. Although adapters are known to facilitate adapting to many downstream tasks compared to fine-tuning the model that require retraining all of the models' parameters- which owes to the adapters' plug and play nature and being parameter efficient-their usage in software engineering is not explored.Here, we explore the knowledge transfer using adapters and based on the Naturalness Hypothesis proposed by Hindle et. al [12]. Thus, studying the bimodality of adapters for two tasks of cloze test and code clone detection, compared to their benchmarks from the CodeXGLUE platform. These adapters are trained using programming languages and are inserted in a PTLM that is pre-trained on English corpora (N-PTLM). Three programming languages, C/C++, Python, and Java, are studied along with extensive experiments on the best setup used for adapters. Improving the results of the N-PTLM confirms the success of the adapters in knowledge transfer to software engineering, which sometimes are in par with or exceed the results of a PTLM trained on source code; while being more efficient in terms of the number of parameters, memory usage, and inference time. Our results can open new directions to build smaller models for more software engineering tasks. We open source all the scripts and the trained adapters.","adapters, transfer learning, parameter efficient models, pre-trained language models","","ICPC '22"
"Conference Paper","Wang K,Yan M,Zhang H,Hu H","Unified Abstract Syntax Tree Representation Learning for Cross-Language Program Classification","","2022","","","390–400","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527915;http://dx.doi.org/10.1145/3524610.3527915","10.1145/3524610.3527915","Program classification can be regarded as a high-level abstraction of code, laying a foundation for various tasks related to source code comprehension, and has a very wide range of applications in the field of software engineering, such as code clone detection, code smell classification, defects classification, etc. The cross-language program classification can realize code transfer in different programming languages, and can also promote cross-language code reuse, thereby helping developers to write code quickly and reduce the development time of code transfer. Most of the existing studies focus on the semantic learning of the code, whilst few studies are devoted to cross-language tasks. The main challenge of cross-language program classification is how to extract semantic features of different programming languages. In order to cope with this difficulty, we propose a Unified Abstract Syntax Tree (namely UAST in this paper) neural network. In detail, the core idea of UAST consists of two unified mechanisms. First, UAST learns an AST representation by unifying the AST traversal sequence and graph-like AST structure for capturing semantic code features. Second, we construct a mechanism called unified vocabulary, which can reduce the feature gap between different programming languages, so it can achieve the role of cross-language program classification. Besides, we collect a dataset containing 20,000 files of five programming languages, which can be used as a benchmark dataset for the cross-language program classification task. We have done experiments on two datasets, and the results show that our proposed approach outperforms the state-of-the-art baselines in terms of four evaluation metrics (Precision, Recall, F1-score, and Accuracy).","code representation learning, program comprehension, program classification, cross-language program classification","","ICPC '22"
"Conference Paper","Sharma R,Chen F,Fard F,Lo D","An Exploratory Study on Code Attention in BERT","","2022","","","437–448","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527921;http://dx.doi.org/10.1145/3524610.3527921","10.1145/3524610.3527921","Many recent models in software engineering introduced deep neural models based on the Transformer architecture or use transformer-based Pre-trained Language Models (PLM) trained on code. Although these models achieve the state of the arts results in many downstream tasks such as code summarization and bug detection, they are based on Transformer and PLM, which are mainly studied in the Natural Language Processing (NLP) field. The current studies rely on the reasoning and practices from NLP for these models in code, despite the differences between natural languages and programming languages. There is also limited literature on explaining how code is modeled.Here, we investigate the attention behavior of PLM on code and compare it with natural language. We pre-trained BERT, a Transformer based PLM, on code and explored what kind of information it learns, both semantic and syntactic. We run several experiments to analyze the attention values of code constructs on each other and what BERT learns in each layer. Our analyses show that BERT pays more attention to syntactic entities, specifically identifiers and separators, in contrast to the most attended token [CLS] in NLP. This observation motivated us to leverage identifiers to represent the code sequence instead of the [CLS] token when used for code clone detection. Our results show that employing embeddings from identifiers increases the performance of BERT by 605% and 4% F1-score in its lower layers and the upper layers, respectively. When identifiers' embeddings are used in CodeBERT, a code-based PLM, the performance is improved by 21--24% in the F1-score of clone detection. The findings can benefit the research community by using code-specific representations instead of applying the common embeddings used in NLP, and open new directions for developing smaller models with similar performance.","BERT, CodeBERT, attention, pre-trained language models","","ICPC '22"
"Conference Paper","Chourasia P,Ramakrishnan G,Apte V,Kumar S","Algorithm Identification in Programming Assignments","","2022","","","471–481","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527914;http://dx.doi.org/10.1145/3524610.3527914","10.1145/3524610.3527914","Current autograders of programming assignments are typically program output based; they fall short in many ways: e.g. they do not carry out subjective evaluations such as code quality, or whether the code has followed any instructor specified constraints; this is still done manually by teaching assistants. In this paper, we tackle a specific aspect of such evaluation: to verify whether a program implements a specific algorithm that the instructor specified. An algorithm, e.g. bubble sort, can be coded in myriad different ways, but a human can always understand the code and spot, say a bubble sort, vs. a selection sort. We develop and compare four approaches to do precisely this: given the source code of a program known to implement a certain functionality, identify the algorithm used, among a known set of algorithms. The approaches are based on code similarity, Support Vector Machine (SVM) with tree or graph kernels, and transformer neural architectures based only source code (CodeBERT), and the extension of this that includes code structure (GraphCodeBERT). Furthermore, we use a model for explainability (LIME) to generate insights into why certain programs get certain labels. Results based on our datasets of sorting, searching and shortest path codes, show that GraphCodeBERT, fine-tuned with scrambled source code, i.e., where identifiers are replaced consistently with arbitrary words, gives the best performance in algorithm identification, with accuracy of 96--99% depending on the functionality. Additionally, we add uncalled function source code elimination to our pre-processing pipeline of test programs, to improve the accuracy of classification of obfuscated source code.","","","ICPC '22"
"Conference Paper","Yang S,Gu X,Shen B","Self-Supervised Learning of Smart Contract Representations","","2022","","","82–93","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527894;http://dx.doi.org/10.1145/3524610.3527894","10.1145/3524610.3527894","Learning smart contract representations can greatly facilitate the development of smart contracts in many tasks such as bug detection and clone detection. Existing approaches for learning program representations are difficult to apply to smart contracts which have insufficient data and significant homogenization. To overcome these challenges, in this paper, we propose SRCL, a novel, self-supervised approach for learning smart contract representations. Unlike existing supervised methods, which are tied on task-specific data labels, SRCL leverages large-scale unlabeled data by self-supervised learning of both local and global information of smart contracts. It automatically extracts structural sequences from abstract syntax trees (ASTs). Then, two discriminators are designed to guide the Transformer encoder to learn local and global semantic features of smart contracts. We evaluate SRCL on a dataset of 75,006 smart contracts collected from Etherscan. Experimental results show that SRCL considerably outperforms the state-of-the-art code representation models on three downstream tasks.","self-supervised learning, code representation learning, data augmentation, smart contract","","ICPC '22"
"Conference Paper","Cui N,Jiang Y,Gu X,Shen B","Zero-Shot Program Representation Learning","","2022","","","60–70","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527888;http://dx.doi.org/10.1145/3524610.3527888","10.1145/3524610.3527888","Learning program representations has been the core prerequisite of code intelligence tasks (e.g., code search and code clone detection). The state-of-the-art pre-trained models such as CodeBERT require the availability of large-scale code corpora. However, gathering training samples can be costly and infeasible for domain-specific languages such as Solidity for smart contracts. In this paper, we propose Zecoler, a zero-shot learning approach for code representations. Zecoler is built upon a pre-trained programming language model. In order to elicit knowledge from the pre-trained models efficiently, Zecoler casts the downstream tasks to the same form of pre-training tasks by inserting trainable prompts into the original input. Then, it employs the prompt learning technique to optimize the pre-trained model by merely adjusting the original input. This enables the representation model to efficiently fit the scarce task-specific data while reusing pre-trained knowledge. We evaluate Zecoler in three code intelligence tasks in two programming languages that have no training samples, namely, Solidity and Go, with model trained in corpora of common languages such as Java. Experimental results show that our approach significantly outperforms baseline models in both zero-shot and few-shot settings.","code intelligence, learning program representations, zero-shot learning, prompt learning","","ICPC '22"
"Conference Paper","Guo Y,Li P,Luo Y,Wang X,Wang Z","Exploring GNN Based Program Embedding Technologies for Binary Related Tasks","","2022","","","366–377","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527900;http://dx.doi.org/10.1145/3524610.3527900","10.1145/3524610.3527900","With the rapid growth of program scale, program analysis, maintenance and optimization become increasingly diverse and complex. Applying learning-assisted methodologies onto program analysis has attracted ever-increasing attention. However, a large number of program factors including syntax structures, semantics, running platforms and compilation configurations block the effective realization of these methods. To overcome these obstacles, existing works prefer to be on a basis of source code or abstract syntax tree, but unfortunately are sub-optimal for binary-oriented analysis tasks closely related to the compilation process. To this end, we propose a new program analysis approach that aims at solving program-level and procedure-level tasks with one model, by taking advantage of the great power of graph neural networks from the level of binary code. By fusing the semantics of control flow graphs, data flow graphs and call graphs into one model, and embedding instructions and values simultaneously, our method can effectively work around emerging compilation-related problems. By testing the proposed method on two tasks, binary similarity detection and dead store prediction, the results show that our method is able to achieve as high accuracy as 83.25%, and 82.77%.","dead store detection, graph neural network, binary similarity detection, program embedding","","ICPC '22"
"Conference Paper","Sharma R,Chen F,Fard F","LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition","","2022","","","48–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527924;http://dx.doi.org/10.1145/3524610.3527924","10.1145/3524610.3527924","Code comment generation is the task of generating a high-level natural language description for a given code method/function. Although researchers have been studying multiple ways to generate code comments automatically, previous work mainly considers representing a code token in its entirety semantics form only (e.g., a language model is used to learn the semantics of a code token), and additional code properties such as the tree structure of a code are included as an auxiliary input to the model. There are two limitations: 1) Learning the code token in its entirety form may not be able to capture information succinctly in source code, and 2) The code token does not contain additional syntactic information, inherently important in programming languages.In this paper, we present LAnguage Model and Named Entity Recognition (LAMNER), a code comment generator capable of encoding code constructs effectively and capturing the structural property of a code token. A character-level language model is used to learn the semantic representation to encode a code token. For the structural property of a token, a Named Entity Recognition model is trained to learn the different types of code tokens. These representations are then fed into an encoder-decoder architecture to generate code comments. We evaluate the generated comments from LAMNER and other baselines on a popular Java dataset with four commonly used metrics. Our results show that LAMNER is effective and improves over the best baseline model in BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%, 18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally, we fused LAMNER's code representation with the baseline models, and the fused models consistently showed improvement over the non-fused models. The human evaluation further shows that LAMNER produces high-quality code comments.","code comment generation, code summarization, named entity recognition, character language model","","ICPC '22"
"Conference Paper","Chen F,Fard FH,Lo D,Bryksin T","On the Transferability of Pre-Trained Language Models for Low-Resource Programming Languages","","2022","","","401–412","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527917;http://dx.doi.org/10.1145/3524610.3527917","10.1145/3524610.3527917","A recent study by Ahmed and Devanbu reported that using a corpus of code written in multilingual datasets to fine-tune multilingual Pre-trained Language Models (PLMs) achieves higher performance as opposed to using a corpus of code written in just one programming language. However, no analysis was made with respect to fine-tuning monolingual PLMs. Furthermore, some programming languages are inherently different and code written in one language usually cannot be interchanged with the others, i.e., Ruby and Java code possess very different structure. To better understand how monolingual and multilingual PLMs affect different programming languages, we investigate 1) the performance of PLMs on Ruby for two popular Software Engineering tasks: Code Summarization and Code Search, 2) the strategy (to select programming languages) that works well on fine-tuning multilingual PLMs for Ruby, and 3) the performance of the fine-tuned PLMs on Ruby given different code lengths.In this work, we analyze over a hundred of pre-trained and fine-tuned models. Our results show that 1) multilingual PLMs have a lower Performance-to-Time Ratio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as compared to monolingual PLMs, 2) our proposed strategy to select target programming languages to fine-tune multilingual PLMs is effective --- it reduces the time to fine-tune yet achieves higher performance in Code Summarization and Code Search tasks, and 3) our proposed strategy consistently shows good performance on different code lengths.","low-resource languages, pre-trained language models","","ICPC '22"
"Conference Paper","Zhang K,Wang W,Zhang H,Li G,Jin Z","Learning to Represent Programs with Heterogeneous Graphs","","2022","","","378–389","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527905;http://dx.doi.org/10.1145/3524610.3527905","10.1145/3524610.3527905","Code representation, which transforms programs into vectors with semantics, is essential for source code processing. We have witnessed the effectiveness of incorporating structural information (i.e., graph) into code representations in recent years. Specifically, the abstract syntax tree (AST) and the AST-augmented graph of the program contain much structural and semantic information, and most existing studies apply them for code representation. The graph adopted by existing approaches is homogeneous, i.e., it discards the type information of the edges and the nodes lying within AST. That may cause plausible obstruction to the representation model. In this paper, we propose to leverage the type information in the graph for code representation. To be specific, we propose the heterogeneous program graph (HPG), which provides the types of the nodes and the edges explicitly. Furthermore, we employ the heterogeneous graph transformer (HGT) architecture to generate representations based on HPG, considering the type of information during processing. With the additional types in HPG, our approach can capture complex structural information, produce accurate and delicate representations, and finally perform well on certain tasks. Our in-depth evaluations upon four classic datasets for two typical tasks (i.e., method name prediction and code classification) demonstrate that the heterogeneous types in HPG benefit the representation models. Our proposed HPG+HGT also outperforms the SOTA baselines on the subject tasks and datasets.","code representation, graph neural networks, heterogeneous graphs","","ICPC '22"
"Conference Paper","Zhang Y,Xiao Y,Kabir MM,Yao Ddaphne,Meng N","Example-Based Vulnerability Detection and Repair in Java Code","","2022","","","190–201","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527895;http://dx.doi.org/10.1145/3524610.3527895","10.1145/3524610.3527895","The Java libraries JCA and JSSE offer cryptographic APIs to facilitate secure coding. When developers misuse some of the APIs, their code becomes vulnerable to cyber-attacks. To eliminate such vulnerabilities, people built tools to detect security-API misuses via pattern matching. However, most tools do not (1) fix misuses or (2) allow users to extend tools' pattern sets. To overcome both limitations, we created Seader---an example-based approach to detect and repair security-API misuses. Given an exemplar (insecure, secure) code pair, Seader compares the snippets to infer any API-misuse template and corresponding fixing edit. Based on the inferred info, given a program, Seader performs inter-procedural static analysis to search for security-API misuses and to propose customized fixes.For evaluation, we applied Seader to 28 (insecure, secure) code pairs; Seader successfully inferred 21 unique API-misuse templates and related fixes. With these (vulnerability, fix) patterns, we applied Seader to a program benchmark that has 86 known vulnerabilities. Seader detected vulnerabilities with 95% precision, 72% recall, and 82% F-score. We also applied Seader to 100 open-source projects and manually checked 77 suggested repairs; 76 of the repairs were correct. Seader can help developers correctly use security APIs.","vulnerability repair, pattern inference, inter-procedural analysis","","ICPC '22"
"Conference Paper","Wang Y,Dong Y,Lu X,Zhou A","GypSum: Learning Hybrid Representations for Code Summarization","","2022","","","12–23","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527903;http://dx.doi.org/10.1145/3524610.3527903","10.1145/3524610.3527903","Code summarization with deep learning has been widely studied in recent years. Current deep learning models for code summarization generally follow the principle in neural machine translation and adopt the encoder-decoder framework, where the encoder learns the semantic representations from source code and the decoder transforms the learnt representations into human-readable text that describes the functionality of code snippets. Despite they achieve the new state-of-the-art performance, we notice that current models often either generate less fluent summaries, or fail to capture the core functionality, since they usually focus on a single type of code representations. As such we propose GypSum, a new deep learning model that learns hybrid representations using graph attention neural networks and a pre-trained programming and natural language model. We introduce particular edges related to the control flow of a code snippet into the abstract syntax tree for graph construction, and design two encoders to learn from the graph and the token sequence of source code, respectively. We modify the encoder-decoder sublayer in the Transformer's decoder to fuse the representations and propose a dual-copy mechanism to facilitate summary generation. Experimental results demonstrate the superior performance of GypSum over existing code summarization models.","deep neural networks, code summarization, copy mechanisms, graph attention neural networks","","ICPC '22"
"Conference Paper","Wang J,Huang Y,Wang S,Wang Q","Find Bugs in Static Bug Finders","","2022","","","516–527","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527899;http://dx.doi.org/10.1145/3524610.3527899","10.1145/3524610.3527899","Static bug finders (also known as static code analyzers, e.g., Find-Bugs, SonarQube) have been widely-adopted by developers to find bugs in real-world software projects. They leverage predefined heuristic static analysis rules to scan source code or binary code of a software project, and report violations to these rules as warnings to be verified. However, the advantages of static bug finders are overshadowed by such issues as uncovered obvious bugs, false positives, etc. To improve these tools, many techniques have been proposed to filter out false positives reported or design new static analysis rules. Nevertheless, the under-performance of bug finders can also be caused by the incorrectness of current rules contained in the static bug finders, which is not explored yet. In this work, we propose a differential testing approach to detect bugs in the rules of four widely-used static bug finders, i.e., SonarQube, PMD, SpotBugs, and ErrorProne, and conduct a qualitative study about the bugs found. The experiment on 2,728 open source projects reveals 46 bugs in the static bug finders, among which 30 are fixed or confirmed and the left are awaiting confirmation. We also summarize 13 bug patterns in the static analysis rules based on their context and root causes, which can serve as the checklist for designing and implementing other rules and/or in other tools. This study indicates that the commonly-used static bug finders are not as reliable as they might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of the static bug finders.","","","ICPC '22"
"Conference Paper","Hadi MA,Yusuf IN,Thung F,Luong KG,Lingxiao J,Fard FH,Lo D","On the Effectiveness of Pretrained Models for API Learning","","2022","","","309–320","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527886;http://dx.doi.org/10.1145/3524610.3527886","10.1145/3524610.3527886","Developers frequently use APIs to implement certain functionalities, such as parsing Excel Files, reading and writing text files line by line, etc. Developers can greatly benefit from automatic API usage sequence generation based on natural language queries for building applications in a faster and cleaner manner. Existing approaches utilize information retrieval models to search for matching API sequences given a query or use RNN-based encoder-decoder to generate API sequences. As it stands, the first approach treats queries and API names as bags of words. It lacks deep comprehension of the semantics of the queries. The latter approach adapts a neural language model to encode a user query into a fixed-length context vector and generate API sequences from the context vector.We want to understand the effectiveness of recent Pre-trained Transformer based Models (PTMs) for the API learning task. These PTMs are trained on large natural language corpora in an unsupervised manner to retain contextual knowledge about the language and have found success in solving similar Natural Language Processing (NLP) problems. However, the applicability of PTMs has not yet been explored for the API sequence generation task. We use a dataset that contains 7 million annotations collected from GitHub to evaluate the PTMs empirically. This dataset was also used to assess previous approaches. Based on our results, PTMs generate more accurate API sequences and outperform other related methods by 11%. We have also identified two different tokenization approaches that can contribute to a significant boost in PTMs' performance for the API sequence generation task.","API usage, API sequence, API, transformers, deep leaning, code search","","ICPC '22"
"Conference Paper","Yusuf IN,Jiang L,Lo D","Accurate Generation of Trigger-Action Programs with Domain-Adapted Sequence-to-Sequence Learning","","2022","","","99–110","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527922;http://dx.doi.org/10.1145/3524610.3527922","10.1145/3524610.3527922","Trigger-action programming allows end users to write event-driven rules to automate smart devices and internet services. Users can create a trigger-action program (TAP) by specifying triggers and actions from a set of predefined functions along with suitable data fields for the functions. Many trigger-action programming platforms have emerged as the popularity grows, e.g., IFTTT, Microsoft Power Automate, and Samsung SmartThings. Despite their simplicity, composing trigger-action programs (TAPs) can still be challenging for end users due to the domain knowledge needed and enormous search space of many combinations of triggers and actions. We propose RecipeGen, a new deep learning-based approach that leverages Transformer sequence-to-sequence (seq2seq) architecture to generate TAPs on the fine-grained field-level granularity from natural language descriptions. Our approach adapts autoencoding pre-trained models to warm-start the encoder in the seq2seq model to boost the generation performance. We have evaluated RecipeGen on real-world datasets from the IFTTT platform against the prior state-of-the-art approach on the TAP generation task. Our empirical evaluation shows that the overall improvement against the prior best results ranges from 9.5%-26.5%. Our results also show that adopting a pre-trained autoencoding model boosts the MRR@3 further by 2.8%-10.8%. Further, in the field-level generation setting, RecipeGen achieves 0.591 and 0.575 in terms of MRR@3 and BLEU scores respectively.","encoder-decoder, IFTTT, program generation, trigger-action programming, deep learning","","ICPC '22"
"Conference Paper","Zhu H,He X,Xu L","HatCUP: Hybrid Analysis and Attention Based Just-in-Time Comment Updating","","2022","","","619–630","Association for Computing Machinery","New York, NY, USA","Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension","Virtual Event","2022","9781450392983","","https://doi.org/10.1145/3524610.3527901;http://dx.doi.org/10.1145/3524610.3527901","10.1145/3524610.3527901","When changing code, developers sometimes neglect updating the related comments, bringing inconsistent or outdated comments. These comments increase the cost of program understanding and greatly reduce software maintainability. Researchers have put forward some solutions, such as CUP and HEBCUP, which update comments efficiently for simple code changes (i.e. modifying of a single token), but not good enough for complex ones. In this paper, we propose an approach named HatCUP (Hybrid Analysis and Attention based Comment UPdater), to provide a new mechanism for comment updating task. HatCUP pays attention to hybrid analysis and information. First, HatCUP considers the code structure change information and introduces a structure-guided attention mechanism combined with code change graph analysis and optimistic data flow dependency analysis. With a generally popular RNN-based encoder-decoder architecture, HatCUP takes the action of the code edits, the syntax, semantics and structure code changes, and old comments as inputs and generates a structural representation of the changes in the current code snippet. Furthermore, instead of directly generating new comments, HatCUP proposes a new edit or non-edit mechanism to mimic human editing behavior, by generating a sequence of edit actions and constructing a modified RNN model to integrate newly developed components. Evaluation on a popular dataset demonstrates that HatCUP outperforms the state-of-the-art deep learning-based approaches (CUP) by 53.8% for accuracy, 31.3% for recall and 14.3% for METEOR of the original metrics. Compared with the heuristic-based approach (HEBCUP), HatCUP also shows better overall performance.","data flow analysis, deep learning, comment updating, code-comment co-evolution, hybrid analysis","","ICPC '22"
"Conference Paper","Wan Y,He Y,Bi Z,Zhang J,Sui Y,Zhang H,Hashimoto K,Jin H,Xu G,Xiong C,Yu PS","NaturalCC: An Open-Source Toolkit for Code Intelligence","","2022","","","149–153","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings","Pittsburgh, Pennsylvania","2022","9781450392235","","https://doi.org/10.1145/3510454.3516863;http://dx.doi.org/10.1145/3510454.3516863","10.1145/3510454.3516863","We present NaturalCC, an efficient and extensible open-source toolkit for machine-learning-based source code analysis (i.e., code intelligence). Using NaturalCC, researchers can conduct rapid prototyping, reproduce state-of-the-art models, and/or exercise their own algorithms. NaturalCC is built upon Fairseq and PyTorch, providing (1) a collection of code corpus with preprocessing scripts, (2) a modular and extensible framework that makes it easy to reproduce and implement a code intelligence model, and (3) a benchmark of state-of-the-art models. Furthermore, we demonstrate the usability of our toolkit over a variety of tasks (e.g., code summarization, code retrieval, and code completion) through a graphical user interface. The website of this project is http://xcodemind.github.io, where the source code and demonstration video can be found.","open source, code embedding, deep learning, code representation, benchmark, toolkit, code intelligence","","ICSE '22"
"Conference Paper","Wang X,Xiao L,Yu T,Woepse A,Wong S","JMocker: Refactoring Test-Production Inheritance by Mockito","","2022","","","125–129","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings","Pittsburgh, Pennsylvania","2022","9781450392235","","https://doi.org/10.1145/3510454.3516836;http://dx.doi.org/10.1145/3510454.3516836","10.1145/3510454.3516836","Mocking frameworks are dedicated to creating, manipulating, and verifying the execution of ""faked"" objects in unit testing. This helps developers to overcome the challenge of high inter-dependencies among software units. Despite the various benefits offered by existing mocking frameworks, developers often create a subclass of the dependent class and mock its behavior through method overriding. However, this requires tedious implementation and compromises the design quality of unit tests. We contribute a refactoring tool as an Eclipse Plugin, named JMocker, to automatically identify and replace the usage of inheritance by using Mockito---a well received mocking framework for Java projects. We evaluate JMocker on four open source projects and successfully refactored 214 cases in total. The evaluation results show that our framework is efficient, applicable to different projects, and preserves test behaviors. According to the feedback of six real-life developers, JMocker improves the design quality of test cases. JMocker is available at https://github.com/wx930910/JMocker. The tool demo can be found at https://youtu.be/HFoA2ZKCoxM.","","","ICSE '22"
"Conference Paper","Bibiano AC","Completeness of Composite Refactorings for Smell Removal","","2022","","","264–268","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings","Pittsburgh, Pennsylvania","2022","9781450392235","","https://doi.org/10.1145/3510454.3517060;http://dx.doi.org/10.1145/3510454.3517060","10.1145/3510454.3517060","Code smells are problems in the internal structural quality. Refactoring is a technique commonly used to remove code smells. A single refactoring rarely suffices to assist developers in achieving a full removal of a code smell. Thus, developers frequently apply composite refactorings (or, simply, composites) with the goal of fully removing a smell. A composite is formed by two or more interrelated single refactorings. Studies report that developers often fail in fully removing code smells through composite refactoring. In this context, a composite refactoring is considered incomplete whenever it does not fully remove a smell. Either incomplete or complete composite is formed by several refactorings; thus, both may inadvertently degrade other parts of the software. However, the literature on (in)complete composites and their effects on structural quality is still scarce. This lack of knowledge hampers the design of empirically-based recommendations to properly assist developers in performing effective complete composites, i.e., those not causing any harm in related parts of the program. This doctoral research investigates the effect of composite (in)completeness on structural quality and proposes a catalog with composite recommendations for the full removal of popular code smell types. We investigated 618 composites in 20 software projects. We found that 58% of incomplete composites did not change the internal structural quality, and 64% of complete composites are formed by refactoring types that were not actually previously recommended in the literature or elsewhere. The expected contributions are a list of findings, guidelines, and a catalog to support developers on how to successfully perform complete composites.","code smells, refactoring, composite refactoring, composite completeness","","ICSE '22"
"Conference Paper","da Cruz Alves N,Kreuch L,von Wangenheim CG","Analyzing Structural Similarity of User Interface Layouts of Android Apps Using Deep Learning","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems","Diamantina, Brazil","2022","9781450395069","","https://doi.org/10.1145/3554364.3559111;http://dx.doi.org/10.1145/3554364.3559111","10.1145/3554364.3559111","The layout of graphical user interfaces (GUIs) takes into consideration the distribution of buttons, images, texts, and other components that are important factors for a positive experience of interactivity and usability. Therefore, it may be helpful to consult similar layouts during the design of GUIs for inspiration as well as to direct the development of original interfaces. This may be even more important in the context of the design of mobile applications in an educational context or when being developed by end-users with low-code environments, such as App Inventor. Therefore, this article presents an approach for analyzing the similarity of GUI layout using deep learning. First, GUI components and their positions are detected using the YOLO model. Then, the similarity of layouts is analyzed based on the GUI components detected. Results of the evaluation of the developed approach indicated a good accuracy of the detection of GUI components and the analysis of similarity close to human predictions. The results of this research can be used for the automated analysis of the similarity of GUI layouts of mobile applications, which can be used to analyze the design of mobile applications (consistency), search for similar designs, or assess the originality of a GUI design. It can also be applied in an educational context to assess the development of creativity as part of the development of mobile applications in order to teach computing.","deep learning, app inventor, similarity, mobile application, user interface design","","IHC '22"
"Conference Paper","Dutta S,Garbervetsky D,Lahiri SK,Schäfer M","InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript","","2022","","","165–174","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513048;http://dx.doi.org/10.1145/3510457.3513048","10.1145/3510457.3513048","Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities.But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort.Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise.In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub's CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives.","machine learning, taint analysis, JavaScript","","ICSE-SEIP '22"
"Conference Paper","Tang W,Wang Y,Zhang H,Han S,Luo P,Zhang D","LibDB: An Effective and Efficient Framework for Detecting Third-Party Libraries in Binaries","","2022","","","423–434","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3528442;http://dx.doi.org/10.1145/3524842.3528442","10.1145/3524842.3528442","Third-party libraries (TPLs) are reused frequently in software applications for reducing development cost. However, they could introduce security risks as well. Many TPL detection methods have been proposed to detect TPL reuse in Android bytecode or in source code. This paper focuses on detecting TPL reuse in binary code, which is a more challenging task. For a detection target in binary form, libraries may be compiled and linked to separate dynamic-link files or built into a fused binary that contains multiple libraries and project-specific code. This could result in fewer available code features and lower the effectiveness of feature engineering. In this paper, we propose a binary TPL reuse detection framework, LibDB, which can effectively and efficiently detect imported TPLs even in stripped and fused binaries. In addition to the basic and coarse-grained features (string literals and exported function names), LibDB utilizes function contents as a new type of feature. It embeds all functions in a binary file to low-dimensional representations with a trained neural network. It further adopts a function call graph-based comparison method to improve the accuracy of the detection. LibDB is able to support version identification of TPLs contained in the detection target, which is not considered by existing detection methods. To evaluate the performance of LibDB, we construct three datasets for binary-based TPL reuse detection. Our experimental results show that LibDB is more accurate and efficient than state-of-the-art tools on the binary TPL detection task and the version identification task. Our datasets and source code used in this work are anonymously available at https://github.com/DeepSoftwareAnalytics/LibDB.","clone detection, static binary analysis, third-party libraries","","MSR '22"
"Conference Paper","Silavong F,Moran S,Georgiadis A,Saphal R,Otter R","Senatus: A Fast and Accurate Code-to-Code Recommendation Engine","","2022","","","511–523","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3527947;http://dx.doi.org/10.1145/3524842.3527947","10.1145/3524842.3527947","Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with Senatus, a new code-to-code recommendation engine. At the core of Senatus is De-Skew LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example on the CodeSearchNet dataset Senatus improves performance by 31.21% F1 and 147.9x faster query time compared to Facebook Aroma. Senatus also outperforms standard MinHash LSH by 29.2% F1 and 51.02x faster query time.","machine learning on source code, MinHash LSH, code-to-code recommendation, locality sensitive hashing","","MSR '22"
"Conference Paper","Ciniselli M,Pascarella L,Bavota G","To What Extent Do Deep Learning-Based Code Recommenders Generate Predictions by Cloning Code from the Training Set?","","2022","","","167–178","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3528440;http://dx.doi.org/10.1145/3524842.3528440","10.1145/3524842.3528440","Deep Learning (DL) models have been widely used to support code completion. These models, once properly trained, can take as input an incomplete code component (e.g., an incomplete function) and predict the missing tokens to finalize it. GitHub Copilot is an example of code recommender built by training a DL model on millions of open source repositories: The source code of these repositories acts as training data, allowing the model to learn ""how to program"". The usage of such a code is usually regulated by Free and Open Source Software (FOSS) licenses, that establish under which conditions the licensed code can be redistributed or modified. As of Today, it is unclear whether the code generated by DL models trained on open source code should be considered as ""new"" or as ""derivative"" work, with possible implications on license infringements. In this work, we run a large-scale study investigating the extent to which DL models tend to clone code from their training set when recommending code completions. Such an exploratory study can help in assessing the magnitude of the potential licensing issues mentioned before: If these models tend to generate new code that is unseen in the training set, then licensing issues are unlikely to occur. Otherwise, a revision of these licenses urges to regulate how the code generated by these models should be treated when used, for example, in a commercial setting. Highlights from our results show that 10% to 0.1% of the predictions generated by a state-of-the-art DL-based code completion tool are Type-1 clones of instances in the training set, depending on the size of the predicted code. Long predictions are unlikely to be cloned.","code completion, code clones, deep learning","","MSR '22"
"Conference Paper","Hin D,Kan A,Chen H,Babar MA","LineVD: Statement-Level Vulnerability Detection Using Graph Neural Networks","","2022","","","596–607","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3527949;http://dx.doi.org/10.1145/3524842.3527949","10.1145/3524842.3527949","Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.","software vulnerability detection, program representation, deep learning","","MSR '22"
"Conference Paper","Yedida R,Menzies T","How to Improve Deep Learning for Software Analytics: (A Case Study with Code Smell Detection)","","2022","","","156–166","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3528458;http://dx.doi.org/10.1145/3524842.3528458","10.1145/3524842.3528458","To reduce technical debt and make code more maintainable, it is important to be able to warn programmers about code smells. State-of-the-art code small detectors use deep learners, usually without exploring alternatives. For example, one promising alternative is GHOST (from TSE'21) that relies on a combination of hyper-parameter optimization of feedforward neural networks and a novel oversampling technique.The prior study from TSE'21 proposing this novel ""fuzzy sampling"" was somewhat limited in that the method was tested on defect prediction, but nothing else. Like defect prediction, code smell detection datasets have a class imbalance (which motivated ""fuzzy sampling""). Hence, in this work we test if fuzzy sampling is useful for code smell detection.The results of this paper show that we can achieve better than state-of-the-art results on code smell detection with fuzzy oversampling. For example, for ""feature envy"", we were able to achieve 99+% AUC across all our datasets, and on 8/10 datasets for ""misplaced class"". While our specific results refer to code smell detection, they do suggest other lessons for other kinds of analytics. For example: (a) try better preprocessing before trying complex learners (b) include simpler learners as a baseline in software analytics (c) try ""fuzzy sampling"" as one such baseline.In order to support others trying to reproduce/extend/refute this work, all our code and data is available online at https://github.com/yrahul3910/code-smell-detection.","deep learning, code smell detection, autoencoders","","MSR '22"
"Conference Paper","Sivaraman A,Abreu R,Scott A,Akomolede T,Chandra S","Mining Idioms in the Wild","","2022","","","187–196","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513046;http://dx.doi.org/10.1145/3510457.3513046","10.1145/3510457.3513046","Existing code repositories contain numerous instances of code patterns that are idiomatic ways of accomplishing a particular programming task. Sometimes, the programming language in use supports specific operators or APIs that can express the same idiomatic imperative code much more succinctly. However, those code patterns linger in repositories because the developers may be unaware of the new APIs or have not gotten around to them. Detection of idiomatic code can also point to the need for new APIs.We share our experiences in mining imperative idiomatic patterns from the Hack repo at Facebook. We found that existing techniques either cannot identify meaningful patterns from syntax trees or require test-suite-based dynamic analysis to incorporate semantic properties to mine useful patterns. The key insight of the approach proposed in this paper --- Jezero --- is that semantic idioms from a large codebase can be learned from canonicalized dataflow trees. We propose a scalable, lightweight static analysis-based approach to construct such a tree that is well suited to mine semantic idioms using nonparametric Bayesian methods.Our experiments with Jezero on Hack code show a clear advantage of adding canonicalized dataflow information to ASTs: Jezero was significantly more effective in finding new refactoring opportunities from unannotated legacy code than a baseline that did not have the dataflow augmentation.","","","ICSE-SEIP '22"
"Conference Paper","Opdebeeck R,Zerouali A,De Roover C","Smelly Variables in Ansible Infrastructure Code: Detection, Prevalence, and Lifetime","","2022","","","61–72","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3527964;http://dx.doi.org/10.1145/3524842.3527964","10.1145/3524842.3527964","Infrastructure as Code is the practice of automating the provisioning, configuration, and orchestration of network nodes using code in which variable values such as configuration parameters, node hostnames, etc. play a central role. Mistakes in these values are an important cause of infrastructure defects and corresponding outages. Ansible, a popular IaC language, nonetheless features semantics which can cause confusion about the value of variables.In this paper, we identify six novel code smells related to Ansible's intricate variable precedence rules and lazy-evaluated template expressions. Their detection requires an accurate representation of control and data flow, for which we transpose the program dependence graph to Ansible. We use the resulting detector to empirically investigate the prevalence of these variable smells in 21,931 open-source Ansible roles, uncovering 31,334 unique smell instances across 4,260 roles. We observe an upward trend in the number of variable smells over time, that it may take a long time before they are fixed, and that code changes more often introduce new smells than fix existing ones. Our results are a call to arms for more in-depth quality checkers for IaC code, and highlight the importance of transcending syntax in IaC research.","empirical study, ansible, program dependence graphs, infrastructure as code, code smells, software quality","","MSR '22"
"Conference Paper","Epperson W,Wang AY,DeLine R,Drucker SM","Strategies for Reuse and Sharing among Data Scientists in Software Teams","","2022","","","243–252","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513042;http://dx.doi.org/10.1145/3510457.3513042","10.1145/3510457.3513042","Effective sharing and reuse practices have long been hallmarks of proficient software engineering. Yet the exploratory nature of data science presents new challenges and opportunities to support sharing and reuse of analysis code. To better understand current practices, we conducted interviews (N=17) and a survey (N=132) with data scientists at Microsoft, and extract five commonly used strategies for sharing and reuse of past work: personal analysis reuse, personal utility libraries, team shared analysis code, team shared template notebooks, and team shared libraries. We also identify factors that encourage or discourage data scientists from sharing and reusing. Our participants described obstacles to reuse and sharing including a lack of incentives to create shared code, difficulties in making data science code modular, and a lack of tool interoperability. We discuss how future tools might help meet these needs.","code reuse, data science, code sharing, survey","","ICSE-SEIP '22"
"Conference Paper","Zhang H,Cruz L,van Deursen A","Code Smells for Machine Learning Applications","","2022","","","217–228","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI","Pittsburgh, Pennsylvania","2022","9781450392754","","https://doi.org/10.1145/3522664.3528620;http://dx.doi.org/10.1145/3522664.3528620","10.1145/3522664.3528620","The popularity of machine learning has wildly expanded in recent years. Machine learning techniques have been heatedly studied in academia and applied in the industry to create business value. However, there is a lack of guidelines for code quality in machine learning applications. In particular, code smells have rarely been studied in this domain. Although machine learning code is usually integrated as a small part of an overarching system, it usually plays an important role in its core functionality. Hence ensuring code quality is quintessential to avoid issues in the long run. This paper proposes and identifies a list of 22 machine learning-specific code smells collected from various sources, including papers, grey literature, GitHub commits, and Stack Overflow posts. We pinpoint each smell with a description of its context, potential issues in the long run, and proposed solutions. In addition, we link them to their respective pipeline stage and the evidence from both academic and grey literature. The code smell catalog helps data scientists and developers produce and maintain high-quality machine learning application code.","code smell, machine learning, anti-pattern, technical debt, code quality","","CAIN '22"
"Conference Paper","Polese A,Hassan S,Tian Y","Adoption of Third-Party Libraries in Mobile Apps: A Case Study on Open-Source Android Applications","","2022","","","125–135","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems","Pittsburgh, Pennsylvania","2022","9781450393010","","https://doi.org/10.1145/3524613.3527810;http://dx.doi.org/10.1145/3524613.3527810","10.1145/3524613.3527810","Third-party libraries are frequently adopted in open-source Android applications (apps). These libraries are essential to the Android app development ecosystem as they often provide vital functionality that would take significant development time to implement otherwise. Researchers have mainly studied the prevalence and updates of third-party libraries in Android apps. However, no prior work investigates the adoption percentages of third-party libraries in apps and how they evolve. It remains unknown whether there are any patterns in third-party libraries' adoption percentages in Android apps.In this study, we empirically investigate the adoption of third-party libraries in 2,997 open-source Android apps over a six-year study period (2015--2020). We collected 39,882 commits from repositories hosting the target apps, and identified all changes to the adoption percentages of third-party libraries in each app. We then calculated the adoption percentage of each library in each app over specific time periods. Using the collected data, we report adoption statistics of popular libraries, propose a new taxonomy to characterize their evolutionary patterns, investigate the adoption percentages of third-party libraries across different app categories, and explore the groups of libraries that have similar release patterns and version-level adoption patterns. Our findings provide insight on third-party library adoption in open-source Android apps and thus might help researchers create tools to improve the library adoption in mobile apps.","Android application, third-party libraries, mobile applications, adoption percentage, open-source, empirical study","","MOBILESoft '22"
"Conference Paper","Shome A,Cruz L,van Deursen A","Data Smells in Public Datasets","","2022","","","205–216","Association for Computing Machinery","New York, NY, USA","Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI","Pittsburgh, Pennsylvania","2022","9781450392754","","https://doi.org/10.1145/3522664.3528621;http://dx.doi.org/10.1145/3522664.3528621","10.1145/3522664.3528621","The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells.","","","CAIN '22"
"Conference Paper","AlOmar EA,Chouchen M,Mkaouer MW,Ouni A","Code Review Practices for Refactoring Changes: An Empirical Study on OpenStack","","2022","","","689–701","Association for Computing Machinery","New York, NY, USA","Proceedings of the 19th International Conference on Mining Software Repositories","Pittsburgh, Pennsylvania","2022","9781450393034","","https://doi.org/10.1145/3524842.3527932;http://dx.doi.org/10.1145/3524842.3527932","10.1145/3524842.3527932","Modern code review is a widely used technique employed in both industrial and open-source projects to improve software quality, share knowledge, and ensure adherence to coding standards and guidelines. During code review, developers may discuss refactoring activities before merging code changes in the code base. To date, code review has been extensively studied to explore its general challenges, best practices and outcomes, and socio-technical aspects. However, little is known about how refactoring is being reviewed and what developers care about when they review refactored code. Hence, in this work, we present a quantitative and qualitative study to understand what are the main criteria developers rely on to develop a decision about accepting or rejecting a submitted refactored code, and what makes this process challenging. Through a case study of 11,010 refactoring and non-refactoring reviews spread across OpenStack open-source projects, we find that refactoring-related code reviews take significantly longer to be resolved in terms of code review efforts. Moreover, upon performing a thematic analysis on a significant sample of the refactoring code review discussions, we built a comprehensive taxonomy consisting of 28 refactoring review criteria. We envision our findings reaffirming the necessity of developing accurate and efficient tools and techniques that can assist developers in the review process in the presence of refactorings.","refactoring, developer perception, code review, software quality","","MSR '22"
"Conference Paper","Zhu F,Xu L,Ma G,Ji S,Wang J,Wang G,Zhang H,Wan K,Wang M,Zhang X,Wang Y,Li J","An Empirical Study on Quality Issues of EBay's Big Data SQL Analytics Platform","","2022","","","33–42","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513034;http://dx.doi.org/10.1145/3510457.3513034","10.1145/3510457.3513034","Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enterprises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summarize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.","big data, open source, empirical study, SQL on hadoop","","ICSE-SEIP '22"
"Conference Paper","Yang L,Zhang H,Zhang F,Zhang X,Rong G","An Industrial Experience Report on Retro-Inspection","","2022","","","43–52","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513055;http://dx.doi.org/10.1145/3510457.3513055","10.1145/3510457.3513055","To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the experiences and lessons from this software practice. To this end, we collected and analyzed various empirical evidence for data tri-angulation. The results reflect that retro-inspection distinguishes itself from peer code review by identifying more complicated and underlying defects, providing more indicative and suggestive comments. Many experienced inspectors indicate defects together with their rationale behind and offer suggestions for correction and prevention. As a result, retro-inspection can benefit not only quality assurance (like Fagan inspection), but also internal audit, inter-division communication, and competence promotion. On the other side, we identify several lessons of retro-inspection at this stage, e.g., developers' acceptance and organizers' predicament, for next-step improvement of this practice. To be specific, some recommendations are discussed for retro-inspection, e.g., more adequate preparation and more careful publicity. This study concludes that most of the expected benefits of retro-inspection can be empirically confirmed in this enterprise and its value on the progress to continuous maturity can be recognized organization-wide. The experiences on executing this altered practice in a large enterprise provide reference value on code quality assurance to other software organizations.","inspection, experience report, case study, code review, quality assurance, retro-inspection","","ICSE-SEIP '22"
"Conference Paper","An G,Yoon J,Sohn J,Hong J,Hwang D,Yoo S","Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA","","2022","","","65–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice","Pittsburgh, Pennsylvania","2022","9781450392266","","https://doi.org/10.1145/3510457.3513051;http://dx.doi.org/10.1145/3510457.3513051","10.1145/3510457.3513051","Continuous Integration (CI) of a large-scale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using real-world CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as input features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline.","test similarity, root cause analysis, continuous integration","","ICSE-SEIP '22"
"Conference Paper","Ryan B,Soria AM,Dreef K,van der Hoek A","Reading to Write Code: An Experience Report of a Reverse Engineering and Modeling Course","","2022","","","223–234","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training","Pittsburgh, Pennsylvania","2022","9781450392259","","https://doi.org/10.1145/3510456.3514164;http://dx.doi.org/10.1145/3510456.3514164","10.1145/3510456.3514164","A substantial portion of any software engineer's job is reading code. Despite the criticality of this skill in a budding software engineer, reading code---and more specifically, techniques on how to read code when integrating oneself into a large existing software project---is often neglected in the typical software engineering education. As part of a new professional Master of Software Engineering at the University of California, Irvine, we designed and delivered a ""reading to write code"" course from the ground up. Titled Reverse Engineering and Modeling, the course introduces students to techniques they can use to become familiar with a large code base, so they are able to make meaningful contributions. In this paper, we briefly introduce the Master program and its underlying philosophy, articulate the course's learning outcomes, present the design of the course, and provide a detailed reflection on our experiences in terms of what went well, what did not go well, what we do not know yet, and what our next steps are in preparing for the forthcoming incarnation of the course in Spring 2022. In so doing, we hope to provide a baseline together with lessons learned for others who may be interested in instituting a similar course at their institution.","software understanding, large open source systems, reading code","","ICSE-SEET '22"
"Conference Paper","Mohian S,Csallner C","PSDoodle: Fast App Screen Search via Partial Screen Doodle","","2022","","","89–99","Association for Computing Machinery","New York, NY, USA","Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems","Pittsburgh, Pennsylvania","2022","9781450393010","","https://doi.org/10.1145/3524613.3527816;http://dx.doi.org/10.1145/3524613.3527816","10.1145/3524613.3527816","Searching through existing repositories for a specific mobile app screen design is currently either slow or tedious. Such searches are either limited to basic keyword searches (Google Image Search) or require as input a complete query screen image (SWIRE). A promising alternative is interactive partial sketching, which is more structured than keyword search and faster than complete-screen queries. PSDoodle is the first system to allow interactive search of screens via interactive sketching. PSDoodle is built on top of a combination of the Rico repository of some 58k Android app screens, the Google QuickDraw dataset of icon-level doodles, and DoodleUINet, a curated corpus of some 10k app icon doodles collected from hundreds of individuals. In our evaluation with third-party software developers, PSDoodle provided similar top-10 screen retrieval accuracy as the state of the art from the SWIRE line of work, while cutting the average time required about in half.","GUI, sketch-based image retrieval, user interface design, deep learning, SBIR, sketching, design examples","","MOBILESoft '22"
"Conference Paper","Huang J","Code Clone Detection Based on Doc2vec Model and Bagging","","2022","","","649–653","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics","Nanjing, China","2022","9781450397179","","https://doi.org/10.1145/3548608.3559280;http://dx.doi.org/10.1145/3548608.3559280","10.1145/3548608.3559280","For software analysis and maintenance, the code clone detection has an important role. In order to improve the detection rate of code clone, this paper proposes a code cloning detection method based on Doc2vec model and bagging. Firstly, method converts the code into token sequence and abstract syntax tree sequence. Then, the Doc2vec model is used to learn the lexical and grammatical information of the code respectively. Finally, bagging algorithm is used to detect the cloning of code pairs. The results show that the accuracy and recall of the code clone detection method based on Doc2vec model and bagging is better than that of sourcerercc and tree LSTM on bigclonebench data set.","","","ICCIR '22"
"Conference Paper","Blanvillain O","Type-Safe Regular Expressions","","2022","","","1–8","Association for Computing Machinery","New York, NY, USA","Proceedings of the Scala Symposium","Berlin, Germany","2022","9781450394635","","https://doi.org/10.1145/3550198.3550425;http://dx.doi.org/10.1145/3550198.3550425","10.1145/3550198.3550425","Regular expressions can easily go wrong. Capturing groups, in particular, require meticulous care to avoid running into off-by-one errors and null pointer exceptions. In this chapter, we propose a new design for Scala's regular expressions which completely eliminates this class of errors. Our design makes extensive use of match types, Scala's new feature for type-level programming, to statically analyze regular expressions during type checking. We show that our approach has a minor impact on compilation times, which makes it suitable for practical use.","match types, type safety, regular expressions","","Scala '22"
"Conference Paper","Leandro O,Gheyi R,Teixeira L,Ribeiro M,Garcia A","A Technique to Test Refactoring Detection Tools","","2022","","","188–197","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXVI Brazilian Symposium on Software Engineering","Virtual Event, Brazil","2022","9781450397353","","https://doi.org/10.1145/3555228.3555246;http://dx.doi.org/10.1145/3555228.3555246","10.1145/3555228.3555246","Refactoring detection tools, such as RefactoringMiner and RefDiff, are helpful to study refactorings applied to software repositories. To evaluate them, the tools’ authors study software repositories and manually classify transformations as refactorings. However, this is a time-consuming and an error-prone activity. It is unclear to what extent the refactoring mechanics is consistent with refactoring implementations available in IDEs. In this paper, we propose a technique to test refactoring detection tools. In our technique, we apply a single refactoring using a popular IDE, and then we run the refactoring detection tool to check whether it detects the transformation applied by the IDE. We evaluate our technique by automatically performing 9,885 transformations on four real open-source projects using eight Eclipse IDE refactorings. RefactoringMiner and RefDiff detect more refactorings in 20.41% and 14.11% of the analyzed transformations, respectively. In the remaining cases, RefactoringMiner and RefDiff either do not detect the refactoring or classify it as other types of refactorings. We report 34 issues to refactoring detection tools, and developers fixed 16 bugs, and 3 bugs are duplicated. In other cases, 3 issues are not accepted. This study brings evidence for the need of a shared understanding of refactoring mechanics.","IDE., Refactoring Mechanics, Refactoring Detection Tools","","SBES '22"
"Conference Paper","Torres W,Alves E,Machado P","A Configurable Test Case Prioritization Technique for Early Fault Detection and Low Test Case Spreading","","2022","","","178–187","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXVI Brazilian Symposium on Software Engineering","Virtual Event, Brazil","2022","9781450397353","","https://doi.org/10.1145/3555228.3555231;http://dx.doi.org/10.1145/3555228.3555231","10.1145/3555228.3555231","Developers often use test suites as safety nets to avoid functionality regression. However, regression testing can be costly and time-consuming. Test case prioritization (TCP) techniques try to reduce this burden by reordering the tests of a given suite to achieve a certain testing goal. The literature presents a great number of TCP techniques. Most works evaluate the performance of TCP techniques by using the rate of test cases that fail per fault (APFD). However, other aspects can be considered when evaluating prioritization results. For instance, the ability to reduce the spreading of failing test cases (M-Spreading), since a better grouping often provides more information regarding faults. This paper proposes Additional-Spreading, a configurable test case prioritization strategy that allows testers to favor either APFD or M-Spreading. In a study with open-source projects we evidence the configuration power of Additional-Spreading and that it can provide results similar to two traditional techniques (Total and Additional) when properly configured.","metric, M-Spreading, test case, prioritization, APFD","","SBES '22"
"Conference Paper","Albuquerque D,Guimaraes ET,Tonin GS,Perkusich MB,Almeida H,Perkusich A","Perceptions of Technical Debt and Its Management Activities - A Survey of Software Practitioners","","2022","","","220–229","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXVI Brazilian Symposium on Software Engineering","Virtual Event, Brazil","2022","9781450397353","","https://doi.org/10.1145/3555228.3555237;http://dx.doi.org/10.1145/3555228.3555237","10.1145/3555228.3555237","Technical Debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefits but might hurt the long-term health of a software system. Although several research efforts have been carried out, TD-related literature indicates that Technical Debt Management (TDM) is still incipient. Particularly in software organizations, there is still a lack of knowledge regarding how practitioners perceive TD and perform TDM in their projects. Our research focuses on characterizing TD and its management under the perspective of practitioners. For doing so, we conducted an online survey with 120 participants from 86 different organizations located in 5 different countries. Our results indicate that TD conception is widespread among more than 70% of respondents. Most of them (72%) recognized its importance and impact on software artifacts, being able to provide a valid example of three different TD Types (i.e., Design, Code, and Architectural). In addition, at least 65% of respondents consider TD identification, TD Repayment, and TD prevention as TDM activities in the spotlight. However, less than 15% adopt formal approaches to support these activities. This paper contributes to TD discussion and TDM activities by showing the practitioner’s perspective. Finally, further research will support observing how effective and efficient TDM activities can be in different contexts.","Survey, Technical Debt Management, Empirical Study, Technical Debt","","SBES '22"
"Conference Paper","Neto A,Bezerra C,Serafim Martins J","Code Smell Co-Occurrences: A Systematic Mapping","","2022","","","331–336","Association for Computing Machinery","New York, NY, USA","Proceedings of the XXXVI Brazilian Symposium on Software Engineering","Virtual Event, Brazil","2022","9781450397353","","https://doi.org/10.1145/3555228.3555268;http://dx.doi.org/10.1145/3555228.3555268","10.1145/3555228.3555268","Code smells co-occurrences, i.e., occurrences of more than one code smell in the same class or method, can be better indicators of design problems for software quality. Despite its importance as an indicator of design problems, we have little known about the impact of removing the smells co-occurrence via software refactoring on internal quality attributes, such as coupling, cohesion, complexity, and inheritance. There are several literature reviews on code smells and refactoring. However, we did not identify any literature review investigating the impact of co-occurring code smells in primary studies. Thus, this work presents a systematic mapping of code smell co-occurrences state of the art. To guide the mapping, we defined the following research questions: (i) Which code smells are most tend to co-occur?; (ii) How are code smells co-occurrences refactored?; (iii) What is the impact of code smell co-occurrences on internal quality attributes?; (iv) Which datasets are considered to analyze code smells co-occurrences?; (v) How are code smells co-occurrences detected?; and, (vi) What are the most common code smells co-occurrences?. With main findings, we identified that the code smells that most co-occur are: Feature Envy (19 co-occurrences), Long Method (14 co-occurrences), Long Parameter List (11 co-occurrences) and God Class (11 co-occurrences). No studies were found to answer how the code smells co-occurrences are refactored and the co-occurrences that most impacted the quality attributes, all of which were negative. Only three datasets were considered in the studies to analyze the code smell co-occurrences. Manual detection of co-occurrences is the most used. The co-occurrences Class Data Should Be Private + Feature Envy and Feature Envy + Long Method were the most common co-occurrences in the studies.","code smells co-occurrences, refactoring. internal quality attributes","","SBES '22"
"Conference Paper","Liu X,Wu Y,Yu Q,Song S,Liu Y,Zhou Q,Zhuge J","PG-VulNet: Detect Supply Chain Vulnerabilities in IoT Devices Using Pseudo-Code and Graphs","","2022","","","205–215","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","Helsinki, Finland","2022","9781450394277","","https://doi.org/10.1145/3544902.3546240;http://dx.doi.org/10.1145/3544902.3546240","10.1145/3544902.3546240","Background: With the boosting development of IoT technology, the supply chains of IoT devices become more powerful and sophisticated, and the security issues introduced by code reuse are becoming more prominent. Therefore, the detection and management of vulnerabilities through code similarity detection technology is of great significance for protecting the security of IoT devices. Aim: We aim to propose a more accurate, parallel-friendly, and realistic software supply chain vulnerability detection solution for IoT devices. Method: This paper presents PG-VulNet, standing for Vulnerability-detection Network based on Pseudo-code Graphs. It is a ”multi-model” cross-architecture vulnerability detection solution based on pseudo-code and Graph Matching Network (GMN). PG-VulNet extracts both behavioral and structural features of pseudo-code to build customized feature graphs and then uses GMN to detect supply chain vulnerabilities based on these graphs. Results: The experiments show that PG-VulNet achieves an average detection accuracy of 99.14%, significantly higher than existing approaches like Gemini, VulSeeker, FIT, and Asteria. In addition to this, PG-VulNet also excels in detection overhead and false alarms. In the real-world evaluation, PG-VulNet detected 690 known vulnerabilities in 1,611 firmwares. Conclusions: PG-VulNet can effectively detect the vulnerabilities introduced by software supply chain in IoT firmwares and is well suited for large-scale detection. Compared with existing approaches, PG-VulNet has significant advantages.","Binary Code Similarity, Vulnerability Detection, IoT Software Supply Chain, Graph Neural Network","","ESEM '22"
"Conference Paper","Abou Khalil Z,Zacchiroli S","Software Artifact Mining in Software Engineering Conferences: A Meta-Analysis","","2022","","","227–237","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","Helsinki, Finland","2022","9781450394277","","https://doi.org/10.1145/3544902.3546239;http://dx.doi.org/10.1145/3544902.3546239","10.1145/3544902.3546239","Background: Software development results in the production of various types of artifacts: source code, version control system metadata, bug reports, mailing list conversations, test data, etc. Empirical software engineering (ESE) has thrived mining those artifacts to uncover the inner workings of software development and improve its practices. But which artifacts are studied in the field is a moving target, which we study empirically in this paper. Aims: We quantitatively characterize the most frequently mined and co-mined software artifacts in ESE research and the research purposes they support. Method: We conduct a meta-analysis of artifact mining studies published in 11 top conferences in ESE, for a total of 9621 papers. We use natural language processing (NLP) techniques to characterize the types of software artifacts that are most often mined and their evolution over a 16-year period (2004–2020). We analyze the combinations of artifact types that are most often mined together, as well as the relationship between study purposes and mined artifacts. Results: We find that: (1) mining happens in the vast majority of analyzed papers, (2) source code and test data are the most mined artifacts, (3) there is an increasing interest in mining novel artifacts, together with source code, (4) researchers are most interested in the evaluation of software systems and use all possible empirical signals to support that goal. Conclusions: Our study presents a meta analysis of the usage of software artifacts in the field over a period of 16 years using NLP techniques.","Research trends, Mining software repository, Software artifacts, Systematic mapping, Academic conferences, Meta-analysis","","ESEM '22"
"Conference Paper","Rahman MM,Satter A,Joarder MM,Sakib K","An Empirical Study on the Occurrences of Code Smells in Open Source and Industrial Projects","","2022","","","289–294","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","Helsinki, Finland","2022","9781450394277","","https://doi.org/10.1145/3544902.3546634;http://dx.doi.org/10.1145/3544902.3546634","10.1145/3544902.3546634","Background: Reusing source code containing code smells can induce significant amount of maintenance time and cost. A list of code smells has been identified in the literature and developers are encouraged to avoid the smells from the very beginning while writing new code or reusing existing code, and it increases time and cost to identify and refactor the code after the development of a system. Again, remembering a long list of smells is difficult specially for the new developers. Besides, two different types of software development environment - open source and industry, might have an effect on the occurrences of code smells. Aims: A study on the occurrences of code smells in open source and industrial systems can provide insights about the most frequently occurring smells in each type of software system. The insights can make developers aware of the most frequent occurring smells, and researchers to focus on the improvement and innovation of automatic refactoring tools or techniques for the smells on priority basis. Method: We have conducted a study on 40 large scale Java systems, where 25 are open source and 15 are industrial systems, for 18 code smells. Results: The results show that 6 smells have not occurred in any system, and 12 smells have occurred 21,182 times in total where 60.66% in the open source systems and 39.34% in the industrial systems. Long Method, Complex Class and Long Parameter List have been seen as frequently occurring code smells. The one tailed t-test with 5% level of significant analysis has shown that there is no difference between the occurrences of 10 code smells in industrial and open source systems, and 2 smells are occurred more frequently in open source systems than industrial systems. Conclusions: Our findings conclude that all smells do not occur at the same frequency and some smells are very frequent. The short list of most frequently occurred smells can help developers to write or reuse source code carefully without inducing the smells from the beginning during software development. Our study also concludes that industry and open source environments do not have significant impact on the occurrences of code smells.","industrial system, open source system, empirical study, code smell","","ESEM '22"
"Conference Paper","Rahman S,Koana UA,Nayebi M","Example Driven Code Review Explanation","","2022","","","307–312","Association for Computing Machinery","New York, NY, USA","Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","Helsinki, Finland","2022","9781450394277","","https://doi.org/10.1145/3544902.3546639;http://dx.doi.org/10.1145/3544902.3546639","10.1145/3544902.3546639","Background: Code reviewing is an essential part of software development to ensure software quality. However, the abundance of review tasks and the intensity of the workload for reviewers negatively impact the quality of the reviews. The short review text is often unactionable. Aims: We propose the Example Driven Review Explanation (EDRE) method to facilitate the code review process by adding additional explanations through examples. EDRE recommends similar code reviews as examples to further explain a review and help a developer to understand the received reviews with less communication overhead. Method: Through an empirical study in an industrial setting and by analyzing 3,722 Code reviews across three open-source projects, we compared five methods of data retrieval, text classification, and text recommendation. Results: EDRE using TF-IDF word embedding along with an SVM classifier can provide practical examples for each code review with 92% F-score and 90% Accuracy. Conclusions: The example-based explanation is an established method for assisting experts in explaining decisions. EDRE can accurately provide a set of context-specific examples to facilitate the code review process in software teams.","Code Review, Decision Explanation, Software Engineering, Natural Language Processing (NLP)","","ESEM '22"
"Journal Article","Han R,Lee J,Sim J,Kim H","COX : Exposing CUDA Warp-Level Functions to CPUs","ACM Trans. Archit. Code Optim.","2022","19","4","","Association for Computing Machinery","New York, NY, USA","","","2022-09","","1544-3566","https://doi.org/10.1145/3554736;http://dx.doi.org/10.1145/3554736","10.1145/3554736","As CUDA becomes the de facto programming language among data parallel applications such as high-performance computing or machine learning applications, running CUDA on other platforms becomes a compelling option. Although several efforts have attempted to support CUDA on devices other than NVIDIA GPUs, due to extra steps in the translation, the support is always a few years behind CUDA’s latest features. In particular, the new CUDA programming model exposes the warp concept in the programming language, which greatly changes the way the CUDA code should be mapped to CPU programs. In this article, hierarchical collapsing that correctly supports CUDA warp-level functions on CPUs is proposed. To verify hierarchical collapsing , we build a framework, COX , that supports executing CUDA source code on the CPU backend. With hierarchical collapsing , 90% of kernels in CUDA SDK samples can be executed on CPUs, much higher than previous works (68%). We also evaluate the performance with benchmarks for real applications and show that hierarchical collapsing can generate CPU programs with comparable or even higher performance than previous projects in general.","code migration, compiler transformations, GPU","",""
"Conference Paper","Huang C,Zhou H,Ye C,Li B","Code Clone Detection Based on Event Embedding and Event Dependency","","2022","","","65–74","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545277;http://dx.doi.org/10.1145/3545258.3545277","10.1145/3545258.3545277","The code clone detection method based on semantic similarity has important value in software engineering tasks (e.g., software evolution, software reuse). Traditional code clone detection technologies pay more attention to the similarity of code at the syntax level, and less attention to the semantic similarity of the code. As a result, candidate codes similar in semantics are ignored. To address this issue, we propose a code clone detection method based on semantic similarity. By treating code as a series of interdependent events that occur continuously, we design a model namely EDAM to encode code semantic information based on event embedding and event dependency. The EDAM model uses the event embedding method to model the execution characteristics of program statements and the data dependence information between all statements. In this way, we can embed the program semantic information into a vector and use the vector to detect codes similar in semantics. Experimental results show that the performance of our EDAM model is superior to state-of-the-art open source models for code clone detection.","event embedding, code clone detection, event dependency","","Internetware '22"
"Conference Paper","Liu B,Chen J,Wang W,Cai S,Chen J,Feng Q","An Adaptive Search Optimization Algorithm for Improving the Detection Capability of Software Vulnerability","","2022","","","212–220","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545283;http://dx.doi.org/10.1145/3545258.3545283","10.1145/3545258.3545283","Deep learning-based vulnerability detection frees human experts from the tedious task of defining features and allows for better detection capabilities. The common practice is to convert program code into vector representation for neural network model training. Since the length of the vector representation varies across program code, finding the optimal vector length is critical to ensuring detection accuracy. This paper proposes an adaptive search optimization algorithm for finding the optimal vector length. It sorts all the vector lengths obtained by word2vec and takes the vector length corresponding to the point where the trend changes from slow to fast as the output. We evaluate our algorithm on three publicly available datasets against state-of-the-art algorithms. The results show that, without significantly increasing the time overhead, our algorithm can more accurately choose an appropriate vector length instead of setting a value empirically or arbitrarily. Furthermore, it shows that while a larger vector length can usually produces a higher detection accuracy, the extra time overhead incurred often does not suffice to compensate for the corresponding accuracy improvement.","Vulnerability detection, Software security, Deep learning, Adaptive search optimization algorithm","","Internetware '22"
"Conference Paper","Wang B,Liu G,Lin Y,Ren S,Li H,Zhang D","Enhanced Evolutionary Automated Program Repair by Finer-Granularity Ingredients and Better Search Algorithms","","2022","","","107–116","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545286;http://dx.doi.org/10.1145/3545258.3545286","10.1145/3545258.3545286","Bug repair is time-consuming and tedious, which hampers software maintenance. To alleviate the burden, automated program repair (APR) is proposed and has been fruitful in the last decade. Evolutionary repair is the seminal work of this field and proliferated a family of approaches. The performance of evolutionary repair approaches is affected by two main factors: (1) search space, which defines all possible patches, and (2) search algorithms, which navigates the space. Although recent approaches have achieved remarkable progress, the main challenges of the two factors still remain. On one hand, the different kinds of search space are very coarse for containing correct patches. On the other hand, the search process guided by genetic algorithms is inefficient to find the correct patches in an appropriate time budget. In this paper, we propose MicroRepair, a new evolutionary repair approach to address the two challenges. Rather than finding statement-level patches like existing genetic repair approaches, MicroRepair enlarges the search space by breaking the statements into finer-granularity ingredients that are consist of AST leaves. As the search space grows exponentially, the former search algorithms may become inefficient to navigate in the larger space. We utilize the best multi-objective search algorithm selected from our empirical comparison on a set of search algorithms. We evaluated MicroRepair on 224 bugs of real-world from the benchmark Defects4J and compare it with several state-of-the-art repair approaches. The evaluation results show that MicroRepair correctly repaired 26 bugs with a precision of 62%, which significantly outperforms the state-of-the-art evolutionary APR approaches in terms of precision.","Automated Program Repair, Genetic Improvement, Evolutionary Program Repair, Genetic Programming","","Internetware '22"
"Conference Paper","Xu S,Yao Y,Xu F,Gu T,Tong H","Combining Code Context and Fine-Grained Code Difference for Commit Message Generation","","2022","","","242–251","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545274;http://dx.doi.org/10.1145/3545258.3545274","10.1145/3545258.3545274","Generating natural language messages for source code changes is an essential task in software development and maintenance. Existing solutions mainly treat a piece of code difference as natural language, and adopt seq2seq learning to translate it into a commit message. The basic assumption of such solutions lies in the naturalness hypothesis, i.e., source code written by programming languages is to some extent similar to natural language text. However, compared with natural language, source code also bears syntactic regularities. In this paper, we propose to simultaneously model the naturalness and syntactic regularities of source code changes for commit message generation. Specifically, to model syntactic regularities, we first enlarge the input with additional context information, i.e., the code statements that have dependency with the variables in the code difference, and then extract the paths in the corresponding ASTs. Moreover, to better model code difference, we align the two versions of code before and after the committed code change at token level, and annotate their differences with fine-grained edit operations. The context and difference are simultaneously encoded in a learning framework to generate the commit messages. We collected from GitHub a large dataset containing 480 Java projects with over 160k commits, and the experimental results demonstrate the effectiveness of the proposed approach.","code regularity, Commit message generation, software naturalness","","Internetware '22"
"Conference Paper","Qian R,Zhang Q,Fang C,Guo L","Investigating Coverage Guided Fuzzing with Mutation Testing","","2022","","","272–281","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545285;http://dx.doi.org/10.1145/3545258.3545285","10.1145/3545258.3545285","Coverage guided fuzzing (CGF) is an effective testing technique which has detected hundreds of thousands of bugs from various software applications. It focuses on maximizing code coverage to reveal more bugs during fuzzing. However, a higher coverage does not necessarily imply a better fault detection capability. Triggering a bug involves not only exercising the specific program path but also reaching interesting program states in that path. In this paper, we use mutation testing to improve CGF in detecting bugs. We use mutation scores as additional feedback to guide fuzzing towards detecting bugs rather than just covering code. To evaluate our approach, we conduct a well-designed experiment on 5 benchmarks. We choose the state-of-the-art fuzzing technique Zest as baseline and construct two modified techniques on it using our approach. The experimental results show that our approach can improve CGF in both code coverage and bug detection.","Coverage Guided Fuzzing, Fuzzing, Mutation testing","","Internetware '22"
"Conference Paper","Chen S,Xu S,Yao Y,Xu F","Untangling Composite Commits by Attributed Graph Clustering","","2022","","","117–126","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th Asia-Pacific Symposium on Internetware","Hohhot, China","2022","9781450397803","","https://doi.org/10.1145/3545258.3545267;http://dx.doi.org/10.1145/3545258.3545267","10.1145/3545258.3545267","During software development, it is considered to be a best practice if each commit represents one distinct concern, such as fixing a bug or adding a new feature. However, developers may not always follow this practice and sometimes tangle multiple concerns into a single composite commit. This makes automatic commit untangling a necessary task, and recent approaches mainly untangle commits via applying graph clustering on the code dependency graph. In this paper, we propose a new commit untangling approach, ComUnt, to decompose the composite commits into atomic ones. Different from existing approaches, ComUnt is built upon the observation that both the textual content of code statements and the dependencies between code statements contain useful semantic information so as to better comprehend the committed code changes. Based on this observation, ComUnt first constructs an attributed graph for each commit, where code statements and various code dependencies are modeled as nodes and edges, respectively, and the textual body of code statements are maintained as node attributes. It then conducts attributed graph clustering on the constructed graph. The used attributed graph clustering algorithm can simultaneously encode both graph structure and node attributes so as to better separate the code changes into clusters with distinct concerns. We evaluate our approach on nine C# projects, and the experimental result shows that ComUnt improves the state-of-the-art by 7.8% in terms of untangling accuracy, and meanwhile it is more than 6 times faster.","Commit untangling, attributed graph clustering, code dependency graph","","Internetware '22"
"Conference Paper","Serafini D,Zacchiroli S","Efficient Prior Publication Identification for Open Source Code","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 18th International Symposium on Open Collaboration","Madrid, Spain","2022","9781450398459","","https://doi.org/10.1145/3555051.3555068;http://dx.doi.org/10.1145/3555051.3555068","10.1145/3555051.3555068","Free/Open Source Software (FOSS) enables large-scale reuse of preexisting software components. The main drawback is increased complexity in software supply chain management. A common approach to tame such complexity is automated open source compliance, which consists in automating the verification of adherence to various open source management best practices about license obligation fulfillment, vulnerability tracking, software composition analysis, and nearby concerns. We consider the problem of auditing a source code base to determine which of its parts have been published before, which is an important building block of automated open source compliance toolchains. Indeed, if source code allegedly developed in house is recognized as having been previously published elsewhere, alerts should be raised to investigate where it comes from and whether this entails that additional obligations shall be fulfilled before product shipment. We propose an efficient approach for prior publication identification that relies on a knowledge base of known source code artifacts linked together in a global Merkle direct acyclic graph and a dedicated discovery protocol. We introduce swh-scanner, a source code scanner that realizes the proposed approach in practice using as knowledge base Software Heritage, the largest public archive of source code artifacts. We validate experimentally the proposed approach, showing its efficiency in both abstract (number of queries) and concrete terms (wall-clock time), performing benchmarks on 16845 real-world public code bases of various sizes, from small to very large.","license compliance, prior art, open source, source code scanning, software supply chain, open compliance","","OpenSym '22"
"Conference Paper","König C,Rosiak K,Linsbauer L,Schaefer I","Synchronizing Software Variants: A Two-Dimensional Approach","","2022","","","82–89","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B","Graz, Austria","2022","9781450392068","","https://doi.org/10.1145/3503229.3547053;http://dx.doi.org/10.1145/3503229.3547053","10.1145/3503229.3547053","Code copying and customization is a common practice to realize variability and serve the demand for custom-tailored software. The clone-and-own-approach is flexible and efficient, but does not scale with the number of variants as developers must transfer changes between clones manually, which is an error-prone and tedious task. This task becomes even more challenging, when developers reuse code not only between variants, but also within variants. As a solution, we propose a novel synchronization technique that supports developers to transfer changes applied to clones to its corresponding clone instances within a variant as well as across other variants. Our technique relies on a common model of clone relationships between and within variants, that can automatically propagate changes two-dimensional into the respective intra- and inter-clone instances. In an empirical evaluation, we demonstrate the need for this two-dimensional clone synchronization and show the usefulness and scalability of our approach using the MobileMedia case study.","software product lines, clone synchronization, clone detection, clone-and-own","","SPLC '22"
"Conference Paper","Schulze S,Krüger J,Wünsche J","Towards Developer Support for Merging Forked Test Cases","","2022","","","131–141","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A","Graz, Austria","2022","9781450394437","","https://doi.org/10.1145/3546932.3547002;http://dx.doi.org/10.1145/3546932.3547002","10.1145/3546932.3547002","Developers rely on branching and forking mechanisms of modern versioning systems to evolve and maintain their software systems. As a result, systems often exist in the form of various short-living or even long-living (i.e., clone & own development) variants. Such variants may have to be merged with the main system or other variants, for instance, to propagate features or bug fixes. Within such merging processes, test cases are highly interesting, since they allow to improve the test coverage and hopefully the reliability of the system (e.g., by merging missing tests and bug fixes in test code). However, as all source code, test cases may evolve independently between two or more variants, which makes it non-trivial to decide what changes of the test cases are relevant for the merging. For instance, some test cases in one variant may be irrelevant in another variant (e.g., because the feature shall not be propagated) or may subsume existing test cases. In this paper, we propose a technique that allows for a fine-grained comparison of test cases to support developers in deciding whether and how to merge these. Precisely, inspired by code-clone detection, we use abstract syntax trees to decide on the relations between test cases of different variants. We evaluate the applicability of our technique qualitatively on five open-source systems written in Java (e.g., JUnit 5, Guava). Our insights into the merge potential of 50 pull requests with test cases from these systems indicate that our technique can support the comprehension of differences in variants' test cases, and also highlight future research opportunities.","feature forks, merging, test cases, variant-rich systems","","SPLC '22"
"Conference Paper","Eggert M,Günther K,Maletschek J,Maxiniuc A,Mann-Wahrenberg A","In Three Steps to Software Product Lines: A Practical Example from the Automotive Industry","","2022","","","170–177","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A","Graz, Austria","2022","9781450394437","","https://doi.org/10.1145/3546932.3547003;http://dx.doi.org/10.1145/3546932.3547003","10.1145/3546932.3547003","In the automotive industry, suppliers aim to increase their revenue and try to keep up with the pace of the market trends to stay competitive by offering off-the-shelf products to car manufacturers. On the other hand those car manufacturers request tailored products to gain unique selling points. Each new customer request may result in a new software project. To save time one might find it a good idea to create the new software project as a copy of an older one. This method guarantees initial functionality, but prevents refactoring and leads to continuous software erosion. The implementations diverge from each other and improvements cannot be shared. Software Product Lines (SPL) can help to maximize reusability and quality by building up shared core assets and customer-specific functionality. In our paper, we propose a method to migrate a customer project landscape into a scalable SPL in three steps.","incremental migration, automotive, migration, active projects, software product line, functionality reuse, adoption","","SPLC '22"
"Conference Paper","Marchezan L,Assunção WK,Michelon G,Herac E,Egyed A","Code Smell Analysis in Cloned Java Variants: The Apo-Games Case Study","","2022","","","250–254","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A","Graz, Austria","2022","9781450394437","","https://doi.org/10.1145/3546932.3547015;http://dx.doi.org/10.1145/3546932.3547015","10.1145/3546932.3547015","Families of software products are usually created using opportunistic reuse (clone-and-own) in which products are cloned and adapted to meet new requirements, user preferences, or non-functional properties. Opportunistic reuse brings short-term benefits, e.g., reduced time-to-market, whereas creating long-term drawbacks, e.g., the need of changing multiple variants for any maintenance and evolution activity. This situation is even worse when the individual products have poor design or implementation choices, the so-called code smells. Due to their harmfulness to software quality, code smells should be detected and removed as early as possible. In a family of software products, the same code smell must be identified and removed in all variants where it is are present. Identifying instances of similar code smells affecting different variants has not been investigated in the literature yet. This is the case of the Apo-Games family, which has the challenge of identifying the flaws in the design and implementation of cloned games. To address this challenge, we applied our inconsistency and repair approach to detect and suggest solutions for six types of code smells in 19 products of the Apo-games family. Our results show that a considerable number of smells were identified, most of them for the long parameter list and data class types. The number of the same smells identified in multiple variants ranged between 2.9 and 20.2 on average, showing that clone-and-own may lead to the replication of code smells in multiple products. Lastly, our approach was able to generate between 4.9 and 28.98 repair alternatives per smell on average.","inconsistency repair, software product line, code smells, consistency checking","","SPLC '22"
"Conference Paper","Mortara J,Collet P,Pinna-Dery AM","Customizable Visualization of Quality Metrics for Object-Oriented Variability Implementations","","2022","","","43–54","Association for Computing Machinery","New York, NY, USA","Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A","Graz, Austria","2022","9781450394437","","https://doi.org/10.1145/3546932.3547073;http://dx.doi.org/10.1145/3546932.3547073","10.1145/3546932.3547073","Many large-scale software systems intensively implement variability to reuse software and speed up development. Such mechanisms, however, bring additional complexity, which eventually leads to technical debt, threatening the software quality, and hampering maintenance and evolution. This is especially the case for variability-rich object-oriented (OO) systems that implement variability in a single codebase. They heavily rely on existing OO mechanisms to implement their variability, making them especially prone to variability debt at the code level. In this paper, we propose VariMetrics, an extension of a visualization relying on the city metaphor to reveal such zones of indebted OO variability implementations. VariMetrics extends the VariCity visualization and displays standard OO quality metrics, such as code duplication, code complexity, or test coverage, as additional visual properties on the buildings representing classes. Extended configuration options allow the user to choose and combine quality metrics, uncovering the critical zones of OO variability implementations. We evaluate VariMetrics both by reporting on the exposed quality-critical zones found on multiple large open-source projects, and by correcting the reported issues in such zones of one project, showing an improvement in quality.","object-oriented systems, reverse-engineering, quality metrics, software variability, technical debt, software visualization","","SPLC '22"
"Journal Article","Gardner DJ,Reynolds DR,Woodward CS,Balos CJ","Enabling New Flexibility in the SUNDIALS Suite of Nonlinear and Differential/Algebraic Equation Solvers","ACM Trans. Math. Softw.","2022","48","3","","Association for Computing Machinery","New York, NY, USA","","","2022-09","","0098-3500","https://doi.org/10.1145/3539801;http://dx.doi.org/10.1145/3539801","10.1145/3539801","In recent years, the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (SUNDIALS) has been redesigned to better enable the use of application-specific and third-party algebraic solvers and data structures. Throughout this work, we have adhered to specific guiding principles that minimized the impact to current users while providing maximum flexibility for later evolution of solvers and data structures. The redesign was done through the addition of new linear and nonlinear solvers classes, enhancements to the vector class, and the creation of modern Fortran interfaces. The vast majority of this work has been performed “behind-the-scenes,” with minimal changes to the user interface and no reduction in solver capabilities or performance. These changes allow SUNDIALS users to more easily utilize external solver libraries and create highly customized solvers, enabling greater flexibility on extreme-scale, heterogeneous computational architectures.","time integration, nonlinear solvers, Numerical software, high-performance computing, object-oriented design","",""
"Journal Article","Jia A,Fan M,Jin W,Xu X,Zhou Z,Tang Q,Nie S,Wu S,Liu T","1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-09","","1049-331X","https://doi.org/10.1145/3561385;http://dx.doi.org/10.1145/3561385","10.1145/3561385","Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. “1-to-1” mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of “1-to-n” (one binary function matches multiple source functions or binary functions) or even “n-to-n” (multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. In this paper, we investigate the effect of function inlining on binary similarity analysis. We carry out three studies to investigate the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Firstly, a scalable and lightweight identification method is designed to recover function inlining in binaries. 88 projects (compiled in 288 versions and resulting in 32,460,156 binary functions) are collected and analyzed to construct 4 inlining-oriented datasets for 4 security tasks in the software supply chain, including code search, OSS (Open Source Software) reuse detection, vulnerability detection, and patch presence test. Datasets reveal that the proportion of function inlining ranges from 30%-40% when using O3 and sometimes can reach nearly 70%. Then, we evaluate 4 existing works on our dataset. Results show most existing works neglect inlining and use the “1-to-1” mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, most inlined functions would be ignored during OSS reuse detection and patch presence test, thus leaving these functions risky. Finally, we analyze 2 inlining-simulation strategies on our dataset. It is shown that they miss nearly 40% of the inlined functions, and there is still a large space for promotion. By precisely recovering when function inlining happens, we discover that inlining is usually cumulative when optimization increases. Thus, conditional inlining and incremental inlining are recommended to design a low-cost and high-coverage inlining-simulation strategy.","1-to-n, 1-to-1, function inlining, binary similarity analysis","Just Accepted",""
"Conference Paper","Sarsa S,Leinonen J,Koutcheme C,Hellas A","Speeding Up Automated Assessment of Programming Exercises","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 Conference on United Kingdom & Ireland Computing Education Research","Dublin, Ireland","2022","9781450397421","","https://doi.org/10.1145/3555009.3555013;http://dx.doi.org/10.1145/3555009.3555013","10.1145/3555009.3555013","Introductory programming courses around the world use automatic assessment. Automatic assessment for programming code is typically performed via unit tests which require computation time to execute, at times in significant amounts, leading to computation costs and delay in feedback to students. We present a step-based approach for speeding up automated assessment to address the issue, consisting of (1) a cache of past programming exercise submissions and their associated test results to avoid retesting equivalent new submissions; (2) static analysis to detect e.g. infinite loops (heuristically) ; (3) a machine learning model to evaluate programs without running them ; and (4) a traditional set of unit tests. When a student submits code for an exercise, the code is evaluated sequentially through each step, providing feedback to the student at the earliest possible time, reducing the need to run tests. We evaluate the impact of the proposed approach using data collected from an introductory programming course and demonstrate a considerable reduction in the number of exercise submissions that require running the tests (up to 80% of exercises). Using the approach leads to faster feedback in a more sustainable way, and also provides opportunities for precise non-exercise specific feedback in steps (2) and (3).","machine learning, source code, sustainability, static analysis, educational data mining, automated assessment, automatic assessment, feedback","","UKICER '22"
"Journal Article","Ko HS,Chen LT,Lin TC","Datatype-Generic Programming Meets Elaborator Reflection","Proc. ACM Program. Lang.","2022","6","ICFP","","Association for Computing Machinery","New York, NY, USA","","","2022-08","","","https://doi.org/10.1145/3547629;http://dx.doi.org/10.1145/3547629","10.1145/3547629","Datatype-generic programming is natural and useful in dependently typed languages such as Agda. However, datatype-generic libraries in Agda are not reused as much as they should be, because traditionally they work only on datatypes decoded from a library’s own version of datatype descriptions; this means that different generic libraries cannot be used together, and they do not work on native datatypes, which are preferred by the practical Agda programmer for better language support and access to other libraries. Based on elaborator reflection, we present a framework in Agda featuring a set of general metaprograms for instantiating datatype-generic programs as, and for, a useful range of native datatypes and functions —including universe-polymorphic ones— in programmer-friendly and customisable forms. We expect that datatype-generic libraries built with our framework will be more attractive to the practical Agda programmer. As the elaborator reflection features used by our framework become more widespread, our design can be ported to other languages too.","universe polymorphism, inductive families, datatype-generic programming, dependently typed programming, elaborator reflection, metaprogramming","",""
"Conference Paper","Ohm M,Boes F,Bungartz C,Meier M","On the Feasibility of Supervised Machine Learning for the Detection of Malicious Software Packages","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 17th International Conference on Availability, Reliability and Security","Vienna, Austria","2022","9781450396707","","https://doi.org/10.1145/3538969.3544415;http://dx.doi.org/10.1145/3538969.3544415","10.1145/3538969.3544415","Modern software development heavily relies on a multitude of externally – often also open source – developed components that constitute a so-called Software Supply Chain. Over the last few years a rise of trojanized (i.e., maliciously manipulated) software packages have been observed and addressed in multiple academic publications. A central issue of this is the timely detection of such malicious packages for which typically single heuristic- or machine learning based approaches have been chosen. Especially the general suitability of supervised machine learning is currently not fully covered. In order to gain insight, we analyze a diverse set of commonly employed supervised machine learning techniques, both quantitatively and qualitatively. More precisely, we leverage a labeled dataset of known malicious software packages on which we measure the performance of each technique. This is followed by an in-depth analysis of the three best performing classifiers on unlabeled data, i.e., the whole npm package repository. Our combination of multiple classifiers indicates a good viability of supervised machine learning for the detection of malicious packages by pre-selecting a feasible number of suspicious packages for further manual analysis. This research effort includes the evaluation of over 25,210 different models which led to True Positive Rates of over 70 % and the detection and reporting of 13 previously unknown malicious packages.","Supervised Machine Learning, Software Supply Chain, Malware Detection","","ARES '22"
"Journal Article","Tian H,Li Y,Pian W,Kaboré AK,Liu K,Habib A,Klein J,Bissyandé TF","Predicting Patch Correctness Based on the Similarity of Failing Test Cases","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-08","","1049-331X","https://doi.org/10.1145/3511096;http://dx.doi.org/10.1145/3511096","10.1145/3511096","How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches. We then propose BATS, an unsupervised learning-based approach to predict patch correctness by checking patch Behavior Against failing Test Specification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets—as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools.","patch semantics, patch correctness, test behavior, Program repair","",""
"Journal Article","Ni C,Xia X,Lo D,Yang X,Hassan AE","Just-In-Time Defect Prediction on JavaScript Projects: A Replication Study","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-08","","1049-331X","https://doi.org/10.1145/3508479;http://dx.doi.org/10.1145/3508479","10.1145/3508479","Change-level defect prediction is widely referred to as just-in-time (JIT) defect prediction since it identifies a defect-inducing change at the check-in time, and researchers have proposed many approaches based on the language-independent change-level features. These approaches can be divided into two types: supervised approaches and unsupervised approaches, and their effectiveness has been verified on Java or C++ projects. However, whether the language-independent change-level features can effectively identify the defects of JavaScript projects is still unknown. Additionally, many researches have confirmed that supervised approaches outperform unsupervised approaches on Java or C++ projects when considering inspection effort. However, whether supervised JIT defect prediction approaches can still perform best on JavaScript projects is still unknown. Lastly, prior proposed change-level features are programming language–independent, whether programming language–specific change-level features can further improve the performance of JIT approaches on identifying defect-prone changes is also unknown.To address the aforementioned gap in knowledge, in this article, we collect and label the top-20 most starred JavaScript projects on GitHub. JavaScript is an extremely popular and widely used programming language in the industry. We propose five JavaScript-specific change-level features and conduct a large-scale empirical study (i.e., involving a total of 176,902 changes) and find that (1) supervised JIT defect prediction approaches (i.e., CBS+) still statistically significantly outperform unsupervised approaches on JavaScript projects when considering inspection effort; (2) JavaScript-specific change-level features can further improve the performance of approach built with language-independent features on identifying defect-prone changes; (3) the change-level features in the dimension of size (i.e., LT), diffusion (i.e., NF), and JavaScript-specific (i.e., SO and TC) are the most important features for indicating the defect-proneness of a change on JavaScript projects; and (4) project-related features (i.e., Stars, Branches, Def Ratio, Changes, Files, Defective, and Forks) have a high association with the probability of a change to be a defect-prone one on JavaScript projects.","JavaScript, just-in-time defect prediction, Defect prediction, empirical study","",""
"Conference Paper","Albuquerque D,Guimaraes E,Tonin G,Perkusich M,Almeida H,Perkusich A","Comprehending the Use of Intelligent Techniques to Support Technical Debt Management","","2022","","","21–30","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Technical Debt","Pittsburgh, Pennsylvania","2022","9781450393041","","https://doi.org/10.1145/3524843.3528097;http://dx.doi.org/10.1145/3524843.3528097","10.1145/3524843.3528097","Technical Debt (TD) refers to the consequences of taking shortcuts when developing software. Technical Debt Management (TDM) becomes complex since it relies on a decision process based on multiple and heterogeneous data, which are not straightforward to be synthesized. In this context, there is a promising opportunity to use Intelligent Techniques to support TDM activities since these techniques explore data for knowledge discovery, reasoning, learning, or supporting decision-making. Although these techniques can be used for improving TDM activities, there is no empirical study exploring this research area. This study aims to identify and analyze solutions based on Intelligent Techniques employed to support TDM activities. A Systematic Mapping Study was performed, covering publications between 2010 and 2020. From 2276 extracted studies, we selected 111 unique studies. We found a positive trend in applying Intelligent Techniques to support TDM activities, being Machine Learning, Reasoning Under Uncertainty, and Natural Language Processing the most recurrent ones. Identification, measurement, and monitoring were the more recurrent TDM activities, whereas Design, Code, and Architectural were the most frequently investigated TD types. Although the research area is up-and-coming, it is still in its infancy, and this study provides a baseline for future research.","intelligent techniques, technical debt management, technical debt, systematic mapping study","","TechDebt '22"
"Conference Paper","Tornhill A,Borg M","Code Red: The Business Impact of Code Quality - a Quantitative Study of 39 Proprietary Production Codebases","","2022","","","11–20","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Technical Debt","Pittsburgh, Pennsylvania","2022","9781450393041","","https://doi.org/10.1145/3524843.3528091;http://dx.doi.org/10.1145/3524843.3528091","10.1145/3524843.3528091","Code quality remains an abstract concept that fails to get traction at the business level. Consequently, software companies keep trading code quality for time-to-market and new features. The resulting technical debt is estimated to waste up to 42% of developers' time. At the same time, there is a global shortage of software developers, meaning that developer productivity is key to software businesses. Our overall mission is to make code quality a business concern, not just a technical aspect. Our first goal is to understand how code quality impacts 1) the number of reported defects, 2) the time to resolve issues, and 3) the predictability of resolving issues on time. We analyze 39 proprietary production codebases from a variety of domains using the CodeScene tool based on a combination of source code analysis, version-control mining, and issue information from Jira. By analyzing activity in 30,737 files, we find that low quality code contains 15 times more defects than high quality code. Furthermore, resolving issues in low quality code takes on average 124% more time in development. Finally, we report that issue resolutions in low quality code involve higher uncertainty manifested as 9 times longer maximum cycle times. This study provides evidence that code quality cannot be dismissed as a technical concern. With 15 times fewer defects, twice the development speed, and substantially more predictable issue resolution times, the business advantage of high quality code should be unmistakably clear.","developer productivity, business impact, mining software repositories, technical debt, code quality, software defects","","TechDebt '22"
"Conference Paper","Jagerman R,Wang X,Zhuang H,Qin Z,Bendersky M,Najork M","Rax: Composable Learning-to-Rank Using JAX","","2022","","","3051–3060","Association for Computing Machinery","New York, NY, USA","Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","Washington DC, USA","2022","9781450393850","","https://doi.org/10.1145/3534678.3539065;http://dx.doi.org/10.1145/3534678.3539065","10.1145/3534678.3539065","Rax is a library for composable Learning-to-Rank (LTR) written entirely in JAX. The goal of Rax is to facilitate easy prototyping of LTR systems by leveraging the flexibility and simplicity of JAX. Rax provides a diverse set of popular ranking metrics and losses that integrate well with the rest of the JAX ecosystem. Furthermore, Rax implements a system of ranking-specific function transformations which allows fine-grained customization of ranking losses and metrics. Most notably Rax provides approx_t12n: a function transformation (t12n) that can transform any of our ranking metrics into an approximate and differentiable form that can be optimized. This provides a systematic way to directly optimize neural ranking models for ranking metrics that are not easily optimizable in other libraries. We empirically demonstrate the effectiveness of Rax by benchmarking neural models implemented using Flax and trained using Rax on two popular LTR benchmarks: WEB30K and Istella. Furthermore, we show that integrating ranking losses with T5, a large language model, can improve overall ranking performance on the MS MARCO passage ranking task. We are sharing the Rax library with the open source community as part of the larger JAX ecosystem at https://github.com/google/rax.","learning to rank, JAX","","KDD '22"
"Conference Paper","Jin Z,Vetter JS","Performance Portability Study of Epistasis Detection Using SYCL on NVIDIA GPU","","2022","","","","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics","Northbrook, Illinois","2022","9781450393867","","https://doi.org/10.1145/3535508.3545591;http://dx.doi.org/10.1145/3535508.3545591","10.1145/3535508.3545591","We describe the experience of converting a CUDA implementation of a high-order epistasis detection algorithm to SYCL. The goals are for our work to be useful to application and compiler developers with a detailed description of migration paths between CUDA and SYCL. Evaluating the CUDA and SYCL applications on an NVIDIA V100 GPU, we find that the optimization of loop unrolling needs to be applied manually to the SYCL kernel for obtaining comparable performance. The performance of the SYCL group reduce function, an alternative to the CUDA warp-based reduction, depends on the problem and work group sizes. The 64-bit popcount operation implemented with tree of adders is slightly faster than the built-in popcount operation. When the number of OpenMP threads is four, the highest performance of the SYCL and CUDA applications are comparable.","portability, programming model, GPU, epistasis","","BCB '22"
"Journal Article","Yu H,Hu X,Li G,Li Y,Wang Q,Xie T","Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3502852;http://dx.doi.org/10.1145/3502852","10.1145/3502852","In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.","dataset collection, Code clone detection, deep learning","",""
"Journal Article","Hu X,Chen Q,Wang H,Xia X,Lo D,Zimmermann T","Correlating Automated and Human Evaluation of Code Documentation Generation Quality","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3502853;http://dx.doi.org/10.1145/3502853","10.1145/3502853","Automatic code documentation generation has been a crucial task in the field of software engineering. It not only relieves developers from writing code documentation but also helps them to understand programs better. Specifically, deep-learning-based techniques that leverage large-scale source code corpora have been widely used in code documentation generation. These works tend to use automatic metrics (such as BLEU, METEOR, ROUGE, CIDEr, and SPICE) to evaluate different models. These metrics compare generated documentation to reference texts by measuring the overlapping words. Unfortunately, there is no evidence demonstrating the correlation between these metrics and human judgment. We conduct experiments on two popular code documentation generation tasks, code comment generation and commit message generation, to investigate the presence or absence of correlations between these metrics and human judgments. For each task, we replicate three state-of-the-art approaches and the generated documentation is evaluated automatically in terms of BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. We also ask 24 participants to rate the generated documentation considering three aspects (i.e., language, content, and effectiveness). Each participant is given Java methods or commit diffs along with the target documentation to be rated. The results show that the ranking of generated documentation from automatic metrics is different from that evaluated by human annotators. Thus, these automatic metrics are not reliable enough to replace human evaluation for code documentation generation tasks. In addition, METEOR shows the strongest correlation (with moderate Pearson correlation r about 0.7) to human evaluation metrics. However, it is still much lower than the correlation observed between different annotators (with a high Pearson correlation r about 0.8) and correlations that are reported in the literature for other tasks (e.g., Neural Machine Translation [39]). Our study points to the need to develop specialized automated evaluation metrics that can correlate more closely to human evaluation metrics for code generation tasks.","evaluation metrics, empirical study, Code documentation generation","",""
"Journal Article","Seitz KA,Foley T,Porumbescu SD,Owens JD","Supporting Unified Shader Specialization by Co-Opting C++ Features","Proc. ACM Comput. Graph. Interact. Tech.","2022","5","3","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","","https://doi.org/10.1145/3543866;http://dx.doi.org/10.1145/3543866","10.1145/3543866","Modern unified programming models (such as CUDA and SYCL) that combine host (CPU) code and GPU code into the same programming language, same file, and same lexical scope lack adequate support for GPU code specialization, which is a key optimization in real-time graphics. Furthermore, current methods used to implement specialization do not translate to a unified environment. In this paper, we create a unified shader programming environment in C++ that provides first-class support for specialization by co-opting C++'s attribute and virtual function features and reimplementing them with alternate semantics to express the services required. By co-opting existing features, we enable programmers to use familiar C++ programming techniques to write host and GPU code together, while still achieving efficient generated C++ and HLSL code via our source-to-source translator.","Unified Programming, Real-Time Rendering, Shading Languages, Shaders, Heterogeneous Programming","",""
"Journal Article","Dramko L,Lacomis J,Yin P,Schwartz EJ,Allamanis M,Neubig G,Vasilescu B,Le Goues C","DIRE and Its Data: Neural Decompiled Variable Renamings with Respect to Software Class","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3546946;http://dx.doi.org/10.1145/3546946","10.1145/3546946","The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.","Decompilation, Machine Learning, Data Provenance","Just Accepted",""
"Journal Article","Zeng C,Yu Y,Li S,Xia X,Wang Z,Geng M,Bai L,Dong W,Liao X","DeGraphCS: Embedding Variable-Based Flow Graph for Neural Code Search","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3546066;http://dx.doi.org/10.1145/3546066","10.1145/3546066","With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). In this paper, we propose a novel learnable deep Graph for Code Search (called deGraphCS) to transfer source code into variable-based flow graphs based on an intermediate representation technique, which can model code semantics more precisely than directly processing the code as text or using the syntax tree representation. Furthermore, we propose a graph optimization mechanism to refine the code representation and apply an improved gated graph neural network to model variable-based flow graphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale dataset from GitHub containing 41,152 code snippets written in the C language and reproduce several typical deep code search methods for comparison. The experimental results show that deGraphCS can achieve state-of-the-art performance and accurately retrieve code snippets satisfying the needs of the users.","graph neural networks, deep learning, code search, intermediate representation","Just Accepted",""
"Conference Paper","Tufano M,Drain D,Svyatkovskiy A,Sundaresan N","Generating Accurate Assert Statements for Unit Test Cases Using Pretrained Transformers","","2022","","","54–64","Association for Computing Machinery","New York, NY, USA","Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test","Pittsburgh, Pennsylvania","2022","9781450392860","","https://doi.org/10.1145/3524481.3527220;http://dx.doi.org/10.1145/3524481.3527220","10.1145/3524481.3527220","Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task.In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests.The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62% of the cases in the first attempt. The results show 80% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature, as well as 33% improvement over the recent Transformer-based T5 approach. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.","software testing, unit test, neural networks","","AST '22"
"Conference Paper","Krauss O","Amaru: A Framework for Combining Genetic Improvement with Pattern Mining","","2022","","","1930–1937","Association for Computing Machinery","New York, NY, USA","Proceedings of the Genetic and Evolutionary Computation Conference Companion","Boston, Massachusetts","2022","9781450392686","","https://doi.org/10.1145/3520304.3534016;http://dx.doi.org/10.1145/3520304.3534016","10.1145/3520304.3534016","We present Amaru, a framework for Genetic Improvement utilizing Abstract Syntax Trees directly at the interpreter and compiler level. Amaru also enables the mining of frequent, discriminative patterns from Genetic Improvement populations. These patterns in turn can be used to improve the crossover and mutation operators to increase population diversity, reduce the number of individuals failing at run-time and increasing the amount of successful individuals in the population.","genetic improvement, framework, interpreter, compiler","","GECCO '22"
"Conference Paper","Wang H,Qu W,Katz G,Zhu W,Gao Z,Qiu H,Zhuge J,Zhang C","JTrans: Jump-Aware Transformer for Binary Code Similarity Detection","","2022","","","1–13","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534367;http://dx.doi.org/10.1145/3533767.3534367","10.1145/3533767.3534367","Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines.","Neural Networks, Similarity Detection, Binary Analysis, Datasets","","ISSTA 2022"
"Conference Paper","Zhang X,Gong Y,Liang B,Huang J,You W,Shi W,Zhang J","Hunting Bugs with Accelerated Optimal Graph Vertex Matching","","2022","","","64–76","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534393;http://dx.doi.org/10.1145/3533767.3534393","10.1145/3533767.3534393","Various techniques based on code similarity measurement have been proposed to detect bugs. Essentially, the code fragment can be regarded as a kind of graph. Performing code graph similarity comparison to identify the potential bugs is a natural choice. However, the logic of a bug often involves only a few statements in the code fragment, while others are bug-irrelevant. They can be considered as a kind of noise, and can heavily interfere with the code similarity measurement. In theory, performing optimal vertex matching can address the problem well, but the task is NP-complete and cannot be applied to a large-scale code base. In this paper, we propose a two-phase strategy to accelerate code graph vertex matching for detecting bugs. In the first phase, a vertex matching embedding model is trained and used to rapidly filter a limited number of candidate code graphs from the target code base, which are likely to have a high vertex matching degree with the seed, i.e., the known buggy code. As a result, the number of code graphs needed to be further analyzed is dramatically reduced. In the second phase, a high-order similarity embedding model based on graph convolutional neural network is built to efficiently get the approximately optimal vertex matching between the seed and candidates. On this basis, the code graph similarity is calculated to identify the potential buggy code. The proposed method is applied to five open source projects. In total, 31 unknown bugs were successfully detected and confirmed by developers. Comparative experiments demonstrate that our method can effectively mitigate the noise problem, and the detection efficiency can be improved dozens of times with the two-phase strategy.","optimal vertex matching, bug detection, graph convolutional neural network, code similarity","","ISSTA 2022"
"Conference Paper","Cheng X,Zhang G,Wang H,Sui Y","Path-Sensitive Code Embedding via Contrastive Learning for Software Vulnerability Detection","","2022","","","519–531","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534371;http://dx.doi.org/10.1145/3533767.3534371","10.1145/3533767.3534371","Machine learning and its promising branch deep learning have shown success in a wide range of application domains. Recently, much effort has been expended on applying deep learning techniques (e.g., graph neural networks) to static vulnerability detection as an alternative to conventional bug detection methods. To obtain the structural information of code, current learning approaches typically abstract a program in the form of graphs (e.g., data-flow graphs, abstract syntax trees), and then train an underlying classification model based on the (sub)graphs of safe and vulnerable code fragments for vulnerability prediction. However, these models are still insufficient for precise bug detection, because the objective of these models is to produce classification results rather than comprehending the semantics of vulnerabilities, e.g., pinpoint bug triggering paths, which are essential for static bug detection. This paper presents ContraFlow, a selective yet precise contrastive value-flow embedding approach to statically detect software vulnerabilities. The novelty of ContraFlow lies in selecting and preserving feasible value-flow (aka program dependence) paths through a pretrained path embedding model using self-supervised contrastive learning, thus significantly reducing the amount of labeled data required for training expensive downstream models for path-based vulnerability detection. We evaluated ContraFlow using 288 real-world projects by comparing eight recent learning-based approaches. ContraFlow outperforms these eight baselines by up to 334.1%, 317.9%, 58.3% for informedness, markedness and F1 Score, and achieves up to 450.0%, 192.3%, 450.0% improvement for mean statement recall, mean statement precision and mean IoU respectively in terms of locating buggy statements.","vulnerabilities, Path sensitive, code embedding, contrastive learning","","ISSTA 2022"
"Conference Paper","Liu J,Zeng J,Wang X,Ji K,Liang Z","TeLL: Log Level Suggestions via Modeling Multi-Level Code Block Information","","2022","","","27–38","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534379;http://dx.doi.org/10.1145/3533767.3534379","10.1145/3533767.3534379","Developers insert logging statements into source code to monitor system execution, which forms the basis for software debugging and maintenance. For distinguishing diverse runtime information, each software log is assigned with a separate verbosity level (e.g., trace and error). However, choosing an appropriate verbosity level is a challenging and error-prone task due to the lack of specifications for log level usages. Prior solutions aim to suggest log levels based on the code block in which a logging statement resides (i.e., intra-block features). Such suggestions, however, do not consider information from surrounding blocks (i.e., inter-block features), which also plays an important role in revealing logging characteristics. To address this issue, we combine multiple levels of code block information (i.e., intra-block and inter-block features) into a joint graph structure called Flow of Abstract Syntax Tree (FAST). To explicitly exploit multi-level block features, we design a new neural architecture, Hierarchical Block Graph Network (HBGN), on the FAST. In particular, it leverages graph neural networks to encode both the intra-block and inter-block features into code block representations and guide log level suggestions. We implement a prototype system, TeLL, and evaluate its effectiveness on nine large-scale software systems. Experimental results showcase TeLL's advantage in predicting log levels over the state-of-the-art approaches.","Multi-level Code Block Information, Log Level Suggestion, Graph Neural Network","","ISSTA 2022"
"Conference Paper","Zeng Z,Tan H,Zhang H,Li J,Zhang Y,Zhang L","An Extensive Study on Pre-Trained Models for Program Understanding and Generation","","2022","","","39–51","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534390;http://dx.doi.org/10.1145/3533767.3534390","10.1145/3533767.3534390","Automatic program understanding and generation techniques could significantly advance the productivity of programmers and have been widely studied by academia and industry. Recently, the advent of pre-trained paradigm enlightens researchers to develop general-purpose pre-trained models which can be applied for a broad range of program understanding and generation tasks. Such pre-trained models, derived by self-supervised objectives on large unlabelled corpora, can be fine-tuned in downstream tasks (such as code search and code generation) with minimal adaptations. Although these pre-trained models claim superiority over the prior techniques, they seldom follow equivalent evaluation protocols, e.g., they are hardly evaluated on the identical benchmarks, tasks, or settings. Consequently, there is a pressing need for a comprehensive study of the pre-trained models on their effectiveness, versatility as well as the limitations to provide implications and guidance for the future development in this area. To this end, we first perform an extensive study of eight open-access pre-trained models over a large benchmark on seven representative code tasks to assess their reproducibility. We further compare the pre-trained models and domain-specific state-of-the-art techniques for validating pre-trained effectiveness. At last, we investigate the robustness of the pre-trained models by inspecting their performance variations under adversarial attacks. Through the study, we find that while we can in general replicate the original performance of the pre-trained models on their evaluated tasks and adopted benchmarks, subtle performance fluctuations can refute the findings in their original papers. Moreover, none of the existing pre-trained models can dominate over all other models. We also find that the pre-trained models can significantly outperform non-pre-trained state-of-the-art techniques in program understanding tasks. Furthermore, we perform the first study for natural language-programming language pre-trained model robustness via adversarial attacks and find that a simple random attack approach can easily fool the state-of-the-art pre-trained models and thus incur security issues. At last, we also provide multiple practical guidelines for advancing future research on pre-trained models for program understanding and generation.","Adversarial Attack, Code Representation, Deep Learning, Pre-Trained Language Models","","ISSTA 2022"
"Conference Paper","Ghanbari A,Marcus A","Patch Correctness Assessment in Automated Program Repair Based on the Impact of Patches on Production and Test Code","","2022","","","654–665","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534368;http://dx.doi.org/10.1145/3533767.3534368","10.1145/3533767.3534368","Test-based generate-and-validate automated program repair (APR) systems often generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, so previous research proposed various techniques for automatic correctness assessment of APR-generated patches. Among them, dynamic patch correctness assessment techniques rely on the assumption that, when running the originally passing test cases, the correct patches will not alter the program behavior in a significant way, e.g., removing the code implementing correct functionality of the program. In this paper, we propose and evaluate a novel technique, named Shibboleth, for automatic correctness assessment of the patches generated by test-based generate-and-validate APR systems. Unlike existing works, the impact of the patches is captured along three complementary facets, allowing more effective patch correctness assessment. Specifically, we measure the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage of passing tests) to separate the patches that result in similar programs and that do not delete desired program elements. Shibboleth assesses the correctness of patches via both ranking and classification. We evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art ranking and classification techniques. Specifically, in our ranking data set, in 43% (66%) of the cases, Shibboleth ranks the correct patch in top-1 (top-2) positions, and in classification mode applied on our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively.","Patch Correctness Assessment, Automated Program Repair, Branch Coverage, Similarity","","ISSTA 2022"
"Conference Paper","Kim G,Hong S,Franz M,Song D","Improving Cross-Platform Binary Analysis Using Representation Learning via Graph Alignment","","2022","","","151–163","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534383;http://dx.doi.org/10.1145/3533767.3534383","10.1145/3533767.3534383","Cross-platform binary analysis requires a common representation of binaries across platforms, on which a specific analysis can be performed. Recent work proposed to learn low-dimensional, numeric vector representations (i.e., embeddings) of disassembled binary code, and perform binary analysis in the embedding space. Unfortunately, however, existing techniques fall short in that they are either (i) specific to a single platform producing embeddings not aligned across platforms, or (ii) not designed to capture the rich contextual information available in a disassembled binary. We present a novel deep learning-based method, XBA, which addresses the aforementioned problems. To this end, we first abstract binaries as typed graphs, dubbed binary disassembly graphs (BDGs), which encode control-flow and other rich contextual information of different entities found in a disassembled binary, including basic blocks, external functions called, and string literals referenced. We then formulate binary code representation learning as a graph alignment problem, i.e., finding the node correspondences between BDGs extracted from two binaries compiled for different platforms. XBA uses graph convolutional networks to learn the semantics of each node, (i) using its rich contextual information encoded in the BDG, and (ii) aligning its embeddings across platforms. Our formulation allows XBA to learn semantic alignments between two BDGs in a semi-supervised manner, requiring only a limited number of node pairs be aligned across platforms for training. Our evaluation shows that XBA can learn semantically-rich embeddings of binaries aligned across platforms without apriori platform-specific knowledge. By training our model only with 50% of the oracle alignments, XBA was able to predict, on average, 75% of the rest. Our case studies further show that the learned embeddings encode knowledge useful for cross-platform binary analysis.","Binary analysis, Graph alignment, Cross-platform","","ISSTA 2022"
"Conference Paper","Zhao B,Ji S,Xu J,Tian Y,Wei Q,Wang Q,Lyu C,Zhang X,Lin C,Wu J,Beyah R","A Large-Scale Empirical Analysis of the Vulnerabilities Introduced by Third-Party Components in IoT Firmware","","2022","","","442–454","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534366;http://dx.doi.org/10.1145/3533767.3534366","10.1145/3533767.3534366","As the core of IoT devices, firmware is undoubtedly vital. Currently, the development of IoT firmware heavily depends on third-party components (TPCs), which significantly improves the development efficiency and reduces the cost. Nevertheless, TPCs are not secure, and the vulnerabilities in TPCs will turn back influence the security of IoT firmware. Currently, existing works pay less attention to the vulnerabilities caused by TPCs, and we still lack a comprehensive understanding of the security impact of TPC vulnerability against firmware. To fill in the knowledge gap, we design and implement FirmSec, which leverages syntactical features and control-flow graph features to detect the TPCs at version-level in firmware, and then recognizes the corresponding vulnerabilities. Based on FirmSec, we present the first large-scale analysis of the usage of TPCs and the corresponding vulnerabilities in firmware. More specifically, we perform an analysis on 34,136 firmware images, including 11,086 publicly accessible firmware images, and 23,050 private firmware images from TSmart. We successfully detect 584 TPCs and identify 128,757 vulnerabilities caused by 429 CVEs. Our in-depth analysis reveals the diversity of security issues for different kinds of firmware from various vendors, and discovers some well-known vulnerabilities are still deeply rooted in many firmware images. We also find that the TPCs used in firmware have fallen behind by five years on average. Besides, we explore the geographical distribution of vulnerable devices, and confirm the security situation of devices in several regions, e.g., South Korea and China, is more severe than in other regions. Further analysis shows 2,478 commercial firmware images have potentially violated GPL/AGPL licensing terms.","Vulnerability, Third-party component, IoT firmware","","ISSTA 2022"
"Conference Paper","Li Z,Xie X,Li H,Xu Z,Li Y,Liu Y","Cross-Lingual Transfer Learning for Statistical Type Inference","","2022","","","239–250","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","Virtual, South Korea","2022","9781450393799","","https://doi.org/10.1145/3533767.3534411;http://dx.doi.org/10.1145/3533767.3534411","10.1145/3533767.3534411","Hitherto statistical type inference systems rely thoroughly on supervised learning approaches, which require laborious manual effort to collect and label large amounts of data. Most Turing-complete imperative languages share similar control- and data-flow structures, which make it possible to transfer knowledge learned from one language to another. In this paper, we propose a cross-lingual transfer learning framework, PLATO, for statistical type inference, which allows us to leverage prior knowledge learned from the labeled dataset of one language and transfer it to the others, e.g., Python to JavaScript, Java to JavaScript, etc. PLATO is powered by a novel kernelized attention mechanism to constrain the attention scope of the backbone Transformer model such that model is forced to base its prediction on commonly shared features among languages. In addition, we propose the syntax enhancement that augments the learning on the feature overlap among language domains. Furthermore, PLATO can also be used to improve the performance of the conventional supervised-based type inference by introducing cross-language augmentation, which enables the model to learn more general features across multiple languages. We evaluated PLATO under two settings: 1) under the cross-domain scenario that the target language data is not labeled or labeled partially, the results show that PLATO outperforms the state-of-the-art domain transfer techniques by a large margin, , it improves the Python to TypeScript baseline by +14.6%@EM, +18.6%@weighted-F1, and 2) under the conventional monolingual supervised scenario, PLATO improves the Python baseline by +4.10%@EM, +1.90%@weighted-F1 with the introduction of the cross-lingual augmentation.","Transfer Learning, Type Inference, Deep Learning","","ISSTA 2022"
"Journal Article","Kim J,Jeon J,Hong S,Yoo S","Predictive Mutation Analysis via the Natural Language Channel in Source Code","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3510417;http://dx.doi.org/10.1145/3510417","10.1145/3510417","Mutation analysis can provide valuable insights into both the system under test and its test suite. However, it is not scalable due to the cost of building and testing a large number of mutants. Predictive Mutation Testing (PMT) has been proposed to reduce the cost of mutation testing, but it can only provide statistical inference about whether a mutant will be killed or not by the entire test suite. We propose Seshat, a Predictive Mutation Analysis (PMA) technique that can accurately predict the entire kill matrix, not just the Mutation Score (MS) of the given test suite. Seshat exploits the natural language channel in code, and learns the relationship between the syntactic and semantic concepts of each test case and the mutants it can kill, from a given kill matrix. The learnt model can later be used to predict the kill matrices for subsequent versions of the program, even after both the source and test code have changed significantly. Empirical evaluation using the programs in Defects4J shows that Seshat can predict kill matrices with an average F-score of 0.83 for versions that are up to years apart. This is an improvement in F-score by 0.14 and 0.45 points over the state-of-the-art PMT technique and a simple coverage-based heuristic, respectively. Seshat also performs as well as PMT for the prediction of the MS only. When applied to a mutant-based fault localisation technique, the predicted kill matrix by Seshat is successfully used to locate faults within the top 10 position, showing its usefulness beyond prediction of MS. Once Seshat trains its model using a concrete mutation analysis, the subsequent predictions made by Seshat are on average 39 times faster than actual test-based analysis. We also show that Seshat can be successfully applied to automatically generated test cases with an experiment using EvoSuite.","Mutation analysis, deep learning","",""
"Journal Article","Zhou Y,Zhang X,Shen J,Han T,Chen T,Gall H","Adversarial Robustness of Deep Code Comment Generation","ACM Trans. Softw. Eng. Methodol.","2022","31","4","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3501256;http://dx.doi.org/10.1145/3501256","10.1145/3501256","Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT(Adversarial Code Comment gENeraTor), an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method.","Code comment generation, adversarial attack, deep learning, robustness","",""
"Conference Paper","Limke A,Milliken A,Cateté V,Gransbury I,Isvik A,Price T,Martens C,Barnes T","Case Studies on the Use of Storyboarding by Novice Programmers","","2022","","","318–324","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1","Dublin, Ireland","2022","9781450392013","","https://doi.org/10.1145/3502718.3524749;http://dx.doi.org/10.1145/3502718.3524749","10.1145/3502718.3524749","Our researchers seek to support students in building block-based programming projects that are motivating and engaging as well as valuable practice in learning to code. A difficult part of the programming process is planning. In this research, we explore how novice programmers used a custom-built planning tool, PlanIT, contrasted against how they used storyboarding when planning games. In a three-part study, we engaged novices in planning and programming three games: a maze game, a break-out game, and a mashup of the two. In a set of five case studies, we show how five pairs of students approached the planning and programming of these three games, illustrating that students felt more creative when storyboarding rather than using PlanIT. We end with a discussion on the implications of this work for designing supports for novices to plan open-ended projects.","open-ended projects, storyboards, planning, block-based programs","","ITiCSE '22"
"Conference Paper","Jeffries B,Lee JA,Koprinska I","115 Ways Not to Say Hello, World! Syntax Errors Observed in a Large-Scale Online CS0 Python Course","","2022","","","337–343","Association for Computing Machinery","New York, NY, USA","Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1","Dublin, Ireland","2022","9781450392013","","https://doi.org/10.1145/3502718.3524809;http://dx.doi.org/10.1145/3502718.3524809","10.1145/3502718.3524809","Online programming courses can provide detailed automatic feedback for code that fails to meet various test conditions, but novice students often struggle with syntax errors and are unable to write valid testable code. Even for very simple exercises, the range of incorrect code can be surprising to educators with mastery of a programming language. This research paper presents an analysis of the error messages from code run by students in an introductory Python 3 programming course, participated in by 8680 primary and high-school students from 680 institutions. The invalid programs demonstrate a wide diversity of mistakes: even for a one-line ""Hello World!'' exercise there were 115 unique invalid programs. The most common errors are identified and compared to the topics introduced in the course. The most generic errors in selected exercises are investigated in greater detail to understand the underlying causes. While the majority of students attempting an exercise reach a successful outcome, many students encounter at least one error in their code. Of these, many such errors indicate basic mistakes, such as unquoted string literals, even in exercises late in the course for which some proficiency of earlier concepts is assumed. These observations suggest there is significant scope to provide greater reinforcement of students' understanding of earlier concepts.","syntax errors, cs0","","ITiCSE '22"
"Journal Article","Shen B,Gulzar MA,He F,Meng N","A Characterization Study of Merge Conflicts in Java Projects","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-07","","1049-331X","https://doi.org/10.1145/3546944;http://dx.doi.org/10.1145/3546944","10.1145/3546944","In collaborative software development, programmers create software branches to add features and fix bugs tentatively, and then merge branches to integrate edits. When edits from different branches textually overlap (i.e., textual conflicts) or lead to compilation and runtime errors (i.e., build and test conflicts), it is challenging for developers to remove such conflicts. Prior work proposed tools to detect and solve conflicts. They investigate how conflicts relate to code smells and the software development process. However, many questions are still not fully investigated, such as what types of conflicts exist in real-world applications and how developers or tools handle them. For this paper, we used automated textual merge, compilation, and testing to reveal 3 types of conflicts in 208 open-source repositories: textual conflicts, build conflicts (i.e., conflicts causing build errors), and test conflicts (i.e., conflicts triggering test failures). We manually inspected 538 conflicts and their resolutions to characterize merge conflicts from different angles. Our analysis revealed three interesting phenomena. First, higher-order conflicts (i.e., build and test conflicts) are harder to detect and resolve, while existing tools mainly focus on textual conflicts. Second, developers manually resolved most higher-order conflicts by applying similar edits to multiple program locations; their conflict resolutions share common editing patterns implying great opportunities for future tool design. Third, developers resolved 64% of true textual conflicts by keeping complete edits from either a left or right branch. Unlike prior studies, our research for the first time thoroughly characterizes three types of conflicts, with a special focus on higher-order conflicts and limitations of existing tool design. Our work will shed light on future research of software merge.","conflict resolution, software merge, conflict detection, empirical","Just Accepted",""
"Conference Paper","Wang D,Jia Z,Li S,Yu Y,Xiong Y,Dong W,Liao X","Bridging Pre-Trained Models and Downstream Tasks for Source Code Understanding","","2022","","","287–298","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510062;http://dx.doi.org/10.1145/3510003.3510062","10.1145/3510003.3510062","With the great success of pre-trained models, the pretrain-then-finetune paradigm has been widely adopted on downstream tasks for source code understanding. However, compared to costly training a large-scale model from scratch, how to effectively adapt pre-trained models to a new task has not been fully explored. In this paper, we propose an approach to bridge pre-trained models and code-related tasks. We exploit semantic-preserving transformation to enrich downstream data diversity, and help pre-trained models learn semantic features invariant to these semantically equivalent transformations. Further, we introduce curriculum learning to organize the transformed data in an easy-to-hard manner to fine-tune existing pre-trained models.We apply our approach to a range of pre-trained models, and they significantly outperform the state-of-the-art models on tasks for source code understanding, such as algorithm classification, code clone detection, and code search. Our experiments even show that without heavy pre-training on code data, natural language pre-trained model RoBERTa fine-tuned with our lightweight approach could outperform or rival existing code pre-trained models fine-tuned on the above tasks, such as CodeBERT and GraphCodeBERT. This finding suggests that there is still much room for improvement in code pre-trained models.","test-time augmentation, fine-tuning, curriculum learning, data augmentation","","ICSE '22"
"Conference Paper","Wan Y,Zhao W,Zhang H,Sui Y,Xu G,Jin H","What Do They Capture? A Structural Analysis of Pre-Trained Language Models for Source Code","","2022","","","2377–2388","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510050;http://dx.doi.org/10.1145/3510003.3510050","10.1145/3510003.3510050","Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.","code representation, deep learning, pre-trained language model, attention analysis, syntax tree induction, probing","","ICSE '22"
"Conference Paper","Li Z,Ma P,Wang H,Wang S,Tang Q,Nie S,Wu S","Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings","","2022","","","2253–2265","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510217;http://dx.doi.org/10.1145/3510003.3510217","10.1145/3510003.3510217","Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs.This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality.We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.","","","ICSE '22"
"Conference Paper","Sejfia A,Schäfer M","Practical Automated Detection of Malicious Npm Packages","","2022","","","1681–1692","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510104;http://dx.doi.org/10.1145/3510003.3510104","10.1145/3510003.3510104","The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.","malware detection, supply chain security","","ICSE '22"
"Conference Paper","Yang C,Xu Z,Chen H,Liu Y,Gong X,Liu B","ModX: Binary Level Partially Imported Third-Party Library Detection via Program Modularization and Semantic Matching","","2022","","","1393–1405","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510627;http://dx.doi.org/10.1145/3510003.3510627","10.1145/3510003.3510627","With the rapid growth of software, using third-party libraries (TPLs) has become increasingly popular. The prosperity of the library usage has provided the software engineers with a handful of methods to facilitate and boost the program development. Unfortunately, it also poses great challenges as it becomes much more difficult to manage the large volume of libraries. Researches and studies have been proposed to detect and understand the TPLs in the software. However, most existing approaches rely on syntactic features, which are not robust when these features are changed or deliberately hidden by the adversarial parties. Moreover, these approaches typically model each of the imported libraries as a whole, therefore, cannot be applied to scenarios where the host software only partially uses the library code segments.To detect both fully and partially imported TPLs at the semantic level, we propose ModX, a framework that leverages novel program modularization techniques to decompose the program into fine-grained functionality-based modules. By extracting both syntactic and semantic features, it measures the distance between modules to detect similar library module reuse in the program. Experimental results show that ModX outperforms other modularization tools by distinguishing more coherent program modules with 353% higher module quality scores and beats other TPL detection tools with on average 17% better in precision and 8% better in recall.","third-party library detection, program modularization, semantic matcing","","ICSE '22"
"Conference Paper","Reid D,Jahanshahi M,Mockus A","The Extent of Orphan Vulnerabilities from Code Reuse in Open Source Software","","2022","","","2104–2115","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510216;http://dx.doi.org/10.1145/3510003.3510216","10.1145/3510003.3510216","Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code.Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed.Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities).Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.","code reuse, CVE, security vulnerabilities, git","","ICSE '22"
"Conference Paper","Wu Y,Zou D,Dou S,Yang W,Xu D,Jin H","VulCNN: An Image-Inspired Scalable Vulnerability Detection System","","2022","","","2365–2376","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510229;http://dx.doi.org/10.1145/3510003.3510229","10.1145/3510003.3510229","Since deep learning (DL) can automatically learn features from source code, it has been widely used to detect source code vulnerability. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. To achieve accurate vulnerability detection, other approaches consider distilling the program semantics into graph representations and using them to detect vulnerability. In practice, text-based techniques are scalable but not accurate due to the lack of program semantics. Graph-based methods are accurate but not scalable since graph analysis is typically time-consuming.In this paper, we aim to achieve both scalability and accuracy on scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose a novel idea that can efficiently convert the source code of a function into an image while preserving the program details. We implement VulCNN and evaluate it on a dataset of 13,687 vulnerable functions and 26,970 non-vulnerable functions. Experimental results report that VulCNN can achieve better accuracy than eight state-of-the-art vulnerability detectors (i.e., Checkmarx, FlawFinder, RATS, TokenCNN, VulDeePecker, SySeVR, VulDeeLocator, and Devign). As for scalability, VulCNN is about four times faster than VulDeePecker and SySeVR, about 15 times faster than VulDeeLocator, and about six times faster than Devign. Furthermore, we conduct a case study on more than 25 million lines of code and the result indicates that VulCNN can detect large-scale vulnerability. Through the scanning reports, we finally discover 73 vulnerabilities that are not reported in NVD.","large scale, vulnerability detection, CNN, image","","ICSE '22"
"Conference Paper","Shi E,Wang Y,Du L,Chen J,Han S,Zhang H,Zhang D,Sun H","On the Evaluation of Neural Code Summarization","","2022","","","1597–1608","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510060;http://dx.doi.org/10.1145/3510003.3510060","10.1145/3510003.3510060","Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Besides, we discover and resolve an important and previously unknown bug in BLEU calculation in a commonly-used software package. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code preprocessing choices can have a large (from -18% to +25%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.","code summarization, deep learning, evaluation, empirical study","","ICSE '22"
"Conference Paper","Yang Z,Shi J,He J,Lo D","Natural Attack for Pre-Trained Models of Code","","2022","","","1482–1493","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510146;http://dx.doi.org/10.1145/3510003.3510146","10.1145/3510003.3510146","Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement.In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pre-trained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively.","genetic algorithm, adversarial attack, pre-trained models","","ICSE '22"
"Conference Paper","Cao S,Sun X,Bo L,Wu R,Li B,Tao C","MVD: Memory-Related Vulnerability Detection Based on Flow-Sensitive Graph Neural Networks","","2022","","","1456–1468","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510219;http://dx.doi.org/10.1145/3510003.3510219","10.1145/3510003.3510219","Memory-related vulnerabilities constitute severe threats to the security of modern software. Despite the success of deep learning-based approaches to generic vulnerability detection, they are still limited by the underutilization of flow information when applied for detecting memory-related vulnerabilities, leading to high false positives.In this paper, we propose MVD, a statement-level Memory-related Vulnerability Detection approach based on flow-sensitive graph neural networks (FS-GNN). FS-GNN is employed to jointly embed both unstructured information (i.e., source code) and structured information (i.e., control- and data-flow) to capture implicit memory-related vulnerability patterns. We evaluate MVD on the dataset which contains 4,353 real-world memory-related vulnerabilities, and compare our approach with three state-of-the-art deep learning-based approaches as well as five popular static analysis-based memory detectors. The experiment results show that MVD achieves better detection accuracy, outperforming both state-of-the-art DL-based and static analysis-based approaches. Furthermore, MVD makes a great trade-off between accuracy and efficiency.","vulnerability detection, graph neural networks, memory-related vulnerability, flow analysis","","ICSE '22"
"Conference Paper","Wyss E,De Carli L,Davidson D","What the Fork? Finding Hidden Code Clones in Npm","","2022","","","2415–2426","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510168;http://dx.doi.org/10.1145/3510003.3510168","10.1145/3510003.3510168","This work presents findings and mitigations on an understudied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or reference to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes.Motivated by these considerations, we propose unwrapper, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. unwrapper uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 different versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.","","","ICSE '22"
"Conference Paper","Liu F,Li G,Fu Z,Lu S,Hao Y,Jin Z","Learning to Recommend Method Names with Global Context","","2022","","","1294–1306","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510154;http://dx.doi.org/10.1145/3510003.3510154","10.1145/3510003.3510154","In programming, the names for the program entities, especially for the methods, are the intuitive characteristic for understanding the functionality of the code. To ensure the readability and maintainability of the programs, method names should be named properly. Specifically, the names should be meaningful and consistent with other names used in related contexts in their codebase. In recent years, many automated approaches are proposed to suggest consistent names for methods, among which neural machine translation (NMT) based models are widely used and have achieved state-of-the-art results. However, these NMT-based models mainly focus on extracting the code-specific features from the method body or the surrounding methods, the project-specific context and documentation of the target method are ignored. We conduct a statistical analysis to explore the relationship between the method names and their contexts. Based on the statistical results, we propose GTNM, a Global Transformer-based Neural Model for method name suggestion, which considers the local context, the project-specific context, and the documentation of the method simultaneously. Experimental results on java methods show that our model can outperform the state-of-the-art results by a large margin on method name suggestion, demonstrating the effectiveness of our proposed model.","deep learning, method name recommendation, global context","","ICSE '22"
"Conference Paper","Izadi M,Gismondi R,Gousios G","CodeFill: Multi-Token Code Completion by Jointly Learning from Structure and Naming Sequences","","2022","","","401–412","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510172;http://dx.doi.org/10.1145/3510003.3510172","10.1145/3510003.3510172","Code completion is an essential feature of IDEs, yet current auto-completers are restricted to either grammar-based or NLP-based single token completions. Both approaches have significant drawbacks: grammar-based autocompletion is restricted in dynamically-typed language environments, whereas NLP-based autocompleters struggle to understand the semantics of the programming language and the developer's code context.In this work, we present CodeFill, a language model for autocompletion that combines learned structure and naming information. Using a parallel Transformer architecture and multi-task learning, CodeFill consumes sequences of source code token names and their equivalent AST token types. Uniquely, CodeFill is trained both for single-token and multi-token (statement) prediction, which enables it to learn long-range dependencies among grammatical and naming elements. We train CodeFill on two datasets, consisting of 29M and 425M lines of code, respectively. To make the evaluation more realistic, we develop a method to automatically infer points in the source code at which completion matters. We compare CodeFill against four baselines and two state-of-the-art models, GPT-C and TravTrans+. CodeFill surpasses all baselines in single token prediction (MRR: 70.9% vs. 66.2% and 67.8%) and outperforms the state of the art for multi-token prediction (ROUGE-L: 63.7% vs. 52.4% and 59.2%, for n = 4 tokens). We publicly release our source code and datasets.","multi-task learning, dynamically-typed languages, types, transformers, automatic code completion","","ICSE '22"
"Conference Paper","Li Y,Wang S,Nguyen TN","DEAR: A Novel Deep Learning-Based Approach for Automated Program Repair","","2022","","","511–523","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510177;http://dx.doi.org/10.1145/3510003.3510177","10.1145/3510003.3510177","The existing deep learning (DL)-based automated program repair (APR) models are limited in fixing general software defects. We present DEAR, a DL-based approach that supports fixing for the general bugs that require dependent changes at once to one or multiple consecutive statements in one or multiple hunks of code. We first design a novel fault localization (FL) technique for multi-hunk, multi-statement fixes that combines traditional spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL model, detects the buggy hunks to be fixed at once, and expands a buggy statement s in a hunk to include other suspicious statements around s. We design a two-tier, tree-based LSTM model that incorporates cycle training and uses a divide-and-conquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate DEAR on three datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset, DEAR outperforms the baselines from 42%--683% in terms of the number of auto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes 31--145 more bugs than existing DL-based APR models with the top-1 patches. On CPatMiner dataset, among 667 fixed bugs, there are 169 (25.3%) multi-hunk/multi-statement bugs. DEAR fixes 71 and 164 more bugs, including 52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art, DL-based APR models.","automated program repair, deep learning, fault localization","","ICSE '22"
"Conference Paper","Georgiou S,Kechagia M,Sharma T,Sarro F,Zou Y","Green AI: Do Deep Learning Frameworks Have Different Costs?","","2022","","","1082–1094","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510221;http://dx.doi.org/10.1145/3510003.3510221","10.1145/3510003.3510221","The use of Artificial Intelligence (ai), and more specifically of Deep Learning (dl), in modern software systems, is nowadays widespread and continues to grow. At the same time, its usage is energy demanding and contributes to the increased CO2 emissions, and has a great financial cost as well. Even though there are many studies that examine the capabilities of dl, only a few focus on its green aspects, such as energy consumption.This paper aims at raising awareness of the costs incurred when using different dl frameworks. To this end, we perform a thorough empirical study to measure and compare the energy consumption and run-time performance of six different dl models written in the two most popular dl frameworks, namely PyTorch and TensorFlow. We use a well-known benchmark of dl models, DeepLearningExamples, created by nvidia, to compare both the training and inference costs of dl. Finally, we manually investigate the functions of these frameworks that took most of the time to execute in our experiments.The results of our empirical study reveal that there is a statistically significant difference between the cost incurred by the two dl frameworks in 94% of the cases studied. While TensorFlow achieves significantly better energy and run-time performance than PyTorch, and with large effect sizes in 100% of the cases for the training phase, PyTorch instead exhibits significantly better energy and run-time performance than TensorFlow in the inference phase for 66% of the cases, always, with large effect sizes. Such a large difference in performance costs does not, however, seem to affect the accuracy of the models produced, as both frameworks achieve comparable scores under the same configurations. Our manual analysis, of the documentation and source code of the functions examined, reveals that such a difference in performance costs is under-documented, in these frameworks. This suggests that developers need to improve the documentation of their dl frameworks, the source code of the functions used in these frameworks, as well as to enhance existing dl algorithms.","deep learning, energy consumption, APIs, run-time performance","","ICSE '22"
"Conference Paper","Meng X,Wang X,Zhang H,Sun H,Liu X","Improving Fault Localization and Program Repair with Deep Semantic Features and Transferred Knowledge","","2022","","","1169–1180","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510147;http://dx.doi.org/10.1145/3510003.3510147","10.1145/3510003.3510147","Automatic software debugging mainly includes two tasks of fault localization and automated program repair. Compared with the traditional spectrum-based and mutation-based methods, deep learning-based methods are proposed to achieve better performance for fault localization. However, the existing methods ignore the deep semantic features or only consider simple code representations. They do not leverage the existing bug-related knowledge from large-scale open-source projects either. In addition, existing template-based program repair techniques can incorporate project specific information better than deep-learning approaches. However, they are weak in selecting the fix templates for efficient program repair. In this work, we propose a novel approach called TRANSFER, which leverages the deep semantic features and transferred knowledge from open-source data to improve fault localization and program repair. First, we build two large-scale open-source bug datasets and design 11 BiLSTM-based binary classifiers and a BiLSTM-based multi-classifier to learn deep semantic features of statements for fault localization and program repair, respectively. Second, we combine semantic-based, spectrum-based and mutation-based features and use an MLP-based model for fault localization. Third, the semantic-based features are leveraged to rank the fix templates for program repair. Our extensive experiments on widely-used benchmark De-fects4J show that TRANSFER outperforms all baselines in fault localization, and is better than existing deep-learning methods in automated program repair. Compared with the typical template-based work TBar, TRANSFER can correctly repair 6 more bugs (47 in total) on Defects4J.","software debugging, program repair, transfer learning, fault localization, neural networks","","ICSE '22"
"Conference Paper","Yu H,Lou Y,Sun K,Ran D,Xie T,Hao D,Li Y,Li G,Wang Q","Automated Assertion Generation via Information Retrieval and Its Integration with Deep Learning","","2022","","","163–174","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510149;http://dx.doi.org/10.1145/3510003.3510149","10.1145/3510003.3510149","Unit testing could be used to validate the correctness of basic units of the software system under test. To reduce manual efforts in conducting unit testing, the research community has contributed with tools that automatically generate unit test cases, including test inputs and test oracles (e.g., assertions). Recently, ATLAS, a deep learning (DL) based approach, was proposed to generate assertions for a unit test based on other already written unit tests. Despite promising, the effectiveness of ATLAS is still limited. To improve the effectiveness, in this work, we make the first attempt to leverage Information Retrieval (IR) in assertion generation and propose an IR-based approach, including the technique of IR-based assertion retrieval and the technique of retrieved-assertion adaptation. In addition, we propose an integration approach to combine our IR-based approach with a DL-based approach (e.g., ATLAS) to further improve the effectiveness. Our experimental results show that our IR-based approach outperforms the state-of-the-art DL-based approach, and integrating our IR-based approach with the DL-based approach can further achieve higher accuracy. Our results convey an important message that information retrieval could be competitive and worthwhile to pursue for software engineering tasks such as assertion generation, and should be seriously considered by the research community given that in recent years deep learning solutions have been over-popularly adopted by the research community for software engineering tasks.","information retrieval, unit testing, deep learning, test assertion","","ICSE '22"
"Conference Paper","Tian Y,Zhang Y,Stol KJ,Jiang L,Liu H","What Makes a Good Commit Message?","","2022","","","2389–2401","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510205;http://dx.doi.org/10.1145/3510003.3510205","10.1145/3510003.3510205","A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an ""audit trail"" by which developers can understand how the source code of a project has changed---and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a ""good"" commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether ""good"" commit messages can be automatically identified; such automation could prompt developers to write better commit messages.","commit-based software development, commit message quality, open collaboration","","ICSE '22"
"Conference Paper","Kang HJ,Aw KL,Lo D","Detecting False Alarms from Automatic Static Analysis Tools: How Far Are We?","","2022","","","698–709","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510214;http://dx.doi.org/10.1145/3510003.3510214","10.1145/3510003.3510214","Automatic static analysis tools (ASATs), such as Findbugs, have a high false alarm rate. The large number of false alarms produced poses a barrier to adoption. Researchers have proposed the use of machine learning to prune false alarms and present only actionable warnings to developers. The state-of-the-art study has identified a set of ""Golden Features"" based on metrics computed over the characteristics and history of the file, code, and warning. Recent studies show that machine learning using these features is extremely effective and that they achieve almost perfect performance.We perform a detailed analysis to better understand the strong performance of the ""Golden Features"". We found that several studies used an experimental procedure that results in data leakage and data duplication, which are subtle issues with significant implications. Firstly, the ground-truth labels have leaked into features that measure the proportion of actionable warnings in a given context. Secondly, many warnings in the testing dataset appear in the training dataset. Next, we demonstrate limitations in the warning oracle that determines the ground-truth labels, a heuristic comparing warnings in a given revision to a reference revision in the future. We show the choice of reference revision influences the warning distribution. Moreover, the heuristic produces labels that do not agree with human oracles. Hence, the strong performance of these techniques previously seen is overoptimistic of their true performance if adopted in practice. Our results convey several lessons and provide guidelines for evaluating false alarm detectors.","data leakage, false alarms, data duplication, static analysis","","ICSE '22"
"Conference Paper","Hu X,Xia X,Lo D,Wan Z,Chen Q,Zimmermann T","Practitioners' Expectations on Automated Code Comment Generation","","2022","","","1693–1705","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510152;http://dx.doi.org/10.1145/3510003.3510152","10.1145/3510003.3510152","Good comments are invaluable assets to software projects, as they help developers understand and maintain projects. However, due to some poor commenting practices, comments are often missing or inconsistent with the source code. Software engineering practitioners often spend a significant amount of time and effort reading and understanding programs without or with poor comments. To counter this, researchers have proposed various techniques to automatically generate code comments in recent years, which can not only save developers time writing comments but also help them better understand existing software projects. However, it is unclear whether these techniques can alleviate comment issues and whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by interviewing and surveying practitioners about their expectations of research in code comment generation. We then compared what practitioners need and the current state-of-the-art research by performing a literature review of papers on code comment generation techniques published in the premier publication venues from 2010 to 2020. From this comparison, we highlighted the directions where researchers need to put effort to develop comment generation techniques that matter to practitioners.","practitioners' expectations, empirical study, code comment generation","","ICSE '22"
"Conference Paper","Rong G,Zhang Y,Yang L,Zhang F,Kuang H,Zhang H","Modeling Review History for Reviewer Recommendation: A Hypergraph Approach","","2022","","","1381–1392","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510213;http://dx.doi.org/10.1145/3510003.3510213","10.1145/3510003.3510213","Modern code review is a critical and indispensable practice in a pull-request development paradigm that prevails in Open Source Software (OSS) development. Finding a suitable reviewer in projects with massive participants thus becomes an increasingly challenging task. Many reviewer recommendation approaches (recommenders) have been developed to support this task which apply a similar strategy, i.e. modeling the review history first then followed by predicting/recommending a reviewer based on the model. Apparently, the better the model reflects the reality in review history, the higher recommender's performance we may expect. However, one typical scenario in a pull-request development paradigm, i.e. one Pull-Request (PR) (such as a revision or addition submitted by a contributor) may have multiple reviewers and they may impact each other through publicly posted comments, has not been modeled well in existing recommenders. We adopted the hypergraph technique to model this high-order relationship (i.e. one PR with multiple reviewers herein) and developed a new recommender, namely HGRec, which is evaluated by 12 OSS projects with more than 87K PRs, 680K comments in terms of accuracy and recommendation distribution. The results indicate that HGRec outperforms the state-of-the-art recommenders on recommendation accuracy. Besides, among the top three accurate recommenders, HGRec is more likely to recommend a diversity of reviewers, which can help to relieve the core reviewers' workload congestion issue. Moreover, since HGRec is based on hypergraph, which is a natural and interpretable representation to model review history, it is easy to accommodate more types of entities and realistic relationships in modern code review scenarios. As the first attempt, this study reveals the potentials of hypergraph on advancing the pragmatic solutions for code reviewer recommendation.","hypergraph, modern code review, reviewer recommendation","","ICSE '22"
"Conference Paper","Sun W,Fang C,Chen Y,Tao G,Han T,Zhang Q","Code Search Based on Context-Aware Code Translation","","2022","","","388–400","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510140;http://dx.doi.org/10.1145/3510003.3510140","10.1145/3510003.3510140","Code search is a widely used technique by developers during software development. It provides semantically similar implementations from a large code corpus to developers based on their queries. Existing techniques leverage deep learning models to construct embedding representations for code snippets and queries, respectively. Features such as abstract syntactic trees, control flow graphs, etc., are commonly employed for representing the semantics of code snippets. However, the same structure of these features does not necessarily denote the same semantics of code snippets, and vice versa. In addition, these techniques utilize multiple different word mapping functions that map query words/code tokens to embedding representations. This causes diverged embeddings of the same word/token in queries and code snippets. We propose a novel context-aware code translation technique that translates code snippets into natural language descriptions (called translations). The code translation is conducted on machine instructions, where the context information is collected by simulating the execution of instructions. We further design a shared word mapping function using one single vocabulary for generating embeddings for both translations and queries. We evaluate the effectiveness of our technique, called TranCS, on the CodeSearchNet corpus with 1,000 queries. Experimental results show that TranCS significantly outperforms state-of-the-art techniques by 49.31% to 66.50% in terms of MRR (mean reciprocal rank).","code translation, deep learning, code search","","ICSE '22"
"Conference Paper","Kharkar A,Moghaddam RZ,Jin M,Liu X,Shi X,Clement C,Sundaresan N","Learning to Reduce False Positives in Analytic Bug Detectors","","2022","","","1307–1316","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510153;http://dx.doi.org/10.1145/3510003.3510153","10.1145/3510003.3510153","Due to increasingly complex software design and rapid iterative development, code defects and security vulnerabilities are prevalent in modern software. In response, programmers rely on static analysis tools to regularly scan their codebases and find potential bugs. In order to maximize coverage, however, these tools generally tend to report a significant number of false positives, requiring developers to manually verify each warning. To address this problem, we propose a Transformer-based learning approach to identify false positive bug warnings. We demonstrate that our models can improve the precision of static analysis by 17.5%. In addition, we validated the generalizability of this approach across two major bug types: null dereference and resource leak.","text tagging, datasets, gaze detection, neural networks","","ICSE '22"
"Conference Paper","Tufano R,Masiero S,Mastropaolo A,Pascarella L,Poshyvanyk D,Bavota G","Using Pre-Trained Models to Boost Code Review Automation","","2022","","","2291–2302","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510621;http://dx.doi.org/10.1145/3510003.3510621","10.1145/3510003.3510621","Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.","code review, empirical study, machine learning on code","","ICSE '22"
"Conference Paper","Mastropaolo A,Pascarella L,Bavota G","Using Deep Learning to Generate Complete Log Statements","","2022","","","2279–2290","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3511561;http://dx.doi.org/10.1145/3510003.3511561","10.1145/3510003.3511561","Logging is a practice widely adopted in several phases of the software lifecycle. For example, during software development log statements allow engineers to verify and debug the system by exposing fine-grained information of the running software. While the benefits of logging are undisputed, taking proper decisions about where to inject log statements, what information to log, and at which log level (e.g., error, warning) is crucial for the logging effectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the first approach supporting developers in all these decisions. LANCE features a Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456 Java methods. LANCE takes as input a Java method and injects in it a full log statement, including a human-comprehensible logging message and properly choosing the needed log level and the statement location. Our results show that LANCE is able to (i) properly identify the location in the code where to inject the statement in 65.9% of Java methods requiring it; (ii) selecting the proper log level in 66.2% of cases; and (iii) generate a completely correct log statement including a meaningful logging message in 15.2% of cases.","machine learning on code, empirical study, logging","","ICSE '22"
"Conference Paper","Zhang C,Chen B,Peng X,Zhao W","BuildSheriff: Change-Aware Test Failure Triage for Continuous Integration Builds","","2022","","","312–324","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510132;http://dx.doi.org/10.1145/3510003.3510132","10.1145/3510003.3510132","Test failures are one of the most common reasons for broken builds in continuous integration. It is expensive to diagnose all test failures in a build. As test failures are usually caused by a few underlying faults, triaging test failures with respect to their underlying root causes can save test failure diagnosis cost. Existing failure triage methods are mostly developed for triaging crash or bug reports, and hence not applicable in the context of test failure triage in continuous integration. In this paper, we first present a large-scale empirical study on 163,371 broken builds caused by test failures to characterize test failures in real-world Java projects. Then, motivated by our study, we propose a new change-aware approach, BuildSheriff, to triage test failures in each continuous integration build such that test failures with the same root cause are put in the same cluster. Our evaluation on 200 broken builds has demonstrated that BuildSheriff can significantly improve the state-of-the-art methods on the triaging effectiveness.","test failures, failure triage, continuous integration","","ICSE '22"
"Conference Paper","Dong J,Lou Y,Zhu Q,Sun Z,Li Z,Zhang W,Hao D","FIRA: FiNe-Grained GRaPh-Based Code Change Representation for Automated Commit Message Generation","","2022","","","970–981","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510069;http://dx.doi.org/10.1145/3510003.3510069","10.1145/3510003.3510069","Commit messages summarize code changes of each commit in natural language, which help developers understand code changes without digging into detailed implementations and play an essential role in comprehending software evolution. To alleviate human efforts in writing commit messages, researchers have proposed various automated techniques to generate commit messages, including template-based, information retrieval-based, and learning-based techniques. Although promising, previous techniques have limited effectiveness due to their coarse-grained code change representations.This work proposes a novel commit message generation technique, FIRA, which first represents code changes via fine-grained graphs and then learns to generate commit messages automatically. Different from previous techniques, FIRA represents the code changes with fine-grained graphs, which explicitly describe the code edit operations between the old version and the new version, and code tokens at different granularities (i.e., sub-tokens and integral tokens). Based on the graph-based representation, FIRA generates commit messages by a generation model, which includes a graph-neural-network-based encoder and a transformer-based decoder. To make both sub-tokens and integral tokens as available ingredients for commit message generation, the decoder is further incorporated with a novel dual copy mechanism. We further perform an extensive study to evaluate the effectiveness of FIRA. Our quantitative results show that FIRA outperforms state-of-the-art techniques in terms of BLEU, ROUGE-L, and METEOR; and our ablation analysis further shows that major components in our technique both positively contribute to the effectiveness of FIRA. In addition, we further perform a human study to evaluate the quality of generated commit messages from the perspective of developers, and the results consistently show the effectiveness of FIRA over the compared techniques.","commit message generation, code change representation, graph neural network","","ICSE '22"
"Conference Paper","Zhu S,Zhang Z,Qin B,Xiong A,Song L","Learning and Programming Challenges of Rust: A Mixed-Methods Study","","2022","","","1269–1281","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510164;http://dx.doi.org/10.1145/3510003.3510164","10.1145/3510003.3510164","Rust is a young systems programming language designed to provide both the safety guarantees of high-level languages and the execution performance of low-level languages. To achieve this design goal, Rust provides a suite of safety rules and checks against those rules at the compile time to eliminate many memory-safety and thread-safety issues. Due to its safety and performance, Rust's popularity has increased significantly in recent years, and it has already been adopted to build many safety-critical software systems.It is critical to understand the learning and programming challenges imposed by Rust's safety rules. For this purpose, we first conducted an empirical study through close, manual inspection of 100 Rust-related Stack Overflow questions. We sought to understand (1) what safety rules are challenging to learn and program with, (2) under which contexts a safety rule becomes more difficult to apply, and (3) whether the Rust compiler is sufficiently helpful in debugging safety-rule violations. We then performed an online survey with 101 Rust programmers to validate the findings of the empirical study. We invited participants to evaluate program variants that differ from each other, either in terms of violated safety rules or the code constructs involved in the violation, and compared the participants' performance on the variants. Our mixed-methods investigation revealed a range of consistent findings that can benefit Rust learners, practitioners, and language designers.","online survey, rust, programming challenges, empirical study","","ICSE '22"
"Conference Paper","Cordy M,Rwemalika R,Franci A,Papadakis M,Harman M","FlakiMe: Laboratory-Controlled Test Flakiness Impact Assessment","","2022","","","982–994","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510194;http://dx.doi.org/10.1145/3510003.3510194","10.1145/3510003.3510194","Much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects. However, this assumption is not always true because some test failures are due to so-called flaky tests, i.e., tests with non-deterministic outcomes. To help testing researchers better investigate flakiness, we introduce a test flakiness assessment and experimentation platform, called FlakiMe. FlakiMe supports the seeding of a (controllable) degree of flakiness into the behaviour of a given test suite. Thereby, FlakiMe equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory-controlled conditions. To demonstrate the application of FlakiMe, we use it to assess the impact of flakiness on mutation testing and program repair (the PRAPR and ARJA methods). These results indicate that a 10% flakiness is sufficient to affect the mutation score, but the effect size is modest (2% - 5% ), while it reduces the number of patches produced for repair by 20% up to 100% of repair problems; a devastating impact on this application of testing. Our experiments with FlakiMe demonstrate that flakiness affects different testing applications in very different ways, thereby motivating the need for a laboratory-controllable flakiness impact assessment platform and approach such as FlakiMe.","","","ICSE '22"
"Conference Paper","Dilhara M,Ketkar A,Sannidhi N,Dig D","Discovering Repetitive Code Changes in Python ML Systems","","2022","","","736–748","Association for Computing Machinery","New York, NY, USA","Proceedings of the 44th International Conference on Software Engineering","Pittsburgh, Pennsylvania","2022","9781450392211","","https://doi.org/10.1145/3510003.3510225;http://dx.doi.org/10.1145/3510003.3510225","10.1145/3510003.3510225","Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use best coding practices.To fill the knowledge gap and advance the science and tooling in ML software evolution, we conducted the first and most fine-grained study on code change patterns in a diverse corpus of 1000 top-rated ML systems comprising 58 million SLOC. To conduct this study we reuse, adapt, and improve upon the state-of-the-art repetitive change mining techniques. Our novel tool, R-CPatMiner, mines over 4M commits and constructs 350K fine-grained change graphs and detects 28K change patterns. Using thematic analysis, we identified 22 pattern groups and we reveal 4 major trends of how ML developers change their code. We surveyed 650 ML developers to further shed light on these patterns and their applications, and we received a 15% response rate. We present actionable, empirically-justified implications for four audiences: (i) researchers, (ii) tool builders, (iii) ML library vendors, and (iv) developers and educators.","code changes, machine learning, refactoring, python, repetition","","ICSE '22"
"Journal Article","Li X,Zhang Y,Leung J,Sun C,Zhao J","EDAssistant: Supporting Exploratory Data Analysis in Computational Notebooks with In-Situ Code Search and Recommendation","ACM Trans. Interact. Intell. Syst.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-06","","2160-6455","https://doi.org/10.1145/3545995;http://dx.doi.org/10.1145/3545995","10.1145/3545995","Using computational notebooks (e.g., Jupyter Notebook), data scientists rationalize their exploratory data analysis (EDA) based on their prior experience and external knowledge such as online examples. For novices or data scientists who lack specific knowledge about the dataset or problem to investigate, effectively obtaining and understanding the external information is critical to carrying out EDA. This paper presents EDAssistant, a JupyterLab extension that supports EDA with in-situ search of example notebooks and recommendation of useful APIs, powered by novel interactive visualization of search results. The code search and recommendation are enabled by advanced machine learning models, trained on a large corpus of EDA notebooks collected online. A user study is conducted to investigate both EDAssistant and data scientists’ current practice (i.e., using external search engines). The results demonstrate the effectiveness and usefulness of EDAssistant, and participants appreciated its smooth and in-context support of EDA. We also report several design implications regarding code recommendation tools.","code search, Exploratory data analysis, computational notebooks., software visualization","Just Accepted",""
"Conference Paper","Liu C,Lin Z,Lou JG,Wen L,Zhang D","Can Neural Clone Detection Generalize to Unseen Functionalities?","","2022","","","617–629","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678907;http://dx.doi.org/10.1109/ASE51524.2021.9678907","10.1109/ASE51524.2021.9678907","Many recently proposed code clone detectors exploit neural networks to capture latent semantics of source code, thus achieving impressive results for detecting semantic clones. These neural clone detectors rely on the availability of large amounts of labeled training data. We identify a key oversight in the current evaluation methodology for neural clone detection: cross-functionality generalization (i.e., detecting semantic clones of which the functionalities are unseen in training). Specifically, we focus on this question: do neural clone detectors truly learn the ability to detect semantic clones, or they just learn how to model specific functionalities in training data while cannot generalize to realistic unseen functionalities? This paper investigates how the generalizability can be evaluated and improved.Our contributions are 3-folds: (1) We propose an evaluation methodology that can systematically measure the cross-functionality generalizability of neural clone detection. Based on this evaluation methodology, an empirical study is conducted and the results indicate that current neural clone detectors cannot generalize well as expected. (2) We conduct empirical analysis to understand key factors that can impact the generalizability. We investigate 3 factors: training data diversity, vocabulary, and locality. Results show that the performance loss on unseen functionalities can be reduced through addressing the out-of-vocabulary problem and increasing training data diversity. (3) We propose a human-in-the-loop mechanism that help adapt neural clone detectors to new code repositories containing lots of unseen functionalities. It improves annotation efficiency with the combination of transfer learning and active learning. Experimental results show that it reduces the amount of annotations by about 88%. Our code and data are publicly available1.","generalization, evaluation methodology, code clone detection, neural network, human-in-the-loop","","ASE '21"
"Conference Paper","Li Z,Zhong H","Understanding Code Fragments with Issue Reports","","2022","","","1312–1316","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678864;http://dx.doi.org/10.1109/ASE51524.2021.9678864","10.1109/ASE51524.2021.9678864","Code comments are vital for software development and maintenance. To supplement the code comments, researchers proposed various approach that generate code comments. The prior approaches take three sources: (1) programming experience, (2) code-comment pairs in source files, and (3) comments of similar code snippets. Most of their generated comments explain code functionalities, but programmers also need comments that explain why a code fragment was developed as it is. To meet the timely needs, in this paper, we introduce a new source, issue reports (e.g. maintenance types, symptoms, and purposes of modifications), to generate code comments. Issue reports contain rich information on how code was maintained. The valuable details of issue reports are useful to understand source code, especially when programmers learn why code was developed in a specific way. Towards this research direction, we propose the first approach, called IssueComm, that builds the links between code fragments and issue reports. Our results show that it links more than 70% issue numbers that are written by programmers in code comments. Furthermore, the links built by our tool covers 4× bugs, and 10× other issues than the links written in manual comments. We present samples of our built links, and explain why our links are useful to describe the functionalities and the purpose of code.","","","ASE '21"
"Conference Paper","Liu Z","Binary Code Similarity Detection","","2022","","","1056–1060","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678518;http://dx.doi.org/10.1109/ASE51524.2021.9678518","10.1109/ASE51524.2021.9678518","Binary code similarity detection is to detect the similarity of code at binary (assembly) level without source code. Existing work still has their limitation when dealing with mutated binary code with different compiling options. We proposed a novel approach to address this problem. By inspecting the binary code, we found that generally, within a function, some instructions aim to calculate (prepare) value for some other instructions. The latter instructions are defined by us the key instructions. Currently, we define four categories of key instructions: calling subfunctions, comparing instruction, returning instruction, and memory address writing instruction. Thus if we symbolically execute similar binary codes, the symbolic value at these key instructions should be similar. We implemented our idea into a tool. This prototype tool can: 1. symbolically execute binary code, 2. extract symbolic values at key instructions into a graph, and 3. compare the symbolic graph similarity. In our implementation, we also address some problems, including path explosion and loop handling.","binary code, symbolic execution, code analysis","","ASE '21"
"Conference Paper","Vagavolu D,Swarna KC,Chimalakonda S","A Mocktail of Source Code Representations","","2022","","","1296–1300","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678551;http://dx.doi.org/10.1109/ASE51524.2021.9678551","10.1109/ASE51524.2021.9678551","Efficient representation of source code is essential for various software engineering tasks such as code classification and code clone detection. Most recent approaches for representing source code still use AST and do not leverage semantic graphs such as CFG and PDG. One effective technique for representing source code involves extracting paths from the AST and using a learning model to capture program properties. Code2vec is one such path-based approach that uses an attention-based neural network to learn code embeddings which can then be used for various downstream tasks. However, this approach uses only AST and does not leverage CFG and PDG. Even though an integrated graph approach (Code Property Graph) exists for representing source code, it has only been explored in the domain of software security. Moreover, it does not leverage the paths from the individual graphs. Our idea is to extend the path-based approach code2vec to include the semantic graphs CFG and PDG with AST, which is largely unexplored in software engineering. We evaluate our approach on the task of MethodNaming using a C dataset of 730K methods collected from GitHub. In comparison to code2vec, our approach improves the F1 score by 11% on the full dataset and up to 100% with individual projects. We show that semantic features from the CFG and PDG paths drastically improve the performance of the software engineering tasks. We envision that looking at a mocktail of source code representations for various software engineering tasks can lay the foundation for a new line of research and a re-haul of existing research.","","","ASE '21"
"Conference Paper","Chakraborty S,Ray B","On Multi-Modal Learning of Editing Source Code","","2022","","","443–455","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678559;http://dx.doi.org/10.1109/ASE51524.2021.9678559","10.1109/ASE51524.2021.9678559","In recent years, Neural Machine Translator (NMT) has shown promise in automatically editing source code. Typical NMT based code editor only considers the code that needs to be changed as input and suggests developers with a ranked list of patched code to choose from - where the correct one may not always be at the top of the list. While NMT based code editing systems generate a broad spectrum of plausible patches, the correct one depends on the developers' requirement and often on the context where the patch is applied. Thus, if developers provide some hints, using natural language, or providing patch context, NMT models can benefit from them.As a proof of concept, in this research, we leverage three modalities of information: edit location, edit code context, commit messages (as a proxy of developers' hint in natural language) to automatically generate edits with NMT models. To that end, we build Modit, a multi-modal NMT based code editing engine. With in-depth investigation and analysis, we show that developers' hint as an input modality can narrow the search space for patches and outperform state-of-the-art models to generate correctly patched code in top-1 position.","automated programming, pretraining, neural networks, neural machine translator, transformers, source code edit","","ASE '21"
"Conference Paper","Li J,Li Y,Li G,Hu X,Xia X,Jin Z","EditSum: A Retrieve-and-Edit Framework for Source Code Summarization","","2022","","","155–166","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678724;http://dx.doi.org/10.1109/ASE51524.2021.9678724","10.1109/ASE51524.2021.9678724","Existing studies show that code summaries help developers understand and maintain source code. Unfortunately, these summaries are often missing or outdated in software projects. Code summarization aims to generate natural language descriptions automatically for source code. According to Gros et al., code summaries are highly structured and have repetitive patterns (e.g. ""return true if...""). Besides the patternized words, a code summary also contains important keywords, which are the key to reflecting the functionality of the code. However, the state-of-the-art approaches perform poorly on predicting the keywords, which leads to the generated summaries suffer a loss in informativeness. To alleviate this problem, this paper proposes a novel retrieve-and-edit approach named EDITSUM for code summarization. Specifically, EditSum first retrieves a similar code snippet from a pre-defined corpus and treats its summary as a prototype summary to learn the pattern. Then, EditSum edits the prototype automatically to combine the pattern in the prototype with the semantic information of input code. Our motivation is that the retrieved prototype provides a good start-point for post-generation because the summaries of similar code snippets often have the same pattern. The post-editing process further reuses the patternized words in prototype and generates keywords based on the semantic information of input code. We conduct experiments on a large-scale Java corpus (2M) and experimental results demonstrate that EditSum outperforms the state-of-the-art approaches by a substantial margin. The human evaluation also proves the summaries generated by EditSum are more informative and useful. We also verify that EditSum performs well on predicting the patternized words and keywords.","deep learning, information retrieval, code summarization","","ASE '21"
"Conference Paper","Hu X,Gao Z,Xia X,Lo D,Yang X","Automating User Notice Generation for Smart Contract Functions","","2022","","","1043–1047","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678552;http://dx.doi.org/10.1109/ASE51524.2021.9678552","10.1109/ASE51524.2021.9678552","Smart contracts have obtained much attention and are crucial for automatic financial and business transactions. For end-users who have never seen the source code, they can read the user notice shown in end-user client to understand what a transaction does of a smart contract function. However, due to time constraints or lack of motivation, user notice is often missing during the development of smart contracts. For end-users who lack the information of the user notices, there is no easy way for them to check the code semantics of the smart contracts. Thus, in this paper, we propose a new approach SmartDoc to generate user notice for smart contract functions automatically. Our tool can help end-users better understand the smart contract and aware of the financial risks, improving the users' confidence on the reliability of the smart contracts. SmartDoc exploits the Transformer to learn the representation of source code and generates natural language descriptions from the learned representation. We also integrate the Pointer mechanism to copy words from the input source code instead of generating words during the prediction process. We extract 7,878 function, notice) pairs from 54,739 smart contracts written in Solidity. Due to the limited amount of collected smart contract functions (i.e., 7,878 functions), we exploit a transfer learning technique to utilize the learned knowledge to improve the performance of SmartDoc. The learned knowledge obtained by the pre-training on a corpus of Java code, that has similar characteristics as Solidity code. The experimental results show that our approach can effectively generate user notice given the source code and significantly outperform the state-of-the-art approaches. To investigate human perspectives on our generated user notice, we also conduct a human evaluation and ask participants to score user notice generated by different approaches. Results show that SmartDoc outperforms baselines from three aspects, naturalness, informativeness, and similarity., booktitle = Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering, pages = 5–17, numpages = 13, keywords = user notice generation, smart contract, deep learning, location = Melbourne, Australia, series = ASE '21","natural language processing, software automation, logging prediction, source code, software systems, logging statement, NLP, deep learning","","ASE '21"
"Conference Paper","Chen B,Abedjan Z","Interactive Cross-Language Code Retrieval with Auto-Encoders","","2022","","","167–178","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678929;http://dx.doi.org/10.1109/ASE51524.2021.9678929","10.1109/ASE51524.2021.9678929","Cross-language code retrieval is necessary in many real-world scenarios. A major application is program translation, e.g., porting codebases from an obsolete or deprecated language to a modern one or re-implementing existing projects in one's preferred programming language. Existing approaches based on the translation model require large amounts of training data and extra information or neglects significant characteristics of programs. Leveraging cross-language code retrieval to assist automatic program translation can make use of Big Code. However, existing code retrieval systems have the barrier to finding the translation with only the features of the input program as the query. In this paper, we present BigPT for interactive cross-language retrieval from Big Code only based on raw code and reusing the retrieved code to assist program translation. We build on existing work on cross-language code representation and propose a novel predictive transformation model based on auto-encoders. The model is trained on Big Code to generate a target-language representation, which will be used as the query to retrieve the most relevant translations for a given program. Our query representation enables the user to easily update and correct the returned results to improve the retrieval process. Our experiments show that BigPT outperforms state-of-the-art baselines in terms of program accuracy. Using our novel querying and retrieving mechanism, BigPT can be scaled to the large dataset and efficiently retrieve the translation.","","","ASE '21"
"Conference Paper","Li Z","Cross-Lingual Transfer Learning Framework for Program Analysis","","2022","","","1074–1078","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678848;http://dx.doi.org/10.1109/ASE51524.2021.9678848","10.1109/ASE51524.2021.9678848","Deep learning-based techniques have been widely applied to program analysis tasks, in fields such as type inference, fault localization, and code summarization. Hitherto deep learning-based software engineering systems rely thoroughly on supervised learning approaches, which require laborious manual effort to collect and label a prohibitively large amount of data. However, most Turing-complete imperative languages share similar control- and data-flow structures, which make it possible to transfer knowledge learned from one language to another. In this paper, we propose a general cross-lingual transfer learning framework PLATO for program analysis by using a series of techniques that are general to different downstream tasks. PLATO allows Bert-based models to leverage prior knowledge learned from the labeled dataset of one language and transfer it to the others. We evaluate our approaches on several downstream tasks such as type inference and code summarization to demonstrate its feasibility.","deep learning, domain adaptation, graph kernel, program analysis, transfer learning","","ASE '21"
"Conference Paper","Terragni V,Salza P","APIzation: Generating Reusable APIs from StackOverflow Code Snippets","","2022","","","542–554","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678576;http://dx.doi.org/10.1109/ASE51524.2021.9678576","10.1109/ASE51524.2021.9678576","Developer forums like StackOverflow have become essential resources to modern software development practices. However, many code snippets lack a well-defined method declaration, and thus they are often incomplete for immediate reuse. Developers must adapt the retrieved code snippets by parameterizing the variables involved and identifying the return value. This activity, which we call APIzation of a code snippet, can be tedious and time-consuming. In this paper, we present APIzator to perform APIzations of Java code snippets automatically. APIzator is grounded by four common patterns that we extracted by studying real APIzations in GitHub. APIzator presents a static analysis algorithm that automatically extracts the method parameters and return statements. We evaluated APIzator with a ground-truth of 200 APIzations collected from 20 developers. For 113 (56.50 %) and 115 (57.50 %) APIzations, APIzator and the developers extracted identical parameters and return statements, respectively. For 163 (81.50 %) APIzations, either the parameters or the return statements were identical.","program synthesis, APIs, StackOverflow, software reuse, code snippets, GitHub, program analysis","","ASE '21"
"Conference Paper","Kimura Y,Akazaki T,Kikuchi S,Mahajan S,Prasad MR","Q&A MAESTRO: Q&A Post Recommendation for Fixing Java Runtime Exceptions","","2022","","","1227–1231","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678893;http://dx.doi.org/10.1109/ASE51524.2021.9678893","10.1109/ASE51524.2021.9678893","Programmers often use Q&A sites (e.g., Stack Overflow) to understand a root cause of program bugs. Runtime exceptions is one of such important class of bugs that is actively discussed on Stack Overflow. However, it may be difficult for beginner programmers to come up with appropriate keywords for search. Moreover, they need to switch their attentions between IDE and browser, and it is time-consuming. To overcome these difficulties, we proposed a method, ""Q&A MAESTRO"", to find suitable Q&A posts automatically for Java runtime exception by utilizing structure information of codes described in programming Q&A website. In this paper, we describe a usage scenario of IDE-plugin, the architecture and user interface of the implementation, and results of user studies. A video is available at https://youtu.be/4X24jJrMUVw. A demo software is available at https://github.com/FujitsuLaboratories/Q-A-MAESTRO.","","","ASE '21"
"Conference Paper","Liu L,Wei L,Zhang W,Wen M,Liu Y,Cheung SC","Characterizing Transaction-Reverting Statements in Ethereum Smart Contracts","","2022","","","630–641","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678597;http://dx.doi.org/10.1109/ASE51524.2021.9678597","10.1109/ASE51524.2021.9678597","Smart contracts are programs stored on blockchains to execute transactions. When input constraints or security properties are violated at runtime, the transaction being executed by a smart contract needs to be reverted to avoid undesirable consequences. On Ethereum, the most popular blockchain that supports smart contracts, developers can choose among three transaction-reverting statements (i.e., require, if...revert, and if...throw) to handle anomalous transactions. While these transaction-reverting statements are vital for preventing smart contracts from exhibiting abnormal behaviors or suffering malicious attacks, there is limited understanding of how they are used in practice. In this work, we perform the first empirical study to characterize transaction-reverting statements in Ethereum smart contracts. We measured the prevalence of these statements in 3,866 verified smart contracts from popular dapps and built a taxonomy of their purposes via manually analyzing 557 transaction-reverting statements. We also compared template contracts and their corresponding custom contracts to understand how developers customize the use of transaction-reverting statements. Finally, we analyzed the security impact of transaction-reverting statements by removing them from smart contracts and comparing the mutated contracts against the original ones. Our study led to important findings. For example, we found that transaction-reverting statements are commonly used to perform seven types of authority verifications or validity checks, and missing such statements may compromise the security of smart contracts. We also found that current smart contract security analyzers cannot effectively handle transaction-reverting statements when detecting security vulnerabilities. Our findings can shed light on further research in the broad area of smart contract quality assurance and provide practical guidance to smart contract developers on the appropriate use of transaction-reverting statements.","ethereum, smart contract, empirical study, transaction-reverting statement, security vulnerability","","ASE '21"
"Conference Paper","Shen B,Zhang W,Yu A,Shi Y,Zhao H,Jin Z","SoManyConflicts: Resolve Many Merge Conflicts Interactively and Systematically","","2022","","","1291–1295","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678937;http://dx.doi.org/10.1109/ASE51524.2021.9678937","10.1109/ASE51524.2021.9678937","Code merging plays an important role in collaborative software development. However, it is often tedious and error-prone for developers to manually resolve merge conflicts, especially when there are many conflicts after merging long-lived branches or parallel versions. In this paper, we present SoMany-Conflicts, a language-agnostic approach to help developers resolve merge conflicts systematically, by utilizing their interrelations (e.g., dependency, similarity, etc.). SoManyConflicts employs a graph representation to model these interrelations and provides 3 major features: 1) cluster and order related conflict based on the graph connectivity; 2) suggest related conflicts of one focused conflict based on the topological sorting, 3) suggest resolution strategies for unresolved conflicts based already resolved ones. We have implemented SoManyConflicts as a Visual Studio Code extension that supports multiple languages (Java, JavaScript, and TypeScript, etc.), which is briefly introduced in the video: https://youtu.be/_asWh_j1KTU. The source code is publicly available at: https://github.com/Symbolk/somanyconflicts.","graph partitioning, version control, conflict resolution, code merging","","ASE '21"
"Conference Paper","Mengin E,Rossi F","Binary Diffing as a Network Alignment Problem via Belief Propagation","","2022","","","967–978","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678782;http://dx.doi.org/10.1109/ASE51524.2021.9678782","10.1109/ASE51524.2021.9678782","In this paper, we address the problem of finding a correspondence, or matching, between the functions of two programs in binary form, which is one of the most common task in binary diffing. We introduce a new formulation of this problem as a particular instance of a graph edit problem over the call graphs of the programs. In this formulation, the quality of a mapping is evaluated simultaneously with respect to both function content and call graph similarities. We show that this formulation is equivalent to a network alignment problem. We propose a solving strategy for this problem based on max-product belief propagation. Finally, we implement a prototype of our method, called QBinDiff, and propose an extensive evaluation which shows that our approach outperforms state of the art diffing tools.","belief propagation, network alignment, binary diffing, binary program analysis, graph edit distance","","ASE '21"
"Conference Paper","Zhou J,Pacheco M,Wan Z,Xia X,Lo D,Wang Y,Hassan AE","Finding a Needle in a Haystack: Automated Mining of Silent Vulnerability Fixes","","2022","","","705–716","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678720;http://dx.doi.org/10.1109/ASE51524.2021.9678720","10.1109/ASE51524.2021.9678720","Following the coordinated vulnerability disclosure model, a vulnerability in open source software (OSS) is suggested to be fixed ""silently"", without disclosing the fix until the vulnerability is disclosed. Yet, it is crucial for OSS users to be aware of vulnerability fixes as early as possible, as once a vulnerability fix is pushed to the source code repository, a malicious party could probe for the corresponding vulnerability to exploit it. In practice, OSS users often rely on the vulnerability disclosure information from security advisories (e.g., National Vulnerability Database) to sense vulnerability fixes. However, the time between the availability of a vulnerability fix and its disclosure can vary from days to months, and in some cases, even years. Due to manpower constraints and the lack of expert knowledge, it is infeasible for OSS users to manually analyze all code changes for vulnerability fix detection. Therefore, it is essential to identify vulnerability fixes automatically and promptly. In a first-of-its-kind study, we propose VulFixMiner, a Transformer-based approach, capable of automatically extracting semantic meaning from commit-level code changes to identify silent vulnerability fixes. We construct our model using sampled commits from 204 projects, and evaluate using the full set of commits from 52 additional projects. The evaluation results show that VulFixMiner outperforms various state-of-the-art baselines in terms of AUC (i.e., 0.81 and 0.73 on Java and Python dataset, respectively) and two effort-aware performance metrics (i.e., EffortCost, Popt). Especially, with an effort of inspecting 5% of total LOC, VulFixMiner can identify 49% of total vulnerability fixes. Additionally, with manual verification of sampled commits that were identified as vulnerability fixes, but not marked as such in our dataset, we observe that 35% (29 out of 82) of the commits are for fixing vulnerabilities, indicating VulFixMiner is also capable of identifying unreported vulnerability fixes.","vulnerability fix, deep learning, open source software, software security","","ASE '21"
"Conference Paper","Nguyen PT,Di Sipio C,Di Rocco J,Di Penta M,Di Ruscio D","Adversarial Attacks to API Recommender Systems: Time to Wake up and Smell the Coffee?","","2022","","","253–265","IEEE Press","Melbourne, Australia","Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering","","2022","9781665403375","","https://doi.org/10.1109/ASE51524.2021.9678946;http://dx.doi.org/10.1109/ASE51524.2021.9678946","10.1109/ASE51524.2021.9678946","Recommender systems in software engineering provide developers with a wide range of valuable items to help them complete their tasks. Among others, API recommender systems have gained momentum in recent years as they became more successful at suggesting API calls or code snippets. While these systems have proven to be effective in terms of prediction accuracy, there has been less attention for what concerns such recommenders' resilience against adversarial attempts. In fact, by crafting the recommenders' learning material, e.g., data from large open-source software (OSS) repositories, hostile users may succeed in injecting malicious data, putting at risk the software clients adopting API recommender systems. In this paper, we present an empirical investigation of adversarial machine learning techniques and their possible influence on recommender systems. The evaluation performed on three state-of-the-art API recommender systems reveals a worrying outcome: all of them are not immune to malicious data. The obtained result triggers the need for effective countermeasures to protect recommender systems against hostile attacks disguised in training data.","recommender systems, adversarial attacks, adversarial machine learning, API mining","","ASE '21"
"Conference Paper","Aumpansub A,Huang Z","Learning-Based Vulnerability Detection in Binary Code","","2022","","","266–271","Association for Computing Machinery","New York, NY, USA","2022 14th International Conference on Machine Learning and Computing (ICMLC)","Guangzhou, China","2022","9781450395700","","https://doi.org/10.1145/3529836.3529926;http://dx.doi.org/10.1145/3529836.3529926","10.1145/3529836.3529926","Cyberattacks typically exploit software vulnerabilities to compromise computers and smart devices. To address vulnerabilities, many approaches have been developed to detect vulnerabilities using deep learning. However, most learning-based approaches detect vulnerabilities in source code instead of binary code. In this paper, we present our approach on detecting vulnerabilities in binary code. Our approach uses binary code compiled from the SARD dataset to build deep learning models to detect vulnerabilities. It extracts features on the syntax information of the assembly instructions in binary code, and trains two deep learning models on the features for vulnerability detection. From our evaluation, we find that the BLSTM model has the best performance, which achieves an accuracy rate of 81% in detecting vulnerabilities. Particularly the F1-score, recall, and specificity of the BLSTM model are 75%, 95% and 75% respectively. This indicates that the model is balanced in detecting both vulnerable code and non-vulnerable code.","software vulnerability, machine learning, deep learning, vulnerability detection, neural network","","ICMLC 2022"
"Conference Paper","Ko Y,Bradbury A,Burgstaller B,Mullins R","Trace-and-Brace (TAB): Bespoke Software Countermeasures against Soft Errors","","2022","","","73–85","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems","San Diego, CA, USA","2022","9781450392662","","https://doi.org/10.1145/3519941.3535070;http://dx.doi.org/10.1145/3519941.3535070","10.1145/3519941.3535070","Lower voltage levels and higher clock frequencies together with a tight hardware area budget make modern processors more susceptible to soft errors. Existing generic software countermeasures against soft errors are application-agnostic or applied uniformely to the application code, which results in insufficient coverage of errors and excessive performance and code size overhead. In this paper, we observe that the degree and the types of vulnerabilities against soft errors vary by application and operational scenarios. We propose a software method, Trace-and-Brace (TAB), that diagnoses the types and locations of vulnerabilities in software and allows accurate and precise application of countermeasures. TAB consists of two phases. The trace phase performs exhaustive fault injection on program traces to diagnose vulnerable code regions and their vulnerability types (i.e., for control-flow, data-flow, and memory data corruption). TAB provides an enhanced and comprehensive set of countermeasures that the user employs in the brace phase through high-level program annotations on vulnerable code regions. Bespoke countermeasures avoid the protection overhead for robust code and focus on vulnerable regions instead. We implemented TAB within CLANG and LLVM, and evaluated it on the RISC-V architecture. TAB detected 99.82 % of data-flow errors and 10.94 % more control-flow errors (96.61 % in total) than the state-of-the-art approach while TAB’s generated programs run 1.15 × faster. TAB provides selective memory area protection which induced only 16 % performance overhead to detect 99.99 % of soft errors on memory.","LLVM, reliability, soft errors, RISC-V, compiler","","LCTES 2022"
"Conference Paper","Ayers H,Laufer E,Mure P,Park J,Rodelo E,Rossman T,Pronin A,Levis P,Van Why J","Tighten Rust’s Belt: Shrinking Embedded Rust Binaries","","2022","","","121–132","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems","San Diego, CA, USA","2022","9781450392662","","https://doi.org/10.1145/3519941.3535075;http://dx.doi.org/10.1145/3519941.3535075","10.1145/3519941.3535075","Rust is a promising programming language for embedded software, providing low-level primitives and performance similar to C/C++ alongside type safety, memory safety, and modern high-level language features. We find naive use of Rust leads to binaries much larger than their C equivalents. For flash-constrained embedded microcontrollers, this is prohibitive. We find four major causes of this growth: monomorphization, inefficient derivations, implicit data structures, and missing compiler optimizations. We present a set of embedded Rust programming principles which reduce Rust binary sizes. We apply these principles to an industrial Rust firmware application, reducing size by 76kB (19%), and an open source Rust OS kernel binary, reducing size by 23kB (26%). We explore compiler optimizations that could further shrink embedded Rust.","binary size, embedded systems, Rust","","LCTES 2022"
"Conference Paper","Ge T,Mo Z,Wu K,Zhang X,Lu Y","RollBin: Reducing Code-Size via Loop Rerolling at Binary Level","","2022","","","99–110","Association for Computing Machinery","New York, NY, USA","Proceedings of the 23rd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems","San Diego, CA, USA","2022","9781450392662","","https://doi.org/10.1145/3519941.3535072;http://dx.doi.org/10.1145/3519941.3535072","10.1145/3519941.3535072","Code size is an increasing concern on resource constrained systems, ranging from embedded devices to cloud servers. To address the issue, lowering memory occupancy has become a priority in developing and deploying applications, and accordingly compiler-based optimizations have been proposed to reduce program footprint. However, prior arts are generally dealing with source codes or intermediate representations, and thus are very limited in scope in real scenarios where only binary files are commonly provided. To fill the gap, this paper presents a novel code-size optimization RollBin to reroll loops at binary level. RollBin first locates the unrolled loops in binary files, and then probes to decide the unrolling factor by identifying regular memory address patterns. To reconstruct the iterations, we propose a customized data dependency analysis that tackles the challenges brought by shuffled instructions and loop-carry dependencies. Next, the recognized iterations are rolled up through instruction removal and update, which are generally reverting the normal unrolling procedure. The evaluations on standard SPEC2006/2017 and MiBench demonstrate that RollBin effectively shrinks code size by 1.7% and 2.2% on average (up to 7.8%), which respectively outperforms the state-of-the-arts by 31% and 38%. In addition, the use cases of representative realistic applications manifest that RollBin can be applicable in practices.","Binary Optimization, Loop Rerolling, Code-Size Reduction","","LCTES 2022"
"Conference Paper","Armengol-Estapé J,Woodruff J,Brauckmann A,Magalhães JW,O'Boyle MF","ExeBench: An ML-Scale Dataset of Executable C Functions","","2022","","","50–59","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming","San Diego, CA, USA","2022","9781450392730","","https://doi.org/10.1145/3520312.3534867;http://dx.doi.org/10.1145/3520312.3534867","10.1145/3520312.3534867","Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.","Code Dataset, C, Mining Software Repositories, Compilers, Program Synthesis, Machine Learning for Code","","MAPS 2022"
"Conference Paper","Xu FF,Alon U,Neubig G,Hellendoorn VJ","A Systematic Evaluation of Large Language Models of Code","","2022","","","1–10","Association for Computing Machinery","New York, NY, USA","Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming","San Diego, CA, USA","2022","9781450392730","","https://doi.org/10.1145/3520312.3534862;http://dx.doi.org/10.1145/3520312.3534862","10.1145/3520312.3534862","Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.","open-source, pretraining, code generation, evaluation, code language model","","MAPS 2022"
"Conference Paper","Das D,Mathews NS,Chimalakonda S","Exploring Security Vulnerabilities in Competitive Programming: An Empirical Study","","2022","","","110–119","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022","Gothenburg, Sweden","2022","9781450396134","","https://doi.org/10.1145/3530019.3530031;http://dx.doi.org/10.1145/3530019.3530031","10.1145/3530019.3530031","Insecure code leading to software vulnerabilities can result in damages of the order of millions of dollars, and in critical systems, the loss of life. Hence, developing secure systems free of exploitable vulnerabilities has been a thrust area of research in recent years. Understanding developers’ approach towards vulnerabilities in their code can pave the way for improvements in insecure coding practices. Recent studies have explored online Q&A forums, open-source code repositories, and other code information sources to gain important insights into the pervasiveness of security vulnerabilities. However, to the best of our knowledge, competitive programming (CP) data, a rich source of information about coding practices, has not been explored from the perspective of insecure coding practices. The evaluation and assessment of coding practices used in CP is particularly intriguing because it has become a key player in developer recruitment in recent times. In this paper, we make one of the first attempts to draw the attention of the community to the emergent concern of insecure coding practices in CP. We use static analysis tools to identify the prevalence and nature of vulnerabilities in a large amount of CP data (6.1 million submissions) obtained from a top-rated CP platform, CodeChef, and find that 34.2% of submissions contain vulnerabilities. We observe that many programmers consistently follow insecure coding practices and most of the detected vulnerabilities are characterized by security standards (CWE, CVSS) based on real-world software.","Empirical Study, Security Vulnerabilities, Software Security, Static Analysis, Competitive Programming","","EASE '22"
"Journal Article","Karnalim O,Simon,Chivers W,Panca BS","Educating Students about Programming Plagiarism and Collusion via Formative Feedback","ACM Trans. Comput. Educ.","2022","22","3","","Association for Computing Machinery","New York, NY, USA","","","2022-06","","","https://doi.org/10.1145/3506717;http://dx.doi.org/10.1145/3506717","10.1145/3506717","To help address programming plagiarism and collusion, students should be informed about acceptable practices and about program similarity, both coincidental and non-coincidental. However, current approaches are usually manual, brief, and delivered well before students are in a situation where they might commit academic misconduct. This article presents an assessment submission system with automated, personalized, and timely formative feedback that can be used in institutions that apply some leniency in early instances of plagiarism and collusion. If a student’s submission shares coincidental or non-coincidental similarity with other submissions, then personalized similarity reports are generated for the involved submissions and the students are expected to explain the similarity and resubmit the work. Otherwise, a report simulating similarities is sent just to the author of the submitted program to enhance their knowledge. Results from two quasi-experiments involving two academic semesters suggest that students with our approach are more aware of programming plagiarism and collusion, including the futility of some program disguises. Further, their submitted programs have lower similarity even at the level of program flow, suggesting that they are less likely to have engaged in programming plagiarism and collusion. Student behavior while using the system is also analyzed based on the statistics of the generated reports and student justifications for the reported similarities.","code similarity, programming, plagiarism, Formative feedback, computing education, collusion","",""
"Conference Paper","Woodruff J,Armengol-Estapé J,Ainsworth S,O'Boyle MF","Bind the Gap: Compiling Real Software to Hardware FFT Accelerators","","2022","","","687–702","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation","San Diego, CA, USA","2022","9781450392655","","https://doi.org/10.1145/3519939.3523439;http://dx.doi.org/10.1145/3519939.3523439","10.1145/3519939.3523439","Specialized hardware accelerators continue to be a source of performance improvement. However, such specialization comes at a programming price. The fundamental issue is that of a mismatch between the diversity of user code and the functionality of fixed hardware, limiting its wider uptake. Here we focus on a particular set of accelerators: those for Fast Fourier Transforms. We present FACC (Fourier ACcelerator Compiler), a novel approach to automatically map legacy code to Fourier Transform accelerators. It automatically generates drop-in replacement adapters using Input-Output (IO)-based program synthesis that bridge the gap between user code and accelerators. We apply FACC to unmodified GitHub C programs of varying complexity and compare against two existing approaches. We target FACC to a high-performance library, FFTW, and two hardware accelerators, the NXP PowerQuad and the Analog Devices FFTA, and demonstrate mean speedups of 9x, 17x and 27x respectively","FFT, program synthesis, hardware accelerator","","PLDI 2022"
"Journal Article","Paiva JC,Leal JP,Figueira Á","Automated Assessment in Computer Science Education: A State-of-the-Art Review","ACM Trans. Comput. Educ.","2022","22","3","","Association for Computing Machinery","New York, NY, USA","","","2022-06","","","https://doi.org/10.1145/3513140;http://dx.doi.org/10.1145/3513140","10.1145/3513140","Practical programming competencies are critical to the success in computer science (CS) education and go-to-market of fresh graduates. Acquiring the required level of skills is a long journey of discovery, trial and error, and optimization seeking through a broad range of programming activities that learners must perform themselves. It is not reasonable to consider that teachers could evaluate all attempts that the average learner should develop multiplied by the number of students enrolled in a course, much less in a timely, deep, and fair fashion. Unsurprisingly, exploring the formal structure of programs to automate the assessment of certain features has long been a hot topic among CS education practitioners. Assessing a program is considerably more complex than asserting its functional correctness, as the proliferation of tools and techniques in the literature over the past decades indicates. Program efficiency, behavior, and readability, among many other features, assessed either statically or dynamically, are now also relevant for automatic evaluation. The outcome of an evaluation evolved from the primordial Boolean values to information about errors and tips on how to advance, possibly taking into account similar solutions. This work surveys the state of the art in the automated assessment of CS assignments, focusing on the supported types of exercises, security measures adopted, testing techniques used, type of feedback produced, and the information they offer the teacher to understand and optimize learning. A new era of automated assessment, capitalizing on static analysis techniques and containerization, has been identified. Furthermore, this review presents several other findings from the conducted review, discusses the current challenges of the field, and proposes some future research directions.","learning analytics, computer science, programming, feedback, Automated assessment","",""
"Conference Paper","Milano M,Turcotti J,Myers AC","A Flexible Type System for Fearless Concurrency","","2022","","","458–473","Association for Computing Machinery","New York, NY, USA","Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation","San Diego, CA, USA","2022","9781450392655","","https://doi.org/10.1145/3519939.3523443;http://dx.doi.org/10.1145/3519939.3523443","10.1145/3519939.3523443","This paper proposes a new type system for concurrent programs, allowing threads to exchange complex object graphs without risking destructive data races. While this goal is shared by a rich history of past work, existing solutions either rely on strictly enforced heap invariants that prohibit natural programming patterns or demand pervasive annotations even for simple programming tasks. As a result, past systems cannot express intuitively simple code without unnatural rewrites or substantial annotation burdens. Our work avoids these pitfalls through a novel type system that provides sound reasoning about separation in the heap while remaining flexible enough to support a wide range of desirable heap manipulations. This new sweet spot is attained by enforcing a heap domination invariant similarly to prior work, but tempering it by allowing complex exceptions that add little annotation burden. Our results include: (1) code examples showing that common data structure manipulations which are difficult or impossible to express in prior work are natural and direct in our system, (2) a formal proof of correctness demonstrating that well-typed programs cannot encounter destructive data races at run time, and (3) an efficient type checker implemented in Gallina and OCaml.","type systems, aliasing, concurrency","","PLDI 2022"
"Journal Article","Ding Z,Li H,Shang W,Chen TH","Towards Learning Generalizable Code Embeddings Using Task-Agnostic Graph Convolutional Networks","ACM Trans. Softw. Eng. Methodol.","2022","","","","Association for Computing Machinery","New York, NY, USA","","","2022-06","","1049-331X","https://doi.org/10.1145/3542944;http://dx.doi.org/10.1145/3542944","10.1145/3542944","Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this paper, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn a more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec, we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.","Neural network, Machine learning, Code embeddings, Source code representation","Just Accepted",""
"Conference Paper","Cao L,Zhang C","Implementation of Domain-Oriented Microservices Decomposition Based on Node-Attributed Network","","2022","","","136–142","Association for Computing Machinery","New York, NY, USA","2022 11th International Conference on Software and Computer Applications","Melaka, Malaysia","2022","9781450385770","","https://doi.org/10.1145/3524304.3524325;http://dx.doi.org/10.1145/3524304.3524325","10.1145/3524304.3524325","The features of microservices, such as scalability and maintainability, have attracted the industry to migrate monolithic projects to microservices. However, how to decompose microservices during migration is a tricky problem. At present, microservices decomposition mainly relies on architects or domain experts, which is more subjective and time-consuming. Followed by semi-automated or automated microservice decomposition, such methods produce coarse-grained results affected by different system characteristics, which cannot make desired decomposition according to the migration requirements of domains. Therefore, this paper proposes a domain-oriented fine-grained microservices decomposition resolution scheme. It uses dynamic and static analysis to obtain the invocation relationships and invocation times between entity methods and the response time of entity methods to represent three main system characteristics concerned during the migration: function, inter-service communications, and performance. And express the above information of monolith by the node-attributed network. Then it uses the community detection algorithm and the proposed similar hierarchical clustering algorithm to complete objective and effective decomposition. Finally, the rationality and feasibility of the proposed approach are verified using the JPetStore case.","Node-attributed Network, Domain, Microservices Decomposition, Static analysis, Dynamic analysis","","ICSCA 2022"
"Conference Paper","Zou H,Shi M,Li T,Qu W","Towards Implementing RTL Microprocessor Agile Design Using Feature Oriented Programming","","2022","","","472–477","European Design and Automation Association","Leuven, BEL","Proceedings of the 2022 Conference & Exhibition on Design, Automation & Test in Europe","Antwerp, Belgium","2022","9783981926361","","","","Recently, hardware agile design methods have been developed to improve the design productivity. However, the modeling methods hinder further design productivity improvements. In this paper, we propose and implement a microprocessor agile design method using feature oriented programming technology to improve design productivity. In this method, designs could be uniquely partitioned and constructed incrementally to explore various functional design features flexibly and efficiently. The key techniques to improve design productivity are flexible modeling extension and on-the-fly feature composing mechanisms. The evaluations on RISC-V and OR1200 CPU pipelines show the effectiveness of the proposed method on duplicate codes reduction and flexible feature composing while avoiding design resource overheads.","agile design method, feature oriented programming, PyRTL","","DATE '22"
"Conference Paper","Dib M,Torabi S,Bou-Harb E,Bouguila N,Assi C","EVOLIoT: A Self-Supervised Contrastive Learning Framework for Detecting and Characterizing Evolving IoT Malware Variants","","2022","","","452–466","Association for Computing Machinery","New York, NY, USA","Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security","Nagasaki, Japan","2022","9781450391405","","https://doi.org/10.1145/3488932.3517393;http://dx.doi.org/10.1145/3488932.3517393","10.1145/3488932.3517393","Recent years have witnessed the emergence of new and more sophisticated malware targeting the Internet of Things. Moreover, the public release of the source code of popular malware families such as Mirai has spawned diverse variants, making it harder to disambiguate their ownership, lineage, and correct label. Such a rapidly evolving landscape makes it also harder to deploy and generalize effective learning models against retired, updated, and/or new threat campaigns. In this paper, we present EVOLIoT, a novel approach aiming at combating ""concept drift"" and the limitations of inter-family IoT malware classification by detecting drifting IoT malware families and understanding their diverse evolutionary trajectories. We introduce a robust and effective contrastive method that learns and compares semantically meaningful representations of IoT malware binaries and codes without the need for expensive target labels. We find that the evolution of IoT binaries can be used as an augmentation strategy to learn effective representations to contrast (dis)similar variant pairs. We discuss the impact and findings of our analysis and present several evaluation studies to highlight the tangled relationships of IoT malware, as well as the efficiency of our contrastively learned feature vectors in preserving semantics and reducing out-of-vocabulary size in cross-architecture IoT malware binaries.","contrastive learning, concept drift, iot malware classification","","ASIA CCS '22"
"Conference Paper","Pascuzzi VR,Goli M","Benchmarking a Proof-of-Concept Performance Portable SYCL-Based Fast Fourier Transformation Library","","2022","","","","Association for Computing Machinery","New York, NY, USA","International Workshop on OpenCL","Bristol, United Kingdom, United Kingdom","2022","9781450396585","","https://doi.org/10.1145/3529538.3529996;http://dx.doi.org/10.1145/3529538.3529996","10.1145/3529538.3529996","In this paper, we present an early version of a SYCL-based FFT library, capable of running on all major vendor hardware, including CPUs and GPUs from AMD, ARM, Intel and NVIDIA. The current limitations of our library is it supports single-dimension FFTs up to 211 in length and base-2 input sequences. Although preliminary, the aim of this work is to seed further developments for a rich set of features for calculating FFTs. The library has the advantage over existing portable FFT libraries in that it is single-source, and therefore removes the complexities that arise due to abundant use of pre-processor macros and auto-generated kernels to target different architectures. We exercise two SYCL-enabled compilers, Codeplay ComputeCpp and Intel’s open-source LLVM project, to evaluate performance portability of our SYCL-based FFT on various heterogeneous architectures.We provide studies comparing our portable library with highly optimized vendor-specific FFT libraries, and discuss potential sources hindering performance.","hpc, performance, portability, sycl, algorithms, fft","","IWOCL'22"
"Conference Paper","Hardy DJ,Choi J,Jiang W,Tajkhorshid E","Experiences Porting NAMD to the Data Parallel C++ Programming Model","","2022","","","","Association for Computing Machinery","New York, NY, USA","International Workshop on OpenCL","Bristol, United Kingdom, United Kingdom","2022","9781450396585","","https://doi.org/10.1145/3529538.3529560;http://dx.doi.org/10.1145/3529538.3529560","10.1145/3529538.3529560","HPC applications have a growing need to leverage heterogeneous computing resources with a vendor-neutral programming paradigm. Data Parallel C++ is a programming language based on open standards SYCL, providing a vendor-neutral solution. We describe our experiences porting the NAMD molecular dynamics application with its GPU-offload force kernels to SYCL/DPC++. Results are shown that demonstrate correctness of the porting effort.","Molecular Dynamics, DPC++, oneAPI, NAMD, SYCL","","IWOCL'22"
"Journal Article","Funke H,Mühlig J,Teubner J","Low-Latency Query Compilation","The VLDB Journal","2022","31","6","1171–1184","Springer-Verlag","Berlin, Heidelberg","","","2022-05","","1066-8888","https://doi.org/10.1007/s00778-022-00741-5;http://dx.doi.org/10.1007/s00778-022-00741-5","10.1007/s00778-022-00741-5","Query compilation is a processing technique that achieves very high processing speeds but has the disadvantage of introducing additional compilation latencies. These latencies cause an overhead that is relatively high for short-running and high-complexity queries. In this work, we present Flounder IR and ReSQL, our new approach to query compilation. Instead of using a general purpose intermediate representation (e.g., LLVM IR) during compilation, ReSQL uses Flounder IR, which is specifically designed for database processing. Flounder IR is lightweight and close to machine assembly. This simplifies the translation from IR to machine code, which otherwise is a costly translation step. Despite simple translation, compiled queries still benefit from the high processing speeds of the query compilation technique. We analyze the performance of our approach with micro-benchmarks and with ReSQL, which employs a full translation stack from SQL to machine code. We show reductions in compilation times up to two orders of magnitude over LLVM and show improvements in overall execution time for TPC-H queries up to 5.5 × over state-of-the-art systems.","Query compilation, Query processing, Just-in-time compilation","",""
"Conference Paper","Tian L,Shi Y,Chen L,Yang Y,Shi G","Gadgets Splicing: Dynamic Binary Transformation for Precise Rewriting","","2022","","","155–167","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization","","2022","9781665405843","","https://doi.org/10.1109/CGO53902.2022.9741259;http://dx.doi.org/10.1109/CGO53902.2022.9741259","10.1109/CGO53902.2022.9741259","Many systems and applications depend on binary rewriting technology to analyze and retrofit software binaries when source code is not available, including binary instrumentation, profiling and security policy reinforcement. However, the investigations have found that many static binary rewriters still fail to accurately transform all legal instructions in binaries. Dynamic binary rewriters allow for accuracy, but coverage and rewriting efficiency are limited. Therefore, the existing binary rewriting technology cannot meet all the needs of binary rewriting. In this paper, we present GRIN, a novel binary rewriting tool that allows for high-precision instruction identification. In GRIN, we propose a gadget-based entry address analysis technique. It identifies the entry addresses of the basic blocks in the binary by gathering and executing the basic blocks related to the computation of the entry addresses of the basic blocks. By traversing from these entries as the new entries of the program, we guarantee the correctness of the identified instructions. We have implemented the prototype of GRIN and evaluated on the SPEC2006 and the whole set of GNU Coreutils. We demonstrate that the precision of GRIN is improved to 99.92% compared to current state-of-the-art techniques.","","","CGO '22"
"Conference Paper","Xu X,Wang X,Xue J","M3V: Multi-Modal Multi-View Context Embedding for Repair Operator Prediction","","2022","","","266–277","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization","","2022","9781665405843","","https://doi.org/10.1109/CGO53902.2022.9741261;http://dx.doi.org/10.1109/CGO53902.2022.9741261","10.1109/CGO53902.2022.9741261","We address the problem of finding context embeddings for faulty locations to allow a learning-based APR tool to learn and predict the repair operators used at the faulty locations. We introduce M3V, a new multi-modal multi-view context embedding approach, which represents the context of a faulty location in two modalities: (1) texts that capture its signature in a natural language using the tree-LSTM model, and (2) graphs that capture its structure with two views, data and control dependences, using the GNN model. We then fuse these two modalities to learn a probabilistic classifier from correct code that, once given a faulty location, will produce a probabilistic distribution over a set of repair operators. We have evaluated M3V against the state-of-the-art context embedding approaches in repairing two common types of bugs in Java, null pointer exceptions (NPE) and index out of bounds (OOB). Trained and tested with 75673 code samples from 20 real-world projects, a learning-based APR tool can predict repair operators more effectively with our context embeddings in repairing NPE bugs, by achieving higher accuracies (11% -- 41%) and higher F1 scores (16% -- 143%). For OOB bugs, these improvements are 9% -- 30% and 15% -- 79%, respectively.","program repair, GNN, program embedding","","CGO '22"
"Conference Paper","Stirling S,Rocha RC,Hazelwood K,Leather H,O'Boyle M,Petoumenos P","F3M: Fast Focused Function Merging","","2022","","","242–253","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization","","2022","9781665405843","","https://doi.org/10.1109/CGO53902.2022.9741269;http://dx.doi.org/10.1109/CGO53902.2022.9741269","10.1109/CGO53902.2022.9741269","From IoT devices to datacenters, code size is important, motivating ongoing research in binary reduction. A key technique is the merging of similar functions to reduce code redundancy. Success, however, depends on accurately identifying functions that can be profitably merged. Attempting to merge all function pairs is prohibitively expensive. Current approaches, therefore, employ summaries to estimate similarity. However these summaries often give little information about how well two programs will merge. To make things worse, they rely on exhaustive search across all summaries; impractical for real-world programs.In this work, we propose a new technique for matching similar functions. We use a hash-based approach that better captures code similarity and, at the same time, significantly reduces the search space by focusing on the most promising candidates. Experimental results show that our similarity metric has a better correlation with merging profitability. This improves the average code size reduction by 6 percentage points, while it reduces the overhead of function merging by 1.8x on average and by as much as 597x for large applications. Faster merging and reduced code size to compile at later stages mean that our approach introduces little to no compile time overhead, while in many cases it makes compilation faster by up to 30%.","compiler optimization, function merging, code-size reduction, LLVM","","CGO '22"
"Conference Paper","Veselý J,Pothukuchi RP,Joshi K,Gupta S,Cohen JD,Bhattacharjee A","Distill: Domain-Specific Compilation for Cognitive Models","","2022","","","301–312","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization","","2022","9781665405843","","https://doi.org/10.1109/CGO53902.2022.9741278;http://dx.doi.org/10.1109/CGO53902.2022.9741278","10.1109/CGO53902.2022.9741278","Computational models of cognition enable a better understanding of the human brain and behavior, psychiatric and neurological illnesses, clinical interventions to treat illnesses, and also offer a path towards human-like artificial intelligence. Cognitive models are also, however, laborious to develop, requiring composition of many types of computational tasks, and suffer from poor performance as they are generally designed using high-level languages like Python. In this work, we present Distill, a domain-specific compilation tool to accelerate cognitive models while continuing to offer cognitive scientists the ability to develop their models in flexible high-level languages. Distill uses domain-specific knowledge to compile Python-based cognitive models into LLVM IR, carefully stripping away features like dynamic typing and memory management that add performance overheads without being necessary for the underlying computation of the models. The net effect is an average of 27X performance improvement in model execution over state-of-the-art techniques using Pyston and PyPy. Distill also repurposes classical compiler data flow analyses to reveal properties about data flow in cognitive models that are useful to cognitive scientists. Distill is publicly available, integrated in the PsyNeuLink cognitive modeling environment, and is already being used by researchers in the brain sciences.","domain-specific compilation, Python, JIT compilers, human brain, cognitive models","","CGO '22"
"Conference Paper","Rocha RC,Petoumenos P,Franke B,Bhatotia P,O'Boyle M","Loop Rolling for Code Size Reduction","","2022","","","217–229","IEEE Press","Virtual Event, Republic of Korea","Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization","","2022","9781665405843","","https://doi.org/10.1109/CGO53902.2022.9741256;http://dx.doi.org/10.1109/CGO53902.2022.9741256","10.1109/CGO53902.2022.9741256","Code size is critical for resource-constrained devices, where memory and storage are limited. Compilers, therefore, should offer optimizations aimed at code reduction. One such optimization is loop rerolling, which transforms a partially unrolled loop into a fully rolled one. However, existing techniques are limited and rarely applicable to real-world programs. They are incapable of handling partial rerolling or straight-line code.In this paper, we propose RoLAG, a novel code-size optimization that creates loops out of straight-line code. It identifies isomorphic code by aligning SSA graphs in a bottom-up fashion. The aligned code is later rolled into a loop. In addition, we propose several optimizations that increase the amount of aligned code by identifying specific patterns of code. Finally, an analysis is used to estimate the profitability of the rolled loop before deciding which version should be kept in the code.Our evaluation of RoLAG on full programs from MiBench and SPEC 2017 show absolute reductions of up to 88 KB while LLVM's technique is hardly distinguishable from the baseline with no rerolling. Finally, our results show that RoLAG is highly applicable to real-world code extracted from popular GitHub repositories. RoLAG is triggered several orders of magnitude more often than LLVM's rerolling, resulting in meaningful reductions on real-world functions.","LLVM, loop optimization, code-size reduction, loop rerolling, compiler optimization","","CGO '22"
"Journal Article","Robe P,Kuttal SK","Designing PairBuddy—A Conversational Agent for Pair Programming","ACM Trans.  Comput. -Hum.  Interact.","2022","29","4","","Association for Computing Machinery","New York, NY, USA","","","2022-05","","1073-0516","https://doi.org/10.1145/3498326;http://dx.doi.org/10.1145/3498326","10.1145/3498326","From automated customer support to virtual assistants, conversational agents have transformed everyday interactions, yet despite phenomenal progress, no agent exists for programming tasks. To understand the design space of such an agent, we prototyped PairBuddy—an interactive pair programming partner—based on research from conversational agents, software engineering, education, human-robot interactions, psychology, and artificial intelligence. We iterated PairBuddy’s design using a series of Wizard-of-Oz studies. Our pilot study of six programmers showed promising results and provided insights toward PairBuddy’s interface design. Our second study of 14 programmers was positively praised across all skill levels. PairBuddy’s active application of soft skills—adaptability, motivation, and social presence—as a navigator increased participants’ confidence and trust, while its technical skills—code contributions, just-in-time feedback, and creativity support—as a driver helped participants realize their own solutions. PairBuddy takes the first step towards an Alexa-like programming partner.","user centered design, pair programming, Conversational agents, Wizard of Oz","",""
"Conference Paper","Wi S,Woo S,Whang JJ,Son S","HiddenCPG: Large-Scale Vulnerable Clone Detection Using Subgraph Isomorphism of Code Property Graphs","","2022","","","755–766","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM Web Conference 2022","Virtual Event, Lyon, France","2022","9781450390965","","https://doi.org/10.1145/3485447.3512235;http://dx.doi.org/10.1145/3485447.3512235","10.1145/3485447.3512235","A code property graph (CPG) is a joint representation of syntax, control flows, and data flows of a target application. Recent studies have demonstrated the promising efficacy of leveraging CPGs for the identification of vulnerabilities. It recasts the problem of implementing a specific static analysis for a target vulnerability as a graph query composition problem. It requires devising coarse-grained graph queries that model vulnerable code patterns. Unfortunately, such coarse-grained queries often leave vulnerabilities due to faulty input sanitization undetected. In this paper, we propose, a scalable system designed to identify various web vulnerabilities, including bugs that stem from incorrect sanitization. We designed to find a subgraph in a target CPG that matches a given CPG query having a known vulnerability, which is known as the subgraph isomorphism problem. To address the scalability challenge that stems from the NP-complete nature of this problem, leverages optimization techniques designed to boost the efficiency of matching vulnerable subgraphs. found confirmed vulnerabilities including CVEs among 2,464 potential vulnerabilities in real-world CPGs having a combined total of 1 billion nodes and 1.2 billion edges.","subgraph isomorphism, clone detection, web vulnerabilities","","WWW '22"
"Conference Paper","Tan X,Zhang Y,Cao J,Sun K,Zhang M,Yang M","Understanding the Practice of Security Patch Management across Multiple Branches in OSS Projects","","2022","","","767–777","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM Web Conference 2022","Virtual Event, Lyon, France","2022","9781450390965","","https://doi.org/10.1145/3485447.3512236;http://dx.doi.org/10.1145/3485447.3512236","10.1145/3485447.3512236","Since the users of open source software (OSS) projects may not use the latest version all the time, OSS development teams often support code maintenance for old versions through maintaining multiple stable branches. Typically, the developers create a stable branch for each old stable version, deploy security patches on the branch, and release fixed versions at regular intervals. As such, old-version applications in production environments are protected from the disclosed vulnerabilities in a long time. However, the rapidly growing number of OSS vulnerabilities has greatly strained this patch deployment model, and a critical need has arisen for the security community to understand the practice of security patch management across stable branches. In this work, we conduct a large-scale empirical study of stable branches in OSS projects and the security patches deployed on them via investigating 608 stable branches belonging to 26 popular OSS projects as well as more than 2,000 security fixes for 806 CVEs deployed on stable branches. Our study distills several important findings: (i) more than 80% affected CVE-Branch pairs are unpatched; (ii) the unpatched vulnerabilities could pose a serious security risk to applications in use, with 47.39% of them achieving a CVSS score over 7 (High or Critical Severity); and (iii) the patch porting process requires great manual efforts and takes an average of 40.46 days, significantly extending the time window for N-day vulnerability attacks. Our results reveal the worrying state of security patch management across stable branches. We hope our study can shed some light on improving the practice of patch management in OSS projects.","OSS Vulnerabilities, Security Patches, Patch Deployment Study","","WWW '22"
"Conference Paper","Lin SC,Chou KH,Chen Y,Hsiao HC,Cassel D,Bauer L,Jia L","Investigating Advertisers’ Domain-Changing Behaviors and Their Impacts on Ad-Blocker Filter Lists","","2022","","","576–587","Association for Computing Machinery","New York, NY, USA","Proceedings of the ACM Web Conference 2022","Virtual Event, Lyon, France","2022","9781450390965","","https://doi.org/10.1145/3485447.3512218;http://dx.doi.org/10.1145/3485447.3512218","10.1145/3485447.3512218","Ad blockers heavily rely on filter lists to block ad domains, which can serve advertisements and trackers. However, recent research has reported that some advertisers keep registering replica ad domains (RAD domains)—new domains that serve the same purpose as the original ones—which tend to slip through ad-blocker filter lists. Although this phenomenon might negatively affect ad blockers’ effectiveness, no study to date has thoroughly investigated its prevalence and the issues caused by RAD domains. In this work, we proposed methods to discover RAD domains and categorized their change patterns. From a crawl of 50,000 websites, we identified 1,748 unique RAD domains, 1,096 of which survived for an average of 410.5 days before they were blocked; the rest have not been blocked as of February 2021. Notably, we found that non-blocked RAD domains could extend the timespan of ad or tracker distribution by more than two years. Our analysis further revealed a taxonomy of four techniques used to create RAD domains, including two less-studied ones. Additionally, we discovered that the RAD domains affected 10.2% of the websites we crawled, and 23.7% of the RAD domains exhibiting privacy-intrusive behaviors, undermining ad blockers’ privacy protection.","ad blocking, filter list, replica ad domain, domain-changing behavior","","WWW '22"
"Conference Paper","Lin Y,Gao D,Lo D","ReSIL: Revivifying Function Signature Inference Using Deep Learning with Domain-Specific Knowledge","","2022","","","107–118","Association for Computing Machinery","New York, NY, USA","Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy","Baltimore, MD, USA","2022","9781450392204","","https://doi.org/10.1145/3508398.3511502;http://dx.doi.org/10.1145/3508398.3511502","10.1145/3508398.3511502","Function signature recovery is important for binary analysis and security enhancement, such as bug finding and control-flow integrity enforcement. However, binary executables typically have crucial information vital for function signature recovery stripped off during compilation. To make things worse, recent studies show that many compiler optimization strategies further complicate the recovery of function signatures with intended violations to function calling conventions.In this paper, we first perform a systematic study to quantify the extent to which compiler optimizations (negatively) impact the accuracy of existing deep learning techniques for function signature recovery. Our experiments show that a state-of-the-art deep learning technique has its accuracy dropped from 98.7% to 87.7% when training and testing optimized binaries. We further identify specific weaknesses in existing approaches and propose an enhanced deep learning approach named sysname (underlineRe vivifying Function underlineS ignature underlineI nference using Deep underlineL earning) to incorporate compiler-optimization-specific domain knowledge into the learning process. Our experimental results show that sysname significantly improves the accuracy and F1 score in inferring function signatures, e.g., with accuracy in inferring the number of arguments for callees compiled with optimization flag O1 from 84.8% to 92.67%. We also demonstrate security implications of sysname in Control-Flow Integrity enforcement in stopping potential Counterfeit Object-Oriented Programming (COOP) attacks.","function signature, recurrent neural network, compiler optimization","","CODASPY '22"
"Journal Article","Zhang H,Fu Z,Li G,Ma L,Zhao Z,Yang H,Sun Y,Liu Y,Jin Z","Towards Robustness of Deep Program Processing Models—Detection, Estimation, and Enhancement","ACM Trans. Softw. Eng. Methodol.","2022","31","3","","Association for Computing Machinery","New York, NY, USA","","","2022-04","","1049-331X","https://doi.org/10.1145/3511887;http://dx.doi.org/10.1145/3511887","10.1145/3511887","Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROTM, which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROTT) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.","Source code processing, robustness enhancement, big code, adversarial attack","",""
"Journal Article","Lin Z,Li G,Zhang J,Deng Y,Zeng X,Zhang Y,Wan Y","XCode: Towards Cross-Language Code Representation with Large-Scale Pre-Training","ACM Trans. Softw. Eng. Methodol.","2022","31","3","","Association for Computing Machinery","New York, NY, USA","","","2022-04","","1049-331X","https://doi.org/10.1145/3506696;http://dx.doi.org/10.1145/3506696","10.1145/3506696","Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture.To address these issues, in this article, we propose a novel Cross-language Code representation with a large-scale pre-training (XCode) method. Concretely, we propose to use several abstract syntax trees and ELMo-enhanced variational autoencoders to obtain multiple pre-trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder (SED) architecture which uses the multi-teacher single-student method to transfer knowledge from the aforementioned pre-trained models to the distilled SED. The pre-trained models and SED will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-to-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.","neural networks, Deep learning, cross-language, code representation, pre-training","",""
"Journal Article","Xie X,Li T,Wang J,Ma L,Guo Q,Juefei-Xu F,Liu Y","NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks","ACM Trans. Softw. Eng. Methodol.","2022","31","3","","Association for Computing Machinery","New York, NY, USA","","","2022-04","","1049-331X","https://doi.org/10.1145/3490489;http://dx.doi.org/10.1145/3490489","10.1145/3490489","Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.","Deep learning testing, model interpretation, testing coverage criteria","",""
"Journal Article","Quaranta L,Calefato F,Lanubile F","Eliciting Best Practices for Collaboration with Computational Notebooks","Proc.  ACM Hum. -Comput.  Interact.","2022","6","CSCW1","","Association for Computing Machinery","New York, NY, USA","","","2022-04","","","https://doi.org/10.1145/3512934;http://dx.doi.org/10.1145/3512934","10.1145/3512934","Despite the widespread adoption of computational notebooks, little is known about best practices for their usage in collaborative contexts. In this paper, we fill this gap by eliciting a catalog of best practices for collaborative data science with computational notebooks. With this aim, we first look for best practices through a multivocal literature review. Then, we conduct interviews with professional data scientists to assess their awareness of these best practices. Finally, we assess the adoption of best practices through the analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform. Findings reveal that experts are mostly aware of the best practices and tend to adopt them in their daily work. Nonetheless, they do not consistently follow all the recommendations as, depending on specific contexts, some are deemed unfeasible or counterproductive due to the lack of proper tool support. As such, we envision the design of notebook solutions that allow data scientists not to have to prioritize exploration and rapid prototyping over writing code of quality.","Kaggle, collaborative systems, Jupyter notebook, data science","",""
"Conference Paper","Thalheim J,Okelmann P,Unnibhavi H,Gouicem R,Bhatotia P","VMSH: Hypervisor-Agnostic Guest Overlays for VMs","","2022","","","678–696","Association for Computing Machinery","New York, NY, USA","Proceedings of the Seventeenth European Conference on Computer Systems","Rennes, France","2022","9781450391627","","https://doi.org/10.1145/3492321.3519589;http://dx.doi.org/10.1145/3492321.3519589","10.1145/3492321.3519589","Lightweight virtual machines (VMs) are prominently adopted for improved performance and dependability in cloud environments. To reduce boot up times and resource utilisation, they are usually ""pre-baked"" with only the minimal kernel and userland strictly required to run an application. This introduces a fundamental trade-off between the advantages of lightweight VMs and available services within a VM, usually leaning towards the former. We propose VMSH, a hypervisor-agnostic abstraction that enables on-demand attachment of services to a running VM---allowing developers to provide minimal, lightweight images without compromising their functionality. The additional applications are made available to the guest via a file system image. To ensure that the newly added services do not affect the original applications in the VM, VMSH uses lightweight isolation mechanisms based on containers. We evaluate VMSH on multiple KVM-based hypervisors and Linux LTS kernels and show that: (i) VMSH adds no overhead for the applications running in the VM, (ii) de-bloating images from the Docker registry can save up to 60% of their size on average, and (iii) VMSH enables cloud providers to offer services to customers, such as recovery shells, without interfering with their VM's execution.","VM introspection, virtual machines","","EuroSys '22"
"Conference Paper","Weisz JD,Muller M,Ross SI,Martinez F,Houde S,Agarwal M,Talamadupula K,Richards JT","Better Together? An Evaluation of AI-Supported Code Translation","","2022","","","369–391","Association for Computing Machinery","New York, NY, USA","27th International Conference on Intelligent User Interfaces","Helsinki, Finland","2022","9781450391443","","https://doi.org/10.1145/3490099.3511157;http://dx.doi.org/10.1145/3490099.3511157","10.1145/3490099.3511157","Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.","imperfect AI, human-AI co-creation, Code translation, generative AI","","IUI '22"
"Conference Paper","Sun J,Liao QV,Muller M,Agarwal M,Houde S,Talamadupula K,Weisz JD","Investigating Explainability of Generative AI for Code through Scenario-Based Design","","2022","","","212–228","Association for Computing Machinery","New York, NY, USA","27th International Conference on Intelligent User Interfaces","Helsinki, Finland","2022","9781450391443","","https://doi.org/10.1145/3490099.3511119;http://dx.doi.org/10.1145/3490099.3511119","10.1145/3490099.3511119","What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.","explainable AI, scenario based design, generative AI, software engineering tooling, human-centered AI","","IUI '22"
"Conference Paper","Srinivasa Ragavan S,Hou Z,Wang Y,Gordon AD,Zhang H,Zhang D","GridBook: Natural Language Formulas for the Spreadsheet Grid","","2022","","","345–368","Association for Computing Machinery","New York, NY, USA","27th International Conference on Intelligent User Interfaces","Helsinki, Finland","2022","9781450391443","","https://doi.org/10.1145/3490099.3511161;http://dx.doi.org/10.1145/3490099.3511161","10.1145/3490099.3511161","Writing formulas on the spreadsheet grid is arguably the most widely practiced form of programming. Still, studies highlight the difficulties experienced by end-user programmers when learning and using traditional formulas, especially for slightly complex tasks. The purpose of GridBook is to ease these difficulties by supporting formulas expressed in natural language within the grid; it is the first system to do so.GridBook builds on a parser utilizing deep learning to understand analysis intents from the natural language input within a spreadsheet cell. GridBook also leverages the spatial context between cells to infer the analysis parameters underspecified in the natural language input. Natural language enables users to analyze data easily and flexibly, to build queries on the results of previous analyses, and to view results intelligibly within the grid—thus taking spreadsheets one step closer to computational notebooks.We evaluated GridBook via two comparative lab studies, with 20 data analysts new only to GridBook. In our studies, there were no significant differences, in terms of time and cognitive load, in participants’ data analysis using GridBook and spreadsheets; however, data analysis with GridBook was significantly faster than with computational notebooks. Our study uncovers insights into the application of natural language as a special purpose programming language for end-user programming in spreadsheets.","","","IUI '22"
"Conference Paper","Lee K,Hoag E,Tillmann N","Efficient Profile-Guided Size Optimization for Native Mobile Applications","","2022","","","243–253","Association for Computing Machinery","New York, NY, USA","Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction","Seoul, South Korea","2022","9781450391832","","https://doi.org/10.1145/3497776.3517764;http://dx.doi.org/10.1145/3497776.3517764","10.1145/3497776.3517764","Positive user experience of mobile apps demands they not only launch fast and run fluidly, but are also small in order to reduce network bandwidth from regular updates. Conventional optimizations often trade off size regressions for performance wins, making them impractical in the mobile space. Indeed, profile-guided optimization (PGO) is successful in server workloads, but is not effective at reducing size and page faults for mobile apps. Also, profiles must be collected from instrumenting builds that are up to 2X larger, so they cannot run normally on real mobile devices. In this paper, we first introduce Machine IR Profile (MIP), a lightweight instrumentation that runs at the machine IR level. Unlike the existing LLVM IR instrumentation counterpart, MIP withholds static metadata from the instrumenting binaries leading to a 2/3 reduction in size overhead. In addition, MIP collects profile data that is more relevant to optimizations in the mobile space. Then we propose three improvements to the LLVM machine outliner: (i) the global outliner overcomes the local scope of the machine outliner when using ThinLTO, (ii) the frame outliner effectively outlines irregular prologues and epilogues, and (iii) the custom outliner outlines frequent patterns occurring in Objective-C and Swift. Lastly, we present our PGO that orders hot start-up functions to minimize page faults, and controls the size optimization level (-Os vs -Oz) for functions based on their estimated execution time driven from MIP. We also order cold functions based on similarity to minimize the compressed app size. Our work improves both the size and performance of real-world mobile apps when compared to the MinSize (-Oz) optimization level: (i) in SocialApp, we reduced the compressed app size by 5.2%, the uncompressed app size by 9.6% and the page faults by 20.6%, and (ii) in ChatApp, we reduced them by 2.4%, 4.6% and 36.4%, respectively.","profile-guided optimizations, size optimizations, iOS, machine outlining, mobile applications","","CC 2022"
"Journal Article","Watson C,Cooper N,Palacio DN,Moran K,Poshyvanyk D","A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research","ACM Trans. Softw. Eng. Methodol.","2022","31","2","","Association for Computing Machinery","New York, NY, USA","","","2022-03","","1049-331X","https://doi.org/10.1145/3485275;http://dx.doi.org/10.1145/3485275","10.1145/3485275","An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE & DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.","literature review, software engineering, machine learning, Deep learning, neural networks","",""
"Conference Paper","Tian Z,Li J,Xue P,Tian J,Mao H,Huang Y","Functionality Recognition on Binary Code with Neural Representation Learning","","2022","","","280–286","Association for Computing Machinery","New York, NY, USA","2021 4th International Conference on Artificial Intelligence and Pattern Recognition","Xiamen, China","2022","9781450384087","","https://doi.org/10.1145/3488933.3489033;http://dx.doi.org/10.1145/3488933.3489033","10.1145/3488933.3489033","The functionality recognition of binary code has important application value in malware analysis, software forensics, binary code similarity analysis and other applications. Most of the existing methods are based on source code or machine learning strategies to carry out program similarity analysis, and this similarity analysis is also applied to a pair of programs, there are limitations in detection accuracy and quantity. Inspired by the recent great success of neural networks and representation learning in various program analysis tasks, We propose NPFI to analyze the binary code of the program and identify its functionality from the perspective of assembly instruction sequence. To evaluate the performance of NPFI, we built a large dataset consisting of 39,000 programs from six different categories collected from Google Code Jam. A large number of experiments show that the accuracy of NPFI in binary code function recognition can reach 95.8%, which is much better than the existing methods.","Program functionality identification, Binary code, Neural network","","AIPR 2021"
"Conference Paper","Tiwari O,Joshi R","Identifying Extract Method Refactorings","","2022","","","","Association for Computing Machinery","New York, NY, USA","15th Innovations in Software Engineering Conference","Gandhinagar, India","2022","9781450396189","","https://doi.org/10.1145/3511430.3511435;http://dx.doi.org/10.1145/3511430.3511435","10.1145/3511430.3511435","Extract method refactoring identifies and extracts a set of statements implementing a specific functionality within a method. Its application enhances the structure of code and provides improved readability and reusability. This paper introduces Segmentation, a new approach for identifying extract method opportunities focusing on achieving higher performance with fewer suggestions. Evaluation of the approach includes six case studies from the open-source domain, and performance is compared against two state-of-the-art approaches. The findings suggest that Segmentation provides improved precision and F measure over both the approaches. Further, improved performance is reflected over long methods too.","Modularity, Restructuring, Segmentation, Segment Graph, Long Methods, Extract Method Refactoring","","ISEC 2022"
"Conference Paper","Kurmaku T,Enoiu EP,Kumrija M","Human-Based Test Design versus Automated Test Generation: A Literature Review and Meta-Analysis","","2022","","","","Association for Computing Machinery","New York, NY, USA","15th Innovations in Software Engineering Conference","Gandhinagar, India","2022","9781450396189","","https://doi.org/10.1145/3511430.3511433;http://dx.doi.org/10.1145/3511430.3511433","10.1145/3511430.3511433","Automated test generation has been proposed to allow test cases to be created with less effort. While much progress has been made, it remains a challenge to automatically generate strong as well as small test suites that are also relevant to engineers. However, how these automated test generation approaches compare to or complement manually written test cases is still an open research question. In the light of the potential benefits of automated test generation in practice, its long history, and the apparent lack of summative evidence supporting its use, the present study aims to systematically review the current body of peer-reviewed publications comparing automated test generation and manual test design performed by humans. We conducted a literature review and meta-analysis to collect data comparing manually written tests with automatically generated ones regarding test efficiency and effectiveness. The overall results of the literature review suggest that automated test generation outperforms manual testing in terms of testing time, the number of tests created and the code coverage achieved. Nevertheless, most of the studies report that manually written tests detect more faults (both injected and naturally occurring ones), are more readable, and detect more specific bugs than those created using automated test generation. Our results suggest that just a few studies report specific statistics (e.g., effect sizes) that can be used in a proper meta-analysis, and therefore, results are inconclusive when comparing automated test generation and manual testing due to the lack of sufficient statistical data and power. Nevertheless, our meta-analysis results suggest that manual and automated test generation are clearly outperforming random testing for all metrics considered.","","","ISEC 2022"
"Conference Paper","Angelou A,Dadaliaris A,Dossis M,Dimitriou G","Branchless Code Generation for Modern Processor Architectures","","2022","","","300–305","Association for Computing Machinery","New York, NY, USA","25th Pan-Hellenic Conference on Informatics","Volos, Greece","2022","9781450395557","","https://doi.org/10.1145/3503823.3503879;http://dx.doi.org/10.1145/3503823.3503879","10.1145/3503823.3503879","Compilers apply transformations to the code they compile in order to make it run faster without changing its behavior. This process is called code optimization. Modern compilers apply many different passes of code optimization to ensure maximum runtime performance and efficiency, at the rather negligible expense of larger compilation times. This study focuses on a particular optimization, called branchless optimization, which eliminates code branches by utilizing different data transformation techniques that have the same effect. Such techniques are explored on their implementation on the LLVM IR and MIPS and partly ARM assembly, and ranked based on their runtime efficiency. Moreover, the stages of implementing the optimization transformation are explored, as well as different instruction set features that some CPU architectures provide that can be used to increase the efficiency of the optimization.","Branchless code generation, Vector code, Branch removal, Compiler optimizations","","PCI 2021"
"Conference Paper","Du J,Song Y,An M,An M,Bogart C,Sakr M","Cheating Detection in Online Assessments via Timeline Analysis","","2022","","","98–104","Association for Computing Machinery","New York, NY, USA","Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1","Providence, RI, USA","2022","9781450390705","","https://doi.org/10.1145/3478431.3499368;http://dx.doi.org/10.1145/3478431.3499368","10.1145/3478431.3499368","The potential for academic integrity violations increases in online courses and instructors must place extra attention on academic integrity, since cheating techniques and costs are different than in the physical classroom. Although students are less supervised and able to study in a self-paced mode in online learning, unauthorized collaboration is still considered to be a serious integrity violation. However, online learning platforms have the advantage that they may capture detailed timelines of student activity. Analysis of these can enable instructors to detect many patterns of collaboration, e.g., working on assessments together, or copying solutions from unauthorized web pages. In this paper, we describe detection methods for several common patterns of alignment between work timelines of pairs of students, and these patterns' relationship with corroborative evidence such as similar answers and unusually fast completion times. We describe data collection necessary to apply the timeline analysis technique to weekly quiz assessments and project submissions, and discuss the strength of evidence the technique can provide in different situations. We have been applying these techniques in an online project-based course over several years, and it has helped instructors to successfully identify potential cheating cases.","online assessment, cheating detection, cheating patterns, timeline analysis, educational data mining, academic integrity","","SIGCSE 2022"
"Conference Paper","Groeneveld W,Martin D,Poncelet T,Aerts K","Are Undergraduate Creative Coders Clean Coders? A Correlation Study","","2022","","","314–320","Association for Computing Machinery","New York, NY, USA","Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1","Providence, RI, USA","2022","9781450390705","","https://doi.org/10.1145/3478431.3499345;http://dx.doi.org/10.1145/3478431.3499345","10.1145/3478431.3499345","Research on global competencies of computing students suggests that next to technical programming knowledge, the teaching of non-technical skills such as creativity is becoming very relevant. Many CS1 courses introduce a layer of creative freedom by employing open project assignments. We are interested in the quality of the submitted projects in relation to the creativity that students show when tackling these open assignments. We have analyzed 110 projects from two academic years to investigate whether there is a relation between creativity and clean code in CS1 student projects. Seven judges were recruited that evaluated the creativity based on Amabile's Consensual Assessment Technique, while the PMD tool was used to explore code quality issues in the Java projects. Results indicate that the more projects are deemed as creative, the more likely code quality issues arise in these projects, and thus the less clean the code will be. We argue that next to promoting creativity in order to solve programming problems, the necessary attention should also be given to the clean code principles.","code quality, software engineering education, clean code, creativity","","SIGCSE 2022"
"Conference Paper","Effenberger T,Pelánek R","Code Quality Defects across Introductory Programming Topics","","2022","","","941–947","Association for Computing Machinery","New York, NY, USA","Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1","Providence, RI, USA","2022","9781450390705","","https://doi.org/10.1145/3478431.3499415;http://dx.doi.org/10.1145/3478431.3499415","10.1145/3478431.3499415","Research on feedback in introductory programming focuses mostly on incomplete and incorrect programs. However, most of the functionally correct programs also contain defects that call for feedback. We analyzed 114,000 solutions to 161 short coding problems in Python and compiled a catalog of 32 defects in code quality. We found that most correct solutions contain some defects and that students do not stop making them if they do not receive targeted feedback. The catalog of defects, together with their prevalence across common topics like expressions, loops, and lists, informs educators which defects to address in which lectures and guides the development of exercises on code quality. Additionally, we describe defect detectors, which can be used to generate valuable feedback to students automatically.","feedback, code quality, python, introductory programming","","SIGCSE 2022"
"Conference Paper","Rodriguez-Rivera G,Turkstra J,Buckmaster J,LeClainche K,Montgomery S,Reed W,Sullivan R,Lee J","Tracking Large Class Projects in Real-Time Using Fine-Grained Source Control","","2022","","","565–570","Association for Computing Machinery","New York, NY, USA","Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1","Providence, RI, USA","2022","9781450390705","","https://doi.org/10.1145/3478431.3499389;http://dx.doi.org/10.1145/3478431.3499389","10.1145/3478431.3499389","Managing a class and identifying common problems becomes significantly more challenging as class sizes increase. Additionally, the increase of online learning requires better methods to track student progress remotely. In this paper, we describe a system that tracks student progress in real time. We propose a method for obtaining a fine-grained commit history by creating a Git repository for each student and automatically running commit/push commands every time a student compiles code. This approach makes a rich source of trace data that can track student progress in real-time, identify common problems students are having, alert faculty of students that are falling behind, and verify project authorship. However, analyzing individual repositories in a large class of students can be tedious and complex, so we have developed a system that provides quick access to all student repositories and summary information and statistics for their projects. This paper describes our approach for obtaining fine-grained source control commits in real-time, a method for tracking student and overall class progress using this data, and our experiences using this system.","source control, classroom management, assessment","","SIGCSE 2022"
"Conference Paper","Wiese E,Rafferty AN,Pyper J","Readable vs. Writable Code: A Survey of Intermediate Students' Structure Choices","","2022","","","321–327","Association for Computing Machinery","New York, NY, USA","Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1","Providence, RI, USA","2022","9781450390705","","https://doi.org/10.1145/3478431.3499413;http://dx.doi.org/10.1145/3478431.3499413","10.1145/3478431.3499413","Since intermediate CS students can use a variety of control struc- tures, why do their choices often not match experts' Students may not realize what choices expert prefer, find non-expert choices easier to read, or simply forget to write with expert structure. To disentangle these explanations, we surveyed 328 2nd and 3rd se- mester undergraduates, with tasks including writing short func- tions, selecting which structure was most readable or best styled, and comprehension questions. Questions focused on seven control structure topics that were important to instructors (e.g., factoring out repeated code between an if-block and its else). Students frequently wrote with non-expert structure, and, for five topics, at least 1/3 of students (48% - 71%) thought a non-expert struc- ture was more readable than the expert one. However, students often made one choice when writing code, but preferred a different choice when reading it. Additionally, for more complex topics, stu- dents often failed to notice (or understand) differences in execution caused by changes in structure. Together, these results suggest that instruction and practice for choosing control structures should be context-specific, and that assessment focused only on code writing may miss underlying misunderstandings.","novice programmers, discourse rules, control structures","","SIGCSE 2022"
"Journal Article","Hückelheim J,Hascoët L","Source-to-Source Automatic Differentiation of OpenMP Parallel Loops","ACM Trans. Math. Softw.","2022","48","1","","Association for Computing Machinery","New York, NY, USA","","","2022-02","","0098-3500","https://doi.org/10.1145/3472796;http://dx.doi.org/10.1145/3472796","10.1145/3472796","This article presents our work toward correct and efficient automatic differentiation of OpenMP parallel worksharing loops in forward and reverse mode. Automatic differentiation is a method to obtain gradients of numerical programs, which are crucial in optimization, uncertainty quantification, and machine learning. The computational cost to compute gradients is a common bottleneck in practice. For applications that are parallelized for multicore CPUs or GPUs using OpenMP, one also wishes to compute the gradients in parallel. We propose a framework to reason about the correctness of the generated derivative code, from which we justify our OpenMP extension to the differentiation model. We implement this model in the automatic differentiation tool Tapenade and present test cases that are differentiated following our extended differentiation procedure. Performance of the generated derivative programs in forward and reverse mode is better than sequential, although our reverse mode often scales worse than the input programs.","Automatic differentiation, shared-memory parallel, multicore, OpenMP","",""
"Journal Article","Corradini F,Marcelletti A,Morichetta A,Polini A,Re B,Tiezzi F","Engineering Trustable and Auditable Choreography-Based Systems Using Blockchain","ACM Trans. Manage. Inf. Syst.","2022","13","3","","Association for Computing Machinery","New York, NY, USA","","","2022-02","","2158-656X","https://doi.org/10.1145/3505225;http://dx.doi.org/10.1145/3505225","10.1145/3505225","A key challenge in engineering distributed systems consists in the integration into their development of a decentralised infrastructure allowing the system participants to trust each other. In this article, we face such a challenge by proposing a model-driven methodology and a related framework to support the engineering of trustable and auditable systems. The approach is based on choreography diagrams specified in the Business Process Model and Notation standard, describing the interactions that should occur among the distributed components of systems. We support the whole lifecycle of choreographies, from their modelling to their distributed execution and auditing. The framework, based on blockchain technology, is named ChorChain. More specifically, ChorChain takes as input a BPMN choreography model and automatically translates it into a Solidity smart contract. The smart contract permits us to enforce the interactions among the cooperating components as prescribed by the choreography model. By leveraging on the auditability of blockchain, ChorChain also supports the activity of auditors continuously. In such a way, ChorChain enables auditors to get some degree of assurance on what happens simultaneously with, or shortly after, information disclosure. We assess the feasibility and effectiveness of the proposed methodology and framework through experiments conducted on the Rinkeby Ethereum Testnet.","BPMN, auditing, choreography, blockchain, trust, execution","",""
"Conference Paper","Floyd B,Jackson J,Probst E,Liu H,Mishra N,Zhong C","Understanding Learners’ Interests in Cybersecurity Competitions on Reddit","","2022","","","444–449","Association for Computing Machinery","New York, NY, USA","Proceedings of the 13th International Conference on Education Technology and Computers","Wuhan, China","2022","9781450385114","","https://doi.org/10.1145/3498765.3498835;http://dx.doi.org/10.1145/3498765.3498835","10.1145/3498765.3498835","Cybersecurity competitions have been recognized as an effective approach to engaging students with real-world challenges. This work aims to explore the cybersecurity competition contestants’ interests and opinions by collecting and analyzing Reddit data. Reddit is a leading hub for students and professionals to exchange information and share experience. We identified a list of keywords of interest and gathered over three thousand relevant public posts. Both qualitative and quantitative data analysis methods are employed to gain insight into learners’ interests and opinions from the data. Our results of analyzing Reddit posts provide a complementary understanding of learners’ interests and opinions compared with the existing survey studies of cybersecurity competitions. The frequency analysis results show that the popular topics include training tools, career advice, penetration testing, CTF, and certifications. Although survey studies are more targeted, our analysis of Reddit posts provides a more complete picture of how learners’ sentiment is impacted by competitions. In the end, our analysis results enable us to discuss several practical implications of the findings and directions of future work.","Reddit, Content Analysis, Text Mining, Cybersecurity Competitions","","ICETC '21"
"Conference Paper","Maier D,Fäßler F,Seifert JP","Uncovering Smart Contract VM Bugs Via Differential Fuzzing","","2022","","","11–22","Association for Computing Machinery","New York, NY, USA","Reversing and Offensive-Oriented Trends Symposium","Vienna, Austria","2022","9781450396028","","https://doi.org/10.1145/3503921.3503923;http://dx.doi.org/10.1145/3503921.3503923","10.1145/3503921.3503923","The ongoing public interest in blockchains and smart contracts has brought a rise to a magnitude of different blockchain implementations. The rate at which new concepts are envisioned and implemented makes it hard to vet their impact on security. Especially smart contract platforms, executing untrusted code, are very complex by design. Still, people put their trust and money into chains that may lack proper testing. A behavior deviation for edge cases of single op-codes is a critical bug class in this brave new world. It can be abused for Denial of Service against the blockchain, chain splits, double-spending, or direct attacks on applications operating on the blockchain. In this paper, we propose an automated methodology to uncover such differences. Through coverage-guided and state-guided fuzzing, we explore smart contract virtual machine behavior against multiple VMs in parallel. We develop NeoDiff, the first framework for feedback-guided differential fuzzing of smart contract VMs. We discuss real, monetary consequences our tool prevents. NeoDiff can be ported to new smart contract platforms with ease. Apart from fuzzing Ethereum VMs, NeoDiff found a range of critical differentials in VMs for the Neo blockchain. Moreover, through a higher-layer semantics mutator, we uncovered semantic discrepancies between Neo smart contracts written in Python when executed on the blockchain vs. classic CPython. Along the way, NeoDiff uncovered memory corruptions in the C# Neo VM.","Differential Fuzzing, Smart Contract VM, State-Aware","","ROOTS'21"
"Conference Paper","Striewe M","Design Patterns for Submission Evaluation within E-Assessment Systems","","2022","","","","Association for Computing Machinery","New York, NY, USA","26th European Conference on Pattern Languages of Programs","Graz, Austria","2022","9781450389976","","https://doi.org/10.1145/3489449.3490010;http://dx.doi.org/10.1145/3489449.3490010","10.1145/3489449.3490010","Many software systems in the area of educational technology can produce grades or other kind of feedback for students’ submissions automatically. Depending on the context of a particular system, there are different software engineering challenges regarding performance or flexibility of the submission evaluation process. During the experimental design of educational technology, these challenges and their consequences are often not considered appropriately, which leads to sub-optimal design decisions that limit productive use. This paper establishes a pattern catalogue that captures available design choices and their consequences in order to support developers and researchers in the domain of educational technology in making their design decisions. Two small case studies demonstrate the usefulness of the catalogue and the gains from applying appropriate patterns for each context.","e-assessment, system architecture, pattern catalogue, educational technology","","EuroPLoP'21"
"Conference Paper","Preschern C","A Pattern Story about C Programming","","2022","","","","Association for Computing Machinery","New York, NY, USA","26th European Conference on Pattern Languages of Programs","Graz, Austria","2022","9781450389976","","https://doi.org/10.1145/3489449.3489978;http://dx.doi.org/10.1145/3489449.3489978","10.1145/3489449.3489978","In the world of patterns, it is sometimes difficult to see how the described patterns can be applied in a real-world context. To show an example of such pattern application, this paper tells a story of applying C programming patterns to implement a logging functionality.","pattern story, C programming, patterns","","EuroPLoP'21"
"Journal Article","Das D,Maruf AA,Islam R,Lambaria N,Kim S,Abdelfattah AS,Cerny T,Frajtak K,Bures M,Tisnovsky P","Technical Debt Resulting from Architectural Degradation and Code Smells: A Systematic Mapping Study","SIGAPP Appl. Comput. Rev.","2022","21","4","20–36","Association for Computing Machinery","New York, NY, USA","","","2022-01","","1559-6915","https://doi.org/10.1145/3512753.3512755;http://dx.doi.org/10.1145/3512753.3512755","10.1145/3512753.3512755","Poor design choices, bad coding practices, or the need to produce software quickly can stand behind technical debt. Unfortunately, manually identifying and managing technical debt gets more difficult as the software matures. Recent research offers various techniques to automate the process of detecting and managing technical debt to address these challenges. This manuscript presents a mapping study of the many aspects of technical debt that have been discovered in this field of study. This includes looking at the various forms of technical debt, as well as detection methods, the financial implications, and mitigation strategies. The findings and outcomes of this study are applicable to a wide range of software development life-cycle decisions.","architectural debt, code smells, architectural degradation, design debt, code debt, technical debt","",""
"Conference Paper","Patnaik A,Padhy N","Does Code Complexity Affect the Quality of Real-Time Projects? Detection of Code Smell on Software Projects Using Machine Learning Algorithms","","2022","","","178–185","Association for Computing Machinery","New York, NY, USA","Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence","Windhoek, Namibia","2022","9781450387637","","https://doi.org/10.1145/3484824.3484911;http://dx.doi.org/10.1145/3484824.3484911","10.1145/3484824.3484911","Code smell targets to identify bugs that occur due to incorrect analysis of code during software development life cycle. It is the task of analyzing a code design problem. The primary causes of code smell are complexity in structural design, violation of programming paradigm, and lack of unit-level testing by the software programmer. Our research focuses on the identification of code smell using different machine learning classifiers. We have considered 15 software code metrics of the Junit open source project and developed a hybrid model for code smell detection. Our dataset consists of 45 features which is further reduced by 15 using various feature selection techniques. Random sampling is used to handle the imbalance in the dataset. The project's performance is evaluated using 10 machine learning techniques which including regression, ensemble methods, and classification. Based on the statistical analysis, it is analyzed that the Random forest ensemble classifiers give best result with an accuracy of 99.12 % is the most appropriate technique for detecting different types of bad smells like god class, duplicate code, long method, large class, and refused bequest.","Source Code Metrics, Feature Selection, Software Quality, Code smell, Machine Learning","","DSMLAI '21'"
"Conference Paper","Shi Y,Nie X,Zhou Q,Zou L,Yin Y","Deep Adaptive Attention Triple Hashing","","2022","","","","Association for Computing Machinery","New York, NY, USA","ACM Multimedia Asia","Gold Coast, Australia","2022","9781450386074","","https://doi.org/10.1145/3469877.3495646;http://dx.doi.org/10.1145/3469877.3495646","10.1145/3469877.3495646","Recent studies have verified that learning compact hash codes can facilitate big data retrieval processing. In particular, learning the deep hash function can greatly improve the retrieval performance. However, the existing deep supervised hashing algorithm treats all the samples in the same way, which leads to insufficient learning of difficult samples. Therefore, we cannot obtain the accurate learning of the similarity relation, making it difficult to achieve satisfactory performance. In light of this, this work proposes a deep supervised hashing model, called deep adaptive attention triple hashing (DAATH), which weights the similarity prediction scores of positive and negative samples in the form of triples, thus giving different degrees of attention to different samples. Compared with the traditional triple loss, it places a greater emphasis on the difficult triple, dramatically reducing the redundant calculation. Extensive experiments have been conducted to show that DAAH consistently outperforms the state-of-the-arts, confirmed its the effectiveness.","adaptive attention triple, deep supervised hashing, Hashing","","MMAsia '21"
"Conference Paper","Russell S","Automated Code Tracing Exercises for CS1","","2022","","","13–16","Association for Computing Machinery","New York, NY, USA","Proceedings of 6th Conference on Computing Education Practice","Durham, United Kingdom","2022","9781450395618","","https://doi.org/10.1145/3498343.3498347;http://dx.doi.org/10.1145/3498343.3498347","10.1145/3498343.3498347","The ability for students to read and comprehend code is a fundamental skill in computer programming. Relying on students to build this skill through typical programming assignments can lead to many persevering through trial and error rather than understanding. This paper describes Trace Generator, a work-in-progress application for generating automatically graded code tracing questions for Python and C programs. The fundamental principles behind this work are mastery through repetition and providing comprehensive and understandable feedback to enable students to learn from their mistakes. Feedback and reflections from the use of the generated questions with two introductory procedural programming classes (200 students) are also discussed. Analysis of student attempts suggests a willingness to complete quizzes multiple times until they achieved a satisfactory score (average final result of 91%).","generated feedback, automatic grading, code tracing, code comprehension","","CEP '22"
